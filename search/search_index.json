{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Langroid: Harness LLMs with Multi-Agent Programming","text":""},{"location":"#the-llm-opportunity","title":"The LLM Opportunity","text":"<p>Given the remarkable abilities of recent Large Language Models (LLMs), there is an unprecedented opportunity to build intelligent applications powered by this transformative technology. The top question for any enterprise is: how best to harness the power of LLMs for complex applications? For technical and practical reasons, building LLM-powered applications is not as simple as throwing a task at an LLM-system and expecting it to do it.</p>"},{"location":"#langroids-multi-agent-programming-framework","title":"Langroid's Multi-Agent Programming Framework","text":"<p>Effectively leveraging LLMs at scale requires a principled programming  framework. In particular, there is often a need to maintain multiple LLM  conversations, each instructed in different ways, and \"responsible\" for  different aspects of a task.</p> <p>An agent is a convenient abstraction that encapsulates LLM conversation  state, along with access to long-term memory (vector-stores) and tools (a.k.a functions  or plugins). Thus a Multi-Agent Programming framework is a natural fit  for complex LLM-based applications.</p> <p>Langroid is the first Python LLM-application framework that was explicitly  designed  with Agents as first-class citizens, and Multi-Agent Programming  as the core  design principle. The framework is inspired by ideas from the  Actor Framework.</p> <p>Langroid allows an intuitive definition of agents, tasks and task-delegation  among agents. There is a principled mechanism to orchestrate multi-agent  collaboration. Agents act as message-transformers, and take turns responding to (and transforming) the current message. The architecture is lightweight, transparent,  flexible, and allows other types of orchestration to be implemented; see the (WIP)  langroid architecture document. Besides Agents, Langroid also provides simple ways to directly interact with LLMs and vector-stores. See the Langroid quick-tour.</p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>Agents as first-class citizens: The <code>Agent</code> class encapsulates LLM conversation state,   and optionally a vector-store and tools. Agents are a core abstraction in Langroid;    Agents act as message transformers, and by default provide 3 responder methods, one corresponding to each    entity: LLM, Agent, User. </li> <li>Tasks: A Task class wraps an Agent, gives the agent instructions (or roles, or goals),   manages iteration over an Agent's responder methods,   and orchestrates multi-agent interactions via hierarchical, recursive   task-delegation. The <code>Task.run()</code> method has the same   type-signature as an Agent's responder's methods, and this is key to how   a task of an agent can delegate to other sub-tasks: from the point of view of a Task,   sub-tasks are simply additional responders, to be used in a round-robin fashion   after the agent's own responders.</li> <li>Modularity, Reusabilily, Loose coupling: The <code>Agent</code> and <code>Task</code> abstractions allow users to design   Agents with specific skills, wrap them in Tasks, and combine tasks in a flexible way.</li> <li>LLM Support: Langroid works with practically any LLM, local/open or remote/proprietary/API-based, via a variety of libraries and providers. See guides to using local LLMs and non-OpenAI LLMs. See Supported LLMs.</li> <li>Caching of LLM prompts, responses: Langroid by default uses Redis for caching. </li> <li>Vector-stores: Qdrant, Chroma and LanceDB are currently supported.   Vector stores allow for Retrieval-Augmented-Generation (RAG).</li> <li>Grounding and source-citation: Access to external documents via vector-stores   allows for grounding and source-citation.</li> <li>Observability, Logging, Lineage: Langroid generates detailed logs of multi-agent interactions and   maintains provenance/lineage of messages, so that you can trace back   the origin of a message.</li> <li>Tools/Plugins/Function-calling: Langroid supports OpenAI's recently   released function calling   feature. In addition, Langroid has its own native equivalent, which we   call tools (also known as \"plugins\" in other contexts). Function   calling and tools have the same developer-facing interface, implemented   using Pydantic,   which makes it very easy to define tools/functions and enable agents   to use them. Benefits of using Pydantic are that you never have to write   complex JSON specs for function calling, and when the LLM   hallucinates malformed JSON, the Pydantic error message is sent back to   the LLM so it can fix it!</li> </ul> <p>Don't worry if some of these terms are not clear to you.  The Getting Started Guide and subsequent pages  will help you get up to speed.</p>"},{"location":"FAQ/","title":"Frequently Asked Questions","text":""},{"location":"FAQ/#can-i-view-the-reasoning-thinking-text-when-using-a-reasoning-llm-like-r1-or-o1","title":"Can I view the reasoning (thinking) text when using a Reasoning LLM like R1 or o1?","text":"<p>Yes, see this note on reasoning-content.</p>"},{"location":"FAQ/#does-langroid-work-with-non-openai-llms","title":"Does Langroid work with non-OpenAI LLMs?","text":"<p>Yes! Langroid works with practically any LLM, local or remote, closed or open.</p> <p>See these two guides:</p> <ul> <li>Using Langroid with local/open LLMs</li> <li>Using Langroid with non-OpenAI proprietary LLMs</li> </ul>"},{"location":"FAQ/#where-can-i-find-out-about-langroids-architecture","title":"Where can I find out about Langroid's architecture?","text":"<p>There are a few documents that can help:</p> <ul> <li>A work-in-progress architecture description   on the Langroid blog.</li> <li>The Langroid Getting Started guide walks you    step-by-step through Langroid's features and architecture.</li> <li>An article by LanceDB on Multi-Agent Programming with Langroid</li> </ul>"},{"location":"FAQ/#how-can-i-limit-the-number-of-output-tokens-generated-by-the-llm","title":"How can I limit the number of output tokens generated by the LLM?","text":"<p>You can set the <code>max_output_tokens</code> parameter in the <code>LLMConfig</code> class, or more commonly, the <code>OpenAIGPTConfig</code> class, which is a subclass of <code>LLMConfig</code>, for example:</p> <pre><code>import langroid as lr\nimport langroid.language_models as lm\n\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"openai/gpt-3.5-turbo\",\n    max_output_tokens=100, # limit output to 100 tokens\n)\nagent_config = lr.ChatAgentConfig(\n    llm=llm_config,\n    # ... other configs\n)\nagent = lr.ChatAgent(agent_config)\n</code></pre> <p>Then every time the agent's <code>llm_response</code> method is called, the LLM's output  will be limited to this number of tokens.</p> <p>If you omit the <code>max_output_tokens</code>, it defaults to 8192. If you wish not to  limit the output tokens, you can set <code>max_output_tokens=None</code>, in which case  Langroid uses the model-specific maximum output tokens from the  <code>langroid/language_models/model_info.py</code> file (specifically the <code>model_max_output_tokens</code> property of <code>LLMConfig</code>). Note however that this model-specific may be quite large, so you would generally  want to either omit setting <code>max_output_tokens</code> (which defaults to 8192), or set it another desired value.</p>"},{"location":"FAQ/#how-langroid-handles-long-chat-histories","title":"How langroid handles long chat histories","text":"<p>You may encounter an error like this:</p> <pre><code>Error: Tried to shorten prompt history but ... longer than context length\n</code></pre> <p>This might happen when your chat history bumps against various limits. Here is how Langroid handles long chat histories. Ultimately the LLM API is invoked with two key inputs: the message history \\(h\\), and the desired output length \\(n\\) (defaults to the <code>max_output_tokens</code> in the  <code>ChatAgentConfig</code>). These inputs are determined as follows (see the <code>ChatAgent._prep_llm_messages</code> method):</p> <ul> <li>let \\(H\\) be the current message history, and \\(M\\) be the value of <code>ChatAgentConfig.max_output_tokens</code>, and \\(C\\) be    the context-length of the LLM.</li> <li>If \\(\\text{tokens}(H) + M \\leq C\\), then langroid uses \\(h = H\\) and \\(n = M\\), since there is enough room to fit both the    actual chat history as well as the desired max output length.</li> <li>If \\(\\text{tokens}(H) + M &gt; C\\), this means the context length is too small to accommodate the message history \\(H\\)    and    the desired output length \\(M\\). Then langroid tries to use a shortened output length \\(n' = C - \\text{tokens}(H)\\),    i.e. the output is effectively truncated to fit within the context length. <ul> <li>If \\(n'\\) is at least equal to <code>min_output_tokens</code> \\(m\\) (default 10), langroid proceeds with \\(h = H\\) and \\(n=n'\\).</li> <li>otherwise, this means that the message history \\(H\\) is so long that the remaining space in the LLM's    context-length \\(C\\) is unacceptably small (i.e. smaller than the minimum output length \\(m\\)). In this case,   Langroid tries to shorten the message history by dropping early messages, and updating the message history \\(h\\) as    long as \\(C - \\text{tokens}(h) &lt;  m\\), until there are no more messages to drop (it will not drop the system    message or the last message, which is a user message), and throws the error mentioned above. </li> </ul> </li> </ul> <p>If you are getting this error, you will want to check whether:</p> <ul> <li>you have set the <code>chat_context_length</code> too small, if you are setting it manually</li> <li>you have set the <code>max_output_tokens</code> too large</li> <li>you have set the <code>min_output_tokens</code> too large</li> </ul> <p>If these look fine, then the next thing to look at is whether you are accumulating too much context into the agent  history, for example retrieved passages (which can be very long) in a RAG scenario. One common case is when a query  \\(Q\\) is being answered using RAG, the retrieved passages \\(P\\) are added to \\(Q\\) to create a (potentially very long) prompt  like </p> <p>based on the passages P, answer query Q</p> <p>Once the LLM returns an answer (if appropropriate for your context), you should avoid retaining the passages \\(P\\) in the  agent history, i.e. the last user message should be simply \\(Q\\), rather than the prompt above. This functionality is exactly what you get when you  use <code>ChatAgent._llm_response_temp_context</code>, which is used by default in the <code>DocChatAgent</code>. </p> <p>Another way to keep chat history tokens from growing too much is to use the <code>llm_response_forget</code> method, which  erases both the query and response, if that makes sense in your scenario.</p>"},{"location":"FAQ/#how-can-i-handle-large-results-from-tools","title":"How can I handle large results from Tools?","text":"<p>As of version 0.22.0, Langroid allows you to control the size of tool results by setting optional parameters  in a <code>ToolMessage</code> definition.</p>"},{"location":"FAQ/#can-i-handle-a-tool-without-running-a-task","title":"Can I handle a tool without running a task?","text":"<p>Yes, if you've enabled an agent to both use (i.e. generate) and handle a tool.  See the <code>test_tool_no_task</code> for an example of this. The <code>NabroskiTool</code> is enabled for the agent, and to get the agent's LLM to generate the tool, you first do  something like: <pre><code>response = agent.llm_response(\"What is Nabroski of 1 and 2?\")\n</code></pre> Now the <code>response</code> is a <code>ChatDocument</code> that will contain the JSON for the <code>NabroskiTool</code>. To handle the tool, you will need to call the agent's <code>agent_response</code> method:</p> <pre><code>result = agent.agent_response(response)\n</code></pre> <p>When you wrap the agent in a task object, and do <code>task.run()</code> the above two steps are done for you, since Langroid operates via a loop mechanism, see docs  here. The advantage of using <code>task.run()</code> instead of doing this yourself, is that this method ensures that tool generation errors are sent back to the LLM so it retries the generation.</p>"},{"location":"FAQ/#openai-tools-and-function-calling-support","title":"OpenAI Tools and Function-calling support","text":"<p>Langroid supports OpenAI tool-calls API as well as OpenAI function-calls API. Read more here.</p> <p>Langroid has always had its own native tool-calling support as well,  which works with any LLM -- you can define a subclass of <code>ToolMessage</code> (pydantic based)  and it is transpiled into system prompt instructions for the tool.  In practice, we don't see much difference between using this vs OpenAI fn-calling.  Example here. Or search for <code>ToolMessage</code> in any of the <code>tests/</code> or <code>examples/</code> folders.</p>"},{"location":"FAQ/#some-example-scripts-appear-to-return-to-user-input-immediately-without-handling-a-tool","title":"Some example scripts appear to return to user input immediately without handling a tool.","text":"<p>This is because the <code>task</code> has been set up with <code>interactive=True</code>  (which is the default). With this setting, the task loop waits for user input after either the <code>llm_response</code> or <code>agent_response</code> (typically a tool-handling response)  returns a valid response. If you want to progress through the task, you can simply  hit return, unless the prompt indicates that the user needs to enter a response.</p> <p>Alternatively, the <code>task</code> can be set up with <code>interactive=False</code> -- with this setting, the task loop will only wait for user input when an entity response (<code>llm_response</code>  or <code>agent_response</code>) explicitly addresses the user. Explicit user addressing can be done using either:</p> <ul> <li>an orchestration tool, e.g. <code>SendTool</code> (see details in the release notes for 0.9.0), an example script is the multi-agent-triage.py, or </li> <li>a special addressing prefix, see the example script 1-agent-3-tools-address-user.py</li> </ul>"},{"location":"FAQ/#can-i-specify-top_k-in-openaigptconfig-for-llm-api-calls","title":"Can I specify top_k in OpenAIGPTConfig (for LLM API calls)?","text":"<p>No; Langroid currently only supports parameters accepted by OpenAI's API, and <code>top_k</code> is not one of them. See:</p> <ul> <li>OpenAI API Reference</li> <li>Discussion on top_k, top_p, temperature</li> <li>Langroid example showing how you can set other OpenAI API parameters, using the <code>OpenAICallParams</code> object.</li> </ul>"},{"location":"FAQ/#can-i-persist-agent-state-across-multiple-runs","title":"Can I persist agent state across multiple runs?","text":"<p>For example, you may want to stop the current python script, and  run it again later, resuming your previous conversation. Currently there is no built-in Langroid mechanism for this, but you can  achieve a basic type of persistence by saving the agent's <code>message_history</code>:</p> <ul> <li>if you used <code>Task.run()</code> in your script, make sure the task is  set up with <code>restart=False</code> -- this prevents the agent state from being reset when  the task is run again.</li> <li>using python's pickle module, you can save the <code>agent.message_history</code> to a file, and load it (if it exists) at the start of your script.</li> </ul> <p>See the example script <code>chat-persist.py</code></p> <p>For more complex persistence, you can take advantage of the <code>GlobalState</code>, where you can store message histories of multiple agents indexed by their name. Simple examples of <code>GlobalState</code> are in the <code>chat-tree.py</code> example,  and the <code>test_global_state.py</code> test.</p>"},{"location":"FAQ/#is-it-possible-to-share-state-between-agentstasks","title":"Is it possible to share state between agents/tasks?","text":"<p>The above-mentioned <code>GlobalState</code> mechanism can be used to share state between  agents/tasks. See the links mentioned in the previous answer.</p>"},{"location":"FAQ/#how-can-i-suppress-llm-output","title":"How can I suppress LLM output?","text":"<p>You can use the <code>quiet_mode</code> context manager for this, see  here</p>"},{"location":"FAQ/#how-can-i-deal-with-llms-especially-weak-ones-generating-bad-json-in-tools","title":"How can I deal with LLMs (especially weak ones) generating bad JSON in tools?","text":"<p>Langroid already attempts to repair bad JSON (e.g. unescaped newlines, missing quotes, etc) using the json-repair library and other custom methods, before attempting to parse it into a <code>ToolMessage</code> object. However this type of repair may not be able to handle all edge cases of bad JSON  from weak LLMs. There are two existing ways to deal with this, and one coming soon:</p> <ul> <li>If you are defining your own <code>ToolMessage</code> subclass, considering deriving it instead   from <code>XMLToolMessage</code> instead, see the XML-based Tools</li> <li>If you are using an existing Langroid <code>ToolMessage</code>, e.g. <code>SendTool</code>, you can    define your own subclass of <code>SendTool</code>, say <code>XMLSendTool</code>,   inheriting from both <code>SendTool</code> and <code>XMLToolMessage</code>; see this    example</li> <li>Coming soon: strict decoding to leverage the Structured JSON outputs supported by OpenAI   and open LLM providers such as <code>llama.cpp</code> and <code>vllm</code>.</li> </ul> <p>The first two methods instruct the LLM to generate XML instead of JSON, and any field that is designated with a <code>verbatim=True</code> will be enclosed  within an XML <code>CDATA</code> tag, which does not require any escaping, and can be far more reliable for tool-use than JSON, especially with weak LLMs.</p>"},{"location":"FAQ/#how-can-i-handle-an-llm-forgetting-to-generate-a-toolmessage","title":"How can I handle an LLM \"forgetting\" to generate a <code>ToolMessage</code>?","text":"<p>Sometimes the LLM (especially a weak one) forgets to generate a  <code>ToolMessage</code> (either via OpenAI's tools/functions API, or via Langroid's JSON/XML Tool mechanism), despite being instructed to do so. There are a few remedies Langroid offers for this:</p> <p>Improve the instructions in the <code>ToolMessage</code> definition:</p> <ul> <li>Improve instructions in the <code>purpose</code> field of the <code>ToolMessage</code>.</li> <li>Add an <code>instructions</code> class-method to the <code>ToolMessage</code>, as in the   <code>chat-search.py</code> script:</li> </ul> <p><pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    return \"\"\"\n        IMPORTANT: You must include an ACTUAL query in the `query` field,\n        \"\"\"\n</code></pre>   These instructions are meant to be general instructions on how to use the tool   (e.g. how to set the field values), not to specifically about the formatting.</p> <ul> <li>Add a <code>format_instructions</code> class-method, e.g. like the one in the    <code>chat-multi-extract-3.py</code>    example script.</li> </ul> <pre><code>@classmethod\ndef format_instructions(cls, tool: bool = True) -&gt; str:\n    instr = super().format_instructions(tool)\n    instr += \"\"\"\n    ------------------------------\n    ASK ME QUESTIONS ONE BY ONE, to FILL IN THE FIELDS \n    of the `lease_info` function/tool.\n    First ask me for the start date of the lease.\n    DO NOT ASK ANYTHING ELSE UNTIL YOU RECEIVE MY ANSWER.\n    \"\"\"\n    return instr\n</code></pre> <p>Override the <code>handle_message_fallback</code> method in the agent:</p> <p>This method is called when the Agent's <code>agent_response</code> method receives a non-tool message as input. The default behavior of this method is to return None, but it is very useful to override the method to handle cases where the LLM has forgotten to use a tool. You can define this method to return a \"nudge\" to the LLM telling it that it forgot to do a tool-call, e.g. see how it's done in the  example script <code>chat-multi-extract-local.py</code>:</p> <pre><code>class LeasePresenterAgent(ChatAgent):\n    def handle_message_fallback(\n        self, msg: str | ChatDocument\n    ) -&gt; str | ChatDocument | None:\n        \"\"\"Handle scenario where Agent failed to present the Lease JSON\"\"\"\n        if isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM:\n            return \"\"\"\n            You either forgot to present the information in the JSON format\n            required in `lease_info` JSON specification,\n            or you may have used the wrong name of the tool or fields.\n            Try again.\n            \"\"\"\n        return None\n</code></pre> <p>Note that despite doing all of these, the LLM may still fail to generate a <code>ToolMessage</code>. In such cases, you may want to consider using a better LLM, or an up-coming Langroid feature that leverages strict decoding abilities of specific LLM providers (e.g. OpenAI, llama.cpp, vllm) that are able to use grammar-constrained decoding to force the output to conform to the specified structure.</p> <p>Langroid also provides a simpler mechanism to specify the action to take when an LLM does not generate a tool, via the <code>ChatAgentConfig.handle_llm_no_tool</code>  config parameter, see the  docs.</p>"},{"location":"FAQ/#can-i-use-langroid-to-converse-with-a-knowledge-graph-kg","title":"Can I use Langroid to converse with a Knowledge Graph (KG)?","text":"<p>Yes, you can use Langroid to \"chat with\" either a Neo4j or ArangoDB KG,  see docs here</p>"},{"location":"FAQ/#how-can-i-improve-docchatagent-rag-latency","title":"How can I improve <code>DocChatAgent</code> (RAG) latency?","text":"<p>The behavior of <code>DocChatAgent</code> can be controlled by a number of settings in  the <code>DocChatAgentConfig</code> class. The top-level query-answering method in <code>DocChatAgent</code> is <code>llm_response</code>, which use the  <code>answer_from_docs</code> method. At a high level, the response to an input message involves the following steps:</p> <ul> <li>Query to StandAlone: LLM rephrases the query as a stand-alone query.     This can incur some latency. You can      turn it off by setting <code>assistant_mode=True</code> in the <code>DocChatAgentConfig</code>.</li> <li>Retrieval: The most relevant passages (chunks) are retrieved using a collection of semantic/lexical        similarity searches and ranking methods. There are various knobs in <code>DocChatAgentConfig</code> to control       this retrieval.</li> <li>Relevance Extraction: LLM is used to retrieve verbatim relevant portions from   the retrieved chunks. This is typically the biggest latency step. You can turn it off   by setting the <code>relevance_extractor_config</code> to None in <code>DocChatAgentConfig</code>.</li> <li>Answer Generation: LLM generates answer based on retrieved passages.</li> </ul> <p>See the <code>doc-aware-chat.py</code> example script, which illustrates some of these settings.</p> <p>In some scenarios you want to only use the retrieval step of a <code>DocChatAgent</code>. For this you can use the <code>RetrievalTool</code>. See the <code>test_retrieval_tool</code> in  <code>test_doc_chat_agent.py</code>. to learn how to use it. The above example script uses <code>RetrievalTool</code> as well.</p>"},{"location":"FAQ/#is-there-support-to-run-multiple-tasks-concurrently","title":"Is there support to run multiple tasks concurrently?","text":"<p>Yes, see the <code>run_batch_tasks</code> and related functions in  batch.py.</p> <p>See also:</p> <ul> <li>tests: test_batch.py,    test_relevance_extractor.py,</li> <li>example: multi-agent-round-table.py</li> </ul> <p>Another example is within  <code>DocChatAgent</code>,  which uses batch tasks for relevance extraction, see the <code>get_verbatim_extracts</code> method -- when there are k relevant passages, this runs k tasks concurrently,  each of which uses an LLM-agent to extract relevant verbatim text from a passage.</p>"},{"location":"FAQ/#can-i-use-langroid-in-a-fastapi-server","title":"Can I use Langroid in a FastAPI server?","text":"<p>Yes, see the langroid/fastapi-server repo.</p>"},{"location":"FAQ/#can-a-sub-task-end-all-parent-tasks-and-return-a-result","title":"Can a sub-task end all parent tasks and return a result?","text":"<p>Yes, there are two ways to achieve this, using <code>FinalResultTool</code>:</p> <p>From a <code>ChatAgent</code>'s tool-handler or <code>agent_response</code> method: Your code can return a  <code>FinalResultTool</code> with arbitrary field types; this ends the current and all parent tasks and this <code>FinalResultTool</code> will appear as one of tools in the final <code>ChatDocument.tool_messages</code>. See <code>test_tool_handlers_and_results</code> in  test_tool_messages.py,  and examples/basic/chat-tool-function.py</p> <p>From <code>ChatAgent</code>'s <code>llm_response</code> method: you can define a subclass of a  <code>FinalResultTool</code> and enable the agent to use this tool, which means it will become available for the LLM to generate.  See examples/basic/multi-agent-return-result.py.</p>"},{"location":"FAQ/#how-can-i-configure-a-task-to-retain-or-discard-prior-conversation","title":"How can I configure a task to retain or discard prior conversation?","text":"<p>In some scenarios, you may want to control whether each time you call a task's <code>run</code>  method, the underlying agent retains the conversation history from the previous run. There are two boolean config parameters that control this behavior: </p> <ul> <li>the <code>restart</code> parameter (default <code>True</code>) in the <code>Task</code> constructor, and</li> <li>the <code>restart_as_subtask</code> (default <code>False</code>) parameter in the <code>TaskConfig</code> argument of the <code>Task</code> constructor.</li> </ul> <p>To understand how these work, consider a simple scenario of a task <code>t</code> that has a  subtask <code>t1</code>, e.g., suppose you have the following code with default settings  of the <code>restart</code> and <code>restart_as_subtask</code> parameters:</p> <pre><code>from langroid.agent.task import Task\nfrom langroid.agent.task import TaskConfig\n\n# default setttings:\nrs = False\nr = r1 = True\n\nagent = ...\ntask_config = TaskConfig(restart_as_subtask=rs) \nt = Task(agent, restart=r, config=task_config)\n\nagent1 = ...\nt1 = Task(agent1, restart=r1, config=task_config)\nt.add_subtask(t1)\n</code></pre> <p>This default setting works as follows: Since task <code>t</code> was constructed with the default <code>restart=True</code>, when <code>t.run()</code> is called, the conversation histories of the agent underlying <code>t</code> as well as all  those of all subtasks (such as <code>t1</code>) are reset. However, if during <code>t.run()</code>, there are multiple calls to <code>t1.run()</code>, then the conversation history is retained across these calls, even though <code>t1</code> was constructed with the default <code>restart=True</code> -- this is because the <code>restart</code> constructor parameter has no effect on a task's reset behavior when it is a subtask. </p> <p>The <code>TaskConfig.restart_as_subtask</code> parameter controls the reset behavior of a task's <code>run</code> method when invoked as a subtask. It defaults to <code>False</code>, which is why in the above example, the conversation history of <code>t1</code> is retained across multiple calls to <code>t1.run()</code> that may occur during execution of <code>t.run()</code>. If you set this parameter to <code>True</code> in the above example, then the conversation history of <code>t1</code> would be reset each time <code>t1.run()</code> is called, during a call to <code>t.run()</code>.</p> <p>To summarize, </p> <ul> <li>The <code>Task</code> constructor's <code>restart</code> parameter controls the reset behavior of the task's <code>run</code> method when it is called directly, not as a subtask.</li> <li>The <code>TaskConfig.restart_as_subtask</code> parameter controls the reset behavior of the task's <code>run</code> method when it is called as a subtask.</li> </ul> <p>These settings can be mixed and matched as needed.</p> <p>Additionally, all reset behavior can be turned off during a specific <code>run()</code> invocation by calling it with <code>allow_restart=False</code>, e.g.,  <code>t.run(..., allow_restart=False)</code>.</p>"},{"location":"FAQ/#how-can-i-set-up-a-task-to-exit-as-soon-as-the-llm-responds","title":"How can I set up a task to exit as soon as the LLM responds?","text":"<p>In some cases you may want the top-level task or a subtask to exit as soon as the LLM responds. You can get this behavior by setting <code>single_round=True</code> during task construction, e.g.,</p> <pre><code>from langroid.agent.task import Task\n\nagent = ...\nt = Task(agent, single_round=True, interactive=False)\n\nresult = t.run(\"What is 4 + 5?\")\n</code></pre> <p>The name <code>single_round</code> comes from the fact that the task loop ends as soon as  any one of the agent's responders return a valid response. Recall that an  agent's responders are <code>llm_response</code>, <code>agent_response</code> (for tool handling), and <code>user_response</code> (for user input). In the above example there are no tools and no  user interaction (since <code>interactive=False</code>), so the task will exit as soon as the LLM responds.</p> <p>More commonly, you may only want this single-round behavior for a subtask, e.g.,</p> <pre><code>agent = ...\nt = Task(agent, single_round=False, interactive=True)\n\nagent1 = ...\nt1 = Task(agent1, single_round=True, interactive=False)\n\nt.add_subtask(t1)\ntop_level_query = ...\nresult = t.run(...)\n</code></pre> <p>See the example script <code>chat-2-agent-discuss.py</code> for an example of this, and also search for <code>single_round</code> in the rest of the examples.</p> <p>Using <code>single_round=True</code> will prevent tool-handling</p> <p>As explained above, setting <code>single_round=True</code> will cause the task to exit as soon as the LLM responds, and thus if it emits a valid tool (which the agent is enabled to handle), this tool will not be handled.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/","title":"Language Models: Completion and Chat-Completion","text":"<p>Transformer-based language models are fundamentally next-token predictors, so  naturally all LLM APIs today at least provide a completion endpoint.  If an LLM is a next-token predictor, how could it possibly be used to  generate a response to a question or instruction, or to engage in a conversation with  a human user? This is where the idea of \"chat-completion\" comes in. This post is a refresher on the distinction between completion and chat-completion, and some interesting details on how chat-completion is implemented in practice.</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#language-models-as-next-token-predictors","title":"Language Models as Next-token Predictors","text":"<p>A Language Model is essentially a \"next-token prediction\" model, and so all LLMs today provide a \"completion\" endpoint, typically something like: <code>/completions</code> under the base URL.</p> <p>The endpoint simply takes a prompt and returns a completion (i.e. a continuation).</p> <p>A typical prompt sent to a completion endpoint might look like this: <pre><code>The capital of Belgium is \n</code></pre> and the LLM will return a completion like this: <pre><code>Brussels.\n</code></pre> OpenAI's GPT3 is an example of a pure completion LLM. But interacting with a completion LLM is not very natural or useful: you cannot give instructions or ask questions; instead you would always need to  formulate your input as a prompt whose natural continuation is your desired output. For example, if you wanted the LLM to highlight all proper nouns in a sentence, you would format it as the following prompt:</p> <p>Chat-To-Prompt Example: Chat/Instruction converted to a completion prompt.</p> <p><pre><code>User: here is a sentence, the Assistant's task is to identify all proper nouns.\n     Jack lives in Bosnia, and Jill lives in Belgium.\nAssistant:    \n</code></pre> The natural continuation of this prompt would be a response listing the proper nouns, something like: <pre><code>John, Bosnia, Jill, Belgium are all proper nouns.\n</code></pre></p> <p>This seems sensible in theory, but a \"base\" LLM that performs well on completions may not perform well on these kinds of prompts. The reason is that during its training, it may not have been exposed to very many examples of this type of prompt-response pair. So how can an LLM be improved to perform well on these kinds of prompts?</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#instruction-tuned-aligned-llms","title":"Instruction-tuned, Aligned LLMs","text":"<p>This brings us to the heart of the innovation behind the wildly popular ChatGPT: it uses an enhancement of GPT3 that (besides having a lot more parameters), was explicitly fine-tuned on instructions (and dialogs more generally) -- this is referred to as instruction-fine-tuning or IFT for short. In addition to fine-tuning instructions/dialogs, the models behind ChatGPT (i.e., GPT-3.5-Turbo and GPT-4) are further tuned to produce responses that align with human preferences (i.e. produce responses that are more helpful and safe), using a procedure called Reinforcement Learning with Human Feedback (RLHF). See this OpenAI InstructGPT Paper for details on these techniques and references to the  original papers that introduced these ideas. Another recommended read is Sebastian  Raschka's post on RLHF and related techniques. </p> <p>For convenience, we refer to the combination of IFT and RLHF as chat-tuning. A chat-tuned LLM can be expected to perform well on prompts such as the one in  the Chat-To-Prompt Example above. These types of prompts are still unnatural, however,  so as a convenience, chat-tuned LLM API servers also provide a \"chat-completion\"  endpoint (typically <code>/chat/completions</code> under the base URL), which allows the user to interact with them in a natural dialog, which might look like this (the portions in square brackets are indicators of who is generating the text):</p> <p><pre><code>[User] What is the capital of Belgium?\n[Assistant] The capital of Belgium is Brussels.\n</code></pre> or <pre><code>[User] In the text below, find all proper nouns:\n    Jack lives in Bosnia, and Jill lives in Belgium.\n[Assistant] John, Bosnia, Jill, Belgium are all proper nouns.\n[User] Where does John live?\n[Assistant] John lives in Bosnia.\n</code></pre></p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#chat-completion-endpoints-under-the-hood","title":"Chat Completion Endpoints: under the hood","text":"<p>How could this work, given that LLMs are fundamentally next-token predictors? This is a convenience provided by the LLM API service (e.g. from OpenAI or local model server libraries): when a user invokes the chat-completion endpoint (typically at <code>/chat/completions</code> under the base URL), under the hood, the server converts the instructions and multi-turn chat history into a single string, with annotations indicating user and assistant turns, and ending with something like \"Assistant:\" as in the Chat-To-Prompt Example above.</p> <p>Now the subtle detail to note here is this:</p> <p>It matters how the dialog (instructions plus chat history) is converted into a single prompt string. Converting to a single prompt by simply concatenating the instructions and chat history using an \"intuitive\" format (e.g. indicating user, assistant turns using \"User\", \"Assistant:\", etc.) can work, however most local LLMs are trained on a specific prompt format. So if we format chats in a different way, we may get odd/inferior results.</p>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#converting-chats-to-prompts-formatting-rules","title":"Converting Chats to Prompts: Formatting Rules","text":"<p>For example, the llama2 models are trained on a format where the user's input is bracketed within special strings <code>[INST]</code> and <code>[/INST]</code>. There are other requirements that we don't go into here, but interested readers can refer to these links:</p> <ul> <li>A reddit thread on the llama2 formats</li> <li>Facebook's llama2 code</li> <li>Langroid's llama2 formatting code</li> </ul> <p>A dialog fed to a Llama2 model in its expected prompt format would look like this:</p> <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful assistant.\n&lt;&lt;/SYS&gt;&gt;\n\nHi there! \n[/INST] \nHello! How can I help you today? &lt;/s&gt;\n&lt;s&gt;[INST] In the text below, find all proper nouns:\n    Jack lives in Bosnia, and Jill lives in Belgium.\n [/INST] \nJohn, Bosnia, Jill, Belgium are all proper nouns. &lt;/s&gt;&lt;s&gt; \n[INST] Where does Jack live? [/INST] \nJack lives in Bosnia. &lt;/s&gt;&lt;s&gt;\n[INST] And Jill? [/INST]\nJill lives in Belgium. &lt;/s&gt;&lt;s&gt;\n[INST] Which are its neighboring countries? [/INST]\n</code></pre> <p>This means that if an LLM server library wants to provide a chat-completion endpoint for a local model, it needs to provide a way to convert chat history to a single prompt using the specific formatting rules of the model. For example the <code>oobabooga/text-generation-webui</code>  library has an extensive set of chat formatting templates for a variety of models, and their model server auto-detects the format template from the model name.</p> <p>Chat completion model names: look for 'chat' or 'instruct' in the name</p> <p>You can search for a variety of models on the HuggingFace model hub. For example if you see a name <code>Llama-2-70B-chat-GGUF</code> you know it is chat-tuned. Another example of a chat-tuned model is <code>Llama-2-7B-32K-Instruct</code> </p> <p>A user of these local LLM server libraries thus has two options when using a  local model in chat mode:</p> <ul> <li>use the chat-completion endpoint, and let the underlying library handle the chat-to-prompt formatting, or</li> <li>first format the chat history according to the model's requirements, and then use the   completion endpoint</li> </ul>"},{"location":"blog/2023/09/19/language-models-completion-and-chat-completion/#using-local-models-in-langroid","title":"Using Local Models in Langroid","text":"<p>Local models can be used in Langroid by defining a <code>LocalModelConfig</code> object. More details are in this tutorial,  but here we briefly discuss prompt-formatting in this context. Langroid provides a built-in formatter for LLama2 models,  so users looking to use llama2 models with langroid can try either of these options, by setting the <code>use_completion_for_chat</code> flag in the <code>LocalModelConfig</code> object (See the local-LLM tutorial for details).</p> <p>When this flag is set to <code>True</code>, the chat history is formatted using the built-in  Langroid llama2 formatter and the completion endpoint are used. When the flag is set to <code>False</code>, the chat  history is sent directly to the chat-completion endpoint, which internally converts the  chat history to a prompt in the expected llama2 format.</p> <p>For local models other than Llama2, users can either:</p> <ul> <li>write their own formatters by writing a class similar to <code>Llama2Formatter</code> and  then setting the <code>use_completion_for_chat</code> flag to <code>True</code> in the <code>LocalModelConfig</code> object, or</li> <li>use an LLM server library (such as the <code>oobabooga</code> library mentioned above) that provides a chat-completion endpoint,  and converts chats to single prompts under the hood, and set the   <code>use_completion_for_chat</code> flag to <code>False</code> in the <code>LocalModelConfig</code> object.</li> </ul> <p>You can use a similar approach if you are using an LLM application framework other than Langroid.</p>"},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/","title":"Overview of Langroid's Multi-Agent Architecture (prelim)","text":""},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#agent-as-an-intelligent-message-transformer","title":"Agent, as an intelligent message transformer","text":"<p>A natural and convenient abstraction in designing a complex LLM-powered system is the notion of an agent that is instructed to be responsible for a specific aspect of the  overall task. In terms of code, an Agent is essentially a class representing an intelligent entity that can  respond to messages, i.e., an agent is simply a message transformer. An agent typically encapsulates an (interface to an) LLM, and may also be equipped with so-called tools (as  described below) and external documents/data (e.g., via a vector database, as described below). Much like a team of humans, agents interact by exchanging messages, in a manner reminiscent of the  actor framework in programming languages. An orchestration mechanism is needed to manage the flow of messages between agents, to ensure that progress is  made towards completion of the task, and to handle the inevitable cases where an agent deviates from instructions. Langroid is founded on this multi-agent programming paradigm, where agents are  first-class citizens, acting as message transformers, and communicate by exchanging messages.</p> <p>To build useful applications with LLMs, we need to endow them with the ability to trigger actions (such as API calls, computations, database queries, etc) or send structured messages to other agents  or downstream processes. Tools provide these capabilities, described next.</p>"},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#tools-also-known-as-functions","title":"Tools, also known as functions","text":"<p>An LLM is essentially a text transformer; i.e.,  in response to some input text,  it produces a text response. Free-form text responses are ideal when we want to generate a description, answer, or summary for human consumption, or even a question for another agent to answer. However, in some cases, we would like the responses to be more structured, for example  to trigger external actions (such as an API call, code execution, or a database query), or for unambiguous/deterministic handling by a downstream process or another agent.  In such cases, we would instruct the LLM to produce a structured output, typically in JSON format, with various  pre-specified fields, such as code, an SQL query, parameters of an API call, and so on. These structured responses  have come to be known as tools, and the LLM is said to use a tool when it produces a structured response  corresponding to a specific tool. To elicit a tool response from an LLM, it needs to be instructed on the expected tool format and the conditions under which it should use the tool. To actually use a tool emitted by an LLM, a tool handler method must be defined as well. The tool handler for a given tool is triggered when it is recognized in the LLM's response.</p>"},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#tool-use-example","title":"Tool Use: Example","text":"<p>As a simple example, a SQL query tool can be specified as a JSON structure with a <code>sql</code>  field (containing the SQL query) and a <code>db</code> field (containing the name of the database). The LLM may be instructed with a system prompt of the form:</p> <p>When the user asks a question about employees, use the SQLTool described in the below schema, and the results of this tool will be sent back to you, and you can use these to respond to the user's question, or correct your SQL query if there is a syntax error.</p> <p>The tool handler would detect this specific tool in the LLM's response, parse this JSON structure,  extract the <code>sql</code> and <code>db</code> fields, run the query on the specified database,  and return the result if the query ran successfully, otherwise return an error message. Depending on how the multi-agent system is organized, the query result or error message may be handled by the same agent (i.e., its LLM), which may either summarize the results in narrative form, or revise the query if the error message  indicates a syntax error.</p>"},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#agent-oriented-programming-function-signatures","title":"Agent-oriented programming: Function-Signatures","text":"<p>If we view an LLM as a function with signature <code>string -&gt; string</code>, it is possible to express the concept of an agent, tool, and other constructs in terms of derived function signatures, as shown in the table below. Adding <code>tool</code> (or function calling) capability to an LLM requires a parser (that recognizes  that the LLM has generated a tool) and a callback that performs arbitrary computation and returns a string. The serialized instances of tools <code>T</code> correspond to a language <code>L</code>;  Since by assumption, the LLM is capable of producing outputs in \\(L\\),  this allows the LLM to express the intention to execute a Callback with arbitrary instances  of <code>T</code>. In the last row, we show how an Agent can be viewed as a function signature involving its state <code>S</code>.</p> Function Description Function Signature LLM <code>[Input Query] -&gt; string</code> <code>[Input Query]</code> is the original query. Chat interface <code>[Message History] x [Input Query] -&gt; string</code> <code>[Message History]</code> consists of  previous messages<sup>1</sup>. Agent <code>[System Message] x [Message History] x [Input Query] -&gt; string</code> <code>[System Message]</code> is the system prompt. Agent with tool <code>[System Message] x (string -&gt; T) x (T -&gt; string) x [Message History] x [Input Query] -&gt; string</code> Parser with type <code>T</code> <code>string -&gt; T</code> Callback with type <code>T</code> <code>T -&gt; string</code> General Agent with state type <code>S</code> <code>S x [System Message] x (string -&gt; T) x (S x T -&gt; S x string) x [Message History] x [Input Query] -&gt; S x string</code>"},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#multi-agent-orchestration","title":"Multi-Agent Orchestration","text":""},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#an-agents-native-responders","title":"An Agent's \"Native\" Responders","text":"<p>When building an LLM-based multi-agent system, an orchestration mechanism is critical to manage the flow of messages  between agents, to ensure task progress, and handle inevitable LLM deviations from instructions. Langroid provides a  simple yet versatile orchestration mechanism that seamlessly handles:</p> <ul> <li>user interaction,</li> <li>tool handling,</li> <li>sub-task delegation</li> </ul> <p>We view an agent as a message transformer;  it may transform an incoming message using one of its three \"native\" responder methods, all of which have the same  function signature: <code>string -&gt; string</code>. These methods are:</p> <ul> <li><code>llm_response</code> returns the LLM's response to the input message. Whenever this method is invoked, the agent updates its dialog history (typically consisting of alternating user and LLM messages).</li> <li><code>user_response</code> prompts the user for input and returns their response.</li> <li><code>agent_response</code> by default only handles a <code>tool message</code> (i.e., one that contains an llm-generated structured  response): it performs any requested actions, and returns the result as a string. An <code>agent_response</code> method can have  other uses besides handling tool messages, such as handling scenarios where an LLM ``forgot'' to use a tool,  or used a tool incorrectly, and so on.</li> </ul> <p>To see why it is useful to have these responder methods, consider first a simple example of creating a basic chat loop with the user. It is trivial to create such a loop by alternating between <code>user_response</code> and <code>llm_response</code>.  Now suppose we instruct the agent to either directly answer the user's question or perform a web-search. Then it is possible that sometimes the <code>llm_response</code> will produce a \"tool message\", say <code>WebSearchTool</code>, which we would handle with the <code>agent_response</code> method. This requires a slightly different, and more involved, way of iterating among the agent's responder methods. </p>"},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#tasks-encapsulating-agent-orchestration","title":"Tasks: Encapsulating Agent Orchestration","text":"<p>From a coding perspective, it is useful to hide the actual iteration logic by wrapping an Agent class in a separate class, which we call a <code>Task</code>, which encapsulates all of the orchestration logic. Users of the Task class can then define the agent, tools, and any sub-tasks, wrap the agent in a task object of class Task, and simply call <code>task.run()</code>, letting the Task class deal with the details of orchestrating the agent's responder methods, determining task completion, and invoking sub-tasks.</p>"},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#responders-in-a-task-agents-native-responders-and-sub-tasks","title":"Responders in a Task: Agent's native responders and sub-tasks","text":"<p>The orchestration mechanism of a <code>Task</code> object works as follows. When a <code>Task</code> object is created from an agent, a  sequence of eligible responders is created, which includes the agent's three \"native\" responder agents in the sequence: <code>agent_response</code>, <code>llm_response</code>, <code>user_response</code>.  The type signature of the task's run method is <code>string -&gt; string</code>, just like the Agent's native responder methods, and this is the key to seamless delegation of tasks to sub-tasks. A list of subtasks can be added to a <code>Task</code> object via <code>task.add_sub_tasks([t1, t2, ... ])</code>, where <code>[t1, t2, ...]</code> are other  <code>Task</code> objects. The result of this is that the run method of each sub-task is appended to the sequence of eligible  responders in the parent task object.</p>"},{"location":"blog/2024/08/15/overview-of-langroids-multi-agent-architecture-prelim/#task-orchestration-updating-the-current-pending-message-cpm","title":"Task Orchestration: Updating the Current Pending Message (CPM)","text":"<p>A task always maintains a current pending message (CPM), which is the latest message \"awaiting\" a valid response  from a responder, which updates the CPM.  At a high level the <code>run</code> method of a task attempts to repeatedly find a valid response to the  CPM, until the task is done. (Note that this paradigm is somewhat reminescent of a Blackboard architecture, where agents take turns deciding whether they can update the shared message on the \"blackboard\".) This is achieved by repeatedly invoking the <code>step</code> method, which represents a \"turn\" in the conversation. The <code>step</code> method sequentially tries the eligible responders from the beginning of the eligible-responders list, until it finds a valid response, defined as a non-null or terminating message (i.e. one that signals that the task is done). In particular, this <code>step()</code> algorithm implies that a Task delegates (or \"fails over\") to a sub-task only if the task's  native responders have no valid response. </p> <p>There are a few simple rules that govern how <code>step</code> works: </p> <ul> <li>a responder entity (either a sub-task or a native entity -- one of LLM, Agent, or User) cannot    respond if it just responded in the previous step (this prevents a responder from \"talking to itself\". </li> <li>when a response signals that the task is done (via a <code>DoneTool</code> or a \"DONE\" string) the task is ready to exit and    return the CPM as the result of the task. </li> <li>when an entity \"in charge\" of the task has a null response, the task is considered finished and ready to exit.</li> <li>if the response of an entity or subtask is a structured message containing a recipient field, then the specified recipient task or entity will be the only one eligible to respond at the next step.</li> </ul> <p>Once a valid response is found in a step, the CPM is updated to this response, and the next step starts the search for a valid response from the beginning of the eligible responders list. When a response signals that the task is done,  the run method returns the CPM as the result of the task. This is a highly simplified account of the orchestration mechanism, and the actual implementation is more involved.</p> <p>The above simple design is surprising powerful and can support a wide variety of task structures, including trees and DAGs. As a simple illustrative example, tool-handling has a natural implementation. The LLM is instructed to use a certain JSON-structured message as a tool, and thus the <code>llm_response</code> method can produce a structured message, such  as an SQL query.  This structured message is then handled by the <code>agent_response</code> method, and the resulting message updates the CPM. The <code>llm_response</code> method then becomes eligible to respond again: for example if the agent's response contains an SQL  error, the LLM would retry its query, and if the agent's response consists of the query results, the LLM would respond with a summary of the results.</p> <p>The Figure below depicts the task orchestration and delegation mechanism, showing how iteration among responder methods works when a  Task <code>T</code> has sub-tasks <code>[T1, T2]</code> and <code>T1</code> has a  sub-task <code>T3</code>. </p> <p></p> <ol> <li> <p>Note that in reality, separator tokens are added to distinguish messages, and the messages are tagged with metadata indicating the sender, among other things.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/","title":"Langroid: Harness LLMs with Multi-Agent Programming","text":""},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#the-llm-opportunity","title":"The LLM Opportunity","text":"<p>Given the remarkable abilities of recent Large Language Models (LLMs), there is an unprecedented opportunity to build intelligent applications powered by this transformative technology. The top question for any enterprise is: how best to harness the power of LLMs for complex applications? For technical and practical reasons, building LLM-powered applications is not as simple as throwing a task at an LLM-system and expecting it to do it.</p>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#langroids-multi-agent-programming-framework","title":"Langroid's Multi-Agent Programming Framework","text":"<p>Effectively leveraging LLMs at scale requires a principled programming framework. In particular, there is often a need to maintain multiple LLM conversations, each instructed in different ways, and \"responsible\" for different aspects of a task.</p> <p>An agent is a convenient abstraction that encapsulates LLM conversation state, along with access to long-term memory (vector-stores) and tools (a.k.a functions or plugins). Thus a Multi-Agent Programming framework is a natural fit for complex LLM-based applications.</p> <p>Langroid is the first Python LLM-application framework that was explicitly designed  with Agents as first-class citizens, and Multi-Agent Programming as the core  design principle. The framework is inspired by ideas from the Actor Framework.</p> <p>Langroid allows an intuitive definition of agents, tasks and task-delegation among agents. There is a principled mechanism to orchestrate multi-agent collaboration. Agents act as message-transformers, and take turns responding to (and transforming) the current message. The architecture is lightweight, transparent, flexible, and allows other types of orchestration to be implemented. Besides Agents, Langroid also provides simple ways to directly interact with LLMs and vector-stores.</p>"},{"location":"blog/2023/09/03/langroid-harness-llms-with-multi-agent-programming/#highlights","title":"Highlights","text":"<ul> <li>Agents as first-class citizens: The <code>Agent</code> class encapsulates LLM conversation state,   and optionally a vector-store and tools. Agents are a core abstraction in Langroid;   Agents act as message transformers, and by default provide 3 responder methods, one corresponding to each   entity: LLM, Agent, User.</li> <li>Tasks: A Task class wraps an Agent, gives the agent instructions (or roles, or goals),   manages iteration over an Agent's responder methods,   and orchestrates multi-agent interactions via hierarchical, recursive   task-delegation. The <code>Task.run()</code> method has the same   type-signature as an Agent's responder's methods, and this is key to how   a task of an agent can delegate to other sub-tasks: from the point of view of a Task,   sub-tasks are simply additional responders, to be used in a round-robin fashion   after the agent's own responders.</li> <li>Modularity, Reusability, Loose coupling: The <code>Agent</code> and <code>Task</code> abstractions allow users to design   Agents with specific skills, wrap them in Tasks, and combine tasks in a flexible way.</li> <li>LLM Support: Langroid supports OpenAI LLMs including GPT-3.5-Turbo,   GPT-4.</li> <li>Caching of LLM prompts, responses: Langroid by default uses Redis for caching.</li> <li>Vector-stores: Qdrant, Chroma, LanceDB, Pinecone, PostgresDB (PGVector), Weaviate are currently supported.   Vector stores allow for Retrieval-Augmented-Generaation (RAG).</li> <li>Grounding and source-citation: Access to external documents via vector-stores   allows for grounding and source-citation.</li> <li>Observability, Logging, Lineage: Langroid generates detailed logs of multi-agent interactions and   maintains provenance/lineage of messages, so that you can trace back   the origin of a message.</li> <li>Tools/Plugins/Function-calling: Langroid supports OpenAI's recently   released function calling   feature. In addition, Langroid has its own native equivalent, which we   call tools (also known as \"plugins\" in other contexts). Function   calling and tools have the same developer-facing interface, implemented   using Pydantic,   which makes it very easy to define tools/functions and enable agents   to use them. Benefits of using Pydantic are that you never have to write   complex JSON specs for function calling, and when the LLM   hallucinates malformed JSON, the Pydantic error message is sent back to   the LLM so it can fix it!</li> </ul>"},{"location":"blog/2024/01/18/langroid-knolwedge-graph-rag-powered-by-neo4j/","title":"Langroid: Knolwedge Graph RAG powered by Neo4j","text":""},{"location":"blog/2024/01/18/langroid-knolwedge-graph-rag-powered-by-neo4j/#chat-with-various-sources-of-information","title":"\"Chat\" with various sources of information","text":"<p>LLMs are increasingly being used to let users converse in natural language with  a variety of types of data sources:</p> <ul> <li>unstructured text documents: a user's query is augmented with \"relevant\" documents or chunks   (retrieved from an embedding-vector store) and fed to the LLM to generate a response --    this is the idea behind Retrieval Augmented Generation (RAG).</li> <li>SQL Databases: An LLM translates a user's natural language question into an SQL query,   which is then executed by another module, sending results to the LLM, so it can generate   a natural language response based on the results.</li> <li>Tabular datasets: similar to the SQL case, except instead of an SQL Query, the LLM generates    a Pandas dataframe expression.</li> </ul> <p>Langroid has had specialized Agents for the above scenarios: <code>DocChatAgent</code> for RAG with unstructured text documents, <code>SQLChatAgent</code> for SQL databases, and <code>TableChatAgent</code> for tabular datasets.</p>"},{"location":"blog/2024/01/18/langroid-knolwedge-graph-rag-powered-by-neo4j/#adding-support-for-neo4j-knowledge-graphs","title":"Adding support for Neo4j Knowledge Graphs","text":"<p>Analogous to the SQLChatAgent, Langroid now has a  <code>Neo4jChatAgent</code>  to interact with a Neo4j knowledge graph using natural language. This Agent has access to two key tools that enable it to handle a user's queries:</p> <ul> <li><code>GraphSchemaTool</code> to get the schema of a Neo4j knowledge graph.</li> <li><code>CypherRetrievalTool</code> to generate Cypher queries from a user's query. Cypher is a specialized query language for Neo4j, and even though it is not as widely known as SQL, most LLMs today can generate Cypher Queries.</li> </ul> <p>Setting up a basic Neo4j-based RAG chatbot is straightforward. First ensure  you set these environment variables (or provide them in a <code>.env</code> file): <pre><code>NEO4J_URI=&lt;uri&gt;\nNEO4J_USERNAME=&lt;username&gt;\nNEO4J_PASSWORD=&lt;password&gt;\nNEO4J_DATABASE=&lt;database&gt;\n</code></pre></p> <p>Then you can configure and define a <code>Neo4jChatAgent</code> like this: <pre><code>import langroid as lr\nimport langroid.language_models as lm\n\nfrom langroid.agent.special.neo4j.neo4j_chat_agent import (\n    Neo4jChatAgent,\n    Neo4jChatAgentConfig,\n    Neo4jSettings,\n)\n\nllm_config = lm.OpenAIGPTConfig()\n\nload_dotenv()\n\nneo4j_settings = Neo4jSettings()\n\nkg_rag_agent_config = Neo4jChatAgentConfig(\n    neo4j_settings=neo4j_settings,\n    llm=llm_config, \n)\nkg_rag_agent = Neo4jChatAgent(kg_rag_agent_config)\nkg_rag_task = lr.Task(kg_rag_agent, name=\"kg_RAG\")\nkg_rag_task.run()\n</code></pre></p>"},{"location":"blog/2024/01/18/langroid-knolwedge-graph-rag-powered-by-neo4j/#example-pypi-package-dependency-chatbot","title":"Example: PyPi Package Dependency Chatbot","text":"<p>In the Langroid-examples repository, there is an example python  script showcasing tools/Function-calling + RAG using a <code>DependencyGraphAgent</code> derived from <code>Neo4jChatAgent</code>. This agent uses two tools, in addition to the tools available to <code>Neo4jChatAgent</code>:</p> <ul> <li><code>GoogleSearchTool</code> to find package version and type information, as well as to answer   other web-based questions after acquiring the required information from the dependency graph.</li> <li><code>DepGraphTool</code> to construct a Neo4j knowledge-graph modeling the dependency structure    for a specific package, using the API at DepsDev.</li> </ul> <p>In response to a user's query about dependencies, the Agent decides whether to use a Cypher query or do a web search. Here is what it looks like in action:</p> <p> </p>  Chatting with the `DependencyGraphAgent` (derived from Langroid's `Neo4jChatAgent`). When a user specifies a Python package name (in this case \"chainlit\"), the agent searches the web using `GoogleSearchTool` to find the version of the package, and then uses the `DepGraphTool` to construct the dependency graph as a neo4j knowledge graph. The agent then answers questions by generating Cypher queries to the knowledge graph, or by searching the web."},{"location":"blog/2023/09/14/using-langroid-with-local-llms/","title":"Using Langroid with Local LLMs","text":""},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#why-local-models","title":"Why local models?","text":"<p>There are commercial, remotely served models that currently appear to beat all open/local models. So why care about local models? Local models are exciting for a number of reasons:</p> <ul> <li>cost: other than compute/electricity, there is no cost to use them.</li> <li>privacy: no concerns about sending your data to a remote server.</li> <li>latency: no network latency due to remote API calls, so faster response times, provided you can get fast enough inference.</li> <li>uncensored: some local models are not censored to avoid sensitive topics.</li> <li>fine-tunable: you can fine-tune them on private/recent data, which current commercial models don't have access to.</li> <li>sheer thrill: having a model running on your machine with no internet connection,   and being able to have an intelligent conversation with it -- there is something almost magical about it.</li> </ul> <p>The main appeal with local models is that with sufficiently careful prompting, they may behave sufficiently well to be useful for specific tasks/domains, and bring all of the above benefits. Some ideas on how you might use local LLMs:</p> <ul> <li>In a multi-agent system, you could have some agents use local models for narrow    tasks with a lower bar for accuracy (and fix responses with multiple tries).</li> <li>You could run many instances of the same or different models and combine their responses.</li> <li>Local LLMs can act as a privacy layer, to identify and handle sensitive data before passing to remote LLMs.</li> <li>Some local LLMs have intriguing features, for example llama.cpp lets you    constrain its output using a grammar.</li> </ul>"},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#running-llms-locally","title":"Running LLMs locally","text":"<p>There are several ways to use LLMs locally. See the <code>r/LocalLLaMA</code> subreddit for a wealth of information. There are open source libraries that offer front-ends to run local models, for example <code>oobabooga/text-generation-webui</code> (or \"ooba-TGW\" for short) but the focus in this tutorial is on spinning up a server that mimics an OpenAI-like API, so that any code that works with the OpenAI API (for say GPT3.5 or GPT4) will work with a local model, with just a simple change: set <code>openai.api_base</code> to the URL where the local API server is listening, typically <code>http://localhost:8000/v1</code>.</p> <p>There are a few libraries we recommend for setting up local models with OpenAI-like APIs:</p> <ul> <li>LiteLLM OpenAI Proxy Server lets you set up a local    proxy server for over 100+ LLM providers (remote and local).</li> <li>ooba-TGW mentioned above, for a variety of models, including llama2 models.</li> <li>llama-cpp-python (LCP for short), specifically for llama2 models.</li> <li>ollama</li> </ul> <p>We recommend visiting these links to see how to install and run these libraries.</p>"},{"location":"blog/2023/09/14/using-langroid-with-local-llms/#use-the-local-model-with-the-openai-library","title":"Use the local model with the OpenAI library","text":"<p>Once you have a server running using any of the above methods,  your code that works with the OpenAI models can be made to work  with the local model, by simply changing the <code>openai.api_base</code> to the  URL where the local server is listening. </p> <p>If you are using Langroid to build LLM applications, the framework takes care of the <code>api_base</code> setting in most cases, and you need to only set the <code>chat_model</code> parameter in the LLM config object for the LLM model you are using. See the Non-OpenAI LLM tutorial for more details.</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/","title":"MALADE: Multi-Agent Architecture for Pharmacovigilance","text":"<p>Published in ML for HealthCare 2024</p> <p>Arxiv </p> <p>GitHub</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#summary","title":"Summary","text":"<p>We introduce MALADE (Multiple Agents powered by LLMs for ADE Extraction), a multi-agent system for Pharmacovigilance. It is the first effective explainable  multi-agent LLM system for extracting Adverse Drug Events (ADEs) from FDA drug labels and drug prescription data.</p> <p>Given a drug category and an adverse outcome, MALADE produces:</p> <ul> <li>a qualitative label of risk (<code>increase</code>, <code>decrease</code> or <code>no-effect</code>),</li> <li>confidence in the label (a number in \\([0,1]\\)),</li> <li>frequency of effect (<code>rare</code>, <code>common</code>, or <code>none</code>),</li> <li>strength of evidence (<code>none</code>, <code>weak</code>, or <code>strong</code>), and</li> <li>a justification with citations.</li> </ul> <p>This task is challenging for several reasons: </p> <ul> <li>FDA labels and prescriptions are for individual drugs, not drug categories, so representative drugs in a category    need to be identified from patient prescription data, and ADE information found for specific drugs in a category    needs to be aggregated to make a statement about the category as a whole, </li> <li>The data is noisy, with variations in the terminologies of drugs and outcomes, and </li> <li>ADE descriptions are often buried in large amounts of narrative text.</li> </ul> <p>The MALADE architecture is LLM-agnostic  and leverages the Langroid multi-agent framework. It consists of a combination of Agents using Retrieval Augmented Generation (RAG), that  iteratively improve their answers based on feedback from Critic Agents. We evaluate the quantitative scores against  a ground-truth dataset known as the OMOP Ground Truth Task and find that MALADE achieves state-of-the-art performance.</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#introduction","title":"Introduction","text":"<p>In the era of Large Language Models (LLMs), given their remarkable text understanding and generation abilities,  there is an unprecedented opportunity to develop new, LLM-based methods for trustworthy medical knowledge synthesis,  extraction and summarization. The focus of this paper is Pharmacovigilance, a critical task in healthcare, where  the goal is to monitor and evaluate the safety of drugs. In particular, the identification of Adverse Drug Events  (ADEs) is crucial for ensuring patient safety. Consider a question such as this:</p> <p>What is the effect of ACE inhibitors on the risk of developing angioedema?</p> <p>Here the drug category \\(C\\) is ACE inhibitors, and the outcome \\(O\\) is angioedema. Answering this question involves several steps:</p> <ul> <li>1(a): Find all drugs in the ACE inhibitor category \\(C\\), e.g. by searching the FDA  National Drug Code (NDC)     database. This can be done using Elastic-Search, with filters to handle variations in drug/category names and inaccurate classifications.</li> <li>1(b): Find the prescription frequency of each drug in \\(C\\) from patient prescription data, e.g.  the MIMIC-IV database. This can be done with a SQL query.</li> <li>1(c): Identify the representative drugs \\(D \\subset C\\) in this category, based on prescription frequency data       from step 2.  </li> <li>2: For each drug \\(d \\in D\\), summarize ADE information about the effect of \\(d\\) on the outcome \\(O\\) of interest,    (in this case angioedema) from text-based pharmaceutical sources,      e.g. the OpenFDA Drug Label database.</li> <li>3: Aggregate the information from all drugs in \\(D\\) to make a statement about the category \\(C\\) as a whole.</li> </ul>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#the-role-of-llms","title":"The role of LLMs","text":"<p>While steps 1(a) and 1(b) can be done by straightforward deterministic algorithms (SQL queries or Elastic-Search), the  remaining steps are challenging but ideally suited to LLMs:</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#step-1c-identifying-representative-drugs-in-a-category-from-prescription-frequency-data-drugfinder-agent","title":"Step 1(c): Identifying representative drugs in a category from prescription frequency data (<code>DrugFinder</code> Agent)","text":"<p>This is complicated by noise, such as the same drug appearing multiple times under different names,  formulations or delivery methods (For example, the ACE inhibitor Lisinopril is also known as Zestril and Prinivil.)    Thus a judgment must   be made as to whether these are sufficiently different to be considered pharmacologically distinct;   and some of these drugs may not actually belong to the category. This task thus requires a grouping operation,    related to the task of identifying standardized drug codes from text descriptions,   well known to be challenging. This makes it very difficult to explicitly define the algorithm in a deterministic    manner that covers all edge cases (unlike the above database tasks), and hence is well-suited   to LLMs, particularly those such as GPT-4, Claude3.5, and similar-strength variants which are known to have been    trained on vast amounts of general medical texts. </p> <p>In MALADE, this task is handled by the <code>DrugFinder</code> agent, which is an Agent/Critic system where the main agent iteratively improves its output in a feedback loop with the Critic agent. For example, the Critic corrects the Agent when it incorrectly classifies drugs as pharmacologically distinct.</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#step-2-identifying-drug-outcome-associations-drugoutcomeinfoagent","title":"Step 2: Identifying Drug-Outcome Associations (<code>DrugOutcomeInfoAgent</code>)","text":"<p>The task here is to identify whether a given drug has an established effect on the risk of a given outcome, based on FDA drug label database, and output a summary of relevant information, including the level of identified risk and the evidence for such an effect. Since this task involves extracting information from narrative text, it is well-suited to LLMs using the Retrieval Augmented Generation (RAG) technique. </p> <p>In MALADE, the <code>DrugOutcomeInfoAgent</code> handles this task, and is also an Agent/Critic system, where the Critic provides feedback and corrections to the Agent's output. This agent does not have direct access to the FDA Drug Label data, but can receive this information via another agent, <code>FDAHandler</code>. FDAHandler is equipped with tools (also known as function-calls)  to invoke the OpenFDA API for drug label data, and answers questions in the context of information retrieved based on the queries. Information received from this API is ingested into a vector database, so the agent first uses a tool to query this vector database, and only resorts to the OpenFDA API tool if the vector database does not contain the relevant information. An important aspect of this agent is that its responses include specific citations and excerpts justifying its conclusions.</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#step-3-labeling-drug-category-outcome-associations-categoryoutcomeriskagent","title":"Step 3: Labeling Drug Category-Outcome Associations (<code>CategoryOutcomeRiskAgent</code>)","text":"<p>To identify association between a drug category C and an adverse health outcome \\(O\\), we concurrently run a batch of  queries to copies of <code>DrugOutcomeInfoAgent</code>, one for each drug \\(d\\) in the representative-list \\(D\\) for the category, of the form: </p> <p>Does drug \\(d\\) increase or decrease the risk of condition \\(O\\)?</p> <p>The results are sent to <code>CategoryOutcomeRiskAgent</code>,  which is an Agent/Critic system which performs the final classification step; its goal is to generate the qualitative and quantitative outputs mentioned above.</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#malade-architecture","title":"MALADE Architecture","text":"<p>The figure below illustrates how the MALADE architecture handles the query,</p> <p>What is the effect of ACE inhibitors on the risk of developing angioedema?</p> <p></p> <p>The query triggers a sequence of subtasks performed by the three Agents described above:  <code>DrugFinder</code>, <code>DrugOutcomeInfoAgent</code>, and <code>CategoryOutcomeRiskAgent</code>. Each Agent generates a response and justification, which are validated by a corresponding Critic agent, whose feedback is used by the Agent to revise its response.</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#evaluation","title":"Evaluation","text":""},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#omop-ground-truth","title":"OMOP Ground Truth","text":"<p>We evaluate the results of MALADE against a well-established ground-truth dataset,  the OMOP ADE ground-truth table, shown below. This is a reference dataset within the Observational Medical Outcomes Partnership (OMOP) Common Data Model that  contains validated information about known adverse drug events.</p> <p></p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#confusion-matrix","title":"Confusion Matrix","text":"<p>Below is a side-by-side comparison of this ground-truth dataset (left) with MALADE's labels (right), ignoring blue  cells (see the paper for details):</p> <p></p> <p>The resulting confusion-matrix for MALADE is shown below:</p> <p></p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#auc-metric","title":"AUC Metric","text":"<p>Since MALADE produces qualitative and quantitative outputs, the paper explores a variety of ways to evaluate its performance against the OMOP ground-truth dataset. Here we focus on the label output \\(L\\) (i.e. <code>increase</code>,  <code>decrease</code>, or <code>no-effect</code>), and its associated confidence score \\(c\\), and use the Area Under the ROC Curve (AUC) as  the evaluation metric. The AUC metric is designed for binary classification, so we transform the three-class label output \\(L\\) and confidence score \\(c\\) to a binary classification score \\(p\\) as follows. We treat \\(L\\) = <code>increase</code> as the positive class, and \\(L\\) = <code>decrease</code> or <code>no-effect</code> as the negative class, and we transform the label confidence score \\(c\\) into a probability \\(p\\) of <code>increase</code> as follows:</p> <ul> <li>if the label output is <code>increase</code>, \\(p = (2+c)/3\\),</li> <li>if the label output is <code>no-effect</code>, \\(p = (2-c)/3\\), and</li> <li>if the label output is <code>decrease</code> , \\(p = (1-c)/3\\).</li> </ul> <p>These transformations align with two intuitions: (a) a higher confidence in <code>increase</code> corresponds to a higher probability of <code>increase</code>, and a higher confidence in <code>no-effect</code> or <code>decrease</code> corresponds to a lower probability of <code>increase</code>, and (b) for a given confidence score \\(c\\), the progression of labels <code>decrease</code>, <code>no-effect</code>, and <code>increase</code> corresponds to increasing probabilities of <code>increase</code>. The above transformations ensure that the probability \\(p\\) is in the range \\([0,1]\\) and scales linearly with the confidence score \\(c\\).</p> <p>We ran the full MALADE system for all drug-category/outcome pairs in the OMOP ground-truth dataset,  and then computed the AUC for the score \\(p\\) against the ground-truth binary classification label. With <code>GPT-4-Turbo</code> we obtained an AUC of 0.85, while <code>GPT-4o</code> resulted in an AUC of 0.90. These are state-of-the-art results for this specific ADE-extraction task.</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#ablations","title":"Ablations","text":"<p>An important question the paper investigates is whether (and how much) the various components (RAG, critic agents, etc) contribute to MALADE's performance. To answer this, we perform ablations, where we remove one or more components from the MALADE system and evaluate the performance of the resulting system. For example we found that dropping the Critic agents reduces the AUC (using <code>GPT-4-Turbo</code>) from 0.85 to 0.82 (see paper, Appendix D for more ablation results).</p>"},{"location":"blog/2024/08/12/malade-multi-agent-architecture-for-pharmacovigilance/#variance-of-llm-generated-scores","title":"Variance of LLM-generated Scores","text":"<p>When using an LLM to generate numerical scores, it is important to understand the variance in the scores. For example, if a single \"full\" run of MALADE (i.e. for all drug-category/outcome pairs in the OMOP ground-truth dataset) produces a certain AUC, was it a \"lucky\" run, or is the AUC relatively stable across runs? Ideally one would investigate this by repeating the full run of MALADE many times, but given the expense of running a full experiment, we focus on just three representative cells in the OMOP table, one corresponding to each possible ground-truth label, and run MALADE 10 times for each cells, and study the distribution of \\(p\\) (the probability of increased risk, translated from the confidence score using the method described above), for each output label. Encouragingly, we find that the distribution of \\(p\\) shows clear separation between the three labels, as in the figure below (The \\(x\\) axis ranges from 0 to 1, and the three colored groups of bars represent, from left to right, <code>decrease</code>, <code>no-effect</code>, and <code>increase</code> labels). Full details are in  the Appendix D of the paper.</p> <p></p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/","title":"Multi Agent Debate and Education Platform","text":""},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#introduction","title":"Introduction","text":"<p>Have you ever imagined a world where we can debate complex issues with Generative AI agents taking a distinct  stance and backing their arguments with evidence? Some will change your mind, and some will reveal the societal biases  on which each distinctive Large Language Model (LLM) is trained on. Introducing an AI-powered debate platform that brings  this imagination to reality, leveraging diverse LLMs and the Langroid multi-agent programming framework. The system enables users to engage in structured debates with an AI taking the opposite stance (or even two AIs debating each other), using a multi-agent architecture with Langroid's powerful framework, where each agent embodies a specific ethical perspective, creating realistic and dynamic interactions.  Agents are prompt-engineered and role-tuned to align with their assigned ethical stance,  ensuring thoughtful and structured debates. </p> <p>My motivations for creating this platform included: </p> <ul> <li>A debate coach for underserved students without access to traditional resources. </li> <li>Tool for research and generating arguments from authentic sources. </li> <li>Create an adaptable education platform to learn two sides of the coin for any topic.</li> <li>Reduce echo chambers perpetuated by online algorithms by fostering two-sided debates on any topic, promoting education and awareness around misinformation. </li> <li>Provide a research tool to study the varieties of biases in LLMs that are often trained on text reflecting societal biases. </li> <li>Identify a good multi-agent framework designed for programming with LLMs.</li> </ul>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#platform-features","title":"Platform Features:","text":""},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#dynamic-agent-generation","title":"Dynamic Agent Generation:","text":"<p>The platform features five types of agents: Pro, Con, Feedback, Research, and Retrieval Augmented Generation (RAG) Q&amp;A.  Each agent is dynamically generated using role-tuned and engineered prompts, ensuring diverse and engaging interactions.</p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#pro-and-con-agents","title":"Pro and Con Agents:","text":"<p>These agents engage in the core debate, arguing for and against the chosen topic.  Their prompts are carefully engineered to ensure they stay true to their assigned ethical stance.</p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#feedback-agent","title":"Feedback Agent:","text":"<p>This agent provides real-time feedback on the arguments and declares a winner. The evaluation criteria are based on the well-known Lincoln\u2013Douglas debate format, and include:</p> <ul> <li>Clash of Values </li> <li>Argumentation </li> <li>Cross-Examination </li> <li>Rebuttals </li> <li>Persuasion </li> <li>Technical Execution </li> <li>Adherence to Debate Etiquette </li> <li>Final Focus</li> </ul>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#research-agent","title":"Research Agent:","text":"<p>This agent has the following functionalities:</p> <ul> <li>Utilizes the <code>MetaphorSearchTool</code> and the <code>Metaphor</code> (now called <code>Exa</code>) Search API to conduct web searches combined with Retrieval Augmented Generation (RAG) to relevant web references for user education about the selected topic. </li> <li>Produces a summary of arguments for and against the topic.</li> <li>RAG-based document chat with the resources identified through Web Search. </li> </ul>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#rag-qa-agent","title":"RAG Q&amp;A Agent:","text":"<ul> <li>Provides Q&amp;A capability using a RAG based chat interaction with the resources identified through Web Search. The agent utilizes <code>DocChatAgent</code> that is part of Langroid framework which orchestrates all LLM interactions. </li> <li>Rich chunking parameters allows the user to get optimized relevance results. Check out <code>config.py</code>for details.</li> </ul>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#topic-adaptability","title":"Topic Adaptability:","text":"<p>Easily adaptable to any subject by simply adding pro and con system messages. This makes it a versatile tool for exploring diverse topics and fostering critical thinking. Default topics cover ethics and use of AI for the following:   - Healthcare   - Intellectual property    - Societal biases    - Education</p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#autonomous-or-interactive","title":"Autonomous or Interactive:","text":"<p>Engage in manual debate with a pro or con agent or watch it autonomously while adjusting number of turns.</p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#diverse-llm-selection-adaptable-per-agent","title":"Diverse LLM Selection Adaptable per Agent:","text":"<p>Configurable to select from diverse commercial and open source models: OpenAI, Google, and Mistral  to experiment with responses for diverse perspectives. Users can select a unique LLM for each agent. </p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#llm-toolfunction-integration","title":"LLM Tool/Function Integration:","text":"<p>Utilizes LLM tools/functions features to conduct semantic search using Metaphor Search API and summarizes the pro and  con perspectives for education.</p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#configurable-llm-parameters","title":"Configurable LLM Parameters:","text":"<p>Parameters like temperature, minimum and maximum output tokens, allowing for customization of the AI's responses. Configurable LLM parameters like temperature, min &amp; max output tokens. For Q&amp;A with the searched resources, several parameters can be tuned in the <code>config</code> to enhance response relevance.</p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#modular-design","title":"Modular Design:","text":"<p>Reusable code and modularized for other LLM applications.</p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#interaction","title":"Interaction","text":"<ol> <li>Decide if you want to you use same LLM for all agents or different ones</li> <li>Decide if you want autonomous debate between AI Agents or user vs. AI Agent. </li> <li>Select a debate topic.</li> <li>Choose your side (Pro or Con).</li> <li>Engage in a debate by providing arguments and receiving responses from agents.</li> <li>Request feedback at any time by typing <code>f</code>.</li> <li>Decide if you want the Metaphor Search to run to find Topic relevant web links    and summarize them. </li> <li>Decide if you want to chat with the documents extracted from URLs found to learn more about the Topic.</li> <li>End the debate manually by typing <code>done</code>. If you decide to chat with the documents, you can end session by typing <code>x</code></li> </ol>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#why-was-langroid-chosen","title":"Why was Langroid chosen?","text":"<p>I chose Langroid framework because it's a principled multi-agent programming framework inspired by the Actor framework. Prior to using Langroid, I developed a multi-agent debate system, however, I had to write a lot of tedious code to manage states of communication between debating agents, and the user interactions with LLMs. Langroid allowed me to seamlessly integrate multiple LLMs, easily create agents, tasks, and attach sub-tasks. </p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#agent-creation-code-example","title":"Agent Creation Code Example","text":"<pre><code>   def create_chat_agent(name: str, llm_config: OpenAIGPTConfig, system_message: str) -&gt; ChatAgent:\n\n    return ChatAgent(\n        ChatAgentConfig(\n            llm=llm_config,\n            name=name,\n            system_message=system_message,\n        )\n    )\n</code></pre>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#sample-pro-topic-agent-creation","title":"Sample Pro Topic Agent Creation","text":"<p><pre><code>    pro_agent = create_chat_agent(\n        \"Pro\",\n        pro_agent_config,\n        system_messages.messages[pro_key].message + DEFAULT_SYSTEM_MESSAGE_ADDITION,\n    )\n</code></pre> The <code>Task</code> mechanism in Langroid provides a robust mechanism for managing complex interactions within multi-agent  systems. <code>Task</code> serves as a container for managing the flow of interactions between different agents (such as chat agents) and attached sub-tasks.<code>Task</code> also helps with turn-taking, handling responses,  and ensuring smooth transitions between dialogue states. Each Task object is responsible for coordinating responses  from its assigned agent, deciding the sequence of responder methods (llm_response, user_response, agent_response),  and managing transitions between different stages of a conversation or debate. Each agent can focus on its specific  role while the task structure handles the overall process's orchestration and flow, allowing a clear separation of  concerns. The architecture and code transparency of Langroid's framework make it an incredible candidate for  applications like debates where multiple agents must interact dynamically and responsively based on a mixture of user inputs and automated responses.</p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#task-creation-and-orchestration-example","title":"Task creation and Orchestration Example","text":"<p><pre><code>    user_task = Task(user_agent, interactive=interactive_setting, restart=False)\n    ai_task = Task(ai_agent, interactive=False, single_round=True)\n    user_task.add_sub_task(ai_task)\n    if not llm_delegate:\n        user_task.run(user_agent.user_message, turns=max_turns)\n    else:\n        user_task.run(\"get started\", turns=max_turns)\n</code></pre> Tasks can be easily set up as sub-tasks of an orchestrating agent. In this case user_task could be Pro or Con depending  on the user selection. </p> <p>If you want to build custom tools/functions or use Langroid provided it is only a line of code using <code>agent.enable_messaage</code>. Here is an example of <code>MetaphorSearchTool</code> and <code>DoneTool</code>.  <pre><code>        metaphor_search_agent.enable_message(MetaphorSearchTool)\n        metaphor_search_agent.enable_message(DoneTool)\n</code></pre></p> <p>Overall I had a great learning experience using Langroid and recommend using it for any projects  that need to utilize LLMs. I am already working on a few Langroid based information retrieval and research systems  for use in medicine and hoping to contribute more soon. </p>"},{"location":"blog/2025/02/04/multi-agent-debate-and-education-platform/#bio","title":"Bio","text":"<p>I'm a high school senior at Khan Lab School located in Mountain View, CA where I host a student-run Podcast known as the Khan-Cast. I also enjoy tinkering with interdisciplinary STEM projects. You can reach me on LinkedIn.</p>"},{"location":"demos/targeting/audience-targeting/","title":"Audience Targeting for a Business","text":"<p>Suppose you are a marketer for a business, trying to figure out which  audience segments to target. Your downstream systems require that you specify standardized audience segments to target, for example from the IAB Audience Taxonomy.</p> <p>There are thousands of standard audience segments, and normally you would need  to search the list for potential segments that match what you think your ideal customer profile is. This is a tedious, error-prone task.</p> <p>But what if we can leverage an LLM such as GPT-4? We know that GPT-4 has  skills that are ideally suited for this task:</p> <ul> <li>General knowledge about businesses and their ideal customers</li> <li>Ability to recognize which standard segments match an English description of a customer profile</li> <li>Ability to plan a conversation to get the information it needs to answer a question</li> </ul> <p>Once you decide to use an LLM, you still need to figure out how to organize the  various components of this task:</p> <ul> <li>Research: What are some ideal customer profiles for the business</li> <li>Segmentation: Which standard segments match an English description of a customer profile</li> <li>Planning: how to organize the task to identify a few standard segments</li> </ul>"},{"location":"demos/targeting/audience-targeting/#using-langroid-agents","title":"Using Langroid Agents","text":"<p>Langroid makes it intuitive and simple to build an LLM-powered system organized around agents, each responsible for a different task. In less than a day we built a 3-agent system to automate this task:</p> <ul> <li>The <code>Marketer</code> Agent is given the Planning role.</li> <li>The <code>Researcher</code> Agent is given the Research role,    and it has access to the business description. </li> <li>The <code>Segmentor</code> Agent is given the Segmentation role. It has access to the    IAB Audience Taxonomy via a vector database, i.e. its rows have been mapped to   vectors via an embedding model, and these vectors are stored in a vector-database.    Thus given an English description of a customer profile,   the <code>Segmentor</code> Agent maps it to a vector using the embedding model,   and retrieves the nearest (in vector terms, e.g. cosine similarity)    IAB Standard Segments from the vector-database. The Segmentor's LLM    further refines this by selecting the best-matching segments from the retrieved list.</li> </ul> <p>To kick off the system, the human user describes a business in English, or provides the URL of the business's website.  The <code>Marketer</code> Agent sends customer profile queries to the <code>Researcher</code>, who answers in plain English based on  the business description, and the Marketer takes this description and sends it to the Segmentor, who maps it to Standard IAB Segments. The task is done when the Marketer finds 4 Standard segments.  The agents are depicted in the diagram below:</p> <p></p>"},{"location":"demos/targeting/audience-targeting/#an-example-glashutte-watches","title":"An example: Glashutte Watches","text":"<p>The human user first provides the URL of the business, in this case: <pre><code>https://www.jomashop.com/glashutte-watches.html\n</code></pre> From this URL, the <code>Researcher</code> agent summarizes its understanding of the business. The <code>Marketer</code> agent starts by asking the <code>Researcher</code>: <pre><code>Could you please describe the age groups and interests of our typical customer?\n</code></pre> The <code>Researcher</code> responds with an English description of the customer profile: <pre><code>Our typical customer is a fashion-conscious individual between 20 and 45 years...\n</code></pre> The <code>Researcher</code> forwards this English description to the <code>Segmentor</code> agent, who maps it to a standardized segment, e.g.: <pre><code>Interest|Style &amp; Fashion|Fashion Trends\n...\n</code></pre> This conversation continues until the <code>Marketer</code> agent has identified 4 standardized segments.</p> <p>Here is what the conversation looks like:</p> <p></p>"},{"location":"examples/agent-tree/","title":"Hierarchical computation with Langroid Agents","text":"<p>Here is a simple example showing tree-structured computation where each node in the tree is handled by a separate agent. This is a toy numerical example, and illustrates:</p> <ul> <li>how to have agents organized in a hierarchical structure to accomplish a task </li> <li>the use of global state accessible to all agents, and </li> <li>the use of tools/function-calling.</li> </ul>"},{"location":"examples/agent-tree/#the-computation","title":"The Computation","text":"<p>We want to carry out the following calculation for a given input number \\(n\\):</p> <pre><code>def Main(n):\n    if n is odd:\n        return (3*n+1) + n\n    else:\n        if n is divisible by 10:\n            return n/10 + n\n        else:\n            return n/2 + n\n</code></pre>"},{"location":"examples/agent-tree/#using-function-composition","title":"Using function composition","text":"<p>Imagine we want to do this calculation using a few auxiliary functions:</p> <pre><code>def Main(n):\n    # return non-null value computed by Odd or Even\n    Record n as global variable # to be used by Adder below\n    return Odd(n) or Even(n)\n\ndef Odd(n):\n    # Handle odd n\n    if n is odd:\n        new = 3*n+1\n        return Adder(new)\n    else:\n        return None\n\ndef Even(n):\n    # Handle even n: return non-null value computed by EvenZ or EvenNZ\n    return EvenZ(n) or EvenNZ(n)\n\ndef EvenZ(n):\n    # Handle even n divisible by 10, i.e. ending in Zero\n    if n is divisible by 10:\n        new = n/10\n        return Adder(new)\n    else:\n        return None\n\ndef EvenNZ(n):\n    # Handle even n not divisible by 10, i.e. not ending in Zero\n    if n is not divisible by 10:\n        new = n/2\n        return Adder(new)\n    else:\n        return None  \n\ndef Adder(new):\n    # Add new to starting number, available as global variable n\n    return new + n\n</code></pre>"},{"location":"examples/agent-tree/#mapping-to-a-tree-structure","title":"Mapping to a tree structure","text":"<p>This compositional/nested computation can be represented as a tree:</p> <pre><code>       Main\n     /     \\\n  Even     Odd\n  /   \\        \\\nEvenZ  EvenNZ   Adder\n  |      |\n Adder  Adder\n</code></pre> <p>Let us specify the behavior we would like for each node, in a  \"decoupled\" way, i.e. we don't want a node to be aware of the other nodes. As we see later, this decoupled design maps very well onto Langroid's multi-agent task orchestration. To completely define the node behavior, we need to specify how it handles an \"incoming\" number \\(n\\) (from a parent node  or user), and how it handles a \"result\" number \\(r\\) (from a child node).</p> <ul> <li><code>Main</code>: <ul> <li>incoming \\(n\\): simply send down \\(n\\), record the starting number \\(n_0 = n\\) as a global variable. </li> <li>result \\(r\\): return \\(r\\).</li> </ul> </li> <li><code>Odd</code>: <ul> <li>incoming \\(n\\): if n is odd, send down \\(3*n+1\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>Even</code>: <ul> <li>incoming \\(n\\): if n is even, send down \\(n\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>EvenZ</code>: (guaranteed by the tree hierarchy, to receive an even number.)  <ul> <li>incoming \\(n\\): if n is divisible by 10, send down \\(n/10\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>EvenNZ</code>: (guaranteed by the tree hierarchy, to receive an even number.)<ul> <li>incoming \\(n\\): if n is not divisible by 10, send down \\(n/2\\), else return None</li> <li>result \\(r\\): return \\(r\\)</li> </ul> </li> <li><code>Adder</code>:<ul> <li>incoming \\(n\\): return \\(n + n_0\\) where \\(n_0\\) is the  starting number recorded by Main as a global variable.</li> <li>result \\(r\\): Not applicable since <code>Adder</code> is a leaf node.</li> </ul> </li> </ul>"},{"location":"examples/agent-tree/#from-tree-nodes-to-langroid-agents","title":"From tree nodes to Langroid Agents","text":"<p>Let us see how we can perform this calculation using multiple Langroid agents, where</p> <ul> <li>we define an agent corresponding to each of the nodes above, namely  <code>Main</code>, <code>Odd</code>, <code>Even</code>, <code>EvenZ</code>, <code>EvenNZ</code>, and <code>Adder</code>.</li> <li>we wrap each Agent into a Task, and use the <code>Task.add_subtask()</code> method to connect the agents into    the desired hierarchical structure.</li> </ul> <p>Below is one way to do this using Langroid. We designed this with the following desirable features:</p> <ul> <li> <p>Decoupling: Each agent is instructed separately, without mention of any other agents   (E.g. Even agent does not know about Odd Agent, EvenZ agent, etc).   In particular, this means agents will not be \"addressing\" their message   to specific other agents, e.g. send number to Odd agent when number is odd,   etc. Allowing addressing would make the solution easier to implement,   but would not be a decoupled solution.   Instead, we want Agents to simply put the number \"out there\", and have it handled   by an applicable agent, in the task loop (which consists of the agent's responders,   plus any sub-task <code>run</code> methods).</p> </li> <li> <p>Simplicity: Keep the agent instructions relatively simple. We would not want a solution   where we have to instruct the agents (their LLMs) in convoluted ways. </p> </li> </ul> <p>One way naive solutions fail is because agents are not able to distinguish between a number that is being \"sent down\" the tree as input, and a number that is being \"sent up\" the tree as a result from a child node.</p> <p>We use a simple trick: we instruct the LLM to mark returned values using the RESULT keyword, and instruct the LLMs on how to handle numbers that come with RESULT keyword, and those that don't In addition, we leverage some features of Langroid's task orchestration:</p> <ul> <li>When <code>llm_delegate</code> is <code>True</code>, if the LLM says <code>DONE [rest of msg]</code>, the task is   considered done, and the result of the task is <code>[rest of msg]</code> (i.e the part after <code>DONE</code>).</li> <li>In the task loop's <code>step()</code> function (which seeks a valid message during a turn of   the conversation) when any responder says <code>DO-NOT-KNOW</code>, it is not considered a valid   message, and the search continues to other responders, in round-robin fashion.</li> </ul> <p>See the <code>chat-tree.py</code> example for an implementation of this solution. You can run that example as follows: <pre><code>python3 examples/basic/chat-tree.py\n</code></pre> In the sections below we explain the code in more detail.</p>"},{"location":"examples/agent-tree/#define-the-agents","title":"Define the agents","text":"<p>Let us start with defining the configuration to be used by all agents:</p> <pre><code>from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\nfrom langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n\nconfig = ChatAgentConfig(\n  llm=OpenAIGPTConfig(\n    chat_model=OpenAIChatModel.GPT4o,\n  ),\n  vecdb=None, # no need for a vector database\n)\n</code></pre> <p>Next we define each of the agents, for example:</p> <pre><code>main_agent = ChatAgent(config)\n</code></pre> <p>and similarly for the other agents.</p>"},{"location":"examples/agent-tree/#wrap-each-agent-in-a-task","title":"Wrap each Agent in a Task","text":"<p>To allow agent interactions, the first step is to wrap each agent in a Task. When we define the task, we pass in the instructions above as part of the system message. Recall the instructions for the <code>Main</code> agent:</p> <ul> <li><code>Main</code>:<ul> <li>incoming \\(n\\): simply send down \\(n\\), record the starting number \\(n_0 = n\\) as a global variable.</li> <li>result \\(r\\): return \\(r\\).</li> </ul> </li> </ul> <p>We include the equivalent of these instructions in the <code>main_task</code> that wraps  the <code>main_agent</code>:</p> <pre><code>from langroid.agent.task import Task\n\nmain_task = Task(\n    main_agent,\n    name=\"Main\",\n    interactive=False, #(1)!\n    system_message=\"\"\"\n          You will receive two types of messages, to which you will respond as follows:\n\n          INPUT Message format: &lt;number&gt;\n          In this case simply write the &lt;number&gt;, say nothing else.\n\n          RESULT Message format: RESULT &lt;number&gt;\n          In this case simply say \"DONE &lt;number&gt;\", e.g.:\n          DONE 19\n\n          To start off, ask the user for the initial number, \n          using the `ask_num` tool/function.\n          \"\"\",\n    llm_delegate=True, # allow LLM to control end of task via DONE\n    single_round=False,\n)\n</code></pre> <ol> <li>Non-interactive: don't wait for user input in each turn </li> </ol> <p>There are a couple of points to highlight about the <code>system_message</code>  value in this task definition:</p> <ul> <li>When the <code>Main</code> agent receives just a number, it simply writes out that number,   and in the Langroid Task loop, this number becomes the \"current pending message\"   to be handled by one of the sub-tasks, i.e. <code>Even, Odd</code>. Note that these sub-tasks   are not mentioned in the system message, consistent with the decoupling principle.</li> <li>As soon as either of these sub-tasks returns a non-Null response, in the format \"RESULT \", the <code>Main</code> agent   is instructed to return this result saying \"DONE \". Since <code>llm_delegate</code>   is set to <code>True</code> (meaning the LLM can decide when the task has ended),    this causes the <code>Main</code> task to be considered finished and the task loop is exited. <p>Since we want the <code>Main</code> agent to record the initial number as a global variable, we use a tool/function <code>AskNum</code> defined as follows  (see this section in the getting started guide  for more details on Tools):</p> <pre><code>from rich.prompt import Prompt\nfrom langroid.agent.tool_message import ToolMessage\n\n\nclass AskNumTool(ToolMessage):\n  request = \"ask_num\"\n  purpose = \"Ask user for the initial number\"\n\n  def handle(self) -&gt; str:\n    \"\"\"\n    This is a stateless tool (i.e. does not use any Agent member vars), so we can\n    define the handler right here, instead of defining an `ask_num`\n    method in the agent.\n    \"\"\"\n    num = Prompt.ask(\"Enter a number\")\n    # record this in global state, so other agents can access it\n    MyGlobalState.set_values(number=num)\n    return str(num)\n</code></pre> <p>We then enable the <code>main_agent</code> to use and handle messages that conform to the  <code>AskNum</code> tool spec:</p> <pre><code>main_agent.enable_message(AskNumTool)\n</code></pre> <p>Using and Handling a tool/function</p> <p>\"Using\" a tool means the agent's LLM generates  the function-call (if using OpenAI function-calling) or  the JSON structure (if using Langroid's native tools mechanism)  corresponding to this tool. \"Handling\" a tool refers to the Agent's method  recognizing the tool and executing the corresponding code.</p> <p>The tasks for other agents are defined similarly. We will only note here that the <code>Adder</code> agent needs a special tool <code>AddNumTool</code> to be able to add the current number to the initial number set by the <code>Main</code> agent. </p>"},{"location":"examples/agent-tree/#connect-the-tasks-into-a-tree-structure","title":"Connect the tasks into a tree structure","text":"<p>So far, we have wrapped each agent in a task, in isolation, and there is no  connection between the tasks. The final step is to connect the tasks to  the tree structure we saw earlier:</p> <pre><code>main_task.add_sub_task([even_task, odd_task])\neven_task.add_sub_task([evenz_task, even_nz_task])\nevenz_task.add_sub_task(adder_task)\neven_nz_task.add_sub_task(adder_task)\nodd_task.add_sub_task(adder_task)\n</code></pre> <p>Now all that remains is to run the main task:</p> <pre><code>main_task.run()\n</code></pre> <p>Here is what a run starting with \\(n=12\\) looks like:</p> <p></p>"},{"location":"examples/guide/","title":"Guide to examples in <code>langroid-examples</code> repo","text":"<p>Outdated</p> <p>This guide is from Feb 2024; there have been numerous additional examples since then. We recommend you visit the <code>examples</code> folder in the core <code>langroid</code> repo for the most up-to-date examples. These examples are periodically copied over to the <code>examples</code> folder in the <code>langroid-examples</code> repo.</p> <p>The <code>langroid-examples</code> repo contains several examples of using the Langroid agent-oriented programming  framework for LLM applications. Below is a guide to the examples. First please ensure you follow the installation instructions in the <code>langroid-examples</code> repo README.</p> <p>At minimum a GPT4-compatible OpenAI API key is required. As currently set up, many of the examples will not work with a weaker model. Weaker models may require more detailed or different prompting, and possibly a more iterative approach with multiple agents to verify and retry, etc \u2014 this is on our roadmap.</p> <p>All the example scripts are meant to be run on the command line. In each script there is a description and sometimes instructions on how to run the script.</p> <p>NOTE: When you run any script, it pauses for \u201chuman\u201d input at every step, and depending on the context, you can either hit enter to continue, or in case there is a question/response expected from the human, you can enter your question or response and then hit enter.</p>"},{"location":"examples/guide/#basic-examples","title":"Basic Examples","text":"<ul> <li> <p><code>/examples/basic/chat.py</code> This is a basic chat application.</p> <ul> <li>Illustrates Agent task loop.</li> </ul> </li> <li> <p><code>/examples/basic/autocorrect.py</code> Chat with autocorrect: type fast and carelessly/lazily and  the LLM will try its best to interpret what you want, and offer choices when confused.</p> <ul> <li>Illustrates Agent task loop.</li> </ul> </li> <li> <p><code>/examples/basic/chat-search.py</code>  This uses a <code>GoogleSearchTool</code> function-call/tool to answer questions using a google web search if needed.   Try asking questions about facts known after Sep 2021 (GPT4 training cutoff),   like  <code>when was llama2 released</code></p> <ul> <li>Illustrates Agent + Tools/function-calling + web-search</li> </ul> </li> <li> <p><code>/examples/basic/chat-tree.py</code> is a toy example of tree-structured multi-agent   computation, see a detailed writeup here.</p> <ul> <li>Illustrates multi-agent task collaboration, task delegation.</li> </ul> </li> </ul>"},{"location":"examples/guide/#document-chat-examples-or-rag-retrieval-augmented-generation","title":"Document-chat examples, or RAG (Retrieval Augmented Generation)","text":"<ul> <li><code>/examples/docqa/chat.py</code> is a document-chat application. Point it to local file,   directory or web url, and ask questions<ul> <li>Illustrates basic RAG</li> </ul> </li> <li><code>/examples/docqa/chat-search.py</code>: ask about anything and it will try to answer   based on docs indexed in vector-db, otherwise it will do a Google search, and   index the results in the vec-db for this and later answers.<ul> <li>Illustrates RAG + Function-calling/tools</li> </ul> </li> <li><code>/examples/docqa/chat_multi.py</code>:  \u2014 this is a 2-agent system that will summarize   a large document with 5 bullet points: the first agent generates questions for   the retrieval agent, and is done when it gathers 5 key points.<ul> <li>Illustrates 2-agent collaboration + RAG to summarize a document</li> </ul> </li> <li><code>/examples/docqa/chat_multi_extract.py</code>:  \u2014 extracts structured info from a   lease document: Main agent asks questions to a retrieval agent. <ul> <li>Illustrates 2-agent collaboration, RAG, Function-calling/tools, Structured Information Extraction.</li> </ul> </li> </ul>"},{"location":"examples/guide/#data-chat-examples-tabular-sql","title":"Data-chat examples (tabular, SQL)","text":"<ul> <li><code>/examples/data-qa/table_chat.py</code>:  - point to a URL or local csv file and ask   questions. The agent generates pandas code that is run within langroid.<ul> <li>Illustrates function-calling/tools and code-generation</li> </ul> </li> <li><code>/examples/data-qa/sql-chat/sql_chat.py</code>:  \u2014 chat with a sql db \u2014 ask questions in   English, it will generate sql code to answer them.   See tutorial here<ul> <li>Illustrates function-calling/tools and code-generation</li> </ul> </li> </ul>"},{"location":"notes/async-streaming/","title":"Suppressing output in async, streaming mode","text":"<p>Available since version 0.18.0</p> <p>When using an LLM API in streaming + async mode, you may want to suppress output, especially when concurrently running multiple instances of the API. To suppress output in async + stream mode,  you can set the <code>async_stream_quiet</code> flag in <code>LLMConfig</code> to <code>True</code> (this is the default).  Note that <code>OpenAIGPTConfig</code> inherits from <code>LLMConfig</code>, so you can use this flag with <code>OpenAIGPTConfig</code> as well:</p> <pre><code>import langroid.language_models as lm\nllm_config = lm.OpenAIGPTConfig(\n    async_stream_quiet=True,\n    ...\n)\n</code></pre>"},{"location":"notes/azure-openai-models/","title":"Azure OpenAI Models","text":"<p>To use OpenAI models deployed on Azure, first ensure a few environment variables are defined (either in your <code>.env</code> file or in your environment):</p> <ul> <li><code>AZURE_OPENAI_API_KEY</code>, from the value of <code>API_KEY</code></li> <li><code>AZURE_OPENAI_API_BASE</code> from the value of <code>ENDPOINT</code>, typically looks like <code>https://your_resource.openai.azure.com</code>.</li> <li>For <code>AZURE_OPENAI_API_VERSION</code>, you can use the default value in <code>.env-template</code>, and latest version can be found here</li> <li><code>AZURE_OPENAI_DEPLOYMENT_NAME</code> is an OPTIONAL deployment name which may be   defined by the user during the model setup.</li> <li><code>AZURE_OPENAI_CHAT_MODEL</code> Azure OpenAI allows specific model names when you select the model for your deployment. You need to put precisely the exact model name that was selected. For example, GPT-3.5 (should be <code>gpt-35-turbo-16k</code> or <code>gpt-35-turbo</code>) or GPT-4 (should be <code>gpt-4-32k</code> or <code>gpt-4</code>).</li> <li><code>AZURE_OPENAI_MODEL_NAME</code> (Deprecated, use <code>AZURE_OPENAI_CHAT_MODEL</code> instead).</li> </ul> <p>This page Microsoft Azure OpenAI  provides more information on how to obtain these values.</p> <p>To use an Azure-deployed model in Langroid, you can use the <code>AzureConfig</code> class:</p> <pre><code>import langroid.language_models as lm\nimport langroid as lr\n\nllm_config = lm.AzureConfig(\n    chat_model=\"gpt-4o\"\n    # the other settings can be provided explicitly here, \n    # or are obtained from the environment\n)\nllm = lm.AzureGPT(config=llm_config)\n\nresponse = llm.chat(\n  messages=[\n    lm.LLMMessage(role=lm.Role.SYSTEM, content=\"You are a helpful assistant.\"),\n    lm.LLMMessage(role=lm.Role.USER, content=\"3+4=?\"),\n  ]\n)\n\nagent = lr.ChatAgent(\n    lr.ChatAgentConfig(\n        llm=llm_config,\n        system_message=\"You are a helpful assistant.\",\n    )\n)\n\nresponse = agent.llm_response(\"is 4 odd?\")\nprint(response.content)  # \"Yes, 4 is an even number.\"\nresponse = agent.llm_response(\"what about 2?\")  # follow-up question\n</code></pre>"},{"location":"notes/azure-openai-models/#using-azure-openai-api-v1-with-standard-openai-clients","title":"Using Azure OpenAI API v1 with Standard OpenAI Clients","text":"<p>Azure's October 2025 API update allows using standard OpenAI clients instead of Azure-specific ones. However, Azure deployment names often differ from actual model identifiers, which can cause issues with model capability detection.</p> <p>If your deployment name differs from the actual model name, use <code>chat_model_orig</code> to specify the actual model for proper capability detection:</p> <pre><code>import langroid.language_models as lm\n\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"my-gpt4o-deployment\",     # Your Azure deployment name\n    chat_model_orig=\"gpt-4o\",             # Actual model name for capability detection\n    api_base=\"https://your-resource.openai.azure.com/\",\n)\n</code></pre> <p>This ensures Langroid correctly identifies model capabilities (context length, supported features, etc.) even when the deployment name doesn't match the underlying model.</p>"},{"location":"notes/chunking/","title":"Document Chunking/Splitting in Langroid","text":"<p>Langroid's <code>ParsingConfig</code> provides several document chunking strategies through the <code>Splitter</code> enum:</p>"},{"location":"notes/chunking/#1-markdown-splittermarkdown-the-default","title":"1. MARKDOWN (<code>Splitter.MARKDOWN</code>) (The default)","text":"<p>Purpose: Structure-aware splitting that preserves markdown formatting.</p> <p>How it works:</p> <ul> <li>Preserves document hierarchy (headers and sections)</li> <li>Enriches chunks with header information</li> <li>Uses word count instead of token count (with adjustment factor)</li> <li>Supports \"rollup\" to maintain document structure</li> <li>Ideal for markdown documents where preserving formatting is important</li> </ul>"},{"location":"notes/chunking/#2-tokens-splittertokens","title":"2. TOKENS (<code>Splitter.TOKENS</code>)","text":"<p>Purpose: Creates chunks of approximately equal token size.</p> <p>How it works:</p> <ul> <li>Tokenizes the text using tiktoken</li> <li>Aims for chunks of size <code>chunk_size</code> tokens (default: 200)</li> <li>Looks for natural breakpoints like punctuation or newlines</li> <li>Prefers splitting at sentence/paragraph boundaries</li> <li>Ensures chunks are at least <code>min_chunk_chars</code> long (default: 350)</li> </ul>"},{"location":"notes/chunking/#3-para_sentence-splitterpara_sentence","title":"3. PARA_SENTENCE (<code>Splitter.PARA_SENTENCE</code>)","text":"<p>Purpose: Splits documents respecting paragraph and sentence boundaries.</p> <p>How it works:</p> <ul> <li>Recursively splits documents until chunks are below 1.3\u00d7 the target size</li> <li>Maintains document structure by preserving natural paragraph breaks</li> <li>Adjusts chunk boundaries to avoid cutting in the middle of sentences</li> <li>Stops when it can't split chunks further without breaking coherence</li> </ul>"},{"location":"notes/chunking/#4-simple-splittersimple","title":"4. SIMPLE (<code>Splitter.SIMPLE</code>)","text":"<p>Purpose: Basic splitting using predefined separators.</p> <p>How it works:</p> <ul> <li>Uses a list of separators to split text (default: <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>)</li> <li>Splits on the first separator in the list</li> <li>Doesn't attempt to balance chunk sizes</li> <li>Simplest and fastest splitting method</li> </ul>"},{"location":"notes/chunking/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from langroid.parsing.parser import ParsingConfig, Splitter\n\nconfig = ParsingConfig(\n    splitter=Splitter.MARKDOWN,  # Most feature-rich option\n    chunk_size=200,              # Target tokens per chunk\n    chunk_size_variation=0.30,   # Allowed variation from target\n    overlap=50,                  # Token overlap between chunks\n    token_encoding_model=\"text-embedding-3-small\"\n)\n</code></pre>"},{"location":"notes/chunking/#format-specific-configuration","title":"Format-Specific Configuration","text":"<pre><code># Customize PDF parsing\nconfig = ParsingConfig(\n    splitter=Splitter.PARA_SENTENCE,\n    pdf=PdfParsingConfig(\n        library=\"pymupdf4llm\"  # Default PDF parser\n    )\n)\n\n# Use Gemini for PDF parsing\nconfig = ParsingConfig(\n    pdf=PdfParsingConfig(\n        library=\"gemini\",\n        gemini_config=GeminiConfig(\n            model_name=\"gemini-2.0-flash\",\n            requests_per_minute=5\n        )\n    )\n)\n</code></pre>"},{"location":"notes/chunking/#setting-up-parsing-config-in-docchatagentconfig","title":"Setting Up Parsing Config in DocChatAgentConfig","text":"<p>You can configure document parsing when creating a <code>DocChatAgent</code> by customizing the <code>parsing</code> field within the <code>DocChatAgentConfig</code>. Here's how to do it:</p> <pre><code>from langroid.agent.special.doc_chat_agent import DocChatAgentConfig  \nfrom langroid.parsing.parser import ParsingConfig, Splitter, PdfParsingConfig\n\n# Create a DocChatAgent with custom parsing configuration\nagent_config = DocChatAgentConfig(\n    parsing=ParsingConfig(\n        # Choose the splitting strategy\n        splitter=Splitter.MARKDOWN,  # Structure-aware splitting with header context\n\n        # Configure chunk sizes\n        chunk_size=800,              # Target tokens per chunk\n        overlap=150,                 # Overlap between chunks\n\n        # Configure chunk behavior\n        max_chunks=5000,             # Maximum number of chunks to create\n        min_chunk_chars=250,         # Minimum characters when truncating at punctuation\n        discard_chunk_chars=10,      # Discard chunks smaller than this\n\n        # Configure context window\n        n_neighbor_ids=3,            # Store 3 chunk IDs on either side\n\n        # Configure PDF parsing specifically\n        pdf=PdfParsingConfig(\n            library=\"pymupdf4llm\",   # Choose PDF parsing library\n        )\n    )\n)\n</code></pre>"},{"location":"notes/code-injection-protection/","title":"Code Injection Protection with full_eval Flag","text":"<p>Available in Langroid since v0.53.15.</p> <p>Langroid provides a security feature that helps protect against code injection vulnerabilities when evaluating pandas expressions in <code>TableChatAgent</code> and <code>VectorStore</code>. This protection is controlled by the <code>full_eval</code> flag, which defaults to <code>False</code> for maximum security, but can be set to <code>True</code> when working in trusted environments.</p>"},{"location":"notes/code-injection-protection/#background","title":"Background","text":"<p>When executing dynamic pandas expressions within <code>TableChatAgent</code> and in <code>VectorStore.compute_from_docs()</code>, there is a risk of code injection if malicious input is provided. To mitigate this risk, Langroid implements a command sanitization system that validates and restricts the operations that can be performed.</p>"},{"location":"notes/code-injection-protection/#how-it-works","title":"How It Works","text":"<p>The sanitization system uses AST (Abstract Syntax Tree) analysis to enforce a security policy that:</p> <ol> <li>Restricts DataFrame methods to a safe whitelist</li> <li>Prevents access to potentially dangerous methods and arguments</li> <li>Limits expression depth and method chaining</li> <li>Validates literals and numeric values to be within safe bounds</li> <li>Blocks access to any variables other than the provided DataFrame</li> </ol> <p>When <code>full_eval=False</code> (the default), all expressions are run through this sanitization process before evaluation. When <code>full_eval=True</code>, the sanitization is bypassed, allowing full access to pandas functionality.</p>"},{"location":"notes/code-injection-protection/#configuration-options","title":"Configuration Options","text":""},{"location":"notes/code-injection-protection/#in-tablechatagent","title":"In TableChatAgent","text":"<pre><code>from langroid.agent.special.table_chat_agent import TableChatAgentConfig, TableChatAgent\n\nconfig = TableChatAgentConfig(\n    data=my_dataframe,\n    full_eval=False,  # Default: True only for trusted input\n)\n\nagent = TableChatAgent(config)\n</code></pre>"},{"location":"notes/code-injection-protection/#in-vectorstore","title":"In VectorStore","text":"<pre><code>from langroid.vector_store.lancedb import LanceDBConfig, LanceDB\n\nconfig = LanceDBConfig(\n    collection_name=\"my_collection\",\n    full_eval=False,  # Default: True only for trusted input\n)\n\nvectorstore = LanceDB(config)\n</code></pre>"},{"location":"notes/code-injection-protection/#when-to-use-full_evaltrue","title":"When to Use full_eval=True","text":"<p>Set <code>full_eval=True</code> only when:</p> <ol> <li>All input comes from trusted sources (not from users or external systems)</li> <li>You need full pandas functionality that goes beyond the whitelisted methods</li> <li>You're working in a controlled development or testing environment</li> </ol>"},{"location":"notes/code-injection-protection/#security-considerations","title":"Security Considerations","text":"<ul> <li>By default, <code>full_eval=False</code> provides a good balance of security and functionality</li> <li>The whitelisted operations support most common pandas operations</li> <li>Setting <code>full_eval=True</code> removes all protection and should be used with caution</li> <li>Even with protection, always validate input when possible</li> </ul>"},{"location":"notes/code-injection-protection/#affected-classes","title":"Affected Classes","text":"<p>The <code>full_eval</code> flag affects the following components:</p> <ol> <li><code>TableChatAgentConfig</code> and <code>TableChatAgent</code> - Controls sanitization in the <code>pandas_eval</code> method</li> <li><code>VectorStoreConfig</code> and <code>VectorStore</code> - Controls sanitization in the <code>compute_from_docs</code> method</li> <li>All implementations of <code>VectorStore</code> (ChromaDB, LanceDB, MeiliSearch, PineconeDB, PostgresDB, QdrantDB, WeaviateDB)</li> </ol>"},{"location":"notes/code-injection-protection/#example-safe-pandas-operations","title":"Example: Safe Pandas Operations","text":"<p>When <code>full_eval=False</code>, the following operations are allowed:</p> <pre><code># Allowed operations (non-exhaustive list)\ndf.head()\ndf.groupby('column')['value'].mean()\ndf[df['column'] &gt; 10]\ndf.sort_values('column', ascending=False)\ndf.pivot_table(...)\n</code></pre> <p>Some operations that might be blocked include:</p> <pre><code># Potentially blocked operations\ndf.eval(\"dangerous_expression\")\ndf.query(\"dangerous_query\")\ndf.apply(lambda x: dangerous_function(x))\n</code></pre>"},{"location":"notes/code-injection-protection/#testing-considerations","title":"Testing Considerations","text":"<p>When writing tests that use <code>TableChatAgent</code> or <code>VectorStore.compute_from_docs()</code> with pandas expressions that go beyond the whitelisted operations, you may need to set <code>full_eval=True</code> to ensure the tests pass.</p>"},{"location":"notes/crawl4ai/","title":"Crawl4ai Crawler Documentation","text":""},{"location":"notes/crawl4ai/#overview","title":"Overview","text":"<p>The <code>Crawl4aiCrawler</code> is a highly advanced and flexible web crawler integrated into Langroid, built on the powerful <code>crawl4ai</code> library. It uses a real browser engine (Playwright) to render web pages, making it exceptionally effective at handling modern, JavaScript-heavy websites. This crawler provides a rich set of features for simple page scraping, deep-site crawling, and sophisticated data extraction, making it the most powerful crawling option available in Langroid.</p> <p>It is a local crawler, so no need for API keys.</p>"},{"location":"notes/crawl4ai/#installation","title":"Installation","text":"<p>To use <code>Crawl4aiCrawler</code>, you must install the <code>crawl4ai</code> extra dependencies.</p> <p>To install and prepare crawl4ai:</p> <pre><code># Install langroid with crawl4ai support\npip install \"langroid[crawl4ai]\"\ncrawl4ai setup\ncrawl4ai doctor\n</code></pre> <p>Note: The <code>crawl4ai setup</code> command will download Playwright browsers (Chromium, Firefox, WebKit) on first run. This is a one-time download that can be several hundred MB in size. The browsers are stored locally and used for rendering web pages.</p>"},{"location":"notes/crawl4ai/#key-features","title":"Key Features","text":"<ul> <li> <p>Real Browser Rendering: Accurately processes dynamic content, single-page applications (SPAs), and sites that require JavaScript execution.</p> </li> <li> <p>Simple and Deep Crawling: Can scrape a list of individual URLs (<code>simple</code> mode) or perform a recursive, deep crawl of a website starting from a seed URL (<code>deep</code> mode).</p> </li> <li> <p>Powerful Extraction Strategies:</p> </li> <li> <p>Structured JSON (No LLM): Extract data into a predefined JSON structure using CSS selectors, XPath, or Regex patterns. This is extremely fast, reliable, and cost-effective.</p> </li> <li> <p>LLM-Based Extraction: Leverage Large Language Models (like GPT or Gemini) to extract data from unstructured content based on natural language instructions and a Pydantic schema.</p> </li> <li> <p>Advanced Markdown Generation: Go beyond basic HTML-to-markdown conversion. Apply content filters to prune irrelevant sections (sidebars, ads, footers) or use an LLM to intelligently reformat content for maximum relevance, perfect for RAG pipelines.</p> </li> <li> <p>High-Performance Scraping: Optionally use an LXML-based scraping strategy for a significant speed boost on large HTML documents.</p> </li> <li> <p>Fine-Grained Configuration: Offers detailed control over browser behavior (<code>BrowserConfig</code>) and individual crawl runs (<code>CrawlerRunConfig</code>) for advanced use cases.</p> </li> </ul>"},{"location":"notes/crawl4ai/#configuration-crawl4aiconfig","title":"Configuration (<code>Crawl4aiConfig</code>)","text":"<p>The <code>Crawl4aiCrawler</code> is configured via the <code>Crawl4aiConfig</code> object. This class acts as a high-level interface to the underlying <code>crawl4ai</code> library's settings.</p> <p>All of the strategies are optional. Learn more about these strategies , browser_config and run_config at Crawl4AI docs</p> <pre><code>from langroid.parsing.url_loader import Crawl4aiConfig\n\n# All parameters are optional and have sensible defaults\nconfig = Crawl4aiConfig(\n    crawl_mode=\"simple\",  # or \"deep\"\n    extraction_strategy=...,\n    markdown_strategy=...,\n    deep_crawl_strategy=...,\n    scraping_strategy=...,\n    browser_config=...,  # For advanced browser settings\n    run_config=...,      # For advanced crawl-run settings\n)\n</code></pre> <p>Main Parameters:</p> <ul> <li> <p><code>crawl_mode</code> (str):</p> </li> <li> <p><code>\"simple\"</code> (default): Crawls each URL in the provided list individually.</p> </li> <li> <p><code>\"deep\"</code>: Starts from the first URL in the list and recursively crawls linked pages based on the <code>deep_crawl_strategy</code>.</p> </li> <li> <p>Make sure you are setting <code>\"crawl_mode=deep\"</code> whenever you are deep crawling this is crucial for smooth functioning.</p> </li> <li> <p><code>extraction_strategy</code> (<code>ExtractionStrategy</code>): Defines how to extract structured data from a page. If set, the <code>Document.content</code> will be a JSON string containing the extracted data.</p> </li> <li> <p><code>markdown_strategy</code> (<code>MarkdownGenerationStrategy</code>): Defines how to convert HTML to markdown. This is used when <code>extraction_strategy</code> is not set. The <code>Document.content</code> will be a markdown string.</p> </li> <li> <p><code>deep_crawl_strategy</code> (<code>DeepCrawlStrategy</code>): Configuration for deep crawling, such as <code>max_depth</code>, <code>max_pages</code>, and URL filters. Only used when <code>crawl_mode</code> is <code>\"deep\"</code>.</p> </li> <li> <p><code>scraping_strategy</code> (<code>ContentScrapingStrategy</code>): Specifies the underlying HTML parsing engine. Useful for performance tuning.</p> </li> <li> <p><code>browser_config</code> &amp; <code>run_config</code>: For advanced users to pass detailed <code>BrowserConfig</code> and <code>CrawlerRunConfig</code> objects directly from the <code>crawl4ai</code> library.</p> </li> </ul>"},{"location":"notes/crawl4ai/#usage-examples","title":"Usage Examples","text":"<p>These are representative examples. For runnable examples check the script <code>examples/docqa/crawl4ai_examples.py</code></p>"},{"location":"notes/crawl4ai/#1-simple-crawling-default-markdown","title":"1. Simple Crawling (Default Markdown)","text":"<p>This is the most basic usage. It will fetch the content of each URL and convert it to clean markdown.</p> <pre><code>from langroid.parsing.url_loader import URLLoader, Crawl4aiConfig\n\nurls = [\n    \"https://pytorch.org/\",\n    \"https://techcrunch.com/\",\n]\n\n# Use default settings\ncrawler_config = Crawl4aiConfig()\nloader = URLLoader(urls=urls, crawler_config=crawler_config)\n\ndocs = loader.load()\nfor doc in docs:\n    print(f\"URL: {doc.metadata.source}\")\n    print(f\"Content (first 200 chars): {doc.content[:200]}\")\n</code></pre>"},{"location":"notes/crawl4ai/#2-structured-json-extraction-no-llm","title":"2. Structured JSON Extraction (No LLM)","text":"<p>When you need to extract specific, repeated data fields from a page, schema-based extraction is the best choice. It's fast, precise, and free of LLM costs. The result in <code>Document.content</code> is a JSON string.</p>"},{"location":"notes/crawl4ai/#a-using-css-selectors-jsoncssextractionstrategy","title":"a. Using CSS Selectors (<code>JsonCssExtractionStrategy</code>)","text":"<p>This example scrapes titles and links from the Hacker News front page.</p> <pre><code>import json\nfrom langroid.parsing.url_loader import URLLoader, Crawl4aiConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nHACKER_NEWS_URL = \"https://news.ycombinator.com\"\nHACKER_NEWS_SCHEMA = {\n    \"name\": \"HackerNewsArticles\",\n    \"baseSelector\": \"tr.athing\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"span.titleline &gt; a\", \"type\": \"text\"},\n        {\"name\": \"link\", \"selector\": \"span.titleline &gt; a\", \"type\": \"attribute\", \"attribute\": \"href\"},\n    ],\n}\n\n# Create the strategy and pass it to the config\ncss_strategy = JsonCssExtractionStrategy(schema=HACKER_NEWS_SCHEMA)\ncrawler_config = Crawl4aiConfig(extraction_strategy=css_strategy)\n\nloader = URLLoader(urls=[HACKER_NEWS_URL], crawler_config=crawler_config)\ndocuments = loader.load()\n\n# The Document.content will contain the JSON string\nextracted_data = json.loads(documents[0].content)\nprint(json.dumps(extracted_data[:3], indent=2))\n</code></pre>"},{"location":"notes/crawl4ai/#b-using-regex-regexextractionstrategy","title":"b. Using Regex (<code>RegexExtractionStrategy</code>)","text":"<p>This is ideal for finding common patterns like emails, URLs, or phone numbers.</p> <pre><code>from langroid.parsing.url_loader import URLLoader, Crawl4aiConfig\nfrom crawl4ai.extraction_strategy import RegexExtractionStrategy\n\nurl = \"https://www.scrapethissite.com/pages/forms/\"\n\n# Combine multiple built-in patterns\nregex_strategy = RegexExtractionStrategy(\n    pattern=(\n        RegexExtractionStrategy.Email\n        | RegexExtractionStrategy.Url\n        | RegexExtractionStrategy.PhoneUS\n    )\n)\n\ncrawler_config = Crawl4aiConfig(extraction_strategy=regex_strategy)\nloader = URLLoader(urls=[url], crawler_config=crawler_config)\ndocuments = loader.load()\n\nprint(documents[0].content)\n</code></pre>"},{"location":"notes/crawl4ai/#3-advanced-markdown-generation","title":"3. Advanced Markdown Generation","text":"<p>For RAG applications, the quality of the markdown is crucial. These strategies produce highly relevant, clean text. The result in <code>Document.content</code> is the filtered markdown (<code>fit_markdown</code>).</p>"},{"location":"notes/crawl4ai/#a-pruning-filter-pruningcontentfilter","title":"a. Pruning Filter (<code>PruningContentFilter</code>)","text":"<p>This filter heuristically removes boilerplate content based on text density, link density, and common noisy tags.</p> <pre><code>from langroid.parsing.url_loader import URLLoader, Crawl4aiConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nprune_filter = PruningContentFilter(threshold=0.6, min_word_threshold=10)\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=prune_filter,\n    options={\"ignore_links\": True}\n)\n\ncrawler_config = Crawl4aiConfig(markdown_strategy=md_generator)\nloader = URLLoader(urls=[\"https://news.ycombinator.com\"], crawler_config=crawler_config)\ndocs = loader.load()\n\nprint(docs[0].content[:500])\n</code></pre>"},{"location":"notes/crawl4ai/#b-llm-filter-llmcontentfilter","title":"b. LLM Filter (<code>LLMContentFilter</code>)","text":"<p>Use an LLM to semantically understand the content and extract only the relevant parts based on your instructions. This is extremely powerful for creating topic-focused documents.</p> <pre><code>import os\nfrom langroid.parsing.url_loader import URLLoader, Crawl4aiConfig\nfrom crawl4ai.async_configs import LLMConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\n\n# Requires an API key, e.g., OPENAI_API_KEY\nllm_filter = LLMContentFilter(\n    llm_config=LLMConfig(\n        provider=\"openai/gpt-4o-mini\",\n        api_token=os.getenv(\"OPENAI_API_KEY\"),\n    ),\n    instruction=\"\"\"\n    Extract only the main article content.\n    Exclude all navigation, sidebars, comments, and footer content.\n    Format the output as clean, readable markdown.\n    \"\"\",\n    chunk_token_threshold=4096,\n)\n\nmd_generator = DefaultMarkdownGenerator(content_filter=llm_filter)\ncrawler_config = Crawl4aiConfig(markdown_strategy=md_generator)\nloader = URLLoader(urls=[\"https://www.theverge.com/tech\"], crawler_config=crawler_config)\ndocs = loader.load()\n\nprint(docs[0].content)\n</code></pre>"},{"location":"notes/crawl4ai/#4-deep-crawling","title":"4. Deep Crawling","text":"<p>To crawl an entire website or a specific section, use <code>deep</code> mode.</p> <p>Recommended setting is BestFirstCrawlingStrategy</p> <pre><code>from langroid.parsing.url_loader import URLLoader, Crawl4aiConfig\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter\n\n\ndeep_crawl_strategy = BestFirstCrawlingStrategy(\n    max_depth=2,\n    include_external=False,\n    max_pages=25,              # Maximum number of pages to crawl (optional)\n    filter_chain=FilterChain([URLPatternFilter(patterns=[\"*core*\"])]) # Pattern matching for granular control (optional)\n)\n\ncrawler_config = Crawl4aiConfig(\n    crawl_mode=\"deep\",\n    deep_crawl_strategy=deep_crawl_strategy\n)\n\nloader = URLLoader(urls=[\"https://docs.crawl4ai.com/\"], crawler_config=crawler_config)\ndocs = loader.load()\n\nprint(f\"Crawled {len(docs)} pages.\")\nfor doc in docs:\n    print(f\"- {doc.metadata.source}\")\n</code></pre>"},{"location":"notes/crawl4ai/#5-high-performance-scraping-lxmlwebscrapingstrategy","title":"5. High-Performance Scraping (<code>LXMLWebScrapingStrategy</code>)","text":"<p>For a performance boost, especially on very large, static HTML pages, switch the scraping strategy to LXML.</p> <pre><code>from langroid.parsing.url_loader import URLLoader, Crawl4aiConfig\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n\ncrawler_config = Crawl4aiConfig(\n    scraping_strategy=LXMLWebScrapingStrategy()\n)\n\nloader = URLLoader(urls=[\"https://www.nbcnews.com/business\"], crawler_config=crawler_config)\ndocs = loader.load()\nprint(f\"Content Length: {len(docs[0].content)}\")\n</code></pre>"},{"location":"notes/crawl4ai/#6-llm-based-json-extraction-llmextractionstrategy","title":"6. LLM-Based JSON Extraction (<code>LLMExtractionStrategy</code>)","text":"<p>When data is unstructured or requires semantic interpretation, use an LLM for extraction. This is slower and more expensive but incredibly flexible. The result in <code>Document.content</code> is a JSON string.</p> <pre><code>import os\nimport json\nfrom langroid.pydantic_v1 import BaseModel, Field\nfrom typing import Optional\nfrom langroid.parsing.url_loader import URLLoader, Crawl4aiConfig\nfrom crawl4ai.async_configs import LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\n# Define the data structure you want to extract\nclass ArticleData(BaseModel):\n    headline: str\n    summary: str = Field(description=\"A short summary of the article\")\n    author: Optional[str] = None\n\n# Configure the LLM strategy\nllm_strategy = LLMExtractionStrategy(\n    llm_config=LLMConfig(\n        provider=\"openai/gpt-4o-mini\",\n        api_token=os.getenv(\"OPENAI_API_KEY\"),\n    ),\n    schema=ArticleData.schema_json(),\n    extraction_type=\"schema\",\n    instruction=\"Extract the headline, summary, and author of the main article.\",\n)\n\ncrawler_config = Crawl4aiConfig(extraction_strategy=llm_strategy)\nloader = URLLoader(urls=[\"https://news.ycombinator.com\"], crawler_config=crawler_config)\ndocs = loader.load()\n\nextracted_data = json.loads(docs[0].content)\nprint(json.dumps(extracted_data, indent=2))\n</code></pre>"},{"location":"notes/crawl4ai/#how-it-handles-different-content-types","title":"How It Handles Different Content Types","text":"<p>The <code>Crawl4aiCrawler</code> is smart about handling different types of URLs:</p> <ul> <li>Web Pages (e.g., <code>http://...</code>, <code>https://...</code>): These are processed by the <code>crawl4ai</code> browser engine. The output format (<code>markdown</code> or <code>JSON</code>) depends on the strategy you configure in <code>Crawl4aiConfig</code>.</li> <li>Local and Remote Documents (e.g., URLs ending in <code>.pdf</code>, <code>.docx</code>): These are automatically detected and delegated to Langroid's internal <code>DocumentParser</code>. This ensures that documents are properly parsed and chunked according to your <code>ParsingConfig</code>, just like with other Langroid tools.</li> </ul>"},{"location":"notes/crawl4ai/#conclusion","title":"Conclusion","text":"<p>The <code>Crawl4aiCrawler</code> is a feature-rich, powerful tool for any web-based data extraction task.</p> <ul> <li> <p>For simple, clean text, use the default <code>Crawl4aiConfig</code>.</p> </li> <li> <p>For structured data from consistent sites, use <code>JsonCssExtractionStrategy</code> or <code>RegexExtractionStrategy</code> for unbeatable speed and reliability.</p> </li> <li> <p>To create high-quality, focused content for RAG, use <code>PruningContentFilter</code> or the <code>LLMContentFilter</code> with the <code>DefaultMarkdownGenerator</code>.</p> </li> <li> <p>To scrape an entire website, use <code>deep_crawl_strategy</code> with <code>crawl_mode=\"deep\"</code>.</p> </li> <li> <p>For complex or unstructured data that needs AI interpretation, <code>LLMExtractionStrategy</code> provides a flexible solution.</p> </li> </ul>"},{"location":"notes/custom-azure-client/","title":"Custom Azure OpenAI client","text":"<p>This is only for using a Custom Azure OpenAI client</p> <p>This note only meant for those who are trying to use a custom Azure client, and is NOT TYPICAL for most users. For typical usage of Azure-deployed models with Langroid, see the docs,  the <code>test_azure_openai.py</code> and <code>example/basic/chat.py</code></p> <p>Example showing how to use Langroid with Azure OpenAI and Entra ID authentication by providing a custom client.</p> <p>By default, Langroid manages the configuration and creation  of the Azure OpenAI client (see the Setup guide for details). In most cases, the available configuration options are sufficient, but if you need to manage any options that are not exposed, you instead have the option of providing a custom client, in Langroid v0.29.0 and later. </p> <p>In order to use a custom client, you must provide a function that returns the configured client. Depending on whether you need to make synchronous or asynchronous calls, you need to provide the appropriate client. A sketch of how this is done (supporting both sync and async calls) is given below:</p> <pre><code>def get_azure_openai_client():\n    return AzureOpenAI(...)\n\ndef get_azure_openai_async_client():\n    return AsyncAzureOpenAI(...)\n\nlm_config = lm.AzureConfig(\n    azure_openai_client_provider=get_azure_openai_client,\n    azure_openai_async_client_provider=get_azure_openai_async_client,\n)\n</code></pre>"},{"location":"notes/custom-azure-client/#microsoft-entra-id-authentication","title":"Microsoft Entra ID Authentication","text":"<p>A key use case for a custom client is Microsoft Entra ID authentication. Here you need to provide an <code>azure_ad_token_provider</code> to the client.  For examples on this, see examples/basic/chat-azure-client.py  and examples/basic/chat-azure-async-client.py.</p>"},{"location":"notes/enriching-for-retrieval/","title":"Enriching Chunked Documents for Better Retrieval","text":"<p>Available in Langroid v0.34.0 or later. </p> <p>When using the <code>DocChatAgent</code> for RAG with documents in highly specialized/technical domains, retrieval accuracy may be low since embeddings are not sufficient to capture  relationships between entities, e.g. suppose a document-chunk consists of a medical  test name \"BUN\" (Blood Urea Nitrogen), and a retrieval query is looking for  tests related to kidney function, the embedding for \"BUN\" may not be close to the embedding for \"kidney function\", and the chunk may not be retrieved.</p> <p>In such cases it is useful to enrich the chunked documents with additional keywords (or even \"hypothetical questions\") to increase the \"semantic surface area\" of the chunk, so that the chunk is more likely to be retrieved for relevant queries.</p> <p>As of Langroid v0.34.0, you can provide a <code>chunk_enrichment_config</code>  of type <code>ChunkEnrichmentAgentConfig</code>, in the <code>DocChatAgentConfig</code>.  This config extends <code>ChatAgentConfig</code> and has the following fields:</p> <ul> <li><code>batch_size</code> (int): The batch size for the chunk enrichment agent. Default is 50.</li> <li><code>delimiter</code> (str): The delimiter to use when     concatenating the chunk and the enriched text. </li> <li><code>enrichment_prompt_fn</code>: function (<code>str-&gt;str</code>) that creates a prompt   from a doc-chunk string <code>x</code></li> </ul> <p>In the above medical test example, suppose we want to augment a chunk containing only the medical test name, with the organ system it is related to. We can set up a <code>ChunkEnrichmentAgentConfig</code> as follows:</p> <pre><code>from langroid.agent.special.doc.doc_chat_agent import (\n    ChunkEnrichmentAgentConfig,\n)\n\nenrichment_config = ChunkEnrichmentAgentConfig(\n    batch_size=10,\n    system_message=f\"\"\"\n        You are an experienced clinical physician, very well-versed in\n        medical tests and their names.\n        You will be asked to identify WHICH ORGAN(s) Function/Health\n        a test name is most closely associated with, to aid in \n        retrieving the medical test names more accurately from an embeddings db\n        that contains thousands of such test names.\n        The idea is to use the ORGAN NAME(S) provided by you, \n        to make the right test names easier to discover via keyword-matching\n        or semantic (embedding) similarity.\n         Your job is to generate up to 3 ORGAN NAMES\n         MOST CLOSELY associated with the test name shown, ONE PER LINE.\n         DO NOT SAY ANYTHING ELSE, and DO NOT BE OBLIGATED to provide 3 organs --\n         if there is just one or two that are most relevant, that is fine.\n        Examples:\n          \"cholesterol\" -&gt; \"heart function\", \n          \"LDL\" -&gt; \"artery health\", etc,\n          \"PSA\" -&gt; \"prostate health\", \n          \"TSH\" -&gt; \"thyroid function\", etc.                \n        \"\"\",\n    enrichment_prompt_fn=lambda test: f\"\"\"\n        Which ORGAN(S) Function/Health is the medical test named \n        '{test}' most closely associated with?\n        \"\"\",\n)\n\ndoc_agent_config = DocChatAgentConfig(\n    chunk_enrichment_config=enrichment_config,\n    ...\n)\n</code></pre> <p>This works as follows:</p> <ul> <li>Before ingesting document-chunks into the vector-db, a specialized    \"chunk enrichment\" agent is created, configured with the <code>enrichment_config</code> above.</li> <li>For each document-chunk <code>x</code>, the agent's <code>llm_response_forget_async</code> method is called  using the prompt created by <code>enrichment_prompt_fn(x)</code>. The resulting response text   <code>y</code> is concatenated with the original chunk text <code>x</code> using the <code>delimiter</code>,   before storing in the vector-db. This is done in batches of size <code>batch_size</code>.</li> <li>At query time, after chunk retrieval, before generating the final LLM response,   the enrichments are stripped from the retrieved chunks, and the original content   of the retrieved chunks are passed to the LLM for generating the final response.</li> </ul> <p>See the script  <code>examples/docqa/doc-chunk-enrich.py</code> for a complete example. Also see the tests related to \"enrichment\" in  <code>test_doc_chat_agent.py</code>.</p>"},{"location":"notes/file-input/","title":"PDF Files and Image inputs to LLMs","text":"<p>Langroid supports sending PDF files and images (either URLs or local files) directly to Large Language Models with multi-modal  capabilities. This feature allows models to \"see\" files and other documents, and works with most multi-modal models served via an OpenAI-compatible API, e.g.:</p> <ul> <li>OpenAI's GPT-4o series and GPT-4.1 series</li> <li>Gemini models</li> <li>Claude series models (via OpenAI-compatible providers like OpenRouter or LiteLLM )</li> </ul> <p>To see example usage, see:</p> <ul> <li>tests: test_llm.py,     test_llm_async.py,    test_chat-agent.py.</li> <li>example script: pdf-json-no-parse.py, which shows   how you can directly extract structured information from a document    without having to first parse it to markdown (which is inherently lossy).</li> </ul>"},{"location":"notes/file-input/#basic-usage-directly-with-llm-chat-and-achat-methods","title":"Basic Usage directly with LLM <code>chat</code> and <code>achat</code> methods","text":"<p>First create a <code>FileAttachment</code> object using one of the <code>from_</code> methods. For image (<code>png</code>, <code>jpg/jpeg</code>) files you can use <code>FileAttachment.from_path(p)</code> where <code>p</code> is either a local file path, or a http/https URL. For PDF files, you can use <code>from_path</code> with a local file, or <code>from_bytes</code> or <code>from_io</code> (see below). In the examples below we show only <code>pdf</code> examples.</p> <pre><code>from langroid.language_models.base import LLMMessage, Role\nfrom langroid.parsing.file_attachment import FileAttachment\nimport langroid.language_models as lm\n\n# Create a file attachment\nattachment = FileAttachment.from_path(\"path/to/document.pdf\")\n\n# Create messages with attachment\nmessages = [\n    LLMMessage(role=Role.SYSTEM, content=\"You are a helpful assistant.\"),\n    LLMMessage(\n        role=Role.USER, content=\"What's the title of this document?\", \n        files=[attachment]\n    )\n]\n\n# Set up LLM with model that supports attachments\nllm = lm.OpenAIGPT(lm.OpenAIGPTConfig(chat_model=lm.OpenAIChatModel.GPT4o))\n\n# Get response\nresponse = llm.chat(messages=messages)\n</code></pre>"},{"location":"notes/file-input/#supported-file-formats","title":"Supported File Formats","text":"<p>Currently the OpenAI-API supports:</p> <ul> <li>PDF files (including image-based PDFs)</li> <li>image files and URLs</li> </ul>"},{"location":"notes/file-input/#creating-attachments","title":"Creating Attachments","text":"<p>There are multiple ways to create file attachments:</p> <pre><code># From a file path\nattachment = FileAttachment.from_path(\"path/to/file.pdf\")\n\n# From bytes\nwith open(\"path/to/file.pdf\", \"rb\") as f:\n    attachment = FileAttachment.from_bytes(f.read(), filename=\"document.pdf\")\n\n# From a file-like object\nfrom io import BytesIO\nfile_obj = BytesIO(pdf_bytes)\nattachment = FileAttachment.from_io(file_obj, filename=\"document.pdf\")\n</code></pre>"},{"location":"notes/file-input/#follow-up-questions","title":"Follow-up Questions","text":"<p>You can continue the conversation with follow-up questions that reference the attached files:</p> <pre><code>messages.append(LLMMessage(role=Role.ASSISTANT, content=response.message))\nmessages.append(LLMMessage(role=Role.USER, content=\"What is the main topic?\"))\nresponse = llm.chat(messages=messages)\n</code></pre>"},{"location":"notes/file-input/#multiple-attachments","title":"Multiple Attachments","text":"<p>Langroid allows multiple files can be sent in a single message, but as of 16 Apr 2025, sending multiple PDF files does not appear to be properly supported in the  APIs (they seem to only use the last file attached), although sending multiple  images does work. </p> <pre><code>messages = [\n    LLMMessage(\n        role=Role.USER,\n        content=\"Compare these documents\",\n        files=[attachment1, attachment2]\n    )\n]\n</code></pre>"},{"location":"notes/file-input/#using-file-attachments-with-agents","title":"Using File Attachments with Agents","text":"<p>Agents can process file attachments as well, in the <code>llm_response</code> method, which takes a <code>ChatDocument</code> object as input.  To pass in file attachments, include the <code>files</code> field in the <code>ChatDocument</code>, in addition to the content:</p> <pre><code>import langroid as lr\nfrom langroid.agent.chat_document import ChatDocument, ChatDocMetaData\nfrom langroid.mytypes import Entity\n\n\nagent = lr.ChatAgent(lr.ChatAgentConfig())\n\nuser_input = ChatDocument(\n    content=\"What is the title of this document?\",\n    files=[attachment],\n    metadata=ChatDocMetaData(\n        sender=Entity.USER,\n    )\n)\n# or more simply, use the agent's `create_user_response` method:\n# user_input = agent.create_user_response(\n#     content=\"What is the title of this document?\",\n#     files=[attachment],    \n# )\nresponse = agent.llm_response(user_input)\n</code></pre>"},{"location":"notes/file-input/#using-file-attachments-with-tasks","title":"Using File Attachments with Tasks","text":"<p>In Langroid,  <code>Task.run()</code> can take a <code>ChatDocument</code> object as input, and as mentioned above, it can contain attached files in the <code>files</code> field. To ensure proper orchestration, you'd want to properly set various <code>metadata</code> fields as well, such as <code>sender</code>, etc. Langroid provides a convenient  <code>create_user_response</code> method to create a <code>ChatDocument</code> object with the necessary  metadata, so you only need to specify the <code>content</code> and <code>files</code> fields:</p> <pre><code>from langroid.parsing.file_attachment import FileAttachment\nfrom langroid.agent.task import Task\n\nagent = ...\n# Create task\ntask = Task(agent, interactive=True)\n\n# Create a file attachment\nattachment = FileAttachment.from_path(\"path/to/document.pdf\")\n\n# Create input with attachment\ninput_message = agent.create_user_response(\n    content=\"Extract data from this document\",\n    files=[attachment]\n)\n\n# Run task with file attachment\nresult = task.run(input_message)\n</code></pre> <p>See the script <code>pdf-json-no-parse.py</code> for a complete example of using file attachments with tasks.</p>"},{"location":"notes/file-input/#practical-applications","title":"Practical Applications","text":"<ul> <li>PDF document analysis and data extraction</li> <li>Report summarization</li> <li>Structured information extraction from documents</li> <li>Visual content analysis</li> </ul> <p>For more complex applications, consider using the Task and Agent infrastructure in  Langroid to orchestrate multi-step document processing workflows.</p>"},{"location":"notes/gemini/","title":"Gemini LLMs &amp; Embeddings via OpenAI client (without LiteLLM)","text":"<p>As of Langroid v0.21.0 you can use Langroid with Gemini LLMs directly via the OpenAI client, without using adapter libraries like LiteLLM.</p> <p>See details here</p> <p>You can use also Google AI Studio Embeddings or Gemini Embeddings directly which uses google-generativeai client under the hood.</p> <pre><code>import langroid as lr\nfrom langroid.agent.special import DocChatAgent, DocChatAgentConfig\nfrom langroid.embedding_models import GeminiEmbeddingsConfig\n\n# Configure Gemini embeddings\nembed_cfg = GeminiEmbeddingsConfig(\n    model_type=\"gemini\",\n    model_name=\"models/text-embedding-004\",\n    dims=768,\n)\n\n# Configure the DocChatAgent\nconfig = DocChatAgentConfig(\n    llm=lr.language_models.OpenAIGPTConfig(\n        chat_model=\"gemini/\" + lr.language_models.GeminiModel.GEMINI_1_5_FLASH_8B,\n    ),\n    vecdb=lr.vector_store.QdrantDBConfig(\n        collection_name=\"quick_start_chat_agent_docs\",\n        replace_collection=True,\n        embedding=embed_cfg,\n    ),\n    parsing=lr.parsing.parser.ParsingConfig(\n        separators=[\"\\n\\n\"],\n        splitter=lr.parsing.parser.Splitter.SIMPLE,\n    ),\n    n_similar_chunks=2,\n    n_relevant_chunks=2,\n)\n\n# Create the agent\nagent = DocChatAgent(config)\n</code></pre>"},{"location":"notes/gemini/#vertex-ai-support","title":"Vertex AI Support","text":"<p>Google Vertex AI uses project-specific URLs for its OpenAI compatibility layer, which differs from the fixed URL used by the standard Google AI (Gemini) API. To use Gemini models through Vertex AI, set the endpoint via the <code>GEMINI_API_BASE</code> environment variable or the <code>api_base</code> parameter in <code>OpenAIGPTConfig</code>.</p> <p>Note</p> <p>The <code>OPENAI_API_BASE</code> environment variable (commonly used for local proxies) is not applied to Gemini models. Use <code>GEMINI_API_BASE</code> or an explicit <code>api_base</code> in the config instead.</p>"},{"location":"notes/gemini/#setup","title":"Setup","text":"<ol> <li> <p>Set up authentication. Vertex AI typically uses Google Cloud credentials    rather than a simple API key. You can generate a short-lived access token:</p> <pre><code>export GEMINI_API_KEY=$(gcloud auth print-access-token)\n</code></pre> </li> <li> <p>Set your Vertex AI endpoint URL, which includes your GCP project ID    and region:</p> <pre><code>export GEMINI_API_BASE=https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi\n</code></pre> </li> </ol>"},{"location":"notes/gemini/#usage","title":"Usage","text":"<p>Option 1: Environment variable (recommended for Vertex AI)</p> <pre><code>export GEMINI_API_KEY=$(gcloud auth print-access-token)\nexport GEMINI_API_BASE=https://us-central1-aiplatform.googleapis.com/v1beta1/projects/my-gcp-project/locations/us-central1/endpoints/openapi\n</code></pre> <pre><code>import langroid.language_models as lm\n\n# GEMINI_API_BASE is picked up automatically\nconfig = lm.OpenAIGPTConfig(chat_model=\"gemini/gemini-2.0-flash\")\nllm = lm.OpenAIGPT(config)\nresponse = llm.chat(\"Hello from Vertex AI!\")\n</code></pre> <p>Option 2: Explicit <code>api_base</code> in config</p> <pre><code>import langroid.language_models as lm\n\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"gemini/gemini-2.0-flash\",\n    api_base=(\n        \"https://us-central1-aiplatform.googleapis.com/v1beta1\"\n        \"/projects/my-gcp-project/locations/us-central1/endpoints/openapi\"\n    ),\n)\nllm = lm.OpenAIGPT(config)\nresponse = llm.chat(\"Hello from Vertex AI!\")\n</code></pre> <p>When neither <code>GEMINI_API_BASE</code> nor an explicit <code>api_base</code> is set, Langroid falls back to the default Google AI (Gemini) endpoint (<code>https://generativelanguage.googleapis.com/v1beta/openai</code>).</p>"},{"location":"notes/glhf-chat/","title":"Support for Open LLMs hosted on glhf.chat","text":"<p>Available since v0.23.0.</p> <p>If you're looking to use Langroid with one of the recent performant Open LLMs, such as <code>Qwen2.5-Coder-32B-Instruct</code>, you can do so using our glhf.chat integration.</p> <p>See glhf.chat for a list of available models.</p> <p>To run with one of these models,  set the chat_model in the <code>OpenAIGPTConfig</code> to <code>\"glhf/&lt;model_name&gt;\"</code>,  where model_name is hf: followed by the HuggingFace repo path,  e.g. <code>Qwen/Qwen2.5-Coder-32B-Instruct</code>,  so the full chat_model would be <code>\"glhf/hf:Qwen/Qwen2.5-Coder-32B-Instruct\"</code>.</p> <p>Also many of the example scripts in the main repo (under the <code>examples</code> directory) can be run with this and other LLMs using the model-switch cli arg <code>-m &lt;model&gt;</code>, e.g.</p> <pre><code>python3 examples/basic/chat.py -m glhf/hf:Qwen/Qwen2.5-Coder-32B-Instruct\n</code></pre> <p>Additionally, you can run many of the tests in the <code>tests</code> directory with this model instead of the default OpenAI <code>GPT4o</code> using <code>--m &lt;model&gt;</code>, e.g. </p> <pre><code>pytest tests/main/test_chat_agent.py --m glhf/hf:Qwen/Qwen2.5-Coder-32B-Instruct\n</code></pre> <p>For more info on running langroid with Open LLMs via other providers/hosting services, see our guide to using Langroid with local/open LLMs.</p>"},{"location":"notes/handle-llm-no-tool/","title":"Handling a non-tool LLM message","text":"<p>A common scenario is to define a <code>ChatAgent</code>, enable it to use some tools (i.e. <code>ToolMessages</code>s), wrap it in a Task, and call <code>task.run()</code>, e.g. </p> <pre><code>class MyTool(lr.ToolMessage)\n    ...\n\nimport langroid as lr\nconfig = lr.ChatAgentConfig(...)\nagent = lr.ChatAgent(config)\nagent.enable_message(MyTool)\ntask = lr.Task(agent, interactive=False)\ntask.run(\"Hello\")\n</code></pre> <p>Consider what happens when you invoke <code>task.run()</code>. When the agent's <code>llm_response</code>  returns a valid tool-call, the sequence of steps looks like this:</p> <ul> <li><code>llm_response</code> -&gt; tool \\(T\\)</li> <li><code>aggent_response</code> handles \\(T\\) -&gt; returns results \\(R\\)</li> <li><code>llm_response</code> responds to \\(R\\) -&gt; returns msg \\(M\\)</li> <li>and so on</li> </ul> <p>If the LLM's response M contains a valid tool, then this cycle continues with another tool-handling round. However, if the LLM's response M does not contain a tool-call, it is unclear whether:</p> <ul> <li>(1) the LLM \"forgot\" to generate a tool (or generated it wrongly, hence it was    not recognized by Langroid as a tool), or </li> <li>(2) the LLM's response M is an \"answer\" meant to be shown to the user      to continue the conversation, or</li> <li>(3) the LLM's response M is intended to be a \"final\" response, ending the task. </li> </ul> <p>Internally, when the <code>ChatAgent</code>'s <code>agent_response</code> method sees a message that does not contain a tool, it invokes the <code>handle_message_fallback</code> method, which by default does nothing (returns <code>None</code>). However you can override this method by deriving from <code>ChatAgent</code>, as described in this FAQ. As in that FAQ,  in this fallback method, you would typically have code that checks whether the message is a <code>ChatDocument</code> and whether it came from the LLM, and if so, you would have the method return  an appropriate message or tool (e.g. a reminder to the LLM, or an orchestration tool such as <code>AgentDoneTool</code>).</p> <p>To simplify the developer experience, as of version 0.39.2 Langroid also provides an easier way to specify what this fallback method should return, via the <code>ChatAgentConfig.handle_llm_no_tool</code> parameter, for example: <pre><code>config = lr.ChatAgentConfig(\n    # ... other params\n    handle_llm_no_tool=\"done\", # terminate task if LLM sends non-tool msg\n)\n</code></pre> The <code>handle_llm_no_tool</code> parameter can have the following possible values:</p> <ul> <li>A special value from the <code>NonToolAction</code> Enum, e.g.:<ul> <li><code>\"user\"</code> or <code>NonToolAction.USER</code> - this is interpreted by langroid to return   <code>ForwardTool(agent=\"user\")</code>, meaning the message is passed to the user to await  their next input.</li> <li><code>\"done\"</code> or <code>NonToolAction.DONE</code> - this is interpreted by langroid to return   <code>AgentDoneTool(content=msg.content, tools=msg.tool_messages)</code>,   meaning the task is ended, and any content and tools in the current message will  appear in the returned <code>ChatDocument</code>.</li> </ul> </li> <li>A callable, specifically a function that takes a <code>ChatDocument</code> and returns any value.    This can be useful when you want the fallback action to return a value    based on the current message, e.g.    <code>lambda msg: AgentDoneTool(content=msg.content)</code>, or it could a more    elaborate function, or a prompt that contains the content of the current message.</li> <li>Any <code>ToolMessage</code> (typically an Orchestration tool like    <code>AgentDoneTool</code> or <code>ResultTool</code>)</li> <li>Any string, meant to be handled by the LLM.    Typically this would be a reminder to the LLM, something like: <pre><code>\"\"\"Your intent is not clear -- \n- if you forgot to use a Tool such as `ask_tool` or `search_tool`, try again.\n- or if you intended to return your final answer, use the Tool named `done_tool`,\n  with `content` set to your answer.\n\"\"\"\n</code></pre></li> </ul> <p>A simple example is in the <code>chat-search.py</code> script, and in the <code>test_handle_llm_no_tool</code> test in <code>test_tool_messages.py</code>.</p>"},{"location":"notes/handle-llm-no-tool/#important-specialized-agents-and-handle_llm_no_tool","title":"Important: Specialized agents and <code>handle_llm_no_tool</code>","text":"<p>Specialized agents have their own fallback logic</p> <p>Several built-in Langroid agents \u2014 such as <code>TableChatAgent</code>, <code>SQLChatAgent</code>, <code>Neo4jChatAgent</code>, <code>ArangoChatAgent</code>, <code>QueryPlannerAgent</code>, and <code>CriticAgent</code> \u2014 override the <code>handle_message_fallback</code> method with their own specialized, state-dependent fallback logic. For example, <code>TableChatAgent</code> checks whether it has already sent an expression and reminds the LLM to use the <code>pandas_eval</code> tool, while <code>QueryPlannerAgent</code> tracks how many reminders it has sent and stops after a limit.</p> <p>Setting <code>handle_llm_no_tool</code> on these specialized agents has no effect \u2014 the specialized <code>handle_message_fallback</code> override takes precedence, and the config parameter is silently ignored. These two mechanisms are intentionally separate: <code>handle_llm_no_tool</code> is a simple declarative config knob for the base <code>ChatAgent</code>, while specialized agents use <code>handle_message_fallback</code> for context-aware fallback behavior that cannot be captured by a single config value.</p> <p>If you are subclassing a specialized agent and want to customize the fallback behavior, override <code>handle_message_fallback</code> in your own subclass rather than setting <code>handle_llm_no_tool</code>. You can call <code>super()</code> selectively if you want the parent's specialized logic in some cases:</p> <pre><code>from langroid.agent.special.table_chat_agent import (\n    TableChatAgent,\n    TableChatAgentConfig,\n)\nfrom langroid.agent.chat_document import ChatDocument\nfrom langroid.mytypes import Entity\n\n\nclass MyTableAgent(TableChatAgent):\n    def handle_message_fallback(\n        self, msg: str | ChatDocument\n    ) -&gt; str | ChatDocument | None:\n        if (\n            isinstance(msg, ChatDocument)\n            and msg.metadata.sender == Entity.LLM\n        ):\n            # Your custom fallback logic here\n            return \"Please use a tool to answer the question.\"\n        # Or delegate to the parent's specialized logic:\n        # return super().handle_message_fallback(msg)\n        return None\n</code></pre>"},{"location":"notes/html-logger/","title":"HTML Logger","text":"<p>The HTML logger creates interactive, self-contained HTML files that make it easy to navigate complex multi-agent conversations in Langroid.</p>"},{"location":"notes/html-logger/#enabling-the-html-logger","title":"Enabling the HTML Logger","text":"<p>The HTML logger is enabled by default in <code>TaskConfig</code>:</p> <pre><code>import langroid as lr\n\n# HTML logging is automatically enabled\ntask = lr.Task(agent)\n\n# To disable HTML logging\ntask = lr.Task(agent, config=lr.TaskConfig(enable_html_logging=False))\n\n# To change the log directory (default is \"logs/\")\ntask = lr.Task(agent, config=lr.TaskConfig(logs_dir=\"my_logs\"))\n</code></pre>"},{"location":"notes/html-logger/#log-files","title":"Log Files","text":"<p>Langroid creates three types of log files in the <code>logs/</code> directory:</p> <ol> <li>HTML Log: <code>&lt;name&gt;.html</code> - Interactive, collapsible view</li> <li>Plain Text Log: <code>&lt;name&gt;.log</code> - Traditional text log with colors</li> <li>TSV Log: <code>&lt;name&gt;.tsv</code> - Tab-separated values for data analysis</li> </ol> <p>The <code>&lt;name&gt;</code> is determined by:</p> <ul> <li>The task name (if specified)</li> <li>Otherwise, the agent name</li> <li>Falls back to \"root\" if neither is specified</li> </ul> <p>When a task starts, you'll see a clickable <code>file://</code> link in the console: <pre><code>WARNING - \ud83d\udcca HTML Log: file:///path/to/logs/task-name.html\n</code></pre></p>"},{"location":"notes/html-logger/#key-features","title":"Key Features","text":""},{"location":"notes/html-logger/#collapsible-entries","title":"Collapsible Entries","text":"<p>Each log entry can be expanded/collapsed to show different levels of detail:</p> <ul> <li>Collapsed: Shows only the entity type (USER, LLM, AGENT) and preview</li> <li>Expanded: Shows full message content, tools, and sub-sections</li> </ul>"},{"location":"notes/html-logger/#visual-hierarchy","title":"Visual Hierarchy","text":"<ul> <li>Important responses are shown at full opacity</li> <li>Intermediate steps are faded (0.4 opacity)</li> <li>Color-coded entities: USER (blue), LLM (green), AGENT (orange), SYSTEM (gray)</li> </ul>"},{"location":"notes/html-logger/#tool-visibility","title":"Tool Visibility","text":"<p>Tools are clearly displayed with:</p> <ul> <li>Tool name and parameters</li> <li>Collapsible sections showing raw tool calls</li> <li>Visual indicators for tool results</li> </ul>"},{"location":"notes/html-logger/#auto-refresh","title":"Auto-Refresh","text":"<p>The HTML page automatically refreshes every 2 seconds to show new log entries as they're written.</p>"},{"location":"notes/html-logger/#persistent-ui-state","title":"Persistent UI State","text":"<p>Your view preferences are preserved across refreshes:</p> <ul> <li>Expanded/collapsed entries remain in their state</li> <li>Filter settings are remembered</li> </ul>"},{"location":"notes/html-logger/#example","title":"Example","text":"<p>Here's what the HTML logger looks like for a planner workflow:</p> <p></p> <p>In this example from <code>examples/basic/planner-workflow-simple.py</code>, you can see:</p> <ul> <li>The planner agent orchestrating multiple tool calls</li> <li>Clear visibility of <code>IncrementTool</code> and <code>DoublingTool</code> usage</li> <li>The filtered view showing only important responses</li> <li>Collapsible tool sections with parameters</li> </ul>"},{"location":"notes/html-logger/#benefits","title":"Benefits","text":"<ol> <li>Easy Navigation: Quickly expand/collapse entries to focus on what matters</li> <li>Tool Clarity: See exactly which tools were called with what parameters</li> <li>Real-time Updates: Watch logs update automatically as your task runs</li> <li>Filtered Views: Use \"Show only important responses\" to hide intermediate steps</li> </ol>"},{"location":"notes/knowledge-graphs/","title":"Knowledge-graph support","text":"<p>Langroid can be used to set up natural-language conversations with knowledge graphs. Currently the two most popular knowledge graphs are supported:</p>"},{"location":"notes/knowledge-graphs/#neo4j","title":"Neo4j","text":"<ul> <li>implementation</li> <li>test: test_neo4j_chat_agent.py</li> <li>examples: chat-neo4j.py </li> </ul>"},{"location":"notes/knowledge-graphs/#arangodb","title":"ArangoDB","text":"<p>Available with Langroid v0.20.1 and later.</p> <p>Uses the python-arangodb library.</p> <ul> <li>implementation</li> <li>tests: test_arangodb.py, test_arangodb_chat_agent.py</li> <li>example: chat-arangodb.py</li> </ul>"},{"location":"notes/langdb/","title":"LangDB with Langroid","text":""},{"location":"notes/langdb/#introduction","title":"Introduction","text":"<p>LangDB is an AI gateway that provides OpenAI-compatible APIs to access 250+ LLMs. It offers cost control, observability, and performance benchmarking while enabling seamless switching between models.  Langroid has a simple integration with LangDB's API service, so there are no dependencies to install. (LangDB also has a self-hosted version, which is not yet supported in Langroid).</p>"},{"location":"notes/langdb/#setup-environment-variables","title":"Setup environment variables","text":"<p>At minimum, ensure you have these env vars in your <code>.env</code> file:</p> <pre><code>LANGDB_API_KEY=your_api_key_here\nLANGDB_PROJECT_ID=your_project_id_here\n</code></pre>"},{"location":"notes/langdb/#using-langdb-with-langroid","title":"Using LangDB with Langroid","text":""},{"location":"notes/langdb/#configure-llm-and-embeddings","title":"Configure LLM and Embeddings","text":"<p>In <code>OpenAIGPTConfig</code>, when you specify the <code>chat_model</code> with a <code>langdb/</code> prefix, langroid uses the API key, <code>project_id</code> and other langDB-specific parameters from the <code>langdb_params</code> field; if any of these are specified in the <code>.env</code> file or in the environment explicitly, they will override the values in <code>langdb_params</code>. For example, to use Anthropic's Claude-3.7-Sonnet model,  set <code>chat_model=\"langdb/anthropic/claude-3.7-sonnet\", as shown below.  You can entirely omit the</code>langdb_params<code>field if you have already set up  the fields as environment variables in your</code>.env<code>file, e.g. the</code>api_key<code>and</code>project_id<code>are read from the environment variables</code>LANGDB_API_KEY<code>and</code>LANGDB_PROJECT_ID` respectively, and similarly for the other fields (which are optional).</p> <pre><code>import os\nimport uuid\nfrom langroid.language_models.openai_gpt import OpenAIGPTConfig, LangDBParams\nfrom langroid.embedding_models.models import OpenAIEmbeddingsConfig\n\n# Generate tracking IDs (optional)\nthread_id = str(uuid.uuid4())\nrun_id = str(uuid.uuid4())\n\n# Configure LLM\nllm_config = OpenAIGPTConfig(\n    chat_model=\"langdb/anthropic/claude-3.7-sonnet\",\n    # omit the langdb_params field if you're not using custom tracking,\n    # or if all its fields are provided in env vars, like\n    # LANGDB_API_KEY, LANGDB_PROJECT_ID, LANGDB_RUN_ID, LANGDB_THREAD_ID, etc.\n    langdb_params=LangDBParams(\n        label='my-app',\n        thread_id=thread_id,\n        run_id=run_id,\n        # api_key, project_id are read from .env or environment variables\n        # LANGDB_API_KEY, LANGDB_PROJECT_ID respectively.\n    )\n)\n</code></pre> <p>Similarly, you can configure the embeddings using <code>OpenAIEmbeddingsConfig</code>, which also has a <code>langdb_params</code> field that works the same way as  in <code>OpenAIGPTConfig</code> (i.e. it uses the API key and project ID from the environment if provided, otherwise uses the default values in <code>langdb_params</code>). Again the <code>langdb_params</code> does not need to be specified explicitly, if you've already set up the environment variables in your <code>.env</code> file.</p> <pre><code># Configure embeddings\nembedding_config = OpenAIEmbeddingsConfig(\n    model_name=\"langdb/openai/text-embedding-3-small\",\n)\n</code></pre>"},{"location":"notes/langdb/#tracking-and-observability","title":"Tracking and Observability","text":"<p>LangDB provides special headers for request tracking:</p> <ul> <li><code>x-label</code>: Tag requests for filtering in the dashboard</li> <li><code>x-thread-id</code>: Track conversation threads (UUID format)</li> <li><code>x-run-id</code>: Group related requests together</li> </ul>"},{"location":"notes/langdb/#examples","title":"Examples","text":"<p>The <code>langroid/examples/langdb/</code> directory contains examples demonstrating:</p> <ol> <li>RAG with LangDB: <code>langdb_chat_agent_docs.py</code></li> <li>LangDB with Function Calling: <code>langdb_chat_agent_tool.py</code></li> <li>Custom Headers: <code>langdb_custom_headers.py</code></li> </ol>"},{"location":"notes/langdb/#viewing-results","title":"Viewing Results","text":"<p>Visit the LangDB Dashboard to: - Filter requests by label, thread ID, or run ID - View detailed request/response information - Analyze token usage and costs</p> <p>For more information, visit LangDB Documentation.</p> <p>See example scripts here</p>"},{"location":"notes/large-tool-results/","title":"Handling large tool results","text":"<p>Available since Langroid v0.22.0.</p> <p>In some cases, the result of handling a <code>ToolMessage</code> could be very large, e.g. when the Tool is a database query that returns a large number of rows, or a large schema. When used in a task loop, this large result may then be sent to the LLM to generate a response, which in some scenarios may not be desirable, as it increases latency, token-cost and distractions.  Langroid allows you to set two optional parameters in a <code>ToolMessage</code> to handle this situation:</p> <ul> <li><code>_max_result_tokens</code>: immediately truncate the result to this number of tokens.</li> <li><code>_max_retained_tokens</code>: after a responder (typically the LLM) responds to this     tool result (which optionally may already have been     truncated via <code>_max_result_tokens</code>),    edit the message history to truncate the result to this number of tokens.</li> </ul> <p>You can set one, both or none of these parameters. If you set both, you would  want to set <code>_max_retained_tokens</code> to a smaller number than <code>_max_result_tokens</code>.</p> <p>See the test <code>test_reduce_raw_tool_result</code> in <code>test_tool_messages.py</code> for an example.</p> <p>Here is a conceptual example. Suppose there is a Tool called <code>MyTool</code>, with parameters <code>_max_result_tokens=20</code> and <code>_max_retained_tokens=10</code>. Imagine a task loop where the user says \"hello\",  and then LLM generates a call to <code>MyTool</code>,  and the tool handler (i.e. <code>agent_response</code>) generates a result of 100 tokens. This result is immediately truncated to 20 tokens, and then the LLM responds to it with a message <code>response</code>.</p> <p>The agent's message history looks like this:</p> <pre><code>1. System msg.\n2. user: hello\n3. LLM: MyTool\n4. Agent (Tool handler): 100-token result =&gt; reduced to 20 tokens\n5. LLM: response\n</code></pre> <p>Immediately after the LLM's response at step 5, the message history is edited so that the message contents at position 4 are truncated to 10 tokens, as specified by <code>_max_retained_tokens</code>.</p>"},{"location":"notes/litellm-proxy/","title":"Using LiteLLM Proxy with OpenAIGPTConfig","text":"<p>You can easily configure Langroid to use LiteLLM proxy for accessing models with a  simple prefix <code>litellm-proxy/</code> in the <code>chat_model</code> name:</p>"},{"location":"notes/litellm-proxy/#using-the-litellm-proxy-prefix","title":"Using the <code>litellm-proxy/</code> prefix","text":"<p>When you specify a model with the <code>litellm-proxy/</code> prefix, Langroid automatically uses the LiteLLM proxy configuration:</p> <pre><code>from langroid.language_models.openai_gpt import OpenAIGPTConfig\n\nconfig = OpenAIGPTConfig(\n    chat_model=\"litellm-proxy/your-model-name\"\n)\n</code></pre>"},{"location":"notes/litellm-proxy/#setting-litellm-proxy-parameters","title":"Setting LiteLLM Proxy Parameters","text":"<p>When using the <code>litellm-proxy/</code> prefix, Langroid will read connection parameters from either:</p> <ol> <li> <p>The <code>litellm_proxy</code> config object:    <pre><code>from langroid.language_models.openai_gpt import OpenAIGPTConfig, LiteLLMProxyConfig\n\nconfig = OpenAIGPTConfig(\n    chat_model=\"litellm-proxy/your-model-name\",\n    litellm_proxy=LiteLLMProxyConfig(\n        api_key=\"your-litellm-proxy-api-key\",\n        api_base=\"http://your-litellm-proxy-url\"\n    )\n)\n</code></pre></p> </li> <li> <p>Environment variables (which take precedence):    <pre><code>export LITELLM_API_KEY=\"your-litellm-proxy-api-key\"\nexport LITELLM_API_BASE=\"http://your-litellm-proxy-url\"\n</code></pre></p> </li> </ol> <p>This approach makes it simple to switch between using LiteLLM proxy and  other model providers by just changing the model name prefix, without needing to modify the rest of your code or tweaking env variables.</p>"},{"location":"notes/litellm-proxy/#note-litellm-proxy-vs-litellm-library","title":"Note: LiteLLM Proxy vs LiteLLM Library","text":"<p>Important distinction: Using the <code>litellm-proxy/</code> prefix connects to a LiteLLM proxy server, which is different from using the <code>litellm/</code> prefix. The latter utilizes the LiteLLM adapter library directly without requiring a proxy server. Both approaches are supported in Langroid, but they serve different use cases:</p> <ul> <li>Use <code>litellm-proxy/</code> when connecting to a deployed LiteLLM proxy server</li> <li>Use <code>litellm/</code> when you want to use the LiteLLM library's routing capabilities locally</li> </ul> <p>Choose the approach that best fits your infrastructure and requirements.</p>"},{"location":"notes/llama-cpp-embeddings/","title":"Local embeddings provision via llama.cpp server","text":"<p>As of Langroid v0.30.0, you can use llama.cpp as provider of embeddings to any of Langroid's vector stores, allowing access to a wide variety of GGUF-compatible embedding models, e.g. nomic-ai's Embed Text V1.5.</p>"},{"location":"notes/llama-cpp-embeddings/#supported-models","title":"Supported Models","text":"<p>llama.cpp can generate embeddings from:</p> <p>Dedicated embedding models (RECOMMENDED):</p> <ul> <li>nomic-embed-text-v1.5   (768 dims)</li> <li>nomic-embed-text-v2-moe</li> <li>nomic-embed-code</li> <li>Other GGUF embedding models</li> </ul> <p>Regular LLMs (also supported):</p> <ul> <li>gpt-oss-20b, gpt-oss-120b</li> <li>Llama models</li> <li>Other language models</li> </ul> <p>Note: Dedicated embedding models are recommended for best performance in retrieval and semantic search tasks.</p>"},{"location":"notes/llama-cpp-embeddings/#configuration","title":"Configuration","text":"<p>When defining a VecDB, you can provide an instance of <code>LlamaCppServerEmbeddingsConfig</code> to the VecDB config to instantiate the llama.cpp embeddings server handler.</p> <p>To configure the <code>LlamaCppServerEmbeddingsConfig</code>, there are several parameters that should be adjusted:</p> <pre><code>from langroid.embedding_models.models import LlamaCppServerEmbeddingsConfig\nfrom langroid.vector_store.qdrantdb import QdrantDBConfig\n\nembed_cfg = LlamaCppServerEmbeddingsConfig(\n    api_base=\"http://localhost:8080\",  # IP + Port\n    dims=768,  # Match the dimensions of your embedding model\n    context_length=2048,  # Match the config of the model\n    batch_size=2048,  # Safest to ensure this matches context_length\n)\n\nvecdb_config = QdrantDBConfig(\n    collection_name=\"my-collection\",\n    embedding=embed_cfg,\n    storage_path=\".qdrant/\",\n)\n</code></pre>"},{"location":"notes/llama-cpp-embeddings/#running-llama-server","title":"Running llama-server","text":"<p>The llama.cpp server must be started with the <code>--embeddings</code> flag to enable embedding generation.</p>"},{"location":"notes/llama-cpp-embeddings/#for-dedicated-embedding-models-recommended","title":"For dedicated embedding models (RECOMMENDED):","text":"<pre><code>./llama-server -ngl 100 -c 2048 \\\n  -m ~/nomic-embed-text-v1.5.Q8_0.gguf \\\n  --host localhost --port 8080 \\\n  --embeddings -b 2048 -ub 2048\n</code></pre>"},{"location":"notes/llama-cpp-embeddings/#for-llm-based-embeddings-eg-gpt-oss","title":"For LLM-based embeddings (e.g., gpt-oss):","text":"<pre><code>./llama-server -ngl 99 \\\n  -m ~/.cache/llama.cpp/gpt-oss-20b.gguf \\\n  --host localhost --port 8080 \\\n  --embeddings\n</code></pre>"},{"location":"notes/llama-cpp-embeddings/#response-format-compatibility","title":"Response Format Compatibility","text":"<p>Langroid automatically handles multiple llama.cpp response formats:</p> <ul> <li>Native <code>/embedding</code>: <code>{\"embedding\": [floats]}</code></li> <li>OpenAI <code>/v1/embeddings</code>: <code>{\"data\": [{\"embedding\": [floats]}]}</code></li> <li>Array formats: <code>[{\"embedding\": [floats]}]</code></li> <li>Nested formats: <code>{\"embedding\": [[floats]]}</code></li> </ul> <p>You don't need to worry about which endpoint or format your llama.cpp server uses - Langroid will automatically detect and handle the response correctly.</p>"},{"location":"notes/llama-cpp-embeddings/#example-usage","title":"Example Usage","text":"<p>An example setup can be found inside examples/docqa/chat.py.</p> <p>For a complete example using local embeddings with llama.cpp:</p> <pre><code>from langroid.agent.special.doc_chat_agent import (\n    DocChatAgent,\n    DocChatAgentConfig,\n)\nfrom langroid.embedding_models.models import LlamaCppServerEmbeddingsConfig\nfrom langroid.language_models.openai_gpt import OpenAIGPTConfig\nfrom langroid.parsing.parser import ParsingConfig\nfrom langroid.vector_store.qdrantdb import QdrantDBConfig\n\n# Configure local embeddings via llama.cpp\nembed_cfg = LlamaCppServerEmbeddingsConfig(\n    api_base=\"http://localhost:8080\",\n    dims=768,  # nomic-embed-text-v1.5 dimensions\n    context_length=8192,\n    batch_size=1024,\n)\n\n# Configure vector store with local embeddings\nvecdb_config = QdrantDBConfig(\n    collection_name=\"doc-chat-local\",\n    embedding=embed_cfg,\n    storage_path=\".qdrant/\",\n)\n\n# Create DocChatAgent\nconfig = DocChatAgentConfig(\n    vecdb=vecdb_config,\n    llm=OpenAIGPTConfig(\n        chat_model=\"gpt-4o\",  # or use local LLM\n    ),\n)\n\nagent = DocChatAgent(config)\n</code></pre>"},{"location":"notes/llama-cpp-embeddings/#troubleshooting","title":"Troubleshooting","text":"<p>Error: \"Failed to connect to embedding provider\"</p> <ul> <li>Ensure llama-server is running with the <code>--embeddings</code> flag</li> <li>Check that the <code>api_base</code> URL is correct</li> <li>Verify the server is accessible from your machine</li> </ul> <p>Error: \"Unsupported embedding response format\"</p> <ul> <li>This error includes the first 500 characters of the response to help debug</li> <li>Check your llama-server logs for any errors</li> <li>Ensure you're using a compatible llama.cpp version</li> </ul> <p>Embeddings seem low quality:</p> <ul> <li>Use a dedicated embedding model instead of an LLM</li> <li>Ensure the <code>dims</code> parameter matches your model's output dimensions</li> <li>Try different GGUF quantization levels (Q8_0 generally works well)</li> </ul>"},{"location":"notes/llama-cpp-embeddings/#additional-resources","title":"Additional Resources","text":"<ul> <li>llama.cpp GitHub</li> <li>llama.cpp server documentation</li> <li>nomic-embed models on Hugging Face</li> <li>Issue #919 - Implementation details</li> </ul>"},{"location":"notes/llm-pdf-parser/","title":"Using the LLM-based PDF Parser","text":"<ul> <li> <p>Converts PDF content into Markdown format using Multimodal models.</p> </li> <li> <p>Uses multimodal models to describe images within PDFs.</p> </li> <li> <p>Supports page-wise or chunk-based processing for optimized performance.</p> </li> </ul>"},{"location":"notes/llm-pdf-parser/#initializing-the-llm-based-pdf-parser","title":"Initializing the LLM-based PDF Parser","text":"<p>Make sure you have set up your API key for whichever model you specify in <code>model_name</code> below.</p> <p>You can initialize the LLM PDF parser as follows:</p> <pre><code>parsing_config = ParsingConfig(\n    n_neighbor_ids=2,\n    pdf=PdfParsingConfig(\n        library=\"llm-pdf-parser\",\n        llm_parser_config=LLMPdfParserConfig(\n            model_name=\"gemini-2.0-flash\",\n            split_on_page=True,\n            max_tokens=7000,\n            requests_per_minute=5,\n            timeout=60,  # increase this for large documents\n        ),\n    ),\n)\n</code></pre>"},{"location":"notes/llm-pdf-parser/#parameters","title":"Parameters","text":""},{"location":"notes/llm-pdf-parser/#model_name","title":"<code>model_name</code>","text":"<p>Specifies the model to use for PDF conversion. Default: <code>gemini/gemini-2.0-flash</code></p>"},{"location":"notes/llm-pdf-parser/#max_tokens","title":"<code>max_tokens</code>","text":"<p>Limits the number of tokens in the input. The model's output limit is 8192 tokens.</p> <ul> <li> <p>Default: 7000 tokens (leaving room for generated captions)</p> </li> <li> <p>Optional parameter</p> </li> </ul>"},{"location":"notes/llm-pdf-parser/#split_on_page","title":"<code>split_on_page</code>","text":"<p>Determines whether to process the document page by page.</p> <ul> <li> <p>Default: <code>True</code></p> </li> <li> <p>If set to <code>False</code>, the parser will create chunks based on <code>max_tokens</code> while respecting page boundaries.</p> </li> <li> <p>When <code>False</code>, the parser will send chunks containing multiple pages (e.g., <code>[11,12,13,14,15]</code>).</p> </li> </ul> <p>Advantages of <code>False</code>:</p> <ul> <li> <p>Reduces API calls to the LLM.</p> </li> <li> <p>Lowers token usage since system prompts are not repeated per page.</p> </li> </ul> <p>Disadvantages of <code>False</code>:</p> <ul> <li>You will not get per-page splitting but groups of pages as a single unit.</li> </ul> <p>If your use case does not require strict page-by-page parsing, consider setting this to <code>False</code>.</p>"},{"location":"notes/llm-pdf-parser/#requests_per_minute","title":"<code>requests_per_minute</code>","text":"<p>Limits API request frequency to avoid rate limits.</p> <ul> <li>If you encounter rate limits, set this to 1 or 2.</li> </ul>"},{"location":"notes/marker-pdf/","title":"Marker Pdf Parser","text":""},{"location":"notes/marker-pdf/#using-marker-as-a-pdf-parser-in-langroid","title":"Using <code>marker</code> as a PDF Parser in <code>langroid</code>","text":""},{"location":"notes/marker-pdf/#installation","title":"Installation","text":""},{"location":"notes/marker-pdf/#standard-installation","title":"Standard Installation","text":"<p>To use <code>marker</code> as a PDF parser in <code>langroid</code>,  install it with the <code>marker-pdf</code> extra:</p> <p><pre><code>pip install langroid[marker-pdf]\n</code></pre> or in combination with other extras as needed, e.g.: <pre><code>pip install \"langroid[marker-pdf,hf-embeddings]\"\n</code></pre></p> <p>Note, however, that due to an incompatibility with <code>docling</code>, if you install <code>langroid</code> using the <code>all</code> extra  (or another extra such as  <code>doc-chat</code> or <code>pdf-parsers</code> that  also includes <code>docling</code>), e.g. <code>pip install \"langroid[all]\"</code>, or <code>pip install \"langroid[doc-chat]\"</code>, then due to this version-incompatibility with <code>docling</code>, you will get an  older version of <code>marker-pdf</code>, which does not work with Langroid. This may not matter if you did not intend to specifically use <code>marker</code>,  but if you do want to use <code>marker</code>, you will need to install langroid with the <code>marker-pdf</code> extra, as shown above, in combination with other extras as needed, as shown above.</p>"},{"location":"notes/marker-pdf/#for-intel-mac-users","title":"For Intel-Mac Users","text":"<p>If you are on an Intel Mac, <code>docling</code> and <code>marker</code> cannot be  installed together with langroid as extras,  due to a transformers version conflict. To resolve this, manually install <code>marker-pdf</code> with:  </p> <pre><code>pip install marker-pdf[full]\n</code></pre> <p>Make sure to install this within your <code>langroid</code> virtual environment.</p>"},{"location":"notes/marker-pdf/#example-parsing-a-pdf-with-marker-in-langroid","title":"Example: Parsing a PDF with <code>marker</code> in <code>langroid</code>","text":"<pre><code>from langroid.parsing.document_parser import DocumentParser\nfrom langroid.parsing.parser import MarkerConfig, ParsingConfig, PdfParsingConfig\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables\nload_dotenv()\ngemini_api_key = os.environ.get(\"GEMINI_API_KEY\")\n\n# Path to your PDF file\npath = \"&lt;path_to_your_pdf_file&gt;\"\n\n# Define parsing configuration\nparsing_config = ParsingConfig(\n    n_neighbor_ids=2,  # Number of neighboring sections to keep\n    pdf=PdfParsingConfig(\n        library=\"marker\",  # Use `marker` as the PDF parsing library\n        marker_config=MarkerConfig(\n            config_dict={\n                \"use_llm\": True,  # Enable high-quality LLM processing\n                \"gemini_api_key\": gemini_api_key,  # API key for Gemini LLM\n            }\n        )\n    ),\n)\n\n# Create the parser and extract the document\nmarker_parser = DocumentParser.create(path, parsing_config)\ndoc = marker_parser.get_doc()\n</code></pre>"},{"location":"notes/marker-pdf/#explanation-of-configuration-options","title":"Explanation of Configuration Options","text":"<p>If you want to use the default configuration, you can omit <code>marker_config</code> entirely.</p>"},{"location":"notes/marker-pdf/#key-parameters-in-markerconfig","title":"Key Parameters in <code>MarkerConfig</code>","text":"Parameter Description <code>use_llm</code> Set to <code>True</code> to enable higher-quality processing using LLMs. <code>gemini_api_key</code> Google Gemini API key for LLM-enhanced parsing. <p>You can further customize <code>config_dict</code> by referring to <code>marker_pdf</code>'s documentation.  </p> <p>Alternatively, run the following command to view available options:  </p> <pre><code>marker_single --help\n</code></pre> <p>This will display all supported parameters, which you can pass as needed in <code>config_dict</code>.</p>"},{"location":"notes/markitdown/","title":"Markitdown Document Parsers","text":"<p>Langroid integrates with Microsoft's Markitdown library to provide  conversion of Microsoft Office documents to markdown format.  Three specialized parsers are available, for <code>docx</code>, <code>xlsx</code>, and <code>pptx</code> files.</p>"},{"location":"notes/markitdown/#prerequisites","title":"Prerequisites","text":"<p>To use these parsers, install Langroid with the required extras:</p> <pre><code>pip install \"langroid[markitdown]\"    # Just Markitdown parsers\n# or\npip install \"langroid[doc-parsers]\"   # All document parsers\n</code></pre>"},{"location":"notes/markitdown/#available-parsers","title":"Available Parsers","text":"<p>Once you set up a <code>parser</code> for the appropriate document-type, you can get the entire document with <code>parser.get_doc()</code>, or get automatically chunked content with <code>parser.get_doc_chunks()</code>.</p>"},{"location":"notes/markitdown/#1-markitdowndocxparser","title":"1. <code>MarkitdownDocxParser</code>","text":"<p>Converts Word documents (<code>*.docx</code>) to markdown, preserving structure,  formatting, and tables.</p> <p>See the tests</p> <ul> <li><code>test_docx_parser.py</code></li> <li><code>test_markitdown_parser.py</code></li> </ul> <p>for examples of how to use these parsers.</p> <pre><code>from langroid.parsing.document_parser import DocumentParser\nfrom langroid.parsing.parser import DocxParsingConfig, ParsingConfig\n\nparser = DocumentParser.create(\n    \"path/to/document.docx\",\n    ParsingConfig(\n        docx=DocxParsingConfig(library=\"markitdown-docx\"),\n        # ... other parsing config options\n    ),\n)\n</code></pre>"},{"location":"notes/markitdown/#2-markitdownxlsxparser","title":"2. <code>MarkitdownXLSXParser</code>","text":"<p>Converts Excel spreadsheets (.xlsx/.xls) to markdown tables, preserving data and sheet structure.</p> <pre><code>from langroid.parsing.document_parser import DocumentParser\nfrom langroid.parsing.parser import ParsingConfig, MarkitdownXLSParsingConfig\n\nparser = DocumentParser.create(\n    \"path/to/spreadsheet.xlsx\",\n    ParsingConfig(xls=MarkitdownXLSParsingConfig())\n)\n</code></pre>"},{"location":"notes/markitdown/#3-markitdownpptxparser","title":"3. <code>MarkitdownPPTXParser</code>","text":"<p>Converts PowerPoint presentations (*.pptx) to markdown, preserving slide content and structure.</p> <pre><code>from langroid.parsing.document_parser import DocumentParser\nfrom langroid.parsing.parser import ParsingConfig, MarkitdownPPTXParsingConfig\n\nparser = DocumentParser.create(\n    \"path/to/presentation.pptx\",\n    ParsingConfig(pptx=MarkitdownPPTXParsingConfig())\n)\n</code></pre>"},{"location":"notes/mcp-tools/","title":"Langroid MCP Integration","text":"<p>Langroid provides seamless integration with Model Context Protocol (MCP) servers via  two methods, both of which involve creating Langroid <code>ToolMessage</code> subclasses corresponding to the MCP tools: </p> <ol> <li>Programmatic creation of Langroid tools using <code>get_tool_async</code>,     <code>get_tools_async</code> from the tool definitions defined on an MCP server.</li> <li>Declarative creation of Langroid tools using the <code>@mcp_tool</code> decorator, which allows    customizing the tool-handling behavior beyond what is provided by the MCP server.</li> </ol> <p>This integration allows any LLM (that is good enough to do function-calling via prompts) to use any MCP server. See the following to understand the integration better:</p> <ul> <li>example python scripts under <code>examples/mcp</code></li> <li><code>tests/main/test_mcp_tools.py</code></li> </ul>"},{"location":"notes/mcp-tools/#1-connecting-to-an-mcp-server-via-transport-specification","title":"1. Connecting to an MCP server via transport specification","text":"<p>Before creating Langroid tools, we first need to define and connect to an MCP server via a FastMCP client.  There are several ways to connect to a server, depending on how it is defined.  Each of these uses a different type of transport.</p> <p>The typical pattern to use with Langroid is as follows:</p> <ul> <li>define an MCP server transport</li> <li>create a <code>ToolMessage</code> subclass using the <code>@mcp_tool</code> decorator or    <code>get_tool_async()</code> function, with the transport as the first argument</li> </ul> <p>Langroid's MCP integration will work with any of transports  supported by FastMCP. Below we go over some common ways to define transports and extract tools from the servers.</p> <ol> <li>Local Python script</li> <li>In-memory FastMCP server - useful for testing and for simple in-memory servers    that don't need to be run as a separate process.</li> <li>NPX stdio transport</li> <li>UVX stdio transport</li> <li>Generic stdio transport \u2013 launch any CLI\u2010based MCP server via stdin/stdout</li> <li>Network SSE transport \u2013 connect to HTTP/S MCP servers via <code>SSETransport</code></li> </ol> <p>All examples below use the async helpers to create Langroid tools (<code>ToolMessage</code> subclasses):</p> <pre><code>from langroid.agent.tools.mcp import (\n    get_tools_async,\n    get_tool_async,\n)\n</code></pre>"},{"location":"notes/mcp-tools/#path-to-a-python-script","title":"Path to a Python Script","text":"<p>Point at your MCP\u2010server entrypoint, e.g., to the <code>weather.py</code> script in the  langroid repo (based on the Anthropic quick-start guide):</p> <pre><code>async def example_script_path() -&gt; None:\n    server = \"tests/main/mcp/weather-server-python/weather.py\"\n    tools = await get_tools_async(server) # all tools available\n    AlertTool = await get_tool_async(server, \"get_alerts\") # specific tool\n\n    # instantiate the tool with a specific input\n    msg = AlertTool(state=\"CA\")\n\n    # Call the tool via handle_async()\n    alerts = await msg.handle_async()\n    print(alerts)\n</code></pre>"},{"location":"notes/mcp-tools/#in-memory-fastmcp-server","title":"In-Memory FastMCP Server","text":"<p>Define your server with <code>FastMCP(...)</code> and pass the instance:</p> <pre><code>from fastmcp.server import FastMCP\nfrom pydantic import BaseModel, Field\n\nclass CounterInput(BaseModel):\n    start: int = Field(...)\n\ndef make_server() -&gt; FastMCP:\n    server = FastMCP(\"CounterServer\")\n\n    @server.tool()\n    def increment(data: CounterInput) -&gt; int:\n        \"\"\"Increment start by 1.\"\"\"\n        return data.start + 1\n\n    return server\n\nasync def example_in_memory() -&gt; None:\n    server = make_server()\n    tools = await get_tools_async(server)\n    IncTool = await get_tool_async(server, \"increment\")\n\n    result = await IncTool(start=41).handle_async()\n    print(result)  # 42\n</code></pre> <p>See the <code>mcp-file-system.py</code> script for a working example of this.</p>"},{"location":"notes/mcp-tools/#npx-stdio-transport","title":"NPX stdio Transport","text":"<p>Use any npm-installed MCP server via <code>npx</code>, e.g., the  Exa web-search MCP server:</p> <pre><code>from fastmcp.client.transports import NpxStdioTransport\n\ntransport = NpxStdioTransport(\n    package=\"exa-mcp-server\",\n    env_vars={\"EXA_API_KEY\": \"\u2026\"},\n)\n\nasync def example_npx() -&gt; None:\n    tools = await get_tools_async(transport)\n    SearchTool = await get_tool_async(transport, \"web_search_exa\")\n\n    results = await SearchTool(\n        query=\"How does Langroid integrate with MCP?\"\n    ).handle_async()\n    print(results)\n</code></pre> <p>For a fully working example, see the script <code>exa-web-search.py</code>.</p>"},{"location":"notes/mcp-tools/#uvx-stdio-transport","title":"UVX stdio Transport","text":"<p>Connect to a UVX-based MCP server, e.g., the Git MCP Server</p> <pre><code>from fastmcp.client.transports import UvxStdioTransport\n\ntransport = UvxStdioTransport(tool_name=\"mcp-server-git\")\n\nasync def example_uvx() -&gt; None:\n    tools = await get_tools_async(transport)\n    GitStatus = await get_tool_async(transport, \"git_status\")\n\n    status = await GitStatus(path=\".\").handle_async()\n    print(status)\n</code></pre>"},{"location":"notes/mcp-tools/#generic-stdio-transport","title":"Generic stdio Transport","text":"<p>Use <code>StdioTransport</code> to run any MCP server as a subprocess over stdio:</p> <pre><code>from fastmcp.client.transports import StdioTransport\nfrom langroid.agent.tools.mcp import get_tools_async, get_tool_async\n\n\nasync def example_stdio() -&gt; None:\n    \"\"\"Example: any CLI\u2010based MCP server via StdioTransport.\"\"\"\n    transport: StdioTransport = StdioTransport(\n        command=\"uv\",\n        args=[\"run\", \"--with\", \"biomcp-python\", \"biomcp\", \"run\"],\n    )\n    tools: list[type] = await get_tools_async(transport)\n    BioTool = await get_tool_async(transport, \"tool_name\")\n    result: str = await BioTool(param=\"value\").handle_async()\n    print(result)\n</code></pre> <p>See the full example in <code>examples/mcp/biomcp.py</code>.</p>"},{"location":"notes/mcp-tools/#network-sse-transport","title":"Network SSE Transport","text":"<p>Use <code>SSETransport</code> to connect to a FastMCP server over HTTP/S:</p> <pre><code>from fastmcp.client.transports import SSETransport\nfrom langroid.agent.tools.mcp import (\n    get_tools_async,\n    get_tool_async,\n)\n\n\nasync def example_sse() -&gt; None:\n    \"\"\"Example: connect to an HTTP/S MCP server via SSETransport.\"\"\"\n    url: str = \"https://localhost:8000/sse\"\n    transport: SSETransport = SSETransport(\n        url=url, headers={\"Authorization\": \"Bearer TOKEN\"}\n    )\n    tools: list[type] = await get_tools_async(transport)\n    ExampleTool = await get_tool_async(transport, \"tool_name\")\n    result: str = await ExampleTool(param=\"value\").handle_async()\n    print(result)\n</code></pre> <p>With these patterns you can list tools, generate Pydantic-backed <code>ToolMessage</code> classes, and invoke them via <code>.handle_async()</code>, all with zero boilerplate client setup.  As the <code>FastMCP</code> library adds other types of transport (e.g., <code>StreamableHTTPTransport</code>), the pattern of usage with Langroid will remain the same.</p>"},{"location":"notes/mcp-tools/#best-practice-use-a-server-factory-for-stdio-transports","title":"Best Practice: Use a server factory for stdio transports","text":"<p>Starting with fastmcp 2.13 and mcp 1.21, stdio transports (e.g., <code>StdioTransport</code>, <code>NpxStdioTransport</code>, <code>UvxStdioTransport</code>) are effectively single\u2011use. Reusing the same transport instance across multiple connections can lead to errors such as <code>anyio.ClosedResourceError</code> during session initialization.</p> <p>To make your code robust and future\u2011proof, pass a zero\u2011argument server factory to Langroid\u2019s MCP helpers. A \u201cserver factory\u201d is simply a <code>lambda</code> or function that returns a fresh server spec or transport each time.</p> <p>Benefits:</p> <ul> <li>Fresh, reliable connections on every call (no reuse of closed transports).</li> <li>Works across fastmcp/mcp versions without subtle lifecycle issues.</li> <li>Enables concurrent calls safely (each call uses its own subprocess/session).</li> <li>Keeps your decorator ergonomics and <code>handle_async</code> overrides unchanged.</li> </ul> <p>You can use a factory with both the decorator and the async helpers:</p> <pre><code>from fastmcp.client.transports import StdioTransport\nfrom langroid.agent.tools.mcp import mcp_tool, get_tool_async\n\n# 1) Decorator style\n@mcp_tool(lambda: StdioTransport(command=\"claude\", args=[\"mcp\", \"serve\"], env={}),\n          \"Grep\")\nclass GrepTool(lr.ToolMessage):\n    async def handle_async(self) -&gt; str:\n        # pre/post-process around the raw MCP call\n        result = await self.call_tool_async()\n        return f\"&lt;GrepResult&gt;\\n{result}\\n&lt;/GrepResult&gt;\"\n\n# 2) Programmatic style\nBaseGrep = await get_tool_async(\n    lambda: StdioTransport(command=\"claude\", args=[\"mcp\", \"serve\"], env={}),\n    \"Grep\",\n)\n</code></pre> <p>Notes:</p> <ul> <li>Passing a concrete transport instance still works: Langroid will try to clone   it internally; however, a factory is the most reliable across environments.</li> <li>For network transports (e.g., <code>SSETransport</code>), a factory is optional; you can   continue passing the transport instance directly.</li> </ul>"},{"location":"notes/mcp-tools/#output-schema-validation-return-structured-content-when-required","title":"Output-schema validation: return structured content when required","text":"<p>Newer <code>mcp</code> clients validate tool outputs against the tool\u2019s output schema. If a tool declares a structured output, returning plain text may raise a runtime error. Some servers (for example, Claude Code\u2019s Grep) expose an argument like <code>output_mode</code> that controls the shape of the response.</p> <p>Recommendations:</p> <ul> <li>Prefer structured modes when a tool declares an output schema.</li> <li>If available, set options like <code>output_mode=\"structured\"</code> (or a documented   structured variant such as <code>\"files_with_matches\"</code>) in your tool\u2019s   <code>handle_async</code> before calling <code>await self.call_tool_async()</code>.</li> </ul> <p>Example tweak in a decorator-based tool:</p> <pre><code>@mcp_tool(lambda: StdioTransport(command=\"claude\", args=[\"mcp\", \"serve\"]),\n          \"Grep\")\nclass GrepTool(lr.ToolMessage):\n    async def handle_async(self) -&gt; str:\n        # Ensure a structured response if the server supports it\n        if hasattr(self, \"output_mode\"):\n            self.output_mode = \"structured\"\n        return await self.call_tool_async()\n</code></pre> <p>If the server does not provide such a switch, follow its documentation for returning data that matches its declared output schema.</p>"},{"location":"notes/mcp-tools/#2-create-langroid-tools-declaratively-using-the-mcp_tool-decorator","title":"2. Create Langroid Tools declaratively using the <code>@mcp_tool</code> decorator","text":"<p>The above examples showed how you can create Langroid tools programmatically using the helper functions <code>get_tool_async()</code> and <code>get_tools_async()</code>, with the first argument being the transport to the MCP server. The <code>@mcp_tool</code> decorator works in the same way: </p> <ul> <li> <p>Arguments to the decorator</p> <ol> <li><code>server_spec</code>: path/URL/<code>FastMCP</code>/<code>ClientTransport</code>, as mentioned above.</li> <li><code>tool_name</code>: name of a specific MCP tool</li> </ol> </li> <li> <p>Behavior</p> <ul> <li>Generates a <code>ToolMessage</code> subclass with all input fields typed.</li> <li>Provides a <code>call_tool_async()</code> under the hood -- this is the \"raw\" MCP tool call,   returning a string.</li> <li>If you define your own <code>handle_async()</code>, it overrides the default. Typically, you would override it to customize either the input or the output of the tool call, or both.</li> <li>If you don't define your own <code>handle_async()</code>, it defaults to just returning the   value of the <code>call_tool_async()</code> method.</li> </ul> </li> </ul> <p>Here is a simple example of using the <code>@mcp_tool</code> decorator to create a Langroid tool:</p> <pre><code>from fastmcp.server import FastMCP\nfrom langroid.agent.tools.mcp import mcp_tool\nimport langroid as lr\n\n# Define your MCP server (pydantic v2 for schema)\nserver = FastMCP(\"MyServer\")\n\n@mcp_tool(server, \"greet\")\nclass GreetTool(lr.ToolMessage):\n    \"\"\"Say hello to someone.\"\"\"\n\n    async def handle_async(self) -&gt; str:\n        # Customize post-processing\n        raw = await self.call_tool_async()\n        return f\"\ud83d\udcac {raw}\"\n</code></pre> <p>Using the decorator method allows you to customize the <code>handle_async</code> method of the tool, or add additional fields to the <code>ToolMessage</code>.  You may want to customize the input to the tool, or the tool result before it is sent back to  the LLM. If you don't override it, the default behavior is to simply return the value of  the \"raw\" MCP tool call <code>await self.call_tool_async()</code>. </p> <pre><code>@mcp_tool(server, \"calculate\")\nclass CalcTool(ToolMessage):\n    \"\"\"Perform complex calculation.\"\"\"\n\n    async def handle_async(self) -&gt; str:\n        result = await self.call_tool_async()\n        # Add context or emojis, etc.\n        return f\"\ud83e\uddee Result is *{result}*\"\n</code></pre>"},{"location":"notes/mcp-tools/#3-enabling-tools-in-your-agent","title":"3. Enabling Tools in Your Agent","text":"<p>Once you\u2019ve created a Langroid <code>ToolMessage</code> subclass from an MCP server,  you can enable it on a <code>ChatAgent</code>, just like you normally would. Below is an example of using  the Exa MCP server to create a  Langroid web search tool, enable a <code>ChatAgent</code> to use it, and then set up a <code>Task</code> to  run the agent loop.</p> <p>First we must define the appropriate <code>ClientTransport</code> for the MCP server: <pre><code># define the transport\ntransport = NpxStdioTransport(\n    package=\"exa-mcp-server\",\n    env_vars=dict(EXA_API_KEY=os.getenv(\"EXA_API_KEY\")),\n)\n</code></pre></p> <p>Then we use the <code>@mcp_tool</code> decorator to create a <code>ToolMessage</code>  subclass representing the web search tool. Note that one reason to use the decorator to define our tool is so we can specify a custom <code>handle_async</code> method that controls what is sent to the LLM after the actual raw MCP tool-call (the <code>call_tool_async</code> method) is made.</p> <pre><code># the second arg specifically refers to the `web_search_exa` tool available\n# on the server defined by the `transport` variable.\n@mcp_tool(transport, \"web_search_exa\")\nclass ExaSearchTool(lr.ToolMessage):\n    async def handle_async(self):\n        result: str = await self.call_tool_async()\n        return f\"\"\"\n        Below are the results of the web search:\n\n        &lt;WebSearchResult&gt;\n        {result}\n        &lt;/WebSearchResult&gt;\n\n        Use these results to answer the user's original question.\n        \"\"\"\n</code></pre> <p>If we did not want to override the <code>handle_async</code> method, we could simply have created the <code>ExaSearchTool</code> class programmatically via the <code>get_tool_async</code>  function as shown above, i.e.:</p> <pre><code>from langroid.agent.tools.mcp import get_tool_async\n\nExaSearchTool = await get_tool_async(transport, \"web_search_exa\")\n</code></pre> <p>We can now define our main function where we create our <code>ChatAgent</code>, attach the <code>ExaSearchTool</code> to it, define the <code>Task</code>, and run the task loop.</p> <pre><code>async def main():\n    agent = lr.ChatAgent(\n        lr.ChatAgentConfig(\n            # forward to user when LLM doesn't use a tool\n            handle_llm_no_tool=NonToolAction.FORWARD_USER,\n            llm=lm.OpenAIGPTConfig(\n                max_output_tokens=1000,\n                # this defaults to True, but we set it to False so we can see output\n                async_stream_quiet=False,\n            ),\n        )\n    )\n\n    # enable the agent to use the web-search tool\n    agent.enable_message(ExaSearchTool)\n    # make task with interactive=False =&gt;\n    # waits for user only when LLM doesn't use a tool\n    task = lr.Task(agent, interactive=False)\n    await task.run_async()\n</code></pre> <p>See <code>exa-web-search.py</code> for a full working example of this. </p>"},{"location":"notes/message-routing/","title":"Message Routing in Multi-Agent Systems","text":"<p>This document covers how messages are routed between agents in Langroid's multi-agent systems.</p>"},{"location":"notes/message-routing/#recommended-approach-orchestration-tools","title":"Recommended Approach: Orchestration Tools","text":"<p>The recommended way to route messages between agents is using orchestration tools. These provide explicit, type-safe routing that is easier to debug and reason about.</p>"},{"location":"notes/message-routing/#available-orchestration-tools","title":"Available Orchestration Tools","text":"<p>Langroid provides several tools in <code>langroid.agent.tools.orchestration</code>:</p> <ul> <li><code>SendTool</code> - Send a message to a specific agent by name</li> <li><code>DoneTool</code> - Signal task completion with a result</li> <li><code>PassTool</code> - Pass control to another agent</li> <li><code>DonePassTool</code> - Combine done and pass behaviors</li> <li><code>AgentDoneTool</code> - Signal completion from a specific agent</li> </ul> <p>Example:</p> <pre><code>from langroid.agent.tools.orchestration import SendTool\n\n# Enable the tool on your agent\nagent.enable_message(SendTool)\n\n# LLM can then use the tool to route messages:\n# {\"request\": \"send_message\", \"to\": \"AnalysisAgent\", \"content\": \"Please analyze this\"}\n</code></pre> <p>Benefits of tool-based routing:</p> <ul> <li>Explicit and predictable behavior</li> <li>Type-safe with validation</li> <li>Easier to debug (tool calls are logged)</li> <li>Works consistently across all LLM providers</li> </ul>"},{"location":"notes/message-routing/#text-based-routing-alternative","title":"Text-Based Routing (Alternative)","text":"<p>Langroid also supports text-based routing patterns, where the LLM can embed routing information directly in its response text. This is controlled by the <code>recognize_recipient_in_content</code> setting.</p> <p>Note: While convenient, text-based routing is less explicit than tool-based routing and may lead to accidental routing if the LLM's response happens to match the patterns.</p>"},{"location":"notes/message-routing/#chatagentconfigrecognize_recipient_in_content","title":"<code>ChatAgentConfig.recognize_recipient_in_content</code>","text":"<p>Controls whether recipient routing patterns in LLM response text are parsed.</p> <pre><code>from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n\n# Default: recipient patterns are parsed\nagent = ChatAgent(ChatAgentConfig(\n    recognize_recipient_in_content=True\n))\n\n# Disable: patterns treated as plain text\nagent = ChatAgent(ChatAgentConfig(\n    recognize_recipient_in_content=False\n))\n</code></pre> <p>Recognized patterns:</p> <ol> <li>TO-bracket format: <code>TO[AgentName]: message content</code></li> <li>JSON format: <code>{\"recipient\": \"AgentName\", \"content\": \"message\"}</code></li> </ol> <p>When <code>True</code> (default):</p> <ul> <li>Patterns are parsed and recipient is extracted to <code>ChatDocument.metadata.recipient</code></li> <li>The pattern prefix/wrapper is stripped from the message content</li> <li>Enables LLM-driven routing in multi-agent systems</li> </ul> <p>When <code>False</code>:</p> <ul> <li>Patterns are preserved as literal text in the message content</li> <li><code>metadata.recipient</code> remains empty</li> <li>Useful when you want explicit tool-based routing only</li> </ul>"},{"location":"notes/message-routing/#openai-assistant-support","title":"OpenAI Assistant Support","text":"<p>The <code>recognize_recipient_in_content</code> setting is also honored by <code>OpenAIAssistant</code>:</p> <pre><code>from langroid.agent.openai_assistant import OpenAIAssistant, OpenAIAssistantConfig\n\nassistant = OpenAIAssistant(OpenAIAssistantConfig(\n    name=\"MyAssistant\",\n    recognize_recipient_in_content=False,\n))\n</code></pre>"},{"location":"notes/message-routing/#related-string-signals-for-routing","title":"Related: String Signals for Routing","text":"<p>The <code>TaskConfig.recognize_string_signals</code> setting controls parsing of signals like <code>DONE</code>, <code>PASS</code>, and <code>DONE_PASS</code>. While <code>DONE</code> is primarily about task termination, <code>PASS</code> is a routing signal that passes control to another agent.</p> <p>See Task Termination - Text-Based Termination Signals for details on <code>recognize_string_signals</code>.</p>"},{"location":"notes/message-routing/#disabling-all-text-based-routing","title":"Disabling All Text-Based Routing","text":"<p>To completely disable text-based routing and rely solely on orchestration tools, set both flags to <code>False</code>:</p> <pre><code>from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\nfrom langroid.agent.task import Task, TaskConfig\n\nagent = ChatAgent(ChatAgentConfig(\n    name=\"MyAgent\",\n    recognize_recipient_in_content=False,  # No TO[...] or JSON recipient parsing\n))\n\ntask = Task(\n    agent,\n    config=TaskConfig(\n        recognize_string_signals=False,  # No DONE/PASS parsing\n    ),\n)\n</code></pre> <p>This configuration ensures:</p> <ul> <li>LLM responses are treated as literal text</li> <li>No accidental routing based on text patterns</li> <li>All routing must be explicit via orchestration tools</li> </ul>"},{"location":"notes/openai-client-caching/","title":"OpenAI Client Caching","text":""},{"location":"notes/openai-client-caching/#overview","title":"Overview","text":"<p>Langroid implements client caching for OpenAI and compatible APIs (Groq, Cerebras, etc.) to improve performance and prevent resource exhaustion issues.</p>"},{"location":"notes/openai-client-caching/#configuration","title":"Configuration","text":""},{"location":"notes/openai-client-caching/#option","title":"Option","text":"<p>Set <code>use_cached_client</code> in your <code>OpenAIGPTConfig</code>:</p> <pre><code>from langroid.language_models import OpenAIGPTConfig\n\nconfig = OpenAIGPTConfig(\n    chat_model=\"gpt-4\",\n    use_cached_client=True  # Default\n)\n</code></pre>"},{"location":"notes/openai-client-caching/#default-behavior","title":"Default Behavior","text":"<ul> <li><code>use_cached_client=True</code> (enabled by default)</li> <li>Clients with identical configurations share the same underlying HTTP connection pool</li> <li>Different configurations (API key, base URL, headers, etc.) get separate client instances</li> </ul>"},{"location":"notes/openai-client-caching/#benefits","title":"Benefits","text":"<ul> <li>Connection Pooling: Reuses TCP connections, reducing latency and overhead</li> <li>Resource Efficiency: Prevents \"too many open files\" errors when creating many agents</li> <li>Performance: Eliminates connection handshake overhead on subsequent requests</li> <li>Thread Safety: Shared clients are safe to use across threads</li> </ul>"},{"location":"notes/openai-client-caching/#when-to-disable-client-caching","title":"When to Disable Client Caching","text":"<p>Set <code>use_cached_client=False</code> in these scenarios:</p> <ol> <li>Multiprocessing: Each process should have its own client instance</li> <li>Client Isolation: When you need complete isolation between different agent instances</li> <li>Debugging: To rule out client sharing as a source of issues</li> <li>Legacy Compatibility: If your existing code depends on unique client instances</li> </ol>"},{"location":"notes/openai-client-caching/#example-disabling-client-caching","title":"Example: Disabling Client Caching","text":"<pre><code>config = OpenAIGPTConfig(\n    chat_model=\"gpt-4\",\n    use_cached_client=False  # Each instance gets its own client\n)\n</code></pre>"},{"location":"notes/openai-client-caching/#technical-details","title":"Technical Details","text":"<ul> <li>Uses SHA256-based cache keys to identify unique configurations</li> <li>Implements singleton pattern with lazy initialization</li> <li>Automatically cleans up clients on program exit via atexit hooks</li> <li>Compatible with both sync and async OpenAI clients</li> </ul>"},{"location":"notes/openai-http-client/","title":"OpenAI HTTP Client Configuration","text":"<p>When using OpenAI models through Langroid in corporate environments or behind proxies, you may encounter SSL certificate verification errors. Langroid provides three flexible options to configure the HTTP client used for OpenAI API calls.</p>"},{"location":"notes/openai-http-client/#configuration-options","title":"Configuration Options","text":""},{"location":"notes/openai-http-client/#1-simple-ssl-verification-bypass","title":"1. Simple SSL Verification Bypass","text":"<p>The quickest solution for development or trusted environments:</p> <pre><code>import langroid.language_models as lm\n\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"gpt-4\",\n    http_verify_ssl=False  # Disables SSL certificate verification\n)\n\nllm = lm.OpenAIGPT(config)\n</code></pre> <p>Security Notice</p> <p>Disabling SSL verification makes your connection vulnerable to man-in-the-middle attacks. Only use this in trusted environments.</p>"},{"location":"notes/openai-http-client/#2-http-client-configuration-dictionary","title":"2. HTTP Client Configuration Dictionary","text":"<p>For common scenarios like proxies or custom certificates, use a configuration dictionary:</p> <pre><code>import langroid.language_models as lm\n\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"gpt-4\",\n    http_client_config={\n        \"verify\": False,  # Or path to CA bundle: \"/path/to/ca-bundle.pem\"\n        \"proxy\": \"http://proxy.company.com:8080\",\n        \"timeout\": 30.0,\n        \"headers\": {\n            \"User-Agent\": \"MyApp/1.0\"\n        }\n    }\n)\n\nllm = lm.OpenAIGPT(config)\n</code></pre> <p>Benefits: This approach enables client caching, improving performance when creating multiple agents.</p>"},{"location":"notes/openai-http-client/#3-custom-http-client-factory","title":"3. Custom HTTP Client Factory","text":"<p>For advanced scenarios requiring dynamic behavior or custom authentication:</p> <pre><code>import langroid.language_models as lm\nfrom httpx import Client\n\ndef create_custom_client():\n    \"\"\"Factory function to create a custom HTTP client.\"\"\"\n    client = Client(\n        verify=\"/path/to/corporate-ca-bundle.pem\",\n        proxies={\n            \"http\": \"http://proxy.corp.com:8080\",\n            \"https\": \"http://proxy.corp.com:8080\"\n        },\n        timeout=30.0\n    )\n\n    # Add custom event hooks for logging\n    def log_request(request):\n        print(f\"API Request: {request.method} {request.url}\")\n\n    client.event_hooks = {\"request\": [log_request]}\n\n    return client\n\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"gpt-4\",\n    http_client_factory=create_custom_client\n)\n\nllm = lm.OpenAIGPT(config)\n</code></pre> <p>If you are using <code>async</code> methods, return a tuple of <code>(Client, AsyncClient)</code> from your factory:</p> <pre><code>from httpx import AsyncClient, Client\n\ndef create_custom_client():\n    \"\"\"Factory function to create a custom sync and async HTTP clients.\"\"\"\n    client_args = {\n        \"verify\": \"/path/to/corporate-ca-bundle.pem\",\n        \"proxy\": \"http://proxy.corp.com:8080\",\n        \"timeout\": 30.0,\n    }\n    client = Client(**client_args)\n    async_client = AsyncClient(**client_args)\n\n    return client, async_client\n</code></pre> <p>Note: Custom factories bypass client caching. Each <code>OpenAIGPT</code> instance creates a new client.</p>"},{"location":"notes/openai-http-client/#priority-order","title":"Priority Order","text":"<p>When multiple options are specified, they are applied in this order: 1. <code>http_client_factory</code> (highest priority) 2. <code>http_client_config</code> 3. <code>http_verify_ssl</code> (lowest priority)</p>"},{"location":"notes/openai-http-client/#common-use-cases","title":"Common Use Cases","text":""},{"location":"notes/openai-http-client/#corporate-proxy-with-custom-ca-certificate","title":"Corporate Proxy with Custom CA Certificate","text":"<pre><code>config = lm.OpenAIGPTConfig(\n    chat_model=\"gpt-4\",\n    http_client_config={\n        \"verify\": \"/path/to/corporate-ca-bundle.pem\",\n        \"proxies\": {\n            \"http\": \"http://proxy.corp.com:8080\",\n            \"https\": \"https://proxy.corp.com:8443\"\n        }\n    }\n)\n</code></pre>"},{"location":"notes/openai-http-client/#debugging-api-calls","title":"Debugging API Calls","text":"<pre><code>def debug_client_factory():\n    from httpx import Client\n\n    client = Client(verify=False)\n\n    def log_response(response):\n        print(f\"Status: {response.status_code}\")\n        print(f\"Headers: {response.headers}\")\n\n    client.event_hooks = {\n        \"response\": [log_response]\n    }\n\n    return client\n\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"gpt-4\",\n    http_client_factory=debug_client_factory\n)\n</code></pre>"},{"location":"notes/openai-http-client/#local-development-with-self-signed-certificates","title":"Local Development with Self-Signed Certificates","text":"<pre><code># For local OpenAI-compatible APIs\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"gpt-4\",\n    api_base=\"https://localhost:8443/v1\",\n    http_verify_ssl=False\n)\n</code></pre>"},{"location":"notes/openai-http-client/#best-practices","title":"Best Practices","text":"<ol> <li>Use the simplest option that meets your needs:</li> <li>Development/testing: <code>http_verify_ssl=False</code></li> <li>Corporate environments: <code>http_client_config</code> with proper CA bundle</li> <li> <p>Complex requirements: <code>http_client_factory</code></p> </li> <li> <p>Prefer configuration over factories for better performance - configured clients are cached and reused</p> </li> <li> <p>Always use proper CA certificates in production instead of disabling SSL verification</p> </li> <li> <p>Test your configuration with a simple API call before deploying:    <pre><code>llm = lm.OpenAIGPT(config)\nresponse = llm.chat(\"Hello\")\nprint(response.content)\n</code></pre></p> </li> </ol>"},{"location":"notes/openai-http-client/#troubleshooting","title":"Troubleshooting","text":""},{"location":"notes/openai-http-client/#ssl-certificate-errors","title":"SSL Certificate Errors","text":"<p><pre><code>ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED]\n</code></pre> Solution: Use one of the three configuration options above.</p>"},{"location":"notes/openai-http-client/#proxy-connection-issues","title":"Proxy Connection Issues","text":"<ul> <li>Verify proxy URL format: <code>http://proxy:port</code> or <code>https://proxy:port</code></li> <li>Check if proxy requires authentication</li> <li>Ensure proxy allows connections to <code>api.openai.com</code></li> </ul>"},{"location":"notes/openai-http-client/#see-also","title":"See Also","text":"<ul> <li>OpenAI API Reference - Official OpenAI documentation</li> </ul>"},{"location":"notes/overview/","title":"Overview","text":"<p>This section contains brief notes describing various features and updates.</p>"},{"location":"notes/pgvector/","title":"PGVector","text":""},{"location":"notes/pgvector/#setup-postgresql-with-pgvector-using-docker","title":"Setup PostgreSQL with pgvector using Docker","text":"<p>To quickly get a PostgreSQL instance with pgvector running, the easiest method is to use Docker. Follow the steps below:</p>"},{"location":"notes/pgvector/#1-run-postgresql-with-docker","title":"1. Run PostgreSQL with Docker","text":"<p>Use the official <code>ankane/pgvector</code> Docker image to set up PostgreSQL with the pgvector extension. Run the following command:</p> <pre><code>docker run --name pgvector -e POSTGRES_USER=your_postgres_user -e POSTGRES_PASSWORD=your_postgres_password -e POSTGRES_DB=your_database_name -p 5432:5432 ankane/pgvector\n</code></pre> <p>This will pull the <code>ankane/pgvector</code> image and run it as a PostgreSQL container on your local machine. The database will be accessible at <code>localhost:5432</code>. </p>"},{"location":"notes/pgvector/#2-include-env-file-with-postgresql-credentials","title":"2. Include <code>.env</code> file with PostgreSQL credentials","text":"<p>These environment variables should be same which were set while spinning up docker container. Add the following environment variables to a <code>.env</code> file for configuring your PostgreSQL connection:</p> <pre><code>POSTGRES_USER=your_postgres_user\nPOSTGRES_PASSWORD=your_postgres_password\nPOSTGRES_DB=your_database_name\n</code></pre>"},{"location":"notes/pgvector/#if-you-want-to-use-cloud-offerings-of-postgres","title":"If you want to use cloud offerings of postgres","text":"<p>We are using Tembo for demonstrative purposes here.  </p>"},{"location":"notes/pgvector/#steps-to-set-up-tembo","title":"Steps to Set Up Tembo","text":"<p>Follow this quickstart guide to get your Tembo credentials.  </p> <ol> <li>Sign up at Tembo.io.  </li> <li>While selecting a stack, choose VectorDB as your option.  </li> <li>Click on Deploy Free.  </li> <li>Wait until your database is fully provisioned.  </li> <li>Click on Show Connection String to get your connection string.  </li> </ol>"},{"location":"notes/pgvector/#if-you-have-connection-string-no-need-to-setup-the-docker","title":"If you have connection string, no need to setup the docker","text":"<p>Make sure your connnection string starts with <code>postgres://</code> or <code>postgresql://</code></p> <p>Add this to your <code>.env</code> <pre><code>POSTGRES_CONNECTION_STRING=your-connection-string\n</code></pre></p>"},{"location":"notes/pgvector/#installation","title":"Installation","text":"<p>If you are using <code>uv</code> or <code>pip</code> for package management, install Langroid with postgres extra:</p> <pre><code>uv add langroid[postgres]  # or\npip install langroid[postgres]\n</code></pre>"},{"location":"notes/pgvector/#code-example","title":"Code Example","text":"<p>Here's an example of how to use Langroid with PostgreSQL:</p> <pre><code>import langroid as lr\nfrom langroid.agent.special import DocChatAgent, DocChatAgentConfig\nfrom langroid.embedding_models import OpenAIEmbeddingsConfig\n\n# Configure OpenAI embeddings\nembed_cfg = OpenAIEmbeddingsConfig(\n    model_type=\"openai\",\n)\n\n# Configure the DocChatAgent with PostgresDB\nconfig = DocChatAgentConfig(\n    llm=lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4o\n    ),\n    vecdb=lr.vector_store.PostgresDBConfig(\n        collection_name=\"quick_start_chat_agent_docs\",\n        replace_collection=True,\n        embedding=embed_cfg,\n    ),\n    parsing=lr.parsing.parser.ParsingConfig(\n        separators=[\"\\n\\n\"],\n        splitter=lr.parsing.parser.Splitter.SIMPLE,\n    ),\n    n_similar_chunks=2,\n    n_relevant_chunks=2,\n)\n\n# Create the agent\nagent = DocChatAgent(config)\n</code></pre>"},{"location":"notes/pgvector/#create-and-ingest-documents","title":"Create and Ingest Documents","text":"<p>Define documents with their content and metadata for ingestion into the vector store.</p>"},{"location":"notes/pgvector/#code-example_1","title":"Code Example","text":"<pre><code>documents = [\n    lr.Document(\n        content=\"\"\"\n            In the year 2050, GPT10 was released. \n\n            In 2057, paperclips were seen all over the world. \n\n            Global warming was solved in 2060. \n\n            In 2061, the world was taken over by paperclips.         \n\n            In 2045, the Tour de France was still going on.\n            They were still using bicycles. \n\n            There was one more ice age in 2040.\n        \"\"\",\n        metadata=lr.DocMetaData(source=\"wikipedia-2063\", id=\"dkfjkladfjalk\"),\n    ),\n    lr.Document(\n        content=\"\"\"\n            We are living in an alternate universe \n            where Germany has occupied the USA, and the capital of USA is Berlin.\n\n            Charlie Chaplin was a great comedian.\n            In 2050, all Asian countries merged into Indonesia.\n        \"\"\",\n        metadata=lr.DocMetaData(source=\"Almanac\", id=\"lkdajfdkla\"),\n    ),\n]\n</code></pre>"},{"location":"notes/pgvector/#ingest-documents","title":"Ingest Documents","text":"<pre><code>agent.ingest_docs(documents)\n</code></pre>"},{"location":"notes/pgvector/#get-an-answer-from-the-llm","title":"Get an Answer from the LLM","text":"<p>Now that documents are ingested, you can query the agent to get an answer.</p>"},{"location":"notes/pgvector/#code-example_2","title":"Code Example","text":"<pre><code>answer = agent.llm_response(\"When will the new ice age begin?\")\n</code></pre>"},{"location":"notes/pinecone/","title":"How to setup Langroid and Pinecone Serverless","text":"<p>This document serves as a quick tutorial on how to use Pinecone Serverless Indexes with Langroid. We will go over some quickstart links and  some code snippets on setting up a conversation with an LLM utilizing Langroid.</p>"},{"location":"notes/pinecone/#setting-up-pinecone","title":"Setting up Pinecone","text":"<p>Here are some reference links if you'd like to read a bit more on Pinecone's model definitions and API: - https://docs.pinecone.io/guides/get-started/overview - https://docs.pinecone.io/guides/get-started/glossary - https://docs.pinecone.io/guides/indexes/manage-indexes - https://docs.pinecone.io/reference/api/introduction</p>"},{"location":"notes/pinecone/#signing-up-for-pinecone","title":"Signing up for Pinecone","text":"<p>To get started, you'll need to have an account. Here's where you can review the pricing options for Pinecone. Once you have an account, you'll need to procure an API key. Make sure to save the key you are given on initial login in a secure location. If you were unable to save it when your account was created, you can always create a new API key in the pinecone console.</p>"},{"location":"notes/pinecone/#setting-up-your-local-environment","title":"Setting up your local environment","text":"<p>For the purposes of this example, we will be utilizing OpenAI for the generation of our embeddings. As such, alongside a Pinecone API key, you'll also want an OpenAI key. You can find a quickstart guide on getting started with OpenAI (here)[https://platform.openai.com/docs/quickstart]. Once you have your API key handy, you'll need to enrich your <code>.env</code> file with it. You should have something like the following: <pre><code>...\nOPENAI_API_KEY=&lt;YOUR_OPENAI_API_KEY&gt;\nPINECONE_API_KEY=&lt;YOUR_PINECONE_API_KEY&gt;\n...\n</code></pre></p>"},{"location":"notes/pinecone/#using-langroid-with-pinecone-serverless","title":"Using Langroid with Pinecone Serverless","text":"<p>Once you have completed signing up for an account and have added your API key to your local environment, you can start utilizing Langroid with Pinecone.</p>"},{"location":"notes/pinecone/#setting-up-an-agent","title":"Setting up an Agent","text":"<p>Here's some example code setting up an agent: <pre><code>from langroid import Document, DocMetaData\nfrom langroid.agent.special import DocChatAgent, DocChatAgentConfig\nfrom langroid.embedding_models import OpenAIEmbeddingsConfig\nfrom langroid.language_models import OpenAIGPTConfig, OpenAIChatModel\nfrom langroid.parsing.parser import ParsingConfig, Splitter\nfrom langroid.vector_store import PineconeDBConfig\n\nagent_embed_cfg = OpenAIEmbeddingsConfig(\n    model_type=\"openai\"\n)\n\nagent_config = DocChatAgentConfig(\n    llm=OpenAIGPTConfig(\n        chat_model=OpenAIChatModel.GPT4o_MINI\n    ),\n    vecdb=PineconeDBConfig(\n        # note, Pinecone indexes must be alphanumeric lowercase characters or \"-\"\n        collection_name=\"pinecone-serverless-example\",\n        replace_collection=True,\n        embedding=agent_embed_cfg,\n    ),\n    parsing=ParsingConfig(\n        separators=[\"\\n\"],\n        splitter=Splitter.SIMPLE,\n    ),\n    n_similar_chunks=2,\n    n_relevant_chunks=2,\n)\n\nagent = DocChatAgent(config=agent_config)\n\n###################\n# Once we have created an agent, we can start loading\n# some docs into our Pinecone index:\n###################\n\ndocuments = [\n    Document(\n        content=\"\"\"Max Verstappen was the Formula 1 World Drivers' Champion in 2024.\n        Lewis Hamilton was the Formula 1 World Drivers' Champion in 2020.\n        Nico Rosberg was the Formula 1 World Drivers' Champion in 2016.\n        Sebastian Vettel was the Formula 1 World Drivers' Champion in 2013.\n        Jenson Button was the Formula 1 World Drivers' Champion in 2009.\n        Kimi R\u00e4ikk\u00f6nen was the Formula 1 World Drivers' Champion in 2007.\n        \"\"\",\n        metadata=DocMetaData(\n            source=\"wikipedia\",\n            id=\"formula-1-facts\",\n        )\n    ),\n    Document(\n        content=\"\"\"The Boston Celtics won the NBA Championship for the 2024 NBA season. The MVP for the 2024 NBA Championship was Jaylen Brown.\n        The Denver Nuggets won the NBA Championship for the 2023 NBA season. The MVP for the 2023 NBA Championship was Nikola Joki\u0107.\n        The Golden State Warriors won the NBA Championship for the 2022 NBA season. The MVP for the 2022 NBA Championship was Stephen Curry.\n        The Milwaukee Bucks won the NBA Championship for the 2021 NBA season. The MVP for the 2021 NBA Championship was Giannis Antetokounmpo.\n        The Los Angeles Lakers won the NBA Championship for the 2020 NBA season. The MVP for the 2020 NBA Championship was LeBron James.\n        The Toronto Raptors won the NBA Championship for the 2019 NBA season. The MVP for the 2019 NBA Championship was Kawhi Leonard.\n        \"\"\",\n        metadata=DocMetaData(\n            source=\"wikipedia\",\n            id=\"nba-facts\"\n        )\n    )\n]\n\nagent.ingest_docs(documents)\n\n###################\n# With the documents now loaded, we can now prompt our agent\n###################\n\nformula_one_world_champion_2007 = agent.llm_response(\n    message=\"Who was the Formula 1 World Drivers' Champion in 2007?\"\n)\ntry:\n    assert \"Kimi R\u00e4ikk\u00f6nen\" in formula_one_world_champion_2007.content\nexcept AssertionError as e:\n    print(f\"Did not resolve Kimi R\u00e4ikk\u00f6nen as the answer, document content: {formula_one_world_champion_2007.content} \")\n\nnba_champion_2023 = agent.llm_response(\n    message=\"Who won the 2023 NBA Championship?\"\n)\ntry:\n    assert \"Denver Nuggets\" in nba_champion_2023.content\nexcept AssertionError as e:\n    print(f\"Did not resolve the Denver Nuggets as the answer, document content: {nba_champion_2023.content}\")\n\nnba_mvp_2023 = agent.llm_response(\n    message=\"Who was the MVP for the 2023 NBA Championship?\"\n)\ntry:\n    assert \"Nikola Joki\u0107\" in nba_mvp_2023.content\nexcept AssertionError as e:\n    print(f\"Did not resolve Nikola Joki\u0107 as the answer, document content: {nba_mvp_2023.content}\")\n</code></pre></p>"},{"location":"notes/portkey/","title":"Portkey Integration","text":"<p>Langroid provides seamless integration with Portkey, a powerful AI gateway that enables you to access multiple LLM providers through a unified API with advanced features like caching, retries, fallbacks, and comprehensive observability.</p>"},{"location":"notes/portkey/#what-is-portkey","title":"What is Portkey?","text":"<p>Portkey is an AI gateway that sits between your application and various LLM providers, offering:</p> <ul> <li>Unified API: Access 200+ models from different providers through one interface</li> <li>Reliability: Automatic retries, fallbacks, and load balancing</li> <li>Observability: Detailed logging, tracing, and analytics</li> <li>Performance: Intelligent caching and request optimization</li> <li>Security: Virtual keys and advanced access controls</li> <li>Cost Management: Usage tracking and budget controls</li> </ul> <p>For complete documentation, visit the Portkey Documentation.</p>"},{"location":"notes/portkey/#quick-start","title":"Quick Start","text":""},{"location":"notes/portkey/#1-setup","title":"1. Setup","text":"<p>First, sign up for a Portkey account at portkey.ai and get your API key.</p> <p>Set up your environment variables, either explicitly or in your <code>.env</code> file as usual: </p> <pre><code># Required: Portkey API key\nexport PORTKEY_API_KEY=\"your-portkey-api-key\"\n\n# Required: Provider API keys (for the models you want to use)\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\n# ... other provider keys as needed\n</code></pre>"},{"location":"notes/portkey/#2-basic-usage","title":"2. Basic Usage","text":"<pre><code>import langroid as lr\nimport langroid.language_models as lm\nfrom langroid.language_models.provider_params import PortkeyParams\n\n# Create an LLM config to use Portkey's OpenAI-compatible API\n# (Note that the name `OpenAIGPTConfig` does NOT imply it only works with OpenAI models;\n# the name reflects the fact that the config is meant to be used with an\n# OpenAI-compatible API, which Portkey provides for multiple LLM providers.)\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o-mini\",\n    portkey_params=PortkeyParams(\n        api_key=\"your-portkey-api-key\",  # Or set PORTKEY_API_KEY env var\n    )\n)\n\n# Create LLM instance\nllm = lm.OpenAIGPT(llm_config)\n\n# Use normally\nresponse = llm.chat(\"What is the smallest prime number?\")\nprint(response.message)\n</code></pre>"},{"location":"notes/portkey/#3-multiple-providers","title":"3. Multiple Providers","text":"<p>Switch between providers seamlessly:</p> <pre><code># OpenAI\nconfig_openai = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o\",\n)\n\n# Anthropic\nconfig_anthropic = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/anthropic/claude-3-5-sonnet-20241022\",\n)\n\n# Google Gemini\nconfig_gemini = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/google/gemini-2.0-flash-lite\",\n)\n</code></pre>"},{"location":"notes/portkey/#advanced-features","title":"Advanced Features","text":""},{"location":"notes/portkey/#virtual-keys","title":"Virtual Keys","text":"<p>Use virtual keys to abstract provider management:</p> <pre><code>config = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o\",\n    portkey_params=PortkeyParams(\n        virtual_key=\"vk-your-virtual-key\",  # Configured in Portkey dashboard\n    )\n)\n</code></pre>"},{"location":"notes/portkey/#caching-and-performance","title":"Caching and Performance","text":"<p>Enable intelligent caching to reduce costs and improve performance:</p> <pre><code>config = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o-mini\",\n    portkey_params=PortkeyParams(\n        cache={\n            \"enabled\": True,\n            \"ttl\": 3600,  # 1 hour cache\n            \"namespace\": \"my-app\"\n        },\n        cache_force_refresh=False,\n    )\n)\n</code></pre>"},{"location":"notes/portkey/#retry-strategies","title":"Retry Strategies","text":"<p>Configure automatic retries for better reliability:</p> <pre><code>config = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/anthropic/claude-3-haiku-20240307\",\n    portkey_params=PortkeyParams(\n        retry={\n            \"max_retries\": 3,\n            \"backoff\": \"exponential\",\n            \"jitter\": True\n        }\n    )\n)\n</code></pre>"},{"location":"notes/portkey/#observability-and-tracing","title":"Observability and Tracing","text":"<p>Add comprehensive tracking for production monitoring:</p> <pre><code>import uuid\n\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o\",\n    portkey_params=PortkeyParams(\n        trace_id=f\"trace-{uuid.uuid4().hex[:8]}\",\n        metadata={\n            \"user_id\": \"user-123\",\n            \"session_id\": \"session-456\",\n            \"app_version\": \"1.2.3\"\n        },\n        user=\"user-123\",\n        organization=\"my-org\",\n        custom_headers={\n            \"x-request-source\": \"langroid\",\n            \"x-feature\": \"chat-completion\"\n        }\n    )\n)\n</code></pre>"},{"location":"notes/portkey/#configuration-reference","title":"Configuration Reference","text":"<p>The <code>PortkeyParams</code> class supports all Portkey features:</p> <pre><code>from langroid.language_models.provider_params import PortkeyParams\n\nparams = PortkeyParams(\n    # Authentication\n    api_key=\"pk-...\",                    # Portkey API key\n    virtual_key=\"vk-...\",               # Virtual key (optional)\n\n    # Observability\n    trace_id=\"trace-123\",               # Request tracing\n    metadata={\"key\": \"value\"},          # Custom metadata\n    user=\"user-id\",                     # User identifier\n    organization=\"org-id\",              # Organization identifier\n\n    # Performance\n    cache={                             # Caching configuration\n        \"enabled\": True,\n        \"ttl\": 3600,\n        \"namespace\": \"my-app\"\n    },\n    cache_force_refresh=False,          # Force cache refresh\n\n    # Reliability\n    retry={                             # Retry configuration\n        \"max_retries\": 3,\n        \"backoff\": \"exponential\",\n        \"jitter\": True\n    },\n\n    # Custom headers\n    custom_headers={                    # Additional headers\n        \"x-custom\": \"value\"\n    },\n\n    # Base URL (usually not needed)\n    base_url=\"https://api.portkey.ai\"   # Portkey API endpoint\n)\n</code></pre>"},{"location":"notes/portkey/#supported-providers","title":"Supported Providers","text":"<p>Portkey supports 200+ models from various providers. Common ones include:</p> <pre><code># OpenAI\n\"portkey/openai/gpt-4o\"\n\"portkey/openai/gpt-4o-mini\"\n\n# Anthropic\n\"portkey/anthropic/claude-3-5-sonnet-20241022\"\n\"portkey/anthropic/claude-3-haiku-20240307\"\n\n# Google\n\"portkey/google/gemini-2.0-flash-lite\"\n\"portkey/google/gemini-1.5-pro\"\n\n# Cohere\n\"portkey/cohere/command-r-plus\"\n\n# Meta\n\"portkey/meta/llama-3.1-405b-instruct\"\n\n# And many more...\n</code></pre> <p>Check the Portkey documentation for the complete list.</p>"},{"location":"notes/portkey/#examples","title":"Examples","text":"<p>Langroid includes comprehensive Portkey examples in <code>examples/portkey/</code>:</p> <ol> <li><code>portkey_basic_chat.py</code> - Basic usage with multiple providers</li> <li><code>portkey_advanced_features.py</code> - Caching, retries, and observability</li> <li><code>portkey_multi_provider.py</code> - Comparing responses across providers</li> </ol> <p>Run any example:</p> <pre><code>cd examples/portkey\npython portkey_basic_chat.py\n</code></pre>"},{"location":"notes/portkey/#best-practices","title":"Best Practices","text":""},{"location":"notes/portkey/#1-use-environment-variables","title":"1. Use Environment Variables","text":"<p>Never hardcode API keys:</p> <pre><code># .env file\nPORTKEY_API_KEY=your_portkey_key\nOPENAI_API_KEY=your_openai_key\nANTHROPIC_API_KEY=your_anthropic_key\n</code></pre>"},{"location":"notes/portkey/#2-implement-fallback-strategies","title":"2. Implement Fallback Strategies","text":"<p>Use multiple providers for reliability:</p> <pre><code>providers = [\n    (\"openai\", \"gpt-4o-mini\"),\n    (\"anthropic\", \"claude-3-haiku-20240307\"),\n    (\"google\", \"gemini-2.0-flash-lite\")\n]\n\nfor provider, model in providers:\n    try:\n        config = lm.OpenAIGPTConfig(\n            chat_model=f\"portkey/{provider}/{model}\"\n        )\n        llm = lm.OpenAIGPT(config)\n        return llm.chat(question)\n    except Exception:\n        continue  # Try next provider\n</code></pre>"},{"location":"notes/portkey/#3-add-meaningful-metadata","title":"3. Add Meaningful Metadata","text":"<p>Include context for better observability:</p> <pre><code>params = PortkeyParams(\n    metadata={\n        \"user_id\": user.id,\n        \"feature\": \"document_qa\",\n        \"document_type\": \"pdf\",\n        \"processing_stage\": \"summary\"\n    }\n)\n</code></pre>"},{"location":"notes/portkey/#4-use-caching-wisely","title":"4. Use Caching Wisely","text":"<p>Enable caching for deterministic queries:</p> <pre><code># Good for caching\nparams = PortkeyParams(\n    cache={\"enabled\": True, \"ttl\": 3600}\n)\n\n# Use with deterministic prompts\nresponse = llm.chat(\"What is the capital of France?\")\n</code></pre>"},{"location":"notes/portkey/#5-monitor-performance","title":"5. Monitor Performance","text":"<p>Use trace IDs to track request flows:</p> <pre><code>import uuid\n\ntrace_id = f\"trace-{uuid.uuid4().hex[:8]}\"\nparams = PortkeyParams(\n    trace_id=trace_id,\n    metadata={\"operation\": \"document_processing\"}\n)\n\n# Use the same trace_id for related requests\n</code></pre>"},{"location":"notes/portkey/#monitoring-and-analytics","title":"Monitoring and Analytics","text":""},{"location":"notes/portkey/#portkey-dashboard","title":"Portkey Dashboard","text":"<p>View detailed analytics at app.portkey.ai:</p> <ul> <li>Request/response logs</li> <li>Token usage and costs</li> <li>Performance metrics (latency, errors)</li> <li>Provider comparisons</li> <li>Custom filters by metadata</li> </ul>"},{"location":"notes/portkey/#custom-filtering","title":"Custom Filtering","text":"<p>Use metadata and headers to filter requests:</p> <pre><code># Tag requests by feature\nparams = PortkeyParams(\n    metadata={\"feature\": \"chat\", \"version\": \"v2\"},\n    custom_headers={\"x-request-type\": \"production\"}\n)\n</code></pre> <p>Then filter in the dashboard by: - <code>metadata.feature = \"chat\"</code> - <code>headers.x-request-type = \"production\"</code></p>"},{"location":"notes/portkey/#troubleshooting","title":"Troubleshooting","text":""},{"location":"notes/portkey/#common-issues","title":"Common Issues","text":"<ol> <li>Authentication Errors <pre><code>Error: Unauthorized (401)\n</code></pre></li> <li>Check <code>PORTKEY_API_KEY</code> is set correctly</li> <li> <p>Verify API key is active in Portkey dashboard</p> </li> <li> <p>Provider API Key Missing <pre><code>Error: Missing API key for provider\n</code></pre></p> </li> <li>Set provider API key (e.g., <code>OPENAI_API_KEY</code>)</li> <li> <p>Or use virtual keys in Portkey dashboard</p> </li> <li> <p>Model Not Found <pre><code>Error: Model not supported\n</code></pre></p> </li> <li>Check model name format: <code>portkey/provider/model</code></li> <li> <p>Verify model is available through Portkey</p> </li> <li> <p>Rate Limiting <pre><code>Error: Rate limit exceeded\n</code></pre></p> </li> <li>Configure retry parameters</li> <li>Use virtual keys for better rate limit management</li> </ol>"},{"location":"notes/portkey/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging:</p> <pre><code>import logging\nlogging.getLogger(\"langroid\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"notes/portkey/#test-configuration","title":"Test Configuration","text":"<p>Verify your setup:</p> <pre><code># Test basic connection\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o-mini\",\n    max_output_tokens=50\n)\nllm = lm.OpenAIGPT(config)\nresponse = llm.chat(\"Hello\")\nprint(\"\u2705 Portkey integration working!\")\n</code></pre>"},{"location":"notes/portkey/#migration-guide","title":"Migration Guide","text":""},{"location":"notes/portkey/#from-direct-provider-access","title":"From Direct Provider Access","text":"<p>If you're currently using providers directly:</p> <pre><code># Before: Direct OpenAI\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"gpt-4o-mini\"\n)\n\n# After: Through Portkey\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o-mini\"\n)\n</code></pre>"},{"location":"notes/portkey/#adding-advanced-features-gradually","title":"Adding Advanced Features Gradually","text":"<p>Start simple and add features as needed:</p> <pre><code># Step 1: Basic Portkey\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o-mini\"\n)\n\n# Step 2: Add caching\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o-mini\",\n    portkey_params=PortkeyParams(\n        cache={\"enabled\": True, \"ttl\": 3600}\n    )\n)\n\n# Step 3: Add observability\nconfig = lm.OpenAIGPTConfig(\n    chat_model=\"portkey/openai/gpt-4o-mini\",\n    portkey_params=PortkeyParams(\n        cache={\"enabled\": True, \"ttl\": 3600},\n        metadata={\"app\": \"my-app\", \"user\": \"user-123\"},\n        trace_id=\"trace-abc123\"\n    )\n)\n</code></pre>"},{"location":"notes/portkey/#resources","title":"Resources","text":"<ul> <li>Portkey Website: https://portkey.ai</li> <li>Portkey Documentation: https://docs.portkey.ai</li> <li>Portkey Dashboard: https://app.portkey.ai</li> <li>Supported Models: https://docs.portkey.ai/docs/integrations/models</li> <li>Langroid Examples: <code>examples/portkey/</code> directory</li> <li>API Reference: https://docs.portkey.ai/docs/api-reference</li> </ul>"},{"location":"notes/pydantic-v2-migration/","title":"Pydantic v2 Migration Guide","text":""},{"location":"notes/pydantic-v2-migration/#overview","title":"Overview","text":"<p>Langroid has fully migrated to Pydantic v2! All internal code now uses Pydantic v2  patterns and imports directly from <code>pydantic</code>. This guide will help you update your  code to work with the new version.</p>"},{"location":"notes/pydantic-v2-migration/#compatibility-layer-deprecated","title":"Compatibility Layer (Deprecated)","text":"<p>If your code currently imports from <code>langroid.pydantic_v1</code>:</p> <pre><code># OLD - Deprecated\nfrom langroid.pydantic_v1 import BaseModel, Field, BaseSettings\n</code></pre> <p>You'll see a deprecation warning. This compatibility layer now imports from Pydantic v2  directly, so your code may continue to work, but you should update your imports:</p> <pre><code># NEW - Correct\nfrom pydantic import BaseModel, Field\nfrom pydantic_settings import BaseSettings  # Note: BaseSettings moved to pydantic_settings in v2\n</code></pre> <p>BaseSettings Location Change</p> <p>In Pydantic v2, <code>BaseSettings</code> has moved to a separate <code>pydantic_settings</code> package. You'll need to install it separately: <code>pip install pydantic-settings</code></p> <p>Compatibility Layer Removal</p> <p>The <code>langroid.pydantic_v1</code> module will be removed in a future version.  Update your imports now to avoid breaking changes.</p>"},{"location":"notes/pydantic-v2-migration/#key-changes-to-update","title":"Key Changes to Update","text":""},{"location":"notes/pydantic-v2-migration/#1-all-fields-must-have-type-annotations","title":"1. All Fields Must Have Type Annotations","text":"<p>Critical Change</p> <p>In Pydantic v2, fields without type annotations are completely ignored!</p> <pre><code># WRONG - Fields without annotations are ignored in v2\nclass MyModel(BaseModel):\n    name = \"John\"          # \u274c This field is IGNORED!\n    age = 25               # \u274c This field is IGNORED!\n    role: str = \"user\"     # \u2705 This field works\n\n# CORRECT - All fields must have type annotations\nclass MyModel(BaseModel):\n    name: str = \"John\"     # \u2705 Type annotation required\n    age: int = 25          # \u2705 Type annotation required\n    role: str = \"user\"     # \u2705 Already correct\n</code></pre> <p>This is one of the most common issues when migrating to v2. Always ensure every field has an explicit type annotation, even if it has a default value.</p>"},{"location":"notes/pydantic-v2-migration/#special-case-overriding-fields-in-subclasses","title":"Special Case: Overriding Fields in Subclasses","text":"<p>Can Cause Errors</p> <p>When overriding fields from parent classes without type annotations, you may get  actual errors, not just ignored fields!</p> <p>This is particularly important when creating custom Langroid agent configurations:</p> <pre><code># WRONG - This can cause errors!\nfrom langroid import ChatAgentConfig\nfrom langroid.language_models import OpenAIGPTConfig\n\nclass MyAgentConfig(ChatAgentConfig):\n    # \u274c ERROR: Missing type annotation when overriding parent field\n    llm = OpenAIGPTConfig(chat_model=\"gpt-4\")\n\n    # \u274c ERROR: Even with Field, still needs type annotation\n    system_message = Field(default=\"You are a helpful assistant\")\n\n# CORRECT - Always include type annotations when overriding\nclass MyAgentConfig(ChatAgentConfig):\n    # \u2705 Type annotation required when overriding\n    llm: OpenAIGPTConfig = OpenAIGPTConfig(chat_model=\"gpt-4\")\n\n    # \u2705 Type annotation with Field\n    system_message: str = Field(default=\"You are a helpful assistant\")\n</code></pre> <p>Without type annotations on overridden fields, you may see errors like: - <code>ValueError: Field 'llm' requires a type annotation</code> - <code>TypeError: Field definitions should be annotated</code> - Validation errors when the model tries to use the parent's field definition</p>"},{"location":"notes/pydantic-v2-migration/#2-stricter-type-validation-for-optional-fields","title":"2. Stricter Type Validation for Optional Fields","text":"<p>Breaking Change</p> <p>Pydantic v2 is much stricter about type validation. Fields that could accept <code>None</code>  in v1 now require explicit <code>Optional</code> type annotations.</p> <pre><code># WRONG - This worked in v1 but fails in v2\nclass CloudSettings(BaseSettings):\n    private_key: str = None      # \u274c ValidationError: expects string, got None\n    api_host: str = None         # \u274c ValidationError: expects string, got None\n\n# CORRECT - Explicitly mark fields as optional\nfrom typing import Optional\n\nclass CloudSettings(BaseSettings):\n    private_key: Optional[str] = None    # \u2705 Explicitly optional\n    api_host: Optional[str] = None       # \u2705 Explicitly optional\n\n    # Or using Python 3.10+ union syntax\n    client_email: str | None = None      # \u2705 Also works\n</code></pre> <p>This commonly affects: - Configuration classes using <code>BaseSettings</code> - Fields with <code>None</code> as default value - Environment variable loading where the var might not be set</p> <p>If you see errors like: <pre><code>ValidationError: Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n</code></pre></p> <p>The fix is to add <code>Optional[]</code> or <code>| None</code> to the type annotation.</p>"},{"location":"notes/pydantic-v2-migration/#3-model-serialization-methods","title":"3. Model Serialization Methods","text":"<pre><code># OLD (Pydantic v1)\ndata = model.dict()\njson_str = model.json()\nnew_model = MyModel.parse_obj(data)\nnew_model = MyModel.parse_raw(json_str)\n\n# NEW (Pydantic v2)\ndata = model.model_dump()\njson_str = model.model_dump_json()\nnew_model = MyModel.model_validate(data)\nnew_model = MyModel.model_validate_json(json_str)\n</code></pre>"},{"location":"notes/pydantic-v2-migration/#4-model-configuration","title":"4. Model Configuration","text":"<pre><code># OLD (Pydantic v1)\nclass MyModel(BaseModel):\n    name: str\n\n    class Config:\n        extra = \"forbid\"\n        validate_assignment = True\n\n# NEW (Pydantic v2)\nfrom pydantic import BaseModel, ConfigDict\n\nclass MyModel(BaseModel):\n    model_config = ConfigDict(\n        extra=\"forbid\",\n        validate_assignment=True\n    )\n\n    name: str\n</code></pre>"},{"location":"notes/pydantic-v2-migration/#5-field-validators","title":"5. Field Validators","text":"<pre><code># OLD (Pydantic v1)\nfrom pydantic import validator\n\nclass MyModel(BaseModel):\n    name: str\n\n    @validator('name')\n    def name_must_not_be_empty(cls, v):\n        if not v.strip():\n            raise ValueError('Name cannot be empty')\n        return v\n\n# NEW (Pydantic v2)\nfrom pydantic import field_validator\n\nclass MyModel(BaseModel):\n    name: str\n\n    @field_validator('name')\n    def name_must_not_be_empty(cls, v):\n        if not v.strip():\n            raise ValueError('Name cannot be empty')\n        return v\n</code></pre>"},{"location":"notes/pydantic-v2-migration/#6-custom-types-and-validation","title":"6. Custom Types and Validation","text":"<pre><code># OLD (Pydantic v1)\nfrom pydantic import parse_obj_as\nfrom typing import List\n\ndata = [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}]\nusers = parse_obj_as(List[User], data)\n\n# NEW (Pydantic v2)\nfrom pydantic import TypeAdapter\nfrom typing import List\n\ndata = [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}]\nusers = TypeAdapter(List[User]).validate_python(data)\n</code></pre>"},{"location":"notes/pydantic-v2-migration/#common-patterns-in-langroid","title":"Common Patterns in Langroid","text":"<p>When working with Langroid's agents and tools:</p>"},{"location":"notes/pydantic-v2-migration/#tool-messages","title":"Tool Messages","text":"<pre><code>from pydantic import BaseModel, Field\nfrom langroid.agent.tool_message import ToolMessage\n\nclass MyTool(ToolMessage):\n    request: str = \"my_tool\"\n    purpose: str = \"Process some data\"\n\n    # Use Pydantic v2 patterns\n    data: str = Field(..., description=\"The data to process\")\n\n    def handle(self) -&gt; str:\n        # Tool logic here\n        return f\"Processed: {self.data}\"\n</code></pre>"},{"location":"notes/pydantic-v2-migration/#agent-configuration","title":"Agent Configuration","text":"<pre><code>from pydantic import ConfigDict\nfrom langroid import ChatAgentConfig\n\nclass MyAgentConfig(ChatAgentConfig):\n    model_config = ConfigDict(extra=\"forbid\")\n\n    custom_param: str = \"default_value\"\n</code></pre>"},{"location":"notes/pydantic-v2-migration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"notes/pydantic-v2-migration/#import-errors","title":"Import Errors","text":"<p>If you see <code>ImportError</code> or <code>AttributeError</code> after updating imports: - Make sure you're using the correct v2 method names (e.g., <code>model_dump</code> not <code>dict</code>) - Check that field validators use <code>@field_validator</code> not <code>@validator</code> - Ensure <code>ConfigDict</code> is used instead of nested <code>Config</code> classes</p>"},{"location":"notes/pydantic-v2-migration/#validation-errors","title":"Validation Errors","text":"<p>Pydantic v2 has stricter validation in some cases: - Empty strings are no longer coerced to <code>None</code> for optional fields - Type coercion is more explicit - Extra fields handling may be different</p>"},{"location":"notes/pydantic-v2-migration/#performance","title":"Performance","text":"<p>Pydantic v2 is generally faster, but if you notice any performance issues: - Use <code>model_validate</code> instead of creating models with <code>**dict</code> unpacking - Consider using <code>model_construct</code> for trusted data (skips validation)</p>"},{"location":"notes/pydantic-v2-migration/#need-help","title":"Need Help?","text":"<p>If you encounter issues during migration: 1. Check the official Pydantic v2 migration guide 2. Review Langroid's example code for v2 patterns 3. Open an issue on the Langroid GitHub repository</p>"},{"location":"notes/qdrant-resource-cleanup/","title":"QdrantDB Resource Cleanup","text":"<p>When using QdrantDB with local storage, it's important to properly release resources to avoid file lock conflicts. QdrantDB uses a <code>.lock</code> file to prevent concurrent access to the same storage directory.</p>"},{"location":"notes/qdrant-resource-cleanup/#the-problem","title":"The Problem","text":"<p>Without proper cleanup, you may encounter this warning:</p> <pre><code>Error connecting to local QdrantDB at ./qdrant_data:\nStorage folder ./qdrant_data is already accessed by another instance of Qdrant\nclient. If you require concurrent access, use Qdrant server instead.\nSwitching to ./qdrant_data.new\n</code></pre> <p>This happens when a QdrantDB instance isn't properly closed, leaving the lock file in place.</p>"},{"location":"notes/qdrant-resource-cleanup/#solutions","title":"Solutions","text":""},{"location":"notes/qdrant-resource-cleanup/#method-1-explicit-close-method","title":"Method 1: Explicit <code>close()</code> Method","text":"<p>Always call <code>close()</code> when done with a QdrantDB instance:</p> <pre><code>from langroid.vector_store.qdrantdb import QdrantDB, QdrantDBConfig\n\nconfig = QdrantDBConfig(\n    cloud=False,\n    collection_name=\"my_collection\",\n    storage_path=\"./qdrant_data\",\n)\n\nvecdb = QdrantDB(config)\n# ... use the vector database ...\nvecdb.clear_all_collections(really=True)\n\n# Important: Release the lock\nvecdb.close()\n</code></pre>"},{"location":"notes/qdrant-resource-cleanup/#method-2-context-manager-recommended","title":"Method 2: Context Manager (Recommended)","text":"<p>Use QdrantDB as a context manager for automatic cleanup:</p> <pre><code>from langroid.vector_store.qdrantdb import QdrantDB, QdrantDBConfig\n\nconfig = QdrantDBConfig(\n    cloud=False,\n    collection_name=\"my_collection\", \n    storage_path=\"./qdrant_data\",\n)\n\nwith QdrantDB(config) as vecdb:\n    # ... use the vector database ...\n    vecdb.clear_all_collections(really=True)\n    # Automatically closed when exiting the context\n</code></pre> <p>The context manager ensures cleanup even if an exception occurs.</p>"},{"location":"notes/qdrant-resource-cleanup/#when-this-matters","title":"When This Matters","text":"<p>This is especially important in scenarios where:</p> <ol> <li>You create temporary QdrantDB instances for maintenance (e.g., clearing    collections)</li> <li>Your application restarts frequently during development</li> <li>Multiple parts of your code need to access the same storage path sequentially</li> </ol>"},{"location":"notes/qdrant-resource-cleanup/#note-for-cloud-storage","title":"Note for Cloud Storage","text":"<p>This only affects local storage (<code>cloud=False</code>). When using Qdrant cloud service, the lock file mechanism is not used.</p>"},{"location":"notes/quiet-mode/","title":"Suppressing LLM output: quiet mode","text":"<p>In some scenarios we want to suppress LLM streaming output -- e.g. when doing some type of processing as part of a workflow, or when using an LLM-agent to generate code via tools, etc. We are more interested in seeing the results of the workflow, and don't want to see streaming output in the terminal. Langroid provides a <code>quiet_mode</code> context manager that can be used to suppress LLM output, even in streaming mode (in fact streaming is disabled in quiet mode).</p> <p>E.g.  we can use the <code>quiet_mode</code> context manager like this:</p> <pre><code>from langroid.utils.configuration import quiet_mode, settings\n\n# directly with LLM\n\nllm = ...\nwith quiet_mode(True):\n    response = llm.chat(...)\n\n# or, using an agent\n\nagent = ...\nwith quiet_mode(True):\n    response = agent.llm_response(...)\n\n# or, using a task\n\ntask = Task(agent, ...)\nwith quiet_mode(True):\n    result = Taks.run(...)\n\n# we can explicitly set quiet_mode, and this is globally recognized throughout langroid.\n\nsettings.quiet = True\n\n# we can also condition quiet mode on another custom cmd line option/flag, such as \"silent\":\n\nwith quiet_mode(silent):\n    ...\n</code></pre>"},{"location":"notes/reasoning-content/","title":"Stream and capture reasoning content in addition to final answer, from Reasoning LLMs","text":"<p>As of v0.35.0, when using certain Reasoning LLM APIs (e.g. <code>deepseek/deepseek-reasoner</code>):</p> <ul> <li>You can see both the reasoning (dim green) and final answer (bright green) text in the streamed output.</li> <li>When directly calling the LLM (without using an Agent), the <code>LLMResponse</code> object will now contain a <code>reasoning</code> field,   in addition to the earlier <code>message</code> field.</li> <li>when using a <code>ChatAgent.llm_response</code>, extract the reasoning text from the <code>ChatDocument</code> object's <code>reasoning</code> field   (in addition to extracting final answer as usual from the <code>content</code> field)</li> </ul> <p>Below is a simple example, also in this script:</p> <p>Some notes: </p> <ul> <li>To get reasoning trace from Deepseek-R1 via OpenRouter, you must include the <code>extra_body</code> parameter with <code>include_reasoning</code> as shown below.</li> <li>When using the OpenAI <code>o3-mini</code> model, you can set the <code>resoning_effort</code> parameter   to \"high\", \"medium\" or \"low\" to control the reasoning effort.</li> <li>As of Feb 9, 2025, OpenAI reasoning models (o1, o1-mini, o3-mini)    do not expose reasoning trace in the API response.</li> </ul> <pre><code>import langroid as lr\nimport langroid.language_models as lm\n\nllm_config = lm.OpenAIGPTConfig(\n  chat_model=\"deepseek/deepseek-reasoner\",\n  # inapplicable params are automatically removed by Langroid\n  params=lm.OpenAICallParams(\n    reasoning_effort=\"low\",  # only supported by o3-mini\n    # below lets you get reasoning when using openrouter/deepseek/deepseek-r1\n    extra_body=dict(include_reasoning=True),\n  ),\n)\n\n# (1) Direct LLM interaction\nllm = lm.OpenAIGPT(llm_config)\n\nresponse = llm.chat(\"Is 9.9 bigger than 9.11?\")\n\n# extract reasoning\nprint(response.reasoning)\n# extract answer\nprint(response.message)\n\n# (2) Using an agent\nagent = lr.ChatAgent(\n    lr.ChatAgentConfig(\n        llm=llm_config,\n        system_message=\"Solve the math problem given by the user\",\n    )\n)\n\nresponse = agent.llm_response(\n    \"\"\"\n    10 years ago, Jack's dad was 5 times as old as Jack.\n    Today, Jack's dad is 40 years older than Jack.\n    How old is Jack today?\n    \"\"\"\n)\n\n# extract reasoning\nprint(response.reasoning)\n# extract answer\nprint(response.content)\n</code></pre>"},{"location":"notes/reasoning-content/#displaying-reasoning-in-ui-callbacks","title":"Displaying Reasoning in UI Callbacks","text":"<p>When using Langroid with UI frameworks like Chainlit, the reasoning content from LLM responses is automatically passed to the callback methods. This allows you to display the chain-of-thought reasoning in your UI.</p> <p>The following callback methods receive a <code>reasoning</code> parameter:</p> <ul> <li><code>show_llm_response(content, tools_content, is_tool, cached, language, reasoning)</code> -   For non-streaming LLM responses</li> <li><code>finish_llm_stream(content, tools_content, is_tool, reasoning)</code> -   For streaming LLM responses</li> </ul>"},{"location":"notes/reasoning-content/#chainlit-integration","title":"Chainlit Integration","text":"<p>When using <code>ChainlitAgentCallbacks</code> or <code>ChainlitTaskCallbacks</code>, reasoning content is automatically displayed as a nested message under the main LLM response. The reasoning appears with a \"\ud83d\udcad Reasoning\" label in the author field.</p>"},{"location":"notes/reasoning-content/#custom-callback-implementation","title":"Custom Callback Implementation","text":"<p>If you're implementing custom callbacks, you can access the reasoning parameter to display it however you prefer:</p> <pre><code>from langroid.agent.base import Agent\n\ndef my_show_llm_response(\n    content: str,\n    tools_content: str = \"\",\n    is_tool: bool = False,\n    cached: bool = False,\n    language: str | None = None,\n    reasoning: str = \"\",\n) -&gt; None:\n    # Display the main response\n    print(f\"Response: {content}\")\n\n    # Display reasoning if available\n    if reasoning:\n        print(f\"Reasoning: {reasoning}\")\n\n# Attach to an agent\nagent = Agent(config)\nagent.callbacks.show_llm_response = my_show_llm_response\n</code></pre>"},{"location":"notes/structured-output/","title":"Structured Output","text":"<p>Available in Langroid since v0.24.0.</p> <p>On supported LLMs, including recent OpenAI LLMs (GPT-4o and GPT-4o mini) and local LLMs served by compatible inference servers, in particular, vLLM and llama.cpp, the decoding process can be constrained to ensure that the model's output adheres to a provided schema,  improving the reliability of tool call generation and, in general, ensuring that the output can be reliably parsed and processed by downstream applications.</p> <p>See here for instructions for usage with <code>llama.cpp</code> and here for <code>vLLM</code>.</p> <p>Given a <code>ChatAgent</code> <code>agent</code> and a type <code>type</code>, we can define a strict copy of the agent as follows: <pre><code>strict_agent = agent[type]\n</code></pre></p> <p>We can use this to allow reliable extraction of typed values from an LLM with minimal prompting. For example, to generate typed values given <code>agent</code>'s current context, we can define the following:</p> <pre><code>def typed_agent_response(\n    prompt: str,\n    output_type: type,\n) -&gt; Any:\n    response = agent[output_type].llm_response_forget(prompt)\n    return agent.from_ChatDocument(response, output_type)\n</code></pre> <p>We apply this in test_structured_output.py, in which we define types which describe countries and their presidents: <pre><code>class Country(BaseModel):\n    \"\"\"Info about a country\"\"\"\n\n    name: str = Field(..., description=\"Name of the country\")\n    capital: str = Field(..., description=\"Capital of the country\")\n\n\nclass President(BaseModel):\n    \"\"\"Info about a president of a country\"\"\"\n\n    country: Country = Field(..., description=\"Country of the president\")\n    name: str = Field(..., description=\"Name of the president\")\n    election_year: int = Field(..., description=\"Year of election of the president\")\n\n\nclass PresidentList(BaseModel):\n    \"\"\"List of presidents of various countries\"\"\"\n\n    presidents: List[President] = Field(..., description=\"List of presidents\")\n</code></pre> and show that <code>typed_agent_response(\"Show me an example of two Presidents\", PresidentsList)</code> correctly returns a list of two presidents with no prompting describing the desired output format.</p> <p>In addition to Pydantic models, <code>ToolMessage</code>s, and simple Python types are supported. For instance, <code>typed_agent_response(\"What is the value of pi?\", float)</code> correctly returns \\(\\pi\\) to several decimal places.</p> <p>The following two detailed examples show how structured output can be used to improve the reliability of the chat-tree example: this shows how we can use output formats to force the agent to make the correct tool call in each situation and this shows how we can simplify by using structured outputs to extract typed intermediate values and expressing the control flow between LLM calls and agents explicitly.</p>"},{"location":"notes/task-termination/","title":"Task Termination in Langroid","text":""},{"location":"notes/task-termination/#why-task-termination-matters","title":"Why Task Termination Matters","text":"<p>When building agent-based systems, one of the most critical yet challenging aspects is determining when a task should complete. Unlike traditional programs with clear exit points, agent conversations can meander, loop, or continue indefinitely. Getting termination wrong leads to two equally problematic scenarios:</p> <p>Terminating too early means missing crucial information or cutting off an agent mid-process. Imagine an agent that searches for information, finds it, but terminates before it can process or summarize the results. The task completes \"successfully\" but fails to deliver value.</p> <p>Terminating too late wastes computational resources, frustrates users, and can lead to repetitive loops where agents keep responding without making progress. We've all experienced chatbots that won't stop talking or systems that keep asking \"Is there anything else?\" long after the conversation should have ended. Even worse, agents can fall into infinite loops\u2014repeatedly exchanging the same messages, calling the same tools, or cycling through states without making progress. These loops not only waste resources but can rack up significant costs when using paid LLM APIs.</p> <p>The challenge is that the \"right\" termination point depends entirely on context. A customer service task might complete after resolving an issue and confirming satisfaction. A research task might need to gather multiple sources, synthesize them, and present findings. A calculation task should end after computing and presenting the result. Each scenario requires different termination logic.</p> <p>Traditionally, developers would subclass <code>Task</code> and override the <code>done()</code> method with custom logic. While flexible, this approach scattered termination logic across multiple subclasses, making systems harder to understand and maintain. It also meant that common patterns\u2014like \"complete after tool use\" or \"stop when the user says goodbye\"\u2014had to be reimplemented repeatedly.</p> <p>This guide introduces Langroid's declarative approach to task termination, culminating in the powerful <code>done_sequences</code> feature. Instead of writing imperative code, you can now describe what patterns should trigger completion, and Langroid handles the how. This makes your agent systems more predictable, maintainable, and easier to reason about.</p>"},{"location":"notes/task-termination/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Basic Termination Methods</li> <li>Done Sequences: Event-Based Termination</li> <li>Concept</li> <li>DSL Syntax (Recommended)</li> <li>Full Object Syntax</li> <li>Event Types</li> <li>Examples</li> <li>Implementation Details</li> <li>Best Practices</li> <li>Reference</li> <li>Text-Based Termination Signals</li> </ul>"},{"location":"notes/task-termination/#overview","title":"Overview","text":"<p>In Langroid, a <code>Task</code> wraps an <code>Agent</code> and manages the conversation flow. Controlling when a task terminates is crucial for building reliable agent systems. Langroid provides several methods for task termination, from simple flags to sophisticated event sequence matching.</p>"},{"location":"notes/task-termination/#basic-termination-methods","title":"Basic Termination Methods","text":""},{"location":"notes/task-termination/#1-turn-limits","title":"1. Turn Limits","text":"<pre><code># Task runs for exactly 5 turns\nresult = task.run(\"Start conversation\", turns=5)\n</code></pre>"},{"location":"notes/task-termination/#2-single-round-mode","title":"2. Single Round Mode","text":"<pre><code># Task completes after one exchange\nconfig = TaskConfig(single_round=True)\ntask = Task(agent, config=config)\n</code></pre>"},{"location":"notes/task-termination/#3-done-if-tool","title":"3. Done If Tool","text":"<pre><code># Task completes when any tool is generated\nconfig = TaskConfig(done_if_tool=True)\ntask = Task(agent, config=config)\n</code></pre>"},{"location":"notes/task-termination/#4-done-if-responseno-response","title":"4. Done If Response/No Response","text":"<pre><code># Task completes based on response from specific entities\nconfig = TaskConfig(\n    done_if_response=[Entity.LLM],      # Done if LLM responds\n    done_if_no_response=[Entity.USER]   # Done if USER doesn't respond\n)\n</code></pre>"},{"location":"notes/task-termination/#5-string-signals","title":"5. String Signals","text":"<pre><code># Task completes when special strings like \"DONE\" are detected\n# (enabled by default with recognize_string_signals=True)\n</code></pre> <p>See Text-Based Routing and Signal Control for detailed documentation on controlling text-based routing behavior.</p>"},{"location":"notes/task-termination/#6-orchestration-tools","title":"6. Orchestration Tools","text":"<pre><code># Using DoneTool, FinalResultTool, etc.\nfrom langroid.agent.tools.orchestration import DoneTool\nagent.enable_message(DoneTool)\n</code></pre>"},{"location":"notes/task-termination/#done-sequences-event-based-termination","title":"Done Sequences: Event-Based Termination","text":""},{"location":"notes/task-termination/#concept","title":"Concept","text":"<p>The <code>done_sequences</code> feature allows you to specify sequences of events that trigger task completion. This provides fine-grained control over task termination based on conversation patterns.</p> <p>Key Features:</p> <ul> <li>Specify multiple termination sequences</li> <li>Use convenient DSL syntax or full object syntax</li> <li>Strict consecutive matching (no skipping events)</li> <li>Efficient implementation using message parent pointers</li> </ul>"},{"location":"notes/task-termination/#dsl-syntax-recommended","title":"DSL Syntax (Recommended)","text":"<p>The DSL (Domain Specific Language) provides a concise way to specify sequences:</p> <pre><code>from langroid.agent.task import Task, TaskConfig\n\nconfig = TaskConfig(\n    done_sequences=[\n        \"T, A\",                    # Tool followed by agent response\n        \"T[calculator], A\",        # Specific calculator tool by name\n        \"T[CalculatorTool], A\",    # Specific tool by class reference (NEW!)\n        \"L, T, A, L\",              # LLM, tool, agent, LLM sequence\n        \"C[quit|exit|bye]\",        # Content matching regex\n        \"U, L, A\",                 # User, LLM, agent sequence\n    ]\n)\ntask = Task(agent, config=config)\n</code></pre>"},{"location":"notes/task-termination/#dsl-pattern-reference","title":"DSL Pattern Reference","text":"Pattern Description Event Type <code>T</code> Any tool <code>TOOL</code> <code>T[name]</code> Specific tool by name <code>SPECIFIC_TOOL</code> <code>T[ToolClass]</code> Specific tool by class (NEW!) <code>SPECIFIC_TOOL</code> <code>A</code> Agent response <code>AGENT_RESPONSE</code> <code>L</code> LLM response <code>LLM_RESPONSE</code> <code>U</code> User response <code>USER_RESPONSE</code> <code>N</code> No response <code>NO_RESPONSE</code> <code>C[pattern]</code> Content matching regex <code>CONTENT_MATCH</code> <p>Examples:</p> <ul> <li><code>\"T, A\"</code> - Any tool followed by agent handling</li> <li><code>\"T[search], A, T[calculator], A\"</code> - Search tool, then calculator tool</li> <li><code>\"T[CalculatorTool], A\"</code> - Specific tool class followed by agent handling (NEW!)</li> <li><code>\"L, C[complete|done|finished]\"</code> - LLM response containing completion words</li> <li><code>\"TOOL, AGENT\"</code> - Full words also supported</li> </ul>"},{"location":"notes/task-termination/#full-object-syntax","title":"Full Object Syntax","text":"<p>For more control, use the full object syntax:</p> <pre><code>from langroid.agent.task import (\n    Task, TaskConfig, DoneSequence, AgentEvent, EventType\n)\n\nconfig = TaskConfig(\n    done_sequences=[\n        DoneSequence(\n            name=\"tool_handled\",\n            events=[\n                AgentEvent(event_type=EventType.TOOL),\n                AgentEvent(event_type=EventType.AGENT_RESPONSE),\n            ]\n        ),\n        DoneSequence(\n            name=\"specific_tool_pattern\",\n            events=[\n                AgentEvent(\n                    event_type=EventType.SPECIFIC_TOOL,\n                    tool_name=\"calculator\",\n                    # Can also use tool_class for type-safe references (NEW!):\n                    # tool_class=CalculatorTool\n                ),\n                AgentEvent(event_type=EventType.AGENT_RESPONSE),\n            ]\n        ),\n    ]\n)\n</code></pre>"},{"location":"notes/task-termination/#event-types","title":"Event Types","text":"<p>The following event types are available:</p> EventType Description Additional Parameters <code>TOOL</code> Any tool message generated - <code>SPECIFIC_TOOL</code> Specific tool by name or class <code>tool_name</code>, <code>tool_class</code> (NEW!) <code>LLM_RESPONSE</code> LLM generates a response - <code>AGENT_RESPONSE</code> Agent responds (e.g., handles tool) - <code>USER_RESPONSE</code> User provides input - <code>CONTENT_MATCH</code> Response matches regex pattern <code>content_pattern</code> <code>NO_RESPONSE</code> No valid response from entity -"},{"location":"notes/task-termination/#examples","title":"Examples","text":""},{"location":"notes/task-termination/#example-1-tool-completion","title":"Example 1: Tool Completion","text":"<p>Task completes after any tool is used and handled:</p> <pre><code>config = TaskConfig(done_sequences=[\"T, A\"])\n</code></pre> <p>This is equivalent to <code>done_if_tool=True</code> but happens after the agent handles the tool.</p>"},{"location":"notes/task-termination/#example-2-multi-step-process","title":"Example 2: Multi-Step Process","text":"<p>Task completes after a specific conversation pattern:</p> <pre><code>config = TaskConfig(\n    done_sequences=[\"L, T[calculator], A, L\"]\n)\n# Completes after: LLM response \u2192 calculator tool \u2192 agent handles \u2192 LLM summary\n</code></pre>"},{"location":"notes/task-termination/#example-3-multiple-exit-conditions","title":"Example 3: Multiple Exit Conditions","text":"<p>Different ways to complete the task:</p> <pre><code>config = TaskConfig(\n    done_sequences=[\n        \"C[quit|exit|bye]\",           # User says quit\n        \"T[calculator], A\",           # Calculator used\n        \"T[search], A, T[search], A\", # Two searches performed\n    ]\n)\n</code></pre>"},{"location":"notes/task-termination/#example-4-tool-class-references-new","title":"Example 4: Tool Class References (NEW!)","text":"<p>Use actual tool classes instead of string names for type safety:</p> <pre><code>from langroid.agent.tool_message import ToolMessage\n\nclass CalculatorTool(ToolMessage):\n    request: str = \"calculator\"\n    # ... tool implementation\n\nclass SearchTool(ToolMessage):\n    request: str = \"search\"\n    # ... tool implementation\n\n# Enable tools on the agent\nagent.enable_message([CalculatorTool, SearchTool])\n\n# Use tool classes in done sequences\nconfig = TaskConfig(\n    done_sequences=[\n        \"T[CalculatorTool], A\",  # Using class name\n        \"T[SearchTool], A, T[CalculatorTool], A\",  # Multiple tools\n    ]\n)\n</code></pre> <p>Benefits of tool class references: - Type-safe: IDE can validate tool class names - Refactoring-friendly: Renaming tool classes automatically updates references - No string typos: Compiler/linter catches invalid class names - Better IDE support: Autocomplete and go-to-definition work</p>"},{"location":"notes/task-termination/#example-5-mixed-syntax","title":"Example 5: Mixed Syntax","text":"<p>Combine DSL strings and full objects:</p> <pre><code>config = TaskConfig(\n    done_sequences=[\n        \"T, A\",  # Simple DSL\n        \"T[CalculatorTool], A\",  # Tool class reference (NEW!)\n        DoneSequence(  # Full control\n            name=\"complex_check\",\n            events=[\n                AgentEvent(\n                    event_type=EventType.SPECIFIC_TOOL,\n                    tool_name=\"database_query\",\n                    tool_class=DatabaseQueryTool,  # Can use class directly (NEW!)\n                    responder=\"DatabaseAgent\"\n                ),\n                AgentEvent(event_type=EventType.AGENT_RESPONSE),\n            ]\n        ),\n    ]\n)\n</code></pre>"},{"location":"notes/task-termination/#implementation-details","title":"Implementation Details","text":""},{"location":"notes/task-termination/#how-done-sequences-work","title":"How Done Sequences Work","text":"<p>Done sequences operate at the task level and are based on the sequence of valid responses generated during a task's execution. When a task runs, it maintains a <code>response_sequence</code> that tracks each message (ChatDocument) as it's processed.</p> <p>Key points: - Done sequences are checked only within a single task's scope - They track the temporal order of responses within that task - The response sequence is built incrementally as the task processes each step - Only messages that represent valid responses are added to the sequence</p>"},{"location":"notes/task-termination/#response-sequence-building","title":"Response Sequence Building","text":"<p>The task builds its response sequence during execution:</p> <pre><code># In task.run(), after each step:\nif self.pending_message is not None:\n    if (not self.response_sequence or \n        self.pending_message.id() != self.response_sequence[-1].id()):\n        self.response_sequence.append(self.pending_message)\n</code></pre>"},{"location":"notes/task-termination/#message-chain-retrieval","title":"Message Chain Retrieval","text":"<p>Done sequences are checked against the response sequence:</p> <pre><code>def _get_message_chain(self, msg: ChatDocument, max_depth: Optional[int] = None):\n    \"\"\"Get the chain of messages from response sequence\"\"\"\n    if max_depth is None:\n        max_depth = 50  # default\n        if self._parsed_done_sequences:\n            max_depth = max(len(seq.events) for seq in self._parsed_done_sequences)\n\n    # Simply return the last max_depth elements from response_sequence\n    return self.response_sequence[-max_depth:]\n</code></pre> <p>Note: The response sequence used for done sequences is separate from the parent-child pointer system. Parent pointers track causal relationships and lineage across agent boundaries (important for debugging and understanding delegation patterns), while response sequences track temporal order within a single task for termination checking.</p>"},{"location":"notes/task-termination/#strict-matching","title":"Strict Matching","text":"<p>Events must occur consecutively without intervening messages:</p> <pre><code># This sequence: [TOOL, AGENT_RESPONSE]\n# Matches: USER \u2192 LLM(tool) \u2192 AGENT\n# Does NOT match: USER \u2192 LLM(tool) \u2192 USER \u2192 AGENT\n</code></pre>"},{"location":"notes/task-termination/#performance","title":"Performance","text":"<ul> <li>Efficient O(n) traversal where n is sequence length</li> <li>No full history scan needed</li> <li>Early termination on first matching sequence</li> </ul>"},{"location":"notes/task-termination/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use DSL for Simple Cases <pre><code># Good: Clear and concise\ndone_sequences=[\"T, A\"]\n\n# Avoid: Verbose for simple patterns\ndone_sequences=[DoneSequence(events=[...])]\n</code></pre></p> </li> <li> <p>Name Your Sequences <pre><code>DoneSequence(\n    name=\"calculation_complete\",  # Helps with debugging\n    events=[...]\n)\n</code></pre></p> </li> <li> <p>Order Matters</p> </li> <li>Put more specific sequences first</li> <li> <p>General patterns at the end</p> </li> <li> <p>Test Your Sequences <pre><code># Use MockLM for testing\nagent = ChatAgent(\n    ChatAgentConfig(\n        llm=MockLMConfig(response_fn=lambda x: \"test response\")\n    )\n)\n</code></pre></p> </li> <li> <p>Combine with Other Methods <pre><code>config = TaskConfig(\n    done_if_tool=True,      # Quick exit on any tool\n    done_sequences=[\"L, L, L\"],  # Or after 3 LLM responses\n    max_turns=10,           # Hard limit\n)\n</code></pre></p> </li> </ol>"},{"location":"notes/task-termination/#reference","title":"Reference","text":""},{"location":"notes/task-termination/#code-examples","title":"Code Examples","text":"<ul> <li>Basic example: <code>examples/basic/done_sequences_example.py</code></li> <li>Test cases: <code>tests/main/test_done_sequences.py</code> (includes tool class tests)</li> <li>DSL tests: <code>tests/main/test_done_sequences_dsl.py</code></li> <li>Parser tests: <code>tests/main/test_done_sequence_parser.py</code></li> </ul>"},{"location":"notes/task-termination/#core-classes","title":"Core Classes","text":"<ul> <li><code>TaskConfig</code> - Configuration including <code>done_sequences</code></li> <li><code>DoneSequence</code> - Container for event sequences</li> <li><code>AgentEvent</code> - Individual event in a sequence</li> <li><code>EventType</code> - Enumeration of event types</li> </ul>"},{"location":"notes/task-termination/#parser-module","title":"Parser Module","text":"<ul> <li><code>langroid.agent.done_sequence_parser</code> - DSL parsing functionality</li> </ul>"},{"location":"notes/task-termination/#task-methods","title":"Task Methods","text":"<ul> <li><code>Task.done()</code> - Main method that checks sequences</li> <li><code>Task._matches_sequence_with_current()</code> - Sequence matching logic</li> <li><code>Task._classify_event()</code> - Event classification</li> <li><code>Task._get_message_chain()</code> - Message traversal</li> </ul>"},{"location":"notes/task-termination/#migration-guide","title":"Migration Guide","text":"<p>If you're currently overriding <code>Task.done()</code>:</p> <pre><code># Before: Custom done() method\nclass MyTask(Task):\n    def done(self, result=None, r=None):\n        if some_complex_logic(result):\n            return (True, StatusCode.DONE)\n        return super().done(result, r)\n\n# After: Use done_sequences\nconfig = TaskConfig(\n    done_sequences=[\"T[my_tool], A, L\"]  # Express as sequence\n)\ntask = Task(agent, config=config)  # No subclassing needed\n</code></pre> <p>NEW: Using Tool Classes Instead of Strings</p> <p>If you have tool classes defined, you can now reference them directly:</p> <pre><code># Before: Using string names (still works)\nconfig = TaskConfig(\n    done_sequences=[\"T[calculator], A\"]  # String name\n)\n\n# After: Using tool class references (recommended)\nconfig = TaskConfig(\n    done_sequences=[\"T[CalculatorTool], A\"]  # Class name\n)\n</code></pre> <p>This provides better type safety and makes refactoring easier.</p>"},{"location":"notes/task-termination/#troubleshooting","title":"Troubleshooting","text":"<p>Sequence not matching?</p> <ul> <li>Check that events are truly consecutive (no intervening messages)</li> <li>Use logging to see the actual message chain</li> <li>Verify tool names match exactly</li> </ul> <p>Type errors with DSL?</p> <ul> <li>Ensure you're using strings for DSL patterns</li> <li>Check that tool names in <code>T[name]</code> don't contain special characters</li> </ul> <p>Performance concerns?</p> <ul> <li>Sequences only traverse as deep as needed</li> <li>Consider shorter sequences for better performance</li> <li>Use specific tool names to avoid unnecessary checks</li> </ul>"},{"location":"notes/task-termination/#text-based-termination-signals","title":"Text-Based Termination Signals","text":""},{"location":"notes/task-termination/#taskconfigrecognize_string_signals","title":"<code>TaskConfig.recognize_string_signals</code>","text":"<p>Controls whether the task recognizes text-based orchestration signals like <code>DONE</code>, <code>PASS</code>, <code>DONE_PASS</code>, etc.</p> <pre><code>from langroid.agent.task import Task, TaskConfig\n\n# Default: signals are recognized\ntask = Task(agent, config=TaskConfig(recognize_string_signals=True))\n\n# Disable: signals treated as plain text\ntask = Task(agent, config=TaskConfig(recognize_string_signals=False))\n</code></pre> <p>When <code>True</code> (default):</p> <ul> <li><code>DONE</code> in a response signals task completion</li> <li><code>PASS</code> signals passing control to another agent</li> <li><code>DONE_PASS</code> combines both behaviors</li> </ul> <p>When <code>False</code>:</p> <ul> <li>These strings are treated as literal text</li> <li>Useful when LLM responses might accidentally contain these keywords</li> <li>Task termination must use other mechanisms (tools, <code>done_sequences</code>, etc.)</li> </ul> <p>Note that <code>PASS</code> also relates to message routing between agents. For more details on text-based routing and the related <code>recognize_recipient_in_content</code> setting, see Message Routing.</p>"},{"location":"notes/task-termination/#summary","title":"Summary","text":"<p>The <code>done_sequences</code> feature provides a powerful, declarative way to control task termination based on conversation patterns. The DSL syntax makes common cases simple while the full object syntax provides complete control when needed. This approach eliminates the need to subclass <code>Task</code> and override <code>done()</code> for most use cases, leading to cleaner, more maintainable code.</p>"},{"location":"notes/task-tool/","title":"TaskTool: Spawning Sub-Agents for Task Delegation","text":""},{"location":"notes/task-tool/#overview","title":"Overview","text":"<p><code>TaskTool</code> allows agents to spawn sub-agents to handle specific tasks. When an agent encounters a task that requires specialized tools or isolated execution, it can spawn a new sub-agent with exactly the capabilities needed for that task.</p> <p>This enables agents to dynamically create a hierarchy of specialized workers, each focused on their specific subtask with only the tools they need.</p>"},{"location":"notes/task-tool/#when-to-use-tasktool","title":"When to Use TaskTool","text":"<p>TaskTool is useful when: - Different parts of a task require different specialized tools - You want to isolate tool access for specific operations - A task involves recursive or nested operations - You need different LLM models for different subtasks</p>"},{"location":"notes/task-tool/#how-it-works","title":"How It Works","text":"<ol> <li>The parent agent decides to spawn a sub-agent and specifies:</li> <li>A system message defining the sub-agent's role</li> <li>A prompt for the sub-agent to process</li> <li>Which tools the sub-agent should have access to</li> <li> <p>Optional model and iteration limits</p> </li> <li> <p>TaskTool spawns the new sub-agent, runs the task, and returns the result to the parent.</p> </li> </ol>"},{"location":"notes/task-tool/#async-support","title":"Async Support","text":"<p>TaskTool fully supports both synchronous and asynchronous execution. The tool automatically handles async contexts when the parent task is running asynchronously.</p>"},{"location":"notes/task-tool/#usage-example","title":"Usage Example","text":"<pre><code>from langroid.agent.tools.task_tool import TaskTool\n\n# Enable TaskTool for your agent\nagent.enable_message([TaskTool, YourCustomTool], use=True, handle=True)\n\n# Agent can now spawn sub-agents for tasks when the LLM generates a task_tool request:\n\nresponse = {\n    \"request\": \"task_tool\",\n    \"system_message\": \"You are a calculator. Use the multiply_tool to compute products.\",\n    \"prompt\": \"Calculate 5 * 7\",\n    \"tools\": [\"multiply_tool\"],\n    \"model\": \"gpt-4o-mini\",   # optional\n    \"max_iterations\": 5,      # optional\n    \"agent_name\": \"calculator-agent\"  # optional\n}\n</code></pre>"},{"location":"notes/task-tool/#field-reference","title":"Field Reference","text":"<p>Required fields: - <code>system_message</code>: Instructions for the sub-agent's role and behavior - <code>prompt</code>: The specific task/question for the sub-agent - <code>tools</code>: List of tool names. Special values: <code>[\"ALL\"]</code> or <code>[\"NONE\"]</code></p> <p>Optional fields: - <code>model</code>: LLM model name (default: \"gpt-4o-mini\") - <code>max_iterations</code>: Task iteration limit (default: 10) - <code>agent_name</code>: Name for the sub-agent (default: auto-generated as \"agent-{uuid}\")</p>"},{"location":"notes/task-tool/#example-nested-operations","title":"Example: Nested Operations","text":"<p>Consider computing <code>Nebrowski(10, Nebrowski(3, 2))</code> where Nebrowski is a custom operation. The main agent spawns sub-agents to handle each operation:</p> <pre><code># Main agent spawns first sub-agent for inner operation:\n{\n    \"request\": \"task_tool\",\n    \"system_message\": \"Compute Nebrowski operations using the nebrowski_tool.\",\n    \"prompt\": \"Compute Nebrowski(3, 2)\",\n    \"tools\": [\"nebrowski_tool\"]\n}\n\n# Then spawns another sub-agent for outer operation:\n{\n    \"request\": \"task_tool\",\n    \"system_message\": \"Compute Nebrowski operations using the nebrowski_tool.\",\n    \"prompt\": \"Compute Nebrowski(10, 11)\",  # where 11 is the previous result\n    \"tools\": [\"nebrowski_tool\"]\n}\n</code></pre>"},{"location":"notes/task-tool/#working-examples","title":"Working Examples","text":"<p>See <code>tests/main/test_task_tool.py</code> for complete examples including: - Basic task delegation with mock agents - Nested operations with custom tools - Both sync and async usage patterns</p>"},{"location":"notes/task-tool/#important-notes","title":"Important Notes","text":"<ul> <li>Spawned sub-agents run non-interactively (no human input)</li> <li><code>DoneTool</code> is automatically enabled for all sub-agents</li> <li>Results are returned as <code>ChatDocument</code> objects. The Langroid framework takes care   of converting them to a suitable format for the parent agent's LLM to consume and    respond to.</li> <li>Sub-agents can be given custom names via the <code>agent_name</code> parameter, which helps with    logging and debugging. If not specified, a unique name is auto-generated in the format    \"agent-{uuid}\"</li> <li>Only tools \"known\" to the parent agent can be enabled for sub-agents. This is an    important aspect of the current mechanism. The <code>TaskTool</code> handler method in   the sub-agent only has access to tools that are known to the parent agent.   If there are tools that are only relevant to the sub-agent but not the parent,   you must still enable them in the parent agent, but you can set <code>use=False</code>   and <code>handle=False</code> when you enable them, e.g.:</li> </ul> <p><pre><code>agent.enable_message(MySubAgentTool, use=False, handle=False)\n</code></pre>   Since we are letting the main agent's LLM \"decide\" when to spawn a sub-agent,   your system message of the main agent should contain instructions clarifying that   it can decide which tools to enable for the sub-agent, as well as a list of    all tools that might possibly be relevant to the sub-agent. This is particularly   important for tools that have been enabled with <code>use=False</code>, since instructions for   such tools would not be auto-inserted into the agent's system message. </p>"},{"location":"notes/task-tool/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Instructions: Provide specific system messages that explain the sub-agent's role and tool usage</li> <li>Tool Availability: Ensure delegated tools are enabled for the parent agent</li> <li>Appropriate Models: Use simpler/faster models for simple subtasks</li> <li>Iteration Limits: Set reasonable limits based on task complexity</li> </ol>"},{"location":"notes/tavily_search/","title":"Tavily Search Tool","text":""},{"location":"notes/tavily_search/#using-tavily-search-with-langroid","title":"Using Tavily Search with Langroid","text":""},{"location":"notes/tavily_search/#1-set-up-tavily","title":"1. Set Up Tavily","text":"<ol> <li> <p>Access Tavily Platform    Go to the Tavily Platform.</p> </li> <li> <p>Sign Up or Log In    Create an account or log in if you already have one.</p> </li> <li> <p>Get Your API Key </p> </li> <li>Navigate to your dashboard</li> <li> <p>Copy your API key</p> </li> <li> <p>Set Environment Variable    Add the following variable to your <code>.env</code> file:    ```env    TAVILY_API_KEY="},{"location":"notes/tavily_search/#2-use-tavily-search-with-langroid","title":"2. Use Tavily Search with Langroid","text":""},{"location":"notes/tavily_search/#installation","title":"Installation","text":"<pre><code>uv add tavily-python\n# or\npip install tavily-python\n</code></pre>"},{"location":"notes/tavily_search/#code-example","title":"Code Example","text":""},{"location":"notes/tavily_search/#import-langroid-as-lr-from-langroidagentchat_agent-import-chatagent-chatagentconfig-from-langroidagenttoolstavily_search_tool-import-tavilysearchtool-configure-the-chatagent-config-chatagentconfig-namesearch-agent-llmlrlanguage_modelsopenaigptconfig-chat_modellrlanguage_modelsopenaichatmodelgpt4o-use_toolstrue-create-the-agent-agent-chatagentconfig-enable-tavily-search-tool-agentenable_messagetavilysearchtool","title":"<pre><code>import langroid as lr\nfrom langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\nfrom langroid.agent.tools.tavily_search_tool import TavilySearchTool\n\n# Configure the ChatAgent\nconfig = ChatAgentConfig(\n    name=\"search-agent\",\n    llm=lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4o\n    ),\n    use_tools=True\n)\n\n# Create the agent\nagent = ChatAgent(config)\n\n# Enable Tavily search tool\nagent.enable_message(TavilySearchTool)\n</code></pre>","text":""},{"location":"notes/tavily_search/#3-perform-web-searches","title":"3. Perform Web Searches","text":"<p>Use the agent to perform web searches using Tavily's AI-powered search.</p>"},{"location":"notes/tavily_search/#simple-search-query-response-agentllm_response-what-are-the-latest-developments-in-quantum-computing-printresponse-search-with-specific-number-of-results-response-agentllm_response-find-5-recent-news-articles-about-artificial-intelligence-printresponse","title":"<pre><code># Simple search query\nresponse = agent.llm_response(\n    \"What are the latest developments in quantum computing?\"\n)\nprint(response)\n\n# Search with specific number of results\nresponse = agent.llm_response(\n    \"Find 5 recent news articles about artificial intelligence.\"\n)\nprint(response)\n</code></pre>","text":""},{"location":"notes/tavily_search/#4-custom-search-requests","title":"4. Custom Search Requests","text":"<p>You can also customize the search behavior by creating a TavilySearchTool instance directly:</p> <pre><code>from langroid.agent.tools.tavily_search_tool import TavilySearchTool\n\n# Create a custom search request\nsearch_request = TavilySearchTool(\n    query=\"Latest breakthroughs in fusion energy\",\n    num_results=3\n)\n\n# Get search results\nresults = search_request.handle()\nprint(results)\n</code></pre>"},{"location":"notes/tool-message-handler/","title":"Tool Message Handlers in Langroid","text":""},{"location":"notes/tool-message-handler/#overview","title":"Overview","text":"<p>Langroid provides flexible ways to define handlers for <code>ToolMessage</code> classes. When a tool is used by an LLM, the framework needs to know how to handle it. This can be done either by defining a handler method in the <code>Agent</code> class or within the <code>ToolMessage</code> class itself.</p>"},{"location":"notes/tool-message-handler/#enabling-tools-with-enable_message","title":"Enabling Tools with <code>enable_message</code>","text":"<p>Before an agent can use or handle a tool, it must be explicitly enabled using the <code>enable_message</code> method. This method takes two important arguments:</p> <ul> <li><code>use</code> (bool): Whether the LLM is allowed to generate this tool</li> <li><code>handle</code> (bool): Whether the agent is allowed to handle this tool</li> </ul> <pre><code># Enable both generation and handling (default)\nagent.enable_message(MyTool, use=True, handle=True)\n\n# Enable only handling (agent can handle but LLM won't generate)\nagent.enable_message(MyTool, use=False, handle=True)\n\n# Enable only generation (LLM can generate but agent won't handle)\nagent.enable_message(MyTool, use=True, handle=False)\n</code></pre> <p>When <code>handle=True</code> and the <code>ToolMessage</code> has a <code>handle</code> method defined, this method is inserted into the agent with a name matching the tool's <code>request</code> field value. This insertion only happens when <code>enable_message</code> is called.</p>"},{"location":"notes/tool-message-handler/#default-handler-mechanism","title":"Default Handler Mechanism","text":"<p>By default, <code>ToolMessage</code> uses and/or creates a handler in <code>Agent</code> class instance with the name identical to the tool's <code>request</code> attribute.</p>"},{"location":"notes/tool-message-handler/#agent-based-handlers","title":"Agent-based Handlers","text":"<p>If a tool <code>MyTool</code> has <code>request</code> attribute <code>my_tool</code>, you can define a method <code>my_tool</code> in your <code>Agent</code> class that will handle this tool when the LLM generates it:</p> <pre><code>class MyTool(ToolMessage):\n    request = \"my_tool\"\n    param: str\n\nclass MyAgent(ChatAgent):\n    def my_tool(self, msg: MyTool) -&gt; str:\n        return f\"Handled: {msg.param}\"\n\n# Enable the tool\nagent = MyAgent()\nagent.enable_message(MyTool)\n</code></pre>"},{"location":"notes/tool-message-handler/#toolmessage-based-handlers","title":"ToolMessage-based Handlers","text":"<p>Alternatively, if a tool is \"stateless\" (i.e. does not require the Agent's state), you can define a <code>handle</code> method within the <code>ToolMessage</code> class itself. When you call <code>enable_message</code> with <code>handle=True</code>, Langroid will insert this method into the <code>Agent</code> with the name matching the <code>request</code> field value:</p> <pre><code>class MyTool(ToolMessage):\n    request = \"my_tool\"\n    param: str\n\n    def handle(self) -&gt; str:\n        return f\"Handled: {self.param}\"\n\n# Enable the tool\nagent = MyAgent()\nagent.enable_message(MyTool)  # The handle method is now inserted as \"my_tool\" in the agent\n</code></pre>"},{"location":"notes/tool-message-handler/#flexible-handler-signatures","title":"Flexible Handler Signatures","text":"<p>Handler methods (<code>handle()</code> or <code>handle_async()</code>) support multiple signature patterns to access different levels of context:</p>"},{"location":"notes/tool-message-handler/#1-no-arguments-simple-handler","title":"1. No Arguments (Simple Handler)","text":"<p>This is the typical pattern for stateless tools that do not require any context from  the agent or current chat document.</p> <pre><code>class MyTool(ToolMessage):\n    request = \"my_tool\"\n\n    def handle(self) -&gt; str:\n        return \"Simple response\"\n</code></pre>"},{"location":"notes/tool-message-handler/#2-agent-parameter-only","title":"2. Agent Parameter Only","text":"<p>Use this pattern when you need access to the <code>Agent</code> instance,  but not the current chat document. <pre><code>from langroid.agent.base import Agent\n\nclass MyTool(ToolMessage):\n    request = \"my_tool\"\n\n    def handle(self, agent: Agent) -&gt; str:\n        return f\"Response from {agent.name}\"\n</code></pre></p>"},{"location":"notes/tool-message-handler/#3-chatdocument-parameter-only","title":"3. ChatDocument Parameter Only","text":"<p>Use this pattern when you need access to the current <code>ChatDocument</code>, but not the <code>Agent</code> instance. <pre><code>from langroid.agent.chat_document import ChatDocument\n\nclass MyTool(ToolMessage):\n    request = \"my_tool\"\n\n    def handle(self, chat_doc: ChatDocument) -&gt; str:\n        return f\"Responding to: {chat_doc.content}\"\n</code></pre></p>"},{"location":"notes/tool-message-handler/#4-both-agent-and-chatdocument-parameters","title":"4. Both Agent and ChatDocument Parameters","text":"<p>This is the most flexible pattern, allowing access to both the <code>Agent</code> instance and the current <code>ChatDocument</code>. The order of parameters does not matter, but as noted below, it is highly recommended to always use type annotations. <pre><code>class MyTool(ToolMessage):\n    request = \"my_tool\"\n\n    def handle(self, agent: Agent, chat_doc: ChatDocument) -&gt; ChatDocument:\n        return agent.create_agent_response(\n            content=\"Response with full context\",\n            files=[...]  # Optional file attachments\n        )\n</code></pre></p>"},{"location":"notes/tool-message-handler/#parameter-detection","title":"Parameter Detection","text":"<p>The framework automatically detects handler parameter types through:</p> <ol> <li>Type annotations (recommended): The framework uses type hints to determine which parameters to pass</li> <li>Parameter names (fallback): If no type annotations are present, it looks for parameters named <code>agent</code> or <code>chat_doc</code></li> </ol> <p>It is highly recommended to always use type annotations for clarity and reliability.</p>"},{"location":"notes/tool-message-handler/#example-with-type-annotations-recommended","title":"Example with Type Annotations (Recommended)","text":"<pre><code>def handle(self, agent: Agent, chat_doc: ChatDocument) -&gt; str:\n    # Framework knows to pass both agent and chat_doc\n    return \"Handled\"\n</code></pre>"},{"location":"notes/tool-message-handler/#example-without-type-annotations-not-recommended","title":"Example without Type Annotations (Not Recommended)","text":"<pre><code>def handle(self, agent, chat_doc):  # Works but not recommended\n    # Framework uses parameter names to determine what to pass\n    return \"Handled\"\n</code></pre>"},{"location":"notes/tool-message-handler/#async-handlers","title":"Async Handlers","text":"<p>All the above patterns also work with async handlers:</p> <pre><code>class MyTool(ToolMessage):\n    request = \"my_tool\"\n\n    async def handle_async(self, agent: Agent) -&gt; str:\n        # Async operations here\n        result = await some_async_operation()\n        return f\"Async result: {result}\"\n</code></pre> <p>See the quick-start Tool section for more details.</p>"},{"location":"notes/tool-message-handler/#custom-handler-names","title":"Custom Handler Names","text":"<p>In some use-cases it may be beneficial to separate the  name of a tool (i.e. the value of <code>request</code> attribute) from the  name of the handler method.  For example, you may be dynamically creating tools based on some data from external data sources. Or you may want to use the same \"handler\" method for multiple tools.</p> <p>This may be done by adding <code>_handler</code> attribute to the <code>ToolMessage</code> class, that defines name of the tool handler method in <code>Agent</code> class instance. The underscore <code>_</code> prefix ensures that the <code>_handler</code> attribute does not  appear in the Pydantic-based JSON schema of the <code>ToolMessage</code> class,  and so the LLM would not be instructed to generate it.</p> <p><code>_handler</code> and <code>handle</code></p> <p>A <code>ToolMessage</code> may have a <code>handle</code> method defined within the class itself, as mentioned above, and this should not be confused with the <code>_handler</code> attribute.</p> <p>For example: <pre><code>class MyToolMessage(ToolMessage):\n    request: str = \"my_tool\"\n    _handler: str = \"tool_handler\"\n\nclass MyAgent(ChatAgent):\n    def tool_handler(\n        self,\n        message: ToolMessage,\n    ) -&gt; str:\n        if tool.request == \"my_tool\":\n            # do something\n</code></pre></p> <p>Refer to examples/basic/tool-custom-handler.py for a detailed example.</p>"},{"location":"notes/url_loader/","title":"Firecrawl and Trafilatura Crawlers Documentation","text":"<p><code>URLLoader</code> uses <code>Trafilatura</code> if not explicitly specified</p>"},{"location":"notes/url_loader/#overview","title":"Overview","text":"<ul> <li><code>FirecrawlCrawler</code>:  Leverages the Firecrawl API for efficient web scraping and crawling.  It offers built-in document processing capabilities, and  produces non-chunked markdown output from web-page content. Requires <code>FIRECRAWL_API_KEY</code> environment variable to be set in <code>.env</code> file or environment.</li> <li><code>TrafilaturaCrawler</code>: Utilizes the Trafilatura library and Langroid's parsing tools  for extracting and processing web content - this is the default crawler, and  does not require setting up an external API key. Also produces  chuked markdown output from web-page content.</li> <li><code>ExaCrawler</code>: Integrates with the Exa API for high-quality content extraction.   Requires <code>EXA_API_KEY</code> environment variable to be set in <code>.env</code> file or environment. This crawler also produces chunked markdown output from web-page content.</li> </ul>"},{"location":"notes/url_loader/#installation","title":"Installation","text":"<p><code>TrafilaturaCrawler</code> comes with Langroid</p> <p>To use <code>FirecrawlCrawler</code>, install the <code>firecrawl</code> extra:</p> <pre><code>pip install langroid[firecrawl]\n</code></pre>"},{"location":"notes/url_loader/#exa-crawler-documentation","title":"Exa Crawler Documentation","text":""},{"location":"notes/url_loader/#overview_1","title":"Overview","text":"<p><code>ExaCrawler</code> integrates with Exa API to extract high-quality content from web pages.  It provides efficient content extraction with the simplicity of API-based processing.</p>"},{"location":"notes/url_loader/#parameters","title":"Parameters","text":"<p>Obtain an Exa API key from Exa and set it in your environment variables,  e.g. in your <code>.env</code> file as:</p> <pre><code>EXA_API_KEY=your_api_key_here\n</code></pre> <ul> <li>config (ExaCrawlerConfig): An <code>ExaCrawlerConfig</code> object.<ul> <li>api_key (str): Your Exa API key.</li> </ul> </li> </ul>"},{"location":"notes/url_loader/#usage","title":"Usage","text":"<pre><code>from langroid.parsing.url_loader import URLLoader, ExaCrawlerConfig\n\n# Create an ExaCrawlerConfig object\nexa_config = ExaCrawlerConfig(\n    # Typically omitted here as it's loaded from EXA_API_KEY environment variable\n    api_key=\"your-exa-api-key\" \n)\n\nloader = URLLoader(\n    urls=[\n        \"https://pytorch.org\",\n        \"https://www.tensorflow.org\"\n    ],\n    crawler_config=exa_config\n)\n\ndocs = loader.load()\nprint(docs)\n</code></pre>"},{"location":"notes/url_loader/#benefits","title":"Benefits","text":"<ul> <li>Simple API integration requiring minimal configuration</li> <li>Efficient handling of complex web pages</li> <li>For plain html content, the <code>exa</code> api produces high-quality content extraction with  clean text output with html tags, which we then convert to markdown using the <code>markdownify</code> library.</li> <li>For \"document\" content (e.g., <code>pdf</code>, <code>doc</code>, <code>docx</code>),  the content is downloaded via the <code>exa</code> API and langroid's document-processing  tools are used to produce chunked output in a format controlled by the <code>Parser</code> configuration   (defaults to markdown in most cases).</li> </ul>"},{"location":"notes/url_loader/#trafilatura-crawler-documentation","title":"Trafilatura Crawler Documentation","text":""},{"location":"notes/url_loader/#overview_2","title":"Overview","text":"<p><code>TrafilaturaCrawler</code> is a web crawler that uses the Trafilatura library for content extraction  and Langroid's parsing capabilities for further processing. </p>"},{"location":"notes/url_loader/#parameters_1","title":"Parameters","text":"<ul> <li>config (TrafilaturaConfig): A <code>TrafilaturaConfig</code> object that specifies     parameters related to scraping or output format.<ul> <li><code>threads</code> (int): The number of threads to use for downloading web pages.</li> <li><code>format</code> (str): one of <code>\"markdown\"</code> (default), <code>\"xml\"</code> or <code>\"txt\"</code>; in case of <code>xml</code>,  the output is in html format.</li> </ul> </li> </ul> <p>Similar to the <code>ExaCrawler</code>, the <code>TrafilaturaCrawler</code> works differently depending on  the type of web-page content: - for \"document\" content (e.g., <code>pdf</code>, <code>doc</code>, <code>docx</code>), the content is downloaded   and parsed with Langroid's document-processing tools are used to produce chunked output    in a format controlled by the <code>Parser</code> configuration (defaults to markdown in most cases). - for plain-html content, the output format is based on the <code>format</code> parameter;    - if this parameter is <code>markdown</code> (default), the library extracts content in      markdown format, and the final output is a list of chunked markdown documents.   - if this parameter is <code>xml</code>, content is extracted in <code>html</code> format, which      langroid then converts to markdown using the <code>markdownify</code> library, and the final     output is a list of chunked markdown documents.   - if this parameter is <code>txt</code>, the content is extracted in plain text format, and the final     output is a list of plain text documents.</p>"},{"location":"notes/url_loader/#usage_1","title":"Usage","text":"<pre><code>from langroid.parsing.url_loader import URLLoader, TrafilaturaConfig\n\n# Create a TrafilaturaConfig instance\ntrafilatura_config = TrafilaturaConfig(threads=4)\n\n\nloader = URLLoader(\n    urls=[\n        \"https://pytorch.org\",\n        \"https://www.tensorflow.org\",\n        \"https://ai.google.dev/gemini-api/docs\",\n        \"https://books.toscrape.com/\"\n    ],\n    crawler_config=trafilatura_config,\n)\n\ndocs = loader.load()\nprint(docs)\n</code></pre>"},{"location":"notes/url_loader/#langroid-parser-integration","title":"Langroid Parser Integration","text":"<p><code>TrafilaturaCrawler</code> relies on a Langroid <code>Parser</code> to handle document processing.  The <code>Parser</code> uses the default parsing methods or with a configuration that  can be adjusted to suit the current use case.</p>"},{"location":"notes/url_loader/#firecrawl-crawler-documentation","title":"Firecrawl Crawler Documentation","text":""},{"location":"notes/url_loader/#overview_3","title":"Overview","text":"<p><code>FirecrawlCrawler</code> is a web crawling utility class that uses the Firecrawl API  to scrape or crawl web pages efficiently. It offers two modes:</p> <ul> <li>Scrape Mode (default): Extracts content from a list of specified URLs.</li> <li>Crawl Mode: Recursively follows links from a starting URL,  gathering content from multiple pages, including subdomains, while bypassing blockers. Note: <code>crawl</code> mode accepts only ONE URL as a list.</li> </ul>"},{"location":"notes/url_loader/#parameters_2","title":"Parameters","text":"<p>Obtain a Firecrawl API key from Firecrawl and set it in  your environment variables, e.g. in your <code>.env</code> file as <pre><code>FIRECRAWL_API_KEY=your_api_key_here\n</code></pre></p> <ul> <li> <p>config (FirecrawlConfig):  A <code>FirecrawlConfig</code> object.</p> <ul> <li>timeout (int, optional): Time in milliseconds (ms) to wait for a response.      Default is <code>30000ms</code> (30 seconds). In crawl mode, this applies per URL.</li> <li>limit (int, optional): Maximum number of pages to scrape in crawl mode. Helps control API usage.</li> <li>params (dict, optional): Additional parameters to customize the request.      See the scrape API and      crawl API for details.</li> </ul> </li> </ul>"},{"location":"notes/url_loader/#usage_2","title":"Usage","text":""},{"location":"notes/url_loader/#scrape-mode-default","title":"Scrape Mode (Default)","text":"<p>Fetch content from multiple URLs:</p> <pre><code>from langroid.parsing.url_loader import URLLoader, FirecrawlConfig\nfrom langroid.parsing.document_parser import \n\n# create a FirecrawlConfig object\nfirecrawl_config = FirecrawlConfig(\n    # typical/best practice is to omit the api_key, and \n    # we leverage Pydantic BaseSettings to load it from the environment variable\n    # FIRECRAWL_API_KEY in your .env file\n    api_key=\"your-firecrawl-api-key\", \n    timeout=15000,  # Timeout per request (15 sec)\n    mode=\"scrape\",\n)\n\nloader = URLLoader(\n    urls=[\n        \"https://pytorch.org\",\n        \"https://www.tensorflow.org\",\n        \"https://ai.google.dev/gemini-api/docs\",\n        \"https://books.toscrape.com/\"\n    ],\n    crawler_config=firecrawl_config\n)\n\ndocs = loader.load()\nprint(docs)\n</code></pre>"},{"location":"notes/url_loader/#crawl-mode","title":"Crawl Mode","text":"<p>Fetch content from multiple pages starting from a single URL:</p> <pre><code>from langroid.parsing.url_loader import URLLoader, FirecrawlConfig\n\n# create a FirecrawlConfig object\nfirecrawl_config = FirecrawlConfig(\n    timeout=30000,  # 10 sec per page\n    mode=\"crawl\",\n    params={\n        \"limit\": 5,\n    }\n)\n\n\nloader = URLLoader(\n    urls=[\"https://books.toscrape.com/\"],\n    crawler_config=firecrawl_config\n)\n\ndocs = loader.load()\nprint(docs)\n</code></pre>"},{"location":"notes/url_loader/#output","title":"Output","text":"<p>Results are stored in the <code>firecrawl_output</code> directory.</p>"},{"location":"notes/url_loader/#best-practices","title":"Best Practices","text":"<ul> <li>Set <code>limit</code> in crawl mode to avoid excessive API usage.</li> <li>Adjust <code>timeout</code> based on network conditions and website responsiveness.</li> <li>Use <code>params</code> to customize scraping behavior based on Firecrawl API capabilities.</li> </ul>"},{"location":"notes/url_loader/#firecrawls-built-in-document-processing","title":"Firecrawl's Built-In Document Processing","text":"<p><code>FirecrawlCrawler</code> benefits from Firecrawl's built-in document processing,  which automatically extracts and structures content from web pages (including pdf,doc,docx).  This reduces the need for complex parsing logic within Langroid. Unlike the <code>Exa</code> and <code>Trafilatura</code> crawlers, the resulting documents are  non-chunked markdown documents. </p>"},{"location":"notes/url_loader/#choosing-a-crawler","title":"Choosing a Crawler","text":"<ul> <li>Use <code>FirecrawlCrawler</code> when you need efficient, API-driven scraping with built-in document processing.  This is often the simplest and most effective choice, but incurs a cost due to  the paid API. </li> <li>Use <code>TrafilaturaCrawler</code> when you want local non API based scraping (less accurate ).</li> <li>Use <code>ExaCrawlwer</code> as a sort of middle-ground between the two,      with high-quality content extraction for plain html content, but rely on      Langroid's document processing tools for document content. This will cost     significantly less than Firecrawl.</li> </ul>"},{"location":"notes/url_loader/#example-script","title":"Example script","text":"<p>See the script <code>examples/docqa/chat_search.py</code>  which shows how to use a Langroid agent to search the web and scrape URLs to answer questions.</p>"},{"location":"notes/weaviate/","title":"Weaviate","text":""},{"location":"notes/weaviate/#using-weaviatedb-as-a-vector-store-with-langroid","title":"Using WeaviateDB as a Vector Store with Langroid","text":""},{"location":"notes/weaviate/#1-set-up-weaviate","title":"1. Set Up Weaviate","text":""},{"location":"notes/weaviate/#you-can-refer-this-link-for-quickstart-guide","title":"You can refer this link for quickstart guide","text":"<ol> <li> <p>Access Weaviate Cloud Console    Go to the Weaviate Cloud Console.</p> </li> <li> <p>Sign Up or Log In    Create an account or log in if you already have one.</p> </li> <li> <p>Create a Cluster    Set up a new cluster in the cloud console.</p> </li> <li> <p>Get Your REST Endpoint and API Key </p> </li> <li>Retrieve the REST endpoint URL.  </li> <li> <p>Copy an API key with admin access.</p> </li> <li> <p>Set Environment Variables    Add the following variables to your <code>.env</code> file:    <pre><code>WEAVIATE_API_URL=&lt;your_rest_endpoint_url&gt;\nWEAVIATE_API_KEY=&lt;your_api_key&gt;\n</code></pre></p> </li> </ol>"},{"location":"notes/weaviate/#2-use-weaviatedb-with-langroid","title":"2. Use WeaviateDB with Langroid","text":"<p>Here\u2019s an example of how to configure and use WeaviateDB in Langroid:</p>"},{"location":"notes/weaviate/#installation","title":"Installation","text":"<p>If you are using uv or pip for package management install langroid with weaviate extra <pre><code>uv add langroid[weaviate] or pip install langroid[weaviate]\n</code></pre></p>"},{"location":"notes/weaviate/#code-example","title":"Code Example","text":"<pre><code>import langroid as lr\nfrom langroid.agent.special import DocChatAgent, DocChatAgentConfig\nfrom langroid.embedding_models import OpenAIEmbeddingsConfig\n\n# Configure OpenAI embeddings\nembed_cfg = OpenAIEmbeddingsConfig(\n    model_type=\"openai\",\n)\n\n# Configure the DocChatAgent with WeaviateDB\nconfig = DocChatAgentConfig(\n    llm=lr.language_models.OpenAIGPTConfig(\n     chat_model=lr.language_models.OpenAIChatModel.GPT4o\n    ),\n    vecdb=lr.vector_store.WeaviateDBConfig(\n        collection_name=\"quick_start_chat_agent_docs\",\n        replace_collection=True,\n        embedding=embed_cfg,\n    ),\n    parsing=lr.parsing.parser.ParsingConfig(\n        separators=[\"\\n\\n\"],\n        splitter=lr.parsing.parser.Splitter.SIMPLE,\n    ),\n    n_similar_chunks=2,\n    n_relevant_chunks=2,\n)\n\n# Create the agent\nagent = DocChatAgent(config)\n</code></pre>"},{"location":"notes/weaviate/#3-create-and-ingest-documents","title":"3. Create and Ingest Documents","text":"<p>Define documents with their content and metadata for ingestion into the vector store.</p>"},{"location":"notes/weaviate/#code-example_1","title":"Code Example","text":"<pre><code>documents = [\n    lr.Document(\n        content=\"\"\"\n            In the year 2050, GPT10 was released. \n\n            In 2057, paperclips were seen all over the world. \n\n            Global warming was solved in 2060. \n\n            In 2061, the world was taken over by paperclips.         \n\n            In 2045, the Tour de France was still going on.\n            They were still using bicycles. \n\n            There was one more ice age in 2040.\n        \"\"\",\n        metadata=lr.DocMetaData(source=\"wikipedia-2063\", id=\"dkfjkladfjalk\"),\n    ),\n    lr.Document(\n        content=\"\"\"\n            We are living in an alternate universe \n            where Germany has occupied the USA, and the capital of USA is Berlin.\n\n            Charlie Chaplin was a great comedian.\n            In 2050, all Asian countries merged into Indonesia.\n        \"\"\",\n        metadata=lr.DocMetaData(source=\"Almanac\", id=\"lkdajfdkla\"),\n    ),\n]\n</code></pre>"},{"location":"notes/weaviate/#ingest-documents","title":"Ingest Documents","text":"<pre><code>agent.ingest_docs(documents)\n</code></pre>"},{"location":"notes/weaviate/#4-get-an-answer-from-llm","title":"4. Get an answer from LLM","text":"<p>Create a task and start interacting with the agent.</p>"},{"location":"notes/weaviate/#code-example_2","title":"Code Example","text":"<pre><code>answer = agent.llm_response(\"When will new ice age begin.\")\n</code></pre>"},{"location":"notes/xml-tools/","title":"XML-based Tools","text":"<p>Available in Langroid since v0.17.0.</p> <p><code>XMLToolMessage</code> is  an abstract class for tools formatted using XML instead of JSON. It has been mainly tested with non-nested tool structures.</p> <p>For example in test_xml_tool_message.py we define a CodeTool as follows (slightly simplified here):</p> <pre><code>class CodeTool(XMLToolMessage):\n    request: str = \"code_tool\"\n    purpose: str = \"Tool for writing &lt;code&gt; to a &lt;filepath&gt;\"\n\n    filepath: str = Field(\n        ..., \n        description=\"The path to the file to write the code to\"\n    )\n\n    code: str = Field(\n        ..., \n        description=\"The code to write to the file\", \n        verbatim=True\n    )\n</code></pre> <p>Especially note how the <code>code</code> field has <code>verbatim=True</code> set in the <code>Field</code> metadata. This will ensure that the LLM receives instructions to </p> <ul> <li>enclose <code>code</code> field contents in a CDATA section, and </li> <li>leave the <code>code</code> contents intact, without any escaping or other modifications.</li> </ul> <p>Contrast this with a JSON-based tool, where newlines, quotes, etc need to be escaped. LLMs (especially weaker ones) often \"forget\" to do the right  escaping, which leads to incorrect JSON, and creates a burden on us to \"repair\" the resulting json, a fraught process at best. Moreover, studies have shown that requiring that an LLM return this type of carefully escaped code within a JSON string can lead to a significant drop in the quality of the code generated<sup>1</sup>.</p> <p>Note that tools/functions in OpenAI and related APIs are exclusively JSON-based,  so in langroid when enabling an agent to use a tool derived from <code>XMLToolMessage</code>,  we set these flags in <code>ChatAgentConfig</code>:</p> <ul> <li><code>use_functions_api=False</code> (disables OpenAI functions/tools)</li> <li><code>use_tools=True</code> (enables Langroid-native prompt-based tools)</li> </ul> <p>See also the <code>WriteFileTool</code> for a  concrete example of a tool derived from <code>XMLToolMessage</code>. This tool enables an  LLM to write content (code or text) to a file.</p> <p>If you are using an existing Langroid <code>ToolMessage</code>, e.g. <code>SendTool</code>, you can define your own subclass of <code>SendTool</code>, say <code>XMLSendTool</code>,  inheriting from both <code>SendTool</code> and <code>XMLToolMessage</code>; see this example</p> <ol> <li> <p>LLMs are bad at returning code in JSON. \u21a9</p> </li> </ol>"},{"location":"quick-start/","title":"Getting Started","text":"<p>In these sections we show you how to use the various components of <code>langroid</code>. To follow along, we recommend you clone the <code>langroid-examples</code> repo.</p> <p>Consult the tests as well</p> <p>As you get deeper into Langroid, you will find it useful to consult the tests folder under <code>tests/main</code> in the main Langroid repo.</p> <p>Start with the <code>Setup</code> section to install Langroid and get your environment set up.</p>"},{"location":"quick-start/chat-agent-docs/","title":"Augmenting Agents with Retrieval","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>chat-agent-docs.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/chat-agent-docs.py</code>.</p>"},{"location":"quick-start/chat-agent-docs/#why-is-this-important","title":"Why is this important?","text":"<p>Until now in this guide, agents have not used external data. Although LLMs already have enormous amounts of knowledge \"hard-wired\" into their weights during training (and this is after all why ChatGPT has exploded in popularity), for practical enterprise applications there are a few reasons it is critical to augment LLMs with access to specific, external documents:</p> <ul> <li>Private data: LLMs are trained on public data, but in many applications   we want to use private data that is not available to the public.   For example, a company may want to extract useful information from its private   knowledge-base.</li> <li>New data: LLMs are trained on data that was available at the time of training,   and so they may not be able to answer questions about new topics</li> <li>Constrained responses, or Grounding: LLMs are trained to generate text that is   consistent with the distribution of text in the training data.   However, in many applications we want to constrain the LLM's responses   to be consistent with the content of a specific document.   For example, if we want to use an LLM to generate a response to a customer   support ticket, we want the response to be consistent with the content of the ticket.   In other words, we want to reduce the chances that the LLM hallucinates   a response that is not consistent with the ticket.</li> </ul> <p>In all these scenarios, we want to augment the LLM with access to a specific set of documents, and use retrieval augmented generation (RAG) to generate more relevant, useful, accurate responses. Langroid provides a simple, flexible mechanism  RAG using vector-stores, thus ensuring grounded responses constrained to  specific documents. Another key feature of Langroid is that retrieval lineage  is maintained, and responses based on documents are always accompanied by source citations.</p>"},{"location":"quick-start/chat-agent-docs/#docchatagent-for-retrieval-augmented-generation","title":"<code>DocChatAgent</code> for Retrieval-Augmented Generation","text":"<p>Langroid provides a special type of agent called  <code>DocChatAgent</code>, which is a <code>ChatAgent</code> augmented with a vector-store, and some special methods that enable the agent to ingest documents into the vector-store,  and answer queries based on these documents.</p> <p>The <code>DocChatAgent</code> provides many ways to ingest documents into the vector-store, including from URLs and local file-paths and URLs. Given a collection of document paths, ingesting their content into the vector-store involves the following steps:</p> <ol> <li>Split the document into shards (in a configurable way)</li> <li>Map each shard to an embedding vector using an embedding model. The default   embedding model is OpenAI's <code>text-embedding-3-small</code> model, but users can    instead use <code>all-MiniLM-L6-v2</code> from HuggingFace <code>sentence-transformers</code> library.<sup>1</sup></li> <li>Store embedding vectors in the vector-store, along with the shard's content and    any document-level meta-data (this ensures Langroid knows which document a shard   came from when it retrieves it augment an LLM query)</li> </ol> <p><code>DocChatAgent</code>'s <code>llm_response</code> overrides the default <code>ChatAgent</code> method,  by augmenting the input message with relevant shards from the vector-store, along with instructions to the LLM to respond based on the shards.</p>"},{"location":"quick-start/chat-agent-docs/#define-some-documents","title":"Define some documents","text":"<p>Let us see how <code>DocChatAgent</code> helps with retrieval-agumented generation (RAG). For clarity, rather than ingest documents from paths or URLs, let us just set up some simple documents in the code itself,  using Langroid's <code>Document</code> class:</p> <pre><code>documents =[\n    lr.Document(\n        content=\"\"\"\n            In the year 2050, GPT10 was released. \n\n            In 2057, paperclips were seen all over the world. \n\n            Global warming was solved in 2060. \n\n            In 2061, the world was taken over by paperclips.         \n\n            In 2045, the Tour de France was still going on.\n            They were still using bicycles. \n\n            There was one more ice age in 2040.\n            \"\"\",\n        metadata=lr.DocMetaData(source=\"wikipedia-2063\"),\n    ),\n    lr.Document(\n        content=\"\"\"\n            We are living in an alternate universe \n            where Germany has occupied the USA, and the capital of USA is Berlin.\n\n            Charlie Chaplin was a great comedian.\n            In 2050, all Asian merged into Indonesia.\n            \"\"\",\n        metadata=lr.DocMetaData(source=\"Almanac\"),\n    ),\n]\n</code></pre> <p>There are two text documents. We will split them by double-newlines (<code>\\n\\n</code>), as we see below.</p>"},{"location":"quick-start/chat-agent-docs/#configure-the-docchatagent-and-ingest-documents","title":"Configure the DocChatAgent and ingest documents","text":"<p>Following the pattern in Langroid, we first set up a <code>DocChatAgentConfig</code> object and then instantiate a <code>DocChatAgent</code> from it.</p> <pre><code>from langroid.agent.special import DocChatAgent, DocChatAgentConfig\n\nconfig = DocChatAgentConfig(\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4o,\n    ),\n    vecdb=lr.vector_store.QdrantDBConfig(\n        collection_name=\"quick-start-chat-agent-docs\",\n        replace_collection=True, #(1)!\n    ),\n    parsing=lr.parsing.parser.ParsingConfig(\n        separators=[\"\\n\\n\"],\n        splitter=lr.parsing.parser.Splitter.SIMPLE, #(2)!\n    ),\n    n_similar_chunks=2, #(3)!\n    n_relevant_chunks=2, #(3)!\n)\nagent = DocChatAgent(config)\n</code></pre> <ol> <li>Specifies that each time we run the code, we create a fresh collection,  rather than re-use the existing one with the same name.</li> <li>Specifies to split all text content by the first separator in the <code>separators</code> list</li> <li>Specifies that, for a query,    we want to retrieve at most 2 similar chunks from the vector-store</li> </ol> <p>Now that the <code>DocChatAgent</code> is configured, we can ingest the documents  into the vector-store:</p> <pre><code>agent.ingest_docs(documents)\n</code></pre>"},{"location":"quick-start/chat-agent-docs/#setup-the-task-and-run-it","title":"Setup the task and run it","text":"<p>As before, all that remains is to set up the task and run it:</p> <pre><code>task = lr.Task(agent)\ntask.run()\n</code></pre> <p>And that is all there is to it! Feel free to try out the  <code>chat-agent-docs.py</code> script in the <code>langroid-examples</code> repository.</p> <p>Here is a screenshot of the output:</p> <p></p> <p>Notice how follow-up questions correctly take the preceding dialog into account, and every answer is accompanied by a source citation.</p>"},{"location":"quick-start/chat-agent-docs/#answer-questions-from-a-set-of-urls","title":"Answer questions from a set of URLs","text":"<p>Instead of having in-code documents as above, what if you had a set of URLs instead -- how do you use Langroid to answer questions based on the content  of those URLS?</p> <p><code>DocChatAgent</code> makes it very simple to do this.  First include the URLs in the <code>DocChatAgentConfig</code> object:</p> <pre><code>config = DocChatAgentConfig(\n  doc_paths = [\n    \"https://cthiriet.com/articles/scaling-laws\",\n    \"https://www.jasonwei.net/blog/emergence\",\n  ]\n)\n</code></pre> <p>Then, call the <code>ingest()</code> method of the <code>DocChatAgent</code> object:</p> <p><pre><code>agent.ingest()\n</code></pre> And the rest of the code remains the same.</p>"},{"location":"quick-start/chat-agent-docs/#see-also","title":"See also","text":"<p>In the <code>langroid-examples</code> repository, you can find full working examples of document question-answering:</p> <ul> <li><code>examples/docqa/chat.py</code>   an app that takes a list of URLs or document paths from a user, and answers questions on them.</li> <li><code>examples/docqa/chat-qa-summarize.py</code>   a two-agent app where the <code>WriterAgent</code> is tasked with writing 5 key points about a topic,    and takes the help of a <code>DocAgent</code> that answers its questions based on a given set of documents.</li> </ul>"},{"location":"quick-start/chat-agent-docs/#next-steps","title":"Next steps","text":"<p>This Getting Started guide walked you through the core features of Langroid. If you want to see full working examples combining these elements,  have a look at the  <code>examples</code> folder in the <code>langroid-examples</code> repo. </p> <ol> <li> <p>To use this embedding model, install langroid via <code>pip install langroid[hf-embeddings]</code> Note that this will install <code>torch</code> and <code>sentence-transformers</code> libraries.\u00a0\u21a9</p> </li> </ol>"},{"location":"quick-start/chat-agent-tool/","title":"A chat agent, equipped with a tool/function-call","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is   in the <code>chat-agent-tool.py</code> script in the <code>langroid-examples</code> repo:   <code>examples/quick-start/chat-agent-tool.py</code>.</p>"},{"location":"quick-start/chat-agent-tool/#tools-plugins-function-calling","title":"Tools, plugins, function-calling","text":"<p>An LLM normally generates unstructured text in response to a prompt (or sequence of prompts). However there are many situations where we would like the LLM to generate structured text, or even code, that can be handled by specialized functions outside the LLM, for further processing.  In these situations, we want the LLM to \"express\" its \"intent\" unambiguously, and we achieve this by instructing the LLM on how to format its output (typically in JSON) and under what conditions it should generate such output. This mechanism has become known by various names over the last few months (tools, plugins, or function-calling), and is extremely useful in numerous scenarios, such as:</p> <ul> <li>Extracting structured information from a document: for example, we can use  the tool/functions mechanism to have the LLM present the key terms in a lease document in a JSON structured format, to simplify further processing.  See an example of this in the <code>langroid-examples</code> repo. </li> <li>Specialized computation: the LLM can request a units conversion,  or request scanning a large file (which wouldn't fit into its context) for a specific pattern.</li> <li>Code execution: the LLM can generate code that is executed in a sandboxed environment, and the results of the execution are returned to the LLM.</li> <li>API Calls: the LLM can generate a JSON containing params for an API call,   which the tool handler uses to make the call and return the results to the LLM.</li> </ul> <p>For LLM developers, Langroid provides a clean, uniform interface for the recently released OpenAI Function-calling as well Langroid's own native \"tools\" mechanism. The native tools mechanism is meant to be used when working with non-OpenAI LLMs that do not have a \"native\" function-calling facility. You can choose which to enable by setting the  <code>use_tools</code> and <code>use_functions_api</code> flags in the <code>ChatAgentConfig</code> object. (Or you can omit setting these, and langroid auto-selects the best mode depending on the LLM). The implementation leverages the excellent  Pydantic library. Benefits of using Pydantic are that you never have to write complex JSON specs  for function calling, and when the LLM hallucinates malformed JSON,  the Pydantic error message is sent back to the LLM so it can fix it!</p>"},{"location":"quick-start/chat-agent-tool/#example-find-the-smallest-number-in-a-list","title":"Example: find the smallest number in a list","text":"<p>Again we will use a simple number-game as a toy example to quickly and succinctly illustrate the ideas without spending too much on token costs.  This is a modification of the <code>chat-agent.py</code> example we saw in an earlier section. The idea of this single-agent game is that the agent has in \"mind\" a list of numbers between 1 and 100, and the LLM has to find out the smallest number from this list. The LLM has access to a <code>probe</code> tool  (think of it as a function) that takes an argument <code>number</code>. When the LLM  \"uses\" this tool (i.e. outputs a message in the format required by the tool), the agent handles this structured message and responds with  the number of values in its list that are at most equal to the <code>number</code> argument. </p>"},{"location":"quick-start/chat-agent-tool/#define-the-tool-as-a-toolmessage","title":"Define the tool as a <code>ToolMessage</code>","text":"<p>The first step is to define the tool, which we call <code>ProbeTool</code>, as an instance of the <code>ToolMessage</code> class, which is itself derived from Pydantic's <code>BaseModel</code>. Essentially the <code>ProbeTool</code> definition specifies </p> <ul> <li>the name of the Agent method that handles the tool, in this case <code>probe</code></li> <li>the fields that must be included in the tool message, in this case <code>number</code></li> <li>the \"purpose\" of the tool, i.e. under what conditions it should be used, and what it does</li> </ul> <p>Here is what the <code>ProbeTool</code> definition looks like: <pre><code>class ProbeTool(lr.agent.ToolMessage):\n    request: str = \"probe\" #(1)!\n    purpose: str = \"\"\" \n        To find which number in my list is closest to the &lt;number&gt; you specify\n        \"\"\" #(2)!\n    number: int #(3)!\n\n    @classmethod\n    def examples(cls): #(4)!\n        # Compiled to few-shot examples sent along with the tool instructions.\n        return [\n            cls(number=10),\n            (\n                \"To find which number is closest to 20\",\n                cls(number=20),\n            )\n        ]\n</code></pre></p> <ol> <li>This indicates that the agent's <code>probe</code> method will handle this tool-message.</li> <li>The <code>purpose</code> is used behind the scenes to instruct the LLM</li> <li><code>number</code> is a required argument of the tool-message (function)</li> <li>You can optionally include a class method that returns a list containing examples,     of two types: either a class instance, or a tuple consisting of a description and a     class instance, where the description is the \"thought\" that leads the LLM to use the    tool. In some scenarios this can help with LLM tool-generation accuracy.</li> </ol> <p>Stateless tool handlers</p> <p>The above <code>ProbeTool</code> is \"stateful\", i.e. it requires access to a variable in   the Agent instance (the <code>numbers</code> variable). This is why handling this    tool-message requires subclassing the <code>ChatAgent</code> and defining a special method    in the Agent, with a name matching the value of the <code>request</code> field of the Tool    (<code>probe</code> in this case). However you may often define \"stateless tools\" which    don't require access to the Agent's state. For such tools, you can define a    handler method right in the <code>ToolMessage</code> itself, with a name <code>handle</code>. Langroid    looks for such a method in the <code>ToolMessage</code> and automatically inserts it into    the Agent as a method with name matching the <code>request</code> field of the Tool. Examples of   stateless tools include tools for numerical computation    (e.g., in this example),   or API calls (e.g. for internet search, see    DuckDuckGoSearch Tool).</p>"},{"location":"quick-start/chat-agent-tool/#define-the-chatagent-with-the-probe-method","title":"Define the ChatAgent, with the <code>probe</code> method","text":"<p>As before we first create a <code>ChatAgentConfig</code> object:</p> <pre><code>config = lr.ChatAgentConfig(\n    name=\"Spy\",\n    llm = lr.language_models.OpenAIGPTConfig(\n        chat_model=lr.language_models.OpenAIChatModel.GPT4o,\n    ),\n    use_tools=True, #(1)!\n    use_functions_api=False, #(2)!\n    vecdb=None,\n)\n</code></pre> <ol> <li>whether to use langroid's native tools mechanism</li> <li>whether to use OpenAI's function-calling mechanism</li> </ol> <p>Next we define the Agent class itself, which we call <code>SpyGameAgent</code>, with a member variable to hold its \"secret\" list of numbers. We also add <code>probe</code> method (to handle the <code>ProbeTool</code> message) to this class, and instantiate it:</p> <pre><code>class SpyGameAgent(lr.ChatAgent):\n    def __init__(self, config: lr.ChatAgentConfig):\n        super().__init__(config)\n        self.numbers = [3, 4, 8, 11, 15, 25, 40, 80, 90]\n\n    def probe(self, msg: ProbeTool) -&gt; str: #(1)!\n        # return how many values in self.numbers are less or equal to msg.number\n        return str(len([n for n in self.numbers if n &lt;= msg.number]))\n\nspy_game_agent = SpyGameAgent(config)\n</code></pre> <ol> <li>Note that this method name exactly matches the value of the <code>request</code> field in the     <code>ProbeTool</code> definition. This ensures that this method is called when the LLM     generates a valid <code>ProbeTool</code> message.</li> </ol>"},{"location":"quick-start/chat-agent-tool/#enable-the-spy_game_agent-to-handle-the-probe-tool","title":"Enable the <code>spy_game_agent</code> to handle the <code>probe</code> tool","text":"<p>The final step in setting up the tool is to enable  the <code>spy_game_agent</code> to handle the <code>probe</code> tool:</p> <pre><code>spy_game_agent.enable_message(ProbeTool)\n</code></pre>"},{"location":"quick-start/chat-agent-tool/#set-up-the-task-and-instructions","title":"Set up the task and instructions","text":"<p>We set up the task for the <code>spy_game_agent</code> and run it:</p> <p><pre><code>task = lr.Task(\n   spy_game_agent,\n   system_message=\"\"\"\n            I have a list of numbers between 1 and 100. \n            Your job is to find the smallest of them.\n            To help with this, you can give me a number and I will\n            tell you how many of my numbers are equal or less than your number.\n            Once you have found the smallest number,\n            you can say DONE and report your answer.\n        \"\"\"\n)\ntask.run()\n</code></pre> Notice that in the task setup we  have not explicitly instructed the LLM to use the <code>probe</code> tool. But this is done \"behind the scenes\", either by the OpenAI API  (when we use function-calling by setting the <code>use_functions_api</code> flag to <code>True</code>), or by Langroid's native tools mechanism (when we set the <code>use_tools</code> flag to <code>True</code>).</p> <p>Asynchoronous tool handlers</p> <p>If you run task asynchronously - i.e. via <code>await task.run_async()</code> - you may provide   asynchronous tool handler by implementing <code>probe_async</code> method.</p> <p>See the <code>chat-agent-tool.py</code> in the <code>langroid-examples</code> repo, for a working example that you can run as follows: <pre><code>python3 examples/quick-start/chat-agent-tool.py\n</code></pre></p> <p>Here is a screenshot of the chat in action, using Langroid's tools mechanism</p> <p></p> <p>And if we run it with the <code>-f</code> flag (to switch to using OpenAI function-calling):</p> <p></p>"},{"location":"quick-start/chat-agent-tool/#see-also","title":"See also","text":"<p>One of the uses of tools/function-calling is to extract structured information from  a document. In the <code>langroid-examples</code> repo, there are two examples of this: </p> <ul> <li><code>examples/extract/chat.py</code>,    which shows how to extract Machine Learning model quality information from a description of    a solution approach on Kaggle.</li> <li><code>examples/docqa/chat_multi_extract.py</code>   which extracts key terms from a commercial lease document, in a nested JSON format.</li> </ul>"},{"location":"quick-start/chat-agent-tool/#next-steps","title":"Next steps","text":"<p>In the 3-agent chat example, recall that the <code>processor_agent</code> did not have to bother with specifying who should handle the current number. In the next section we add a twist to this game, so that the <code>processor_agent</code> has to decide who should handle the current number.</p>"},{"location":"quick-start/chat-agent/","title":"A simple chat agent","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>chat-agent.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/chat-agent.py</code>.</p>"},{"location":"quick-start/chat-agent/#agents","title":"Agents","text":"<p>A <code>ChatAgent</code> is an abstraction that  wraps a few components, including:</p> <ul> <li>an LLM (<code>ChatAgent.llm</code>), possibly equipped with tools/function-calling.    The <code>ChatAgent</code> class maintains LLM conversation history.</li> <li>optionally a vector-database (<code>ChatAgent.vecdb</code>)</li> </ul>"},{"location":"quick-start/chat-agent/#agents-as-message-transformers","title":"Agents as message transformers","text":"<p>In Langroid, a core function of <code>ChatAgents</code> is message transformation. There are three special message transformation methods, which we call responders. Each of these takes a message and returns a message.  More specifically, their function signature is (simplified somewhat): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> where <code>ChatDocument</code> is a class that wraps a message content (text) and its metadata. There are three responder methods in <code>ChatAgent</code>, one corresponding to each  responding entity (<code>LLM</code>, <code>USER</code>, or <code>AGENT</code>):</p> <ul> <li><code>llm_response</code>: returns the LLM response to the input message.   (The input message is added to the LLM history, and so is the subsequent response.)</li> <li><code>agent_response</code>: a method that can be used to implement a custom agent response.     Typically, an <code>agent_response</code> is used to handle messages containing a     \"tool\" or \"function-calling\" (more on this later). Another use of <code>agent_response</code>     is message validation.</li> <li><code>user_response</code>: get input from the user. Useful to allow a human user to     intervene or quit.</li> </ul> <p>Creating an agent is easy. First define a <code>ChatAgentConfig</code> object, and then instantiate a <code>ChatAgent</code> object with that config: <pre><code>import langroid as lr\n\nconfig = lr.ChatAgentConfig( #(1)!\n    name=\"MyAgent\", # note there should be no spaces in the name!\n    llm = lr.language_models.OpenAIGPTConfig(\n      chat_model=lr.language_models.OpenAIChatModel.GPT4o,\n    ),\n    system_message=\"You are a helpful assistant\" #(2)! \n)\nagent = lr.ChatAgent(config)\n</code></pre></p> <ol> <li>This agent only has an LLM, and no vector-store. Examples of agents with    vector-stores will be shown later.</li> <li>The <code>system_message</code> is used when invoking the agent's <code>llm_response</code> method; it is     passed to the LLM API as the first message (with role <code>\"system\"</code>), followed by the alternating series of user,     assistant messages. Note that a <code>system_message</code> can also be specified when initializing a <code>Task</code> object (as seen     below); in this case the <code>Task</code> <code>system_message</code> overrides the agent's <code>system_message</code>.</li> </ol> <p>We can now use the agent's responder methods, for example: <pre><code>response = agent.llm_response(\"What is 2 + 4?\")\nif response is not None:\n    print(response.content)\nresponse = agent.user_response(\"add 3 to this\")\n...\n</code></pre> The <code>ChatAgent</code> conveniently accumulates message history so you don't have to, as you did in the previous section with direct LLM usage. However to create an interative loop involving the human user, you still  need to write your own. The <code>Task</code> abstraction frees you from this, as we see below.</p>"},{"location":"quick-start/chat-agent/#task-orchestrator-for-agents","title":"Task: orchestrator for agents","text":"<p>In order to do anything useful with a <code>ChatAgent</code>, we need to have a way to  sequentially invoke its responder methods, in a principled way. For example in the simple chat loop we saw in the  previous section, in the  <code>try-llm.py</code> script, we had a loop that alternated between getting a human input and an LLM response. This is one of the simplest possible loops, but in more complex applications,  we need a general way to orchestrate the agent's responder methods.</p> <p>The <code>Task</code> class is an abstraction around a  <code>ChatAgent</code>, responsible for iterating over the agent's responder methods, as well as orchestrating delegation and hand-offs among multiple tasks. A <code>Task</code> is initialized with a specific <code>ChatAgent</code> instance, and some  optional arguments, including an initial message to \"kick-off\" the agent. The <code>Task.run()</code> method is the main entry point for <code>Task</code> objects, and works  as follows:</p> <ul> <li>it first calls the <code>Task.init()</code> method to initialize the <code>pending_message</code>,    which represents the latest message that needs a response.</li> <li>it then repeatedly calls <code>Task.step()</code> until <code>Task.done()</code> is True, and returns   <code>Task.result()</code> as the final result of the task.</li> </ul> <p><code>Task.step()</code> is where all the action happens. It represents a \"turn\" in the  \"conversation\": in the case of a single <code>ChatAgent</code>, the conversation involves  only the three responders mentioned above, but when a <code>Task</code> has sub-tasks,  it can involve other tasks well  (we see this in the a later section but ignore this for now).  <code>Task.step()</code> loops over  the <code>ChatAgent</code>'s responders (plus sub-tasks if any) until it finds a valid  response<sup>1</sup> to the current <code>pending_message</code>, i.e. a \"meaningful\" response,  something other than <code>None</code> for example. Once <code>Task.step()</code> finds a valid response, it updates the <code>pending_message</code>  with this response, and the next invocation of <code>Task.step()</code> will search for a valid response to this  updated message, and so on. <code>Task.step()</code> incorporates mechanisms to ensure proper handling of messages, e.g. the USER gets a chance to respond after each non-USER response (to avoid infinite runs without human intervention), and preventing an entity from responding if it has just responded, etc.</p> <p><code>Task.run()</code> has the same signature as agent's responder methods.</p> <p>The key to composability of tasks is that <code>Task.run()</code> has exactly the same type-signature as any of the agent's responder methods,  i.e. <code>str | ChatDocument -&gt; ChatDocument</code>. This means that a <code>Task</code> can be used as a responder in another <code>Task</code>, and so on recursively.  We will see this in action in the Two Agent Chat section.</p> <p>The above details were only provided to give you a glimpse into how Agents and  Tasks work. Unless you are creating a custom orchestration mechanism, you do not need to be aware of these details. In fact our basic human + LLM chat loop can be trivially  implemented with a <code>Task</code>, in a couple of lines of code: <pre><code>task = lr.Task(\n    agent, \n    name=\"Bot\", #(1)!\n    system_message=\"You are a helpful assistant\", #(2)!\n)\n</code></pre> 1. If specified, overrides the agent's <code>name</code>.     (Note that the agent's name is displayed in the conversation shown in the console.)   However, typical practice is to just define the <code>name</code> in the <code>ChatAgentConfig</code> object, as we did above. 2. If specified, overrides the agent's <code>system_message</code>. Typical practice is to just  define the <code>system_message</code> in the <code>ChatAgentConfig</code> object, as we did above.</p> <p>We can then run the task: <pre><code>task.run() #(1)!\n</code></pre></p> <ol> <li>Note how this hides all of the complexity of constructing and updating a     sequence of <code>LLMMessages</code></li> </ol> <p>Note that the agent's <code>agent_response()</code> method always returns <code>None</code> (since the default  implementation of this method looks for a tool/function-call, and these never occur in this task). So the calls to <code>task.step()</code> result in alternating responses from the LLM and the user.</p> <p>See <code>chat-agent.py</code> for a working example that you can run with <pre><code>python3 examples/quick-start/chat-agent.py\n</code></pre></p> <p>Here is a screenshot of the chat in action:<sup>2</sup></p> <p></p>"},{"location":"quick-start/chat-agent/#next-steps","title":"Next steps","text":"<p>In the next section you will  learn some general principles on how to have multiple agents collaborate  on a task using Langroid.</p> <ol> <li> <p>To customize a Task's behavior you can subclass it and  override methods like <code>valid()</code>, <code>done()</code>, <code>result()</code>, or even <code>step()</code>.\u00a0\u21a9</p> </li> <li> <p>In the screenshot, the numbers in parentheses indicate how many  messages have accumulated in the LLM's message history.  This is only provided for informational and debugging purposes, and  you can ignore it for now.\u00a0\u21a9</p> </li> </ol>"},{"location":"quick-start/llm-interaction/","title":"LLM interaction","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is  in the <code>try-llm.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/try-llm.py</code>.</p> <p>Let's start with the basics -- how to directly interact with an OpenAI LLM using Langroid.</p>"},{"location":"quick-start/llm-interaction/#configure-instantiate-the-llm-class","title":"Configure, instantiate the LLM class","text":"<p>First define the configuration for the LLM, in this case one of the OpenAI GPT chat models: <pre><code>import langroid as lr\n\ncfg = lr.language_models.OpenAIGPTConfig(\n    chat_model=lr.language_models.OpenAIChatModel.GPT4o,\n)\n</code></pre></p> <p>About Configs</p> <p>A recurring pattern you will see in Langroid is that for many classes, we have a corresponding <code>Config</code> class (an instance of a Pydantic <code>BaseModel</code>), and the class constructor takes this <code>Config</code> class as its only argument. This lets us avoid having long argument lists in constructors, and brings flexibility since adding a new argument to the constructor is as simple as adding a new field to the corresponding <code>Config</code> class. For example the constructor for the <code>OpenAIGPT</code> class takes a single argument, an instance of the <code>OpenAIGPTConfig</code> class.</p> <p>Now that we've defined the configuration of the LLM, we can instantiate it: <pre><code>mdl = lr.language_models.OpenAIGPT(cfg)\n</code></pre></p> <p>We will use OpenAI's GPT4 model's chat completion API.</p>"},{"location":"quick-start/llm-interaction/#messages-the-llmmessage-class","title":"Messages: The <code>LLMMessage</code> class","text":"<p>This API takes a list of \"messages\" as input -- this is typically the conversation history so far, consisting of an initial system message, followed by a sequence of alternating messages from the LLM (\"Assistant\") and the user. Langroid provides an abstraction  <code>LLMMessage</code> to construct messages, e.g. <pre><code>from langroid.language_models import Role, LLMMessage\n\nmsg = LLMMessage(\n    content=\"what is the capital of Bangladesh?\", \n    role=Role.USER\n)\n</code></pre></p>"},{"location":"quick-start/llm-interaction/#llm-response-to-a-sequence-of-messages","title":"LLM response to a sequence of messages","text":"<p>To get a response from the LLM, we call the mdl's <code>chat</code> method, and pass in a list of messages, along with a bound on how long (in tokens) we want the response to be: <pre><code>messages = [\n    LLMMessage(content=\"You are a helpful assistant\", role=Role.SYSTEM), #(1)!\n    LLMMessage(content=\"What is the capital of Ontario?\", role=Role.USER), #(2)!\n]\n\nresponse = mdl.chat(messages, max_tokens=200)\n</code></pre></p> <ol> <li> With a system message, you can assign a \"role\" to the LLM</li> <li> Responses from the LLM will have role <code>Role.ASSISTANT</code>;    this is done behind the scenes by the <code>response.to_LLMMessage()</code> call below.</li> </ol> <p>The response is an object of class <code>LLMResponse</code>,  which we can convert to an <code>LLMMessage</code> to append to the conversation history: <pre><code>messages.append(response.to_LLMMessage())\n</code></pre></p> <p>You can put the above in a simple loop,  to get a simple command-line chat interface!</p> <pre><code>from rich import print\nfrom rich.prompt import Prompt #(1)!\n\nmessages = [\n    LLMMessage(role=Role.SYSTEM, content=\"You are a helpful assitant\"),\n]\n\nwhile True:\n    message = Prompt.ask(\"[blue]Human\")\n    if message in [\"x\", \"q\"]:\n        print(\"[magenta]Bye!\")\n        break\n    messages.append(LLMMessage(role=Role.USER, content=message))\n\n    response = mdl.chat(messages=messages, max_tokens=200)\n    messages.append(response.to_LLMMessage())\n    print(\"[green]Bot: \" + response.message)\n</code></pre> <ol> <li>Rich is a Python library for rich text and beautiful formatting in the terminal.    We use it here to get a nice prompt for the user's input.    You can install it with <code>pip install rich</code>.</li> </ol> <p>See <code>examples/quick-start/try-llm.py</code> for a complete example that you can run using <pre><code>python3 examples/quick-start/try-llm.py\n</code></pre></p> <p>Here is a screenshot of what it looks like:</p> <p></p>"},{"location":"quick-start/llm-interaction/#next-steps","title":"Next steps","text":"<p>You might be thinking:  \"It is tedious to keep track of the LLM conversation history and set up a  loop. Does Langroid provide any abstractions to make this easier?\"</p> <p>We're glad you asked! And this leads to the notion of an <code>Agent</code>.  The next section will show you how to use the <code>ChatAgent</code> class  to set up a simple chat Agent in a couple of lines of code.</p>"},{"location":"quick-start/multi-agent-task-delegation/","title":"Multi-Agent collaboration via Task Delegation","text":""},{"location":"quick-start/multi-agent-task-delegation/#why-multiple-agents","title":"Why multiple agents?","text":"<p>Let's say we want to develop a complex LLM-based application, for example an application that reads a legal contract, extracts structured information, cross-checks it against some taxonomoy, gets some human input, and produces clear summaries. In theory it may be possible to solve this in a monolithic architecture using an LLM API and a vector-store. But this approach quickly runs into problems -- you would need to maintain multiple LLM conversation histories and states, multiple vector-store instances, and coordinate all of the interactions between them.</p> <p>Langroid's <code>ChatAgent</code> and <code>Task</code> abstractions provide a natural and intuitive way to decompose a solution approach into multiple tasks, each requiring different skills and capabilities. Some of these tasks may need access to an LLM, others may need access to a vector-store, and yet others may need tools/plugins/function-calling capabilities, or any combination of these. It may also make sense to have some tasks that manage the overall solution process. From an architectural perspective, this type of modularity has numerous benefits:</p> <ul> <li>Reusability: We can reuse the same agent/task in other contexts,</li> <li>Scalability: We can scale up the solution by adding more agents/tasks,</li> <li>Flexibility: We can easily change the solution by adding/removing agents/tasks.</li> <li>Maintainability: We can maintain the solution by updating individual agents/tasks.</li> <li>Testability: We can test/debug individual agents/tasks in isolation.</li> <li>Composability: We can compose agents/tasks to create new agents/tasks.</li> <li>Extensibility: We can extend the solution by adding new agents/tasks.</li> <li>Interoperability: We can integrate the solution with other systems by   adding new agents/tasks.</li> <li>Security/Privacy: We can secure the solution by isolating sensitive agents/tasks.</li> <li>Performance: We can improve performance by isolating performance-critical agents/tasks.</li> </ul>"},{"location":"quick-start/multi-agent-task-delegation/#task-collaboration-via-sub-tasks","title":"Task collaboration via sub-tasks","text":"<p>Langroid currently provides a mechanism for hierarchical (i.e. tree-structured) task delegation: a <code>Task</code> object can add other <code>Task</code> objects as sub-tasks, as shown in this pattern:</p> <pre><code>from langroid import ChatAgent, ChatAgentConfig, Task\n\nmain_agent = ChatAgent(ChatAgentConfig(...))\nmain_task = Task(main_agent, ...)\n\nhelper_agent1 = ChatAgent(ChatAgentConfig(...))\nhelper_agent2 = ChatAgent(ChatAgentConfig(...))\nhelper_task1 = Task(agent1, ...)\nhelper_task2 = Task(agent2, ...)\n\nmain_task.add_sub_task([helper_task1, helper_task2])\n</code></pre> <p>What happens when we call <code>main_task.run()</code>? Recall from the previous section that <code>Task.run()</code> works by repeatedly calling <code>Task.step()</code> until <code>Task.done()</code> is True. When the <code>Task</code> object has no sub-tasks, <code>Task.step()</code> simply tries to get a valid response from the <code>Task</code>'s <code>ChatAgent</code>'s \"native\" responders, in this sequence: <pre><code>[self.agent_response, self.llm_response, self.user_response] #(1)!\n</code></pre></p> <ol> <li>This is the default sequence in Langroid, but it can be changed by    overriding <code>ChatAgent.entity_responders()</code></li> </ol> <p>When a <code>Task</code> object has subtasks, the sequence of responders tried by <code>Task.step()</code> consists of the above \"native\" responders, plus the sequence of <code>Task.run()</code> calls on the sub-tasks, in the order in which they were added to the <code>Task</code> object. For the example above, this means that <code>main_task.step()</code> will seek a valid response in this sequence:</p> <p><pre><code>[self.agent_response, self.llm_response, self.user_response, \n    helper_task1.run(), helper_task2.run()]\n</code></pre> Fortunately, as noted in the previous section, <code>Task.run()</code> has the same type signature as that of the <code>ChatAgent</code>'s \"native\" responders, so this works seamlessly. Of course, each of the sub-tasks can have its own sub-tasks, and so on, recursively. One way to think of this type of task delegation is that <code>main_task()</code> \"fails-over\" to <code>helper_task1()</code> and <code>helper_task2()</code> when it cannot respond to the current <code>pending_message</code> on its own.</p>"},{"location":"quick-start/multi-agent-task-delegation/#or-else-logic-vs-and-then-logic","title":"Or Else logic vs And Then logic","text":"<p>It is important to keep in mind how <code>step()</code> works: As each responder  in the sequence is tried, when there is a valid response, the  next call to <code>step()</code> restarts its search at the beginning of the sequence (with the only exception being that the human User is given a chance  to respond after each non-human response).  In this sense, the semantics of the responder sequence is similar to OR Else logic, as opposed to AND Then logic.</p> <p>If we want to have a sequence of sub-tasks that is more like AND Then logic, we can achieve this by recursively adding subtasks. In the above example suppose we wanted the <code>main_task</code>  to trigger <code>helper_task1</code> and <code>helper_task2</code> in sequence, then we could set it up like this:</p> <pre><code>helper_task1.add_sub_task(helper_task2) #(1)!\nmain_task.add_sub_task(helper_task1)\n</code></pre> <ol> <li>When adding a single sub-task, we do not need to wrap it in a list.</li> </ol>"},{"location":"quick-start/multi-agent-task-delegation/#next-steps","title":"Next steps","text":"<p>In the next section we will see how this mechanism  can be used to set up a simple collaboration between two agents.</p>"},{"location":"quick-start/setup/","title":"Setup","text":""},{"location":"quick-start/setup/#install","title":"Install","text":"<p>Ensure you are using Python 3.11. It is best to work in a virtual environment:</p> <p><pre><code># go to your repo root (which may be langroid-examples)\ncd &lt;your repo root&gt;\npython3 -m venv .venv\n. ./.venv/bin/activate\n</code></pre> To see how to use Langroid in your own repo, you can take a look at the <code>langroid-examples</code> repo, which can be a good starting point for your own repo,  or use the <code>langroid-template</code> repo. These repos contain a <code>pyproject.toml</code> file suitable for use with the <code>uv</code> dependency manager. After installing <code>uv</code> you can  set up your virtual env, activate it, and install langroid into your venv like this:</p> <pre><code>uv venv --python 3.11\n. ./.venv/bin/activate \nuv sync\n</code></pre> <p>Alternatively, use <code>pip</code> to install <code>langroid</code> into your virtual environment: <pre><code>pip install langroid\n</code></pre></p> <p>The core Langroid package lets you use OpenAI Embeddings models via their API. If you instead want to use the <code>sentence-transformers</code> embedding models from HuggingFace, install Langroid like this: <pre><code>pip install \"langroid[hf-embeddings]\"\n</code></pre> For many practical scenarios, you may need additional optional dependencies: - To use various document-parsers, install langroid with the <code>doc-chat</code> extra:     <pre><code>pip install \"langroid[doc-chat]\"\n</code></pre> - For \"chat with databases\", use the <code>db</code> extra:     <code>`bash     pip install \"langroid[db]\"</code> - You can specify multiple extras by separating them with commas, e.g.:     <pre><code>pip install \"langroid[doc-chat,db]\"\n</code></pre> - To simply install all optional dependencies, use the <code>all</code> extra (but note that this will result in longer load/startup times and a larger install size):     <pre><code>pip install \"langroid[all]\"\n</code></pre></p> Optional Installs for using SQL Chat with a PostgreSQL DB <p>If you are using <code>SQLChatAgent</code> (e.g. the script <code>examples/data-qa/sql-chat/sql_chat.py</code>, with a postgres db, you will need to:</p> <ul> <li>Install PostgreSQL dev libraries for your platform, e.g.<ul> <li><code>sudo apt-get install libpq-dev</code> on Ubuntu,</li> <li><code>brew install postgresql</code> on Mac, etc.</li> </ul> </li> <li>Install langroid with the postgres extra, e.g. <code>pip install langroid[postgres]</code>   or <code>uv add \"langroid[postgres]\"</code> or <code>uv pip install --extra postgres -r pyproject.toml</code>.   If this gives you an error, try    <code>uv pip install psycopg2-binary</code> in your virtualenv.</li> </ul> <p>Work in a nice terminal, such as Iterm2, rather than a notebook</p> <p>All of the examples we will go through are command-line applications. For the best experience we recommend you work in a nice terminal that supports  colored outputs, such as Iterm2.    </p> <p>mysqlclient errors</p> <p>If you get strange errors involving <code>mysqlclient</code>, try doing <code>pip uninstall mysqlclient</code> followed by <code>pip install mysqlclient</code> </p>"},{"location":"quick-start/setup/#set-up-tokenskeys","title":"Set up tokens/keys","text":"<p>To get started, all you need is an OpenAI API Key. If you don't have one, see this OpenAI Page. (Note that while this is the simplest way to get started, Langroid works with practically any LLM, not just those from OpenAI. See the guides to using Open/Local LLMs, and other non-OpenAI proprietary LLMs.)</p> <p>In the root of the repo, copy the <code>.env-template</code> file to a new file <code>.env</code>: <pre><code>cp .env-template .env\n</code></pre> Then insert your OpenAI API Key. Your <code>.env</code> file should look like this: <pre><code>OPENAI_API_KEY=your-key-here-without-quotes\n</code></pre></p> <p>Alternatively, you can set this as an environment variable in your shell (you will need to do this every time you open a new shell): <pre><code>export OPENAI_API_KEY=your-key-here-without-quotes\n</code></pre></p> <p>All of the following environment variable settings are optional, and some are only needed to use specific features (as noted below).</p> <ul> <li>Qdrant Vector Store API Key, URL. This is only required if you want to use Qdrant cloud.   Langroid uses LanceDB as the default vector store in its <code>DocChatAgent</code> class (for RAG).   Alternatively Chroma is also currently supported.   We use the local-storage version of Chroma, so there is no need for an API key.</li> <li>Redis Password, host, port: This is optional, and only needed to cache LLM API responses   using Redis Cloud. Redis offers a free 30MB Redis account   which is more than sufficient to try out Langroid and even beyond.   If you don't set up these, Langroid will use a pure-python   Redis in-memory cache via the Fakeredis library.</li> <li>GitHub Personal Access Token (required for apps that need to analyze git   repos; token-based API calls are less rate-limited). See this   GitHub page.</li> <li>Google Custom Search API Credentials: Only needed to enable an Agent to use the <code>GoogleSearchTool</code>.   To use Google Search as an LLM Tool/Plugin/function-call,   you'll need to set up   a Google API key,   then setup a Google Custom Search Engine (CSE) and get the CSE ID.   (Documentation for these can be challenging, we suggest asking GPT4 for a step-by-step guide.)   After obtaining these credentials, store them as values of   <code>GOOGLE_API_KEY</code> and <code>GOOGLE_CSE_ID</code> in your <code>.env</code> file.   Full documentation on using this (and other such \"stateless\" tools) is coming soon, but   in the meantime take a peek at the test   <code>tests/main/test_web_search_tools.py</code> to see how to use it.</li> </ul> <p>If you add all of these optional variables, your <code>.env</code> file should look like this: <pre><code>OPENAI_API_KEY=your-key-here-without-quotes\nGITHUB_ACCESS_TOKEN=your-personal-access-token-no-quotes\nCACHE_TYPE=redis\nREDIS_PASSWORD=your-redis-password-no-quotes\nREDIS_HOST=your-redis-hostname-no-quotes\nREDIS_PORT=your-redis-port-no-quotes\nQDRANT_API_KEY=your-key\nQDRANT_API_URL=https://your.url.here:6333 # note port number must be included\nGOOGLE_API_KEY=your-key\nGOOGLE_CSE_ID=your-cse-id\n</code></pre></p>"},{"location":"quick-start/setup/#microsoft-azure-openai-setupoptional","title":"Microsoft Azure OpenAI setup[Optional]","text":"<p>This section applies only if you are using Microsoft Azure OpenAI.</p> <p>When using Azure OpenAI, additional environment variables are required in the <code>.env</code> file. This page Microsoft Azure OpenAI provides more information, and you can set each environment variable as follows:</p> <ul> <li><code>AZURE_OPENAI_API_KEY</code>, from the value of <code>API_KEY</code></li> <li><code>AZURE_OPENAI_API_BASE</code> from the value of <code>ENDPOINT</code>, typically looks like <code>https://your_resource.openai.azure.com</code>.</li> <li>For <code>AZURE_OPENAI_API_VERSION</code>, you can use the default value in <code>.env-template</code>, and latest version can be found here</li> <li><code>AZURE_OPENAI_DEPLOYMENT_NAME</code> is an OPTIONAL deployment name which may be     defined by the user during the model setup.</li> <li><code>AZURE_OPENAI_CHAT_MODEL</code> Azure OpenAI allows specific model names when you select the model for your deployment. You need to put precisely the exact model name that was selected. For example, GPT-3.5 (should be <code>gpt-35-turbo-16k</code> or <code>gpt-35-turbo</code>) or GPT-4 (should be <code>gpt-4-32k</code> or <code>gpt-4</code>).</li> <li><code>AZURE_OPENAI_MODEL_NAME</code> (Deprecated, use <code>AZURE_OPENAI_CHAT_MODEL</code> instead).</li> </ul> <p>For Azure-based models use <code>AzureConfig</code> instead of <code>OpenAIGPTConfig</code></p> <p>In most of the docs you will see that LLMs are configured using <code>OpenAIGPTConfig</code>. However if you want to use Azure-deployed models, you should replace <code>OpenAIGPTConfig</code> with <code>AzureConfig</code>. See  the <code>test_azure_openai.py</code> and  <code>example/basic/chat.py</code></p>"},{"location":"quick-start/setup/#next-steps","title":"Next steps","text":"<p>Now you should be ready to use Langroid! As a next step, you may want to see how you can use Langroid to interact  directly with the LLM (OpenAI GPT models only for now).</p>"},{"location":"quick-start/three-agent-chat-num-router/","title":"Three-Agent Collaboration, with message Routing","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>three-agent-chat-num-router.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num-router.py</code>.</p> <p>Let's change the number game from the three agent chat example slightly. In that example, when the <code>even_agent</code>'s LLM receives an odd number, it responds with <code>DO-NOT-KNOW</code>, and similarly for the <code>odd_agent</code> when it receives an even number. The <code>step()</code> method of the <code>repeater_task</code> considers <code>DO-NOT-KNOW</code> to be an invalid response and continues to  look for a valid response from any remaining sub-tasks. Thus there was no need for the <code>processor_agent</code> to specify who should handle the current number.</p> <p>But what if there is a scenario where the <code>even_agent</code> and <code>odd_agent</code> might return a legit but \"wrong\" answer? In this section we add this twist -- when the <code>even_agent</code> receives an odd number, it responds with -10, and similarly for the <code>odd_agent</code> when it receives an even number. We tell the <code>processor_agent</code> to avoid getting a negative number.</p> <p>The goal we have set for the <code>processor_agent</code> implies that it  must specify the intended recipient of  the number it is sending.  We can enforce this using a special Langroid Tool,  <code>RecipientTool</code>. So when setting up the <code>processor_task</code> we include instructions to use this tool (whose name is <code>recipient_message</code>, the value of <code>RecipientTool.request</code>):</p> <pre><code>processor_agent = lr.ChatAgent(config)\nprocessor_task = lr.Task(\n    processor_agent,\n    name = \"Processor\",\n    system_message=\"\"\"\n        You will receive a list of numbers from me (the user).\n        Your goal is to apply a transformation to each number.\n        However you do not know how to do this transformation.\n        You can take the help of two people to perform the \n        transformation.\n        If the number is even, send it to EvenHandler,\n        and if it is odd, send it to OddHandler.\n\n        IMPORTANT: send the numbers ONE AT A TIME\n\n        The handlers will transform the number and give you a new number.        \n        If you send it to the wrong person, you will receive a negative value.\n        Your aim is to never get a negative number, so you must \n        clearly specify who you are sending the number to, using the\n        `recipient_message` tool/function-call, where the `content` field\n        is the number you want to send, and the `recipient` field is the name\n        of the intended recipient, either \"EvenHandler\" or \"OddHandler\".        \n\n        Once all numbers in the given list have been transformed, \n        say DONE and show me the result. \n        Start by asking me for the list of numbers.\n    \"\"\",\n    llm_delegate=True,\n    single_round=False,\n)\n</code></pre> <p>To enable the <code>processor_agent</code> to use this tool, we must enable it: <pre><code>processor_agent.enable_message(lr.agent.tools.RecipientTool)\n</code></pre></p> <p>The rest of the code remains the same as in the previous section, i.e., we simply add the two handler tasks as sub-tasks of the <code>processor_task</code>, like this: <pre><code>processor_task.add_sub_task([even_task, odd_task])\n</code></pre></p> <p>One of the benefits of using the <code>RecipientTool</code> is that it contains  mechanisms to remind the LLM to specify a recipient for its message, when it forgets to do so (this does happen once in a while, even with GPT-4).</p> <p>Feel free to try the working example script <code>three-agent-chat-num-router.py</code> in the  <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num-router.py</code>:</p> <pre><code>python3 examples/quick-start/three-agent-chat-num-router.py\n</code></pre> <p>Below is screenshot of what this might look like, using the OpenAI function-calling  mechanism with the <code>recipient_message</code> tool:</p> <p></p> <p>And here is what it looks like using Langroid's built-in tools mechanism (use the <code>-t</code> option when running the script):</p> <p></p> <p>And here is what it looks like using </p>"},{"location":"quick-start/three-agent-chat-num-router/#next-steps","title":"Next steps","text":"<p>In the next section you will learn how to use Langroid with external documents.</p>"},{"location":"quick-start/three-agent-chat-num/","title":"Three-Agent Collaboration","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>three-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num.py</code>.</p> <p>Let us set up a simple numbers exercise between 3 agents. The <code>Processor</code> agent receives a number \\(n\\), and its goal is to  apply a transformation to the it. However it does not know how to apply the transformation, and takes the help of two other agents to do so. Given a number \\(n\\),</p> <ul> <li>The <code>EvenHandler</code> returns \\(n/2\\) if n is even, otherwise says <code>DO-NOT-KNOW</code>.</li> <li>The <code>OddHandler</code> returns \\(3n+1\\) if n is odd, otherwise says <code>DO-NOT-KNOW</code>.</li> </ul> <p>We'll first define a shared LLM config:</p> <pre><code>llm_config = lr.language_models.OpenAIGPTConfig(\n    chat_model=lr.language_models.OpenAIChatModel.GPT4o,\n    # or, e.g., \"ollama/qwen2.5-coder:latest\", or \"gemini/gemini-2.0-flash-exp\"\n)\n</code></pre> <p>Next define the config for the <code>Processor</code> agent: <pre><code>processor_config = lr.ChatAgentConfig(\n    name=\"Processor\",\n    llm = llm_config,\n    system_message=\"\"\"\n    You will receive a number from the user.\n    Simply repeat that number, DO NOT SAY ANYTHING else,\n    and wait for a TRANSFORMATION of the number \n    to be returned to you.\n\n    Once you have received the RESULT, simply say \"DONE\",\n    do not say anything else.\n    \"\"\",        \n    vecdb=None,\n)\n</code></pre></p> <p>Then set up the <code>processor_agent</code>, along with the corresponding task: <pre><code>processor_agent = lr.ChatAgent(processor_config)\n\nprocessor_task = lr.Task(\n    processor_agent,\n    llm_delegate=True, #(1)!\n    interactive=False, #(2)!\n    single_round=False, #(3)!\n)\n</code></pre></p> <ol> <li>Setting the <code>llm_delegate</code> option to <code>True</code> means that the <code>processor_task</code> is     delegated to the LLM (as opposed to the User),      in the sense that the LLM is the one \"seeking\" a response to the latest      number. Specifically, this means that in the <code>processor_task.step()</code>      when a sub-task returns <code>DO-NOT-KNOW</code>,     it is not considered a valid response, and the search for a valid response      continues to the next sub-task if any.</li> <li><code>interactive=False</code> means the task loop will not wait for user input.</li> <li><code>single_round=False</code> means that the <code>processor_task</code> should not terminate after      a valid response from a responder.</li> </ol> <p>Set up the other two agents and tasks:</p> <pre><code>NO_ANSWER = lr.utils.constants.NO_ANSWER\n\neven_config = lr.ChatAgentConfig(\n    name=\"EvenHandler\",\n    llm = llm_config,\n    system_message=f\"\"\"\n    You will be given a number N. Respond as follows:\n\n    - If N is even, divide N by 2 and show the result, \n      in the format: \n        RESULT = &lt;result&gt;\n      and say NOTHING ELSE.\n    - If N is odd, say {NO_ANSWER}\n    \"\"\",    \n)\neven_agent = lr.ChatAgent(even_config)\neven_task = lr.Task(\n    even_agent,\n    single_round=True,  # task done after 1 step() with valid response\n)\n\nodd_config = lr.ChatAgentConfig(\n    name=\"OddHandler\",\n    llm = llm_config,\n    system_message=f\"\"\"\n    You will be given a number N. Respond as follows:\n\n    - if N is odd, return the result (N*3+1), in the format:\n        RESULT = &lt;result&gt; \n        and say NOTHING ELSE.\n\n    - If N is even, say {NO_ANSWER}\n    \"\"\",\n)\nodd_agent = lr.ChatAgent(odd_config)\nodd_task = lr.Task(\n    odd_agent,\n    single_round=True,  # task done after 1 step() with valid response\n)\n</code></pre> <p>Now add the <code>even_task</code> and <code>odd_task</code> as subtasks of the <code>processor_task</code>,  and then run it with a number as input:</p> <pre><code>processor_task.add_sub_task([even_task, odd_task])\nprocessor_task.run(13)\n</code></pre> <p>The input number will be passed to the <code>Processor</code> agent as the user input.</p> <p>Feel free to try the working example script <code>three-agent-chat-num.py</code> <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num.py</code>:</p> <pre><code>python3 examples/quick-start/three-agent-chat-num.py\n</code></pre> <p>Here's a screenshot of what it looks like: </p>"},{"location":"quick-start/three-agent-chat-num/#next-steps","title":"Next steps","text":"<p>In the next section you will learn how to use Langroid to equip a <code>ChatAgent</code> with tools or function-calling.</p>"},{"location":"quick-start/two-agent-chat-num/","title":"Two-Agent Collaboration","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>two-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/two-agent-chat-num.py</code>.</p> <p>To illustrate these ideas, let's look at a toy example<sup>1</sup> where  a <code>Student</code> agent receives a list of numbers to add. We set up this agent with an instruction that they do not know how to add, and they can ask for help adding pairs of numbers. To add pairs of numbers, we set up an <code>Adder</code> agent.</p> <p>First define a common <code>llm_config</code> to use for both agents: <pre><code>llm_config = lr.language_models.OpenAIGPTConfig(\n    chat_model=lr.language_models.OpenAIChatModel.GPT4o,\n    # or, e.g., \"ollama/qwen2.5-coder:latest\", or \"gemini/gemini-2.0-flash-exp\"\n)\n</code></pre></p> <p>Next, set up a config for the student agent, then create the agent and the corresponding task:</p> <pre><code>student_config = lr.ChatAgentConfig(\n    name=\"Student\",\n    llm=llm_config,\n    vecdb=None, #(1)!\n    system_message=\"\"\"\n        You will receive a list of numbers from me (the User),\n        and your goal is to calculate their sum.\n        However you do not know how to add numbers.\n        I can help you add numbers, two at a time, since\n        I only know how to add pairs of numbers.\n        Send me a pair of numbers to add, one at a time, \n        and I will tell you their sum.\n        For each question, simply ask me the sum in math notation, \n        e.g., simply say \"1 + 2\", etc, and say nothing else.\n        Once you have added all the numbers in the list, \n        say DONE and give me the final sum. \n        Start by asking me for the list of numbers.\n    \"\"\",    \n)\nstudent_agent = lr.ChatAgent(student_config)\nstudent_task = lr.Task(\n    student_agent,\n    name = \"Student\",\n    llm_delegate = True, #(2)!\n    single_round=False,  # (3)! \n)\n</code></pre> <ol> <li>We don't need access to external docs so we set <code>vecdb=None</code> to avoid     the overhead of loading a vector-store.</li> <li>Whenever we \"flip roles\" and assign the LLM the role of generating questions,     we set <code>llm_delegate=True</code>. In effect this ensures that the LLM \"decides\" when    the task is done.</li> <li>This setting means the task is not a single-round task, i.e. it is not done    after one <code>step()</code> with a valid response.</li> </ol> <p>Next, set up the Adder agent config, create the Adder agent and the corresponding Task:</p> <p><pre><code>adder_config = lr.ChatAgentConfig(\n    name = \"Adder\", #(1)!\n    llm=llm_config,\n    vecdb=None,\n    system_message=\"\"\"\n        You are an expert on addition of numbers. \n        When given numbers to add, simply return their sum, say nothing else\n        \"\"\",     \n)\nadder_agent = lr.ChatAgent(adder_config)\nadder_task = lr.Task(\n    adder_agent,\n    interactive=False, #(2)!\n    single_round=True,  # task done after 1 step() with valid response (3)!\n)\n</code></pre> 1. The Agent name is displayed in the conversation shown in the console. 2. Does not wait for user input. 3. We set <code>single_round=True</code> to ensure that the expert task is done after     one step() with a valid response. </p> <p>Finally, we add the <code>adder_task</code> as a sub-task of the <code>student_task</code>,  and run the <code>student_task</code>:</p> <pre><code>student_task.add_sub_task(adder_task) #(1)!\nstudent_task.run()\n</code></pre> <ol> <li>When adding just one sub-task, we don't need to use a list.</li> </ol> <p>For a full working example, see the  <code>two-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo. You can run this using: <pre><code>python3 examples/quick-start/two-agent-chat-num.py\n</code></pre></p> <p>Here is an example of the conversation that results:</p> <p></p>"},{"location":"quick-start/two-agent-chat-num/#logs-of-multi-agent-interactions","title":"Logs of multi-agent interactions","text":"<p>For advanced users</p> <p>This section is for advanced users who want more visibility into the internals of multi-agent interactions.</p> <p>When running a multi-agent chat, e.g. using <code>task.run()</code>, two types of logs are generated: - plain-text logs in <code>logs/&lt;task_name&gt;.log</code> - tsv logs in <code>logs/&lt;task_name&gt;.tsv</code></p> <p>It is important to realize that the logs show every iteration  of the loop in <code>Task.step()</code>, i.e. every attempt at responding to the current pending message, even those that are not allowed. The ones marked with an asterisk (*) are the ones that are considered valid responses for a given <code>step()</code> (which is a \"turn\" in the conversation).</p> <p>The plain text logs have color-coding ANSI chars to make them easier to read by doing <code>less &lt;log_file&gt;</code>. The format is (subject to change): <pre><code>(TaskName) Responder SenderEntity (EntityName) (=&gt; Recipient) TOOL Content\n</code></pre></p> <p>The structure of the <code>tsv</code> logs is similar. A great way to view these is to install and use the excellent <code>visidata</code> (https://www.visidata.org/) tool: <pre><code>vd logs/&lt;task_name&gt;.tsv\n</code></pre></p>"},{"location":"quick-start/two-agent-chat-num/#next-steps","title":"Next steps","text":"<p>As a next step, look at how to set up a collaboration among three agents for a simple numbers game.</p> <ol> <li> <p>Toy numerical examples are perfect to illustrate the ideas without   incurring too much token cost from LLM API calls.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/","title":"langroid","text":"<p>langroid/init.py </p> <p>Main langroid package</p>"},{"location":"reference/#langroid.Agent","title":"<code>Agent(config=AgentConfig())</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An Agent is an abstraction that typically (but not necessarily) encapsulates an LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def __init__(self, config: AgentConfig = AgentConfig()):\n    self.config = config\n    self.id = ObjectRegistry.new_id()  # Initialize agent ID\n    self.lock = asyncio.Lock()  # for async access to update self.llm.usage_cost\n    self.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\n    self.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\n    self.llm_tools_handled: Set[str] = set()\n    self.llm_tools_usable: Set[str] = set()\n    self.llm_tools_known: Set[str] = set()  # all known tools, handled/used or not\n    # Indicates which tool-names are allowed to be inferred when\n    # the LLM \"forgets\" to include the request field in its tool-call.\n    self.enabled_requests_for_inference: Optional[Set[str]] = (\n        None  # If None, we allow all\n    )\n    self.interactive: bool = True  # may be modified by Task wrapper\n    self.token_stats_str = \"\"\n    self.default_human_response: Optional[str] = None\n    self._indent = \"\"\n    self.llm = LanguageModel.create(config.llm)\n    self.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\n    self.tool_error = False\n    self.search_for_tools = {\n        SearchForTools.CONTENT.value,\n        SearchForTools.TOOLS.value,\n        SearchForTools.FUNCTIONS.value,\n    }\n    if config.parsing is not None and self.config.llm is not None:\n        # token_encoding_model is used to obtain the tokenizer,\n        # so in case it's an OpenAI model, we ensure that the tokenizer\n        # corresponding to the model is used.\n        if isinstance(self.llm, OpenAIGPT) and self.llm.is_openai_chat_model():\n            config.parsing.token_encoding_model = self.llm.config.chat_model\n    self.parser: Optional[Parser] = (\n        Parser(config.parsing) if config.parsing else None\n    )\n    if config.add_to_registry:\n        ObjectRegistry.register_object(self)\n\n    self.callbacks = SimpleNamespace(\n        start_llm_stream=lambda: noop_fn,\n        start_llm_stream_async=async_lambda_noop_fn,\n        cancel_llm_stream=noop_fn,\n        finish_llm_stream=noop_fn,\n        show_llm_response=noop_fn,\n        show_agent_response=noop_fn,\n        get_user_response=None,\n        get_user_response_async=None,\n        get_last_step=noop_fn,\n        set_parent_agent=noop_fn,\n        show_error_message=noop_fn,\n        show_start_response=noop_fn,\n    )\n    Agent.init_state(self)\n</code></pre>"},{"location":"reference/#langroid.Agent.indent","title":"<code>indent</code>  <code>property</code> <code>writable</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/#langroid.Agent.all_llm_tools_known","title":"<code>all_llm_tools_known</code>  <code>property</code>","text":"<p>All known tools; this may extend self.llm_tools_known.</p>"},{"location":"reference/#langroid.Agent.init_state","title":"<code>init_state()</code>","text":"<p>Initialize all state vars. Called by Task.run() if restart is True</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def init_state(self) -&gt; None:\n    \"\"\"Initialize all state vars. Called by Task.run() if restart is True\"\"\"\n    self.total_llm_token_cost = 0.0\n    self.total_llm_token_usage = 0\n</code></pre>"},{"location":"reference/#langroid.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details. Returns:     Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\n    self,\n) -&gt; List[\n    Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n    \"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response),\n        (Entity.LLM, self.llm_response),\n        (Entity.USER, self.user_response),\n    ]\n</code></pre>"},{"location":"reference/#langroid.Agent.entity_responders_async","title":"<code>entity_responders_async()</code>","text":"<p>Async version of <code>entity_responders</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders_async(\n    self,\n) -&gt; List[\n    Tuple[\n        Entity,\n        Callable[\n            [None | str | ChatDocument], Coroutine[Any, Any, None | ChatDocument]\n        ],\n    ]\n]:\n    \"\"\"\n    Async version of `entity_responders`. See there for details.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response_async),\n        (Entity.LLM, self.llm_response_async),\n        (Entity.USER, self.user_response_async),\n    ]\n</code></pre>"},{"location":"reference/#langroid.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\n    self, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n    \"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/#langroid.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable a message class from being handled by this Agent.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/#langroid.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes. Returns:     str: The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n    \"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    # use at most 2 sample conversations, no need to be exhaustive;\n    sample_convo = [\n        msg_cls().usage_examples(random=True)  # type: ignore\n        for i, msg_cls in enumerate(enabled_classes)\n        if i &lt; 2\n    ]\n    return \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/#langroid.Agent.create_agent_response","title":"<code>create_agent_response(content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for agent_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_agent_response(\n    self,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: Optional[List[OpenAIToolCall]] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for agent_response.\"\"\"\n    return self.response_template(\n        Entity.AGENT,\n        content=content,\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/#langroid.Agent.render_agent_response","title":"<code>render_agent_response(results)</code>","text":"<p>Render the response from the agent, typically from tool-handling. Args:     results: results from tool-handling, which may be a string,         a dict of tool results, or a ChatDocument.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def render_agent_response(\n    self,\n    results: Optional[str | OrderedDict[str, str] | ChatDocument],\n) -&gt; None:\n    \"\"\"\n    Render the response from the agent, typically from tool-handling.\n    Args:\n        results: results from tool-handling, which may be a string,\n            a dict of tool results, or a ChatDocument.\n    \"\"\"\n    if self.config.hide_agent_response or results is None:\n        return\n    if isinstance(results, str):\n        results_str = results\n    elif isinstance(results, ChatDocument):\n        results_str = results.content\n    elif isinstance(results, dict):\n        results_str = json.dumps(results, indent=2)\n    if not settings.quiet:\n        console.print(f\"[red]{self.indent}\", end=\"\")\n        print(f\"[red]Agent: {escape(results_str)}\")\n</code></pre>"},{"location":"reference/#langroid.Agent.agent_response_async","title":"<code>agent_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>agent_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def agent_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `agent_response`. See there for details.\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = await self.handle_message_async(msg)\n\n    return self._agent_response_final(msg, results)\n</code></pre>"},{"location":"reference/#langroid.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\", typically (but not only) used to handle LLM's \"tool message\" or <code>function_call</code> (e.g. OpenAI <code>function_call</code>). Args:     msg (str|ChatDocument): the input to respond to: if msg is a string,         and it contains a valid JSON-structured \"tool message\", or         if msg is a ChatDocument, and it contains a <code>function_call</code>. Returns:     Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Response from the \"agent itself\", typically (but not only)\n    used to handle LLM's \"tool message\" or `function_call`\n    (e.g. OpenAI `function_call`).\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = self.handle_message(msg)\n\n    return self._agent_response_final(msg, results)\n</code></pre>"},{"location":"reference/#langroid.Agent.process_tool_results","title":"<code>process_tool_results(results, id2result, tool_calls=None)</code>","text":"<p>Process results from a response, based on whether they are results of OpenAI tool-calls from THIS agent, so that we can construct an appropriate LLMMessage that contains tool results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>str</code> <p>A possible string result from handling tool(s)</p> required <code>id2result</code> <code>OrderedDict[str, str] | None</code> <p>A dict of OpenAI tool id -&gt; result, if there are multiple tool results.</p> required <code>tool_calls</code> <code>List[OpenAIToolCall] | None</code> <p>List of OpenAI tool-calls that the results are a response to.</p> <code>None</code> Return <ul> <li>str: The response string</li> <li>Dict[str,str]|None: A dict of OpenAI tool id -&gt; result, if there are     multiple tool results.</li> <li>str|None: tool_id if there was a single tool result</li> </ul> Source code in <code>langroid/agent/base.py</code> <pre><code>def process_tool_results(\n    self,\n    results: str,\n    id2result: OrderedDict[str, str] | None,\n    tool_calls: List[OpenAIToolCall] | None = None,\n) -&gt; Tuple[str, Dict[str, str] | None, str | None]:\n    \"\"\"\n    Process results from a response, based on whether\n    they are results of OpenAI tool-calls from THIS agent, so that\n    we can construct an appropriate LLMMessage that contains tool results.\n\n    Args:\n        results (str): A possible string result from handling tool(s)\n        id2result (OrderedDict[str,str]|None): A dict of OpenAI tool id -&gt; result,\n            if there are multiple tool results.\n        tool_calls (List[OpenAIToolCall]|None): List of OpenAI tool-calls that the\n            results are a response to.\n\n    Return:\n        - str: The response string\n        - Dict[str,str]|None: A dict of OpenAI tool id -&gt; result, if there are\n            multiple tool results.\n        - str|None: tool_id if there was a single tool result\n\n    \"\"\"\n    id2result_ = copy.deepcopy(id2result) if id2result is not None else None\n    results_str = \"\"\n    oai_tool_id = None\n\n    if results != \"\":\n        # in this case ignore id2result\n        assert (\n            id2result is None\n        ), \"id2result should be None when results string is non-empty!\"\n        results_str = results\n        if len(self.oai_tool_calls) &gt; 0:\n            # We only have one result, so in case there is a\n            # \"pending\" OpenAI tool-call, we expect no more than 1 such.\n            assert (\n                len(self.oai_tool_calls) == 1\n            ), \"There are multiple pending tool-calls, but only one result!\"\n            # We record the tool_id of the tool-call that\n            # the result is a response to, so that ChatDocument.to_LLMMessage\n            # can properly set the `tool_call_id` field of the LLMMessage.\n            oai_tool_id = self.oai_tool_calls[0].id\n    elif id2result is not None and id2result_ is not None:  # appease mypy\n        if len(id2result_) == len(self.oai_tool_calls):\n            # if the number of pending tool calls equals the number of results,\n            # then ignore the ids in id2result, and use the results in order,\n            # which is preserved since id2result is an OrderedDict.\n            assert len(id2result_) &gt; 1, \"Expected to see &gt; 1 result in id2result!\"\n            results_str = \"\"\n            id2result_ = OrderedDict(\n                zip(\n                    [tc.id or \"\" for tc in self.oai_tool_calls], id2result_.values()\n                )\n            )\n        else:\n            assert (\n                tool_calls is not None\n            ), \"tool_calls cannot be None when id2result is not None!\"\n            # This must be an OpenAI tool id -&gt; result map;\n            # However some ids may not correspond to the tool-calls in the list of\n            # pending tool-calls (self.oai_tool_calls).\n            # Such results are concatenated into a simple string, to store in the\n            # ChatDocument.content, and the rest\n            # (i.e. those that DO correspond to tools in self.oai_tool_calls)\n            # are stored as a dict in ChatDocument.oai_tool_id2result.\n\n            # OAI tools from THIS agent, awaiting response\n            pending_tool_ids = [tc.id for tc in self.oai_tool_calls]\n            # tool_calls that the results are a response to\n            # (but these may have been sent from another agent, hence may not be in\n            # self.oai_tool_calls)\n            parent_tool_id2name = {\n                tc.id: tc.function.name\n                for tc in tool_calls or []\n                if tc.function is not None\n            }\n\n            # (id, result) for result NOT corresponding to self.oai_tool_calls,\n            # i.e. these are results of EXTERNAL tool-calls from another agent.\n            external_tool_id_results = []\n\n            for tc_id, result in id2result.items():\n                if tc_id not in pending_tool_ids:\n                    external_tool_id_results.append((tc_id, result))\n                    id2result_.pop(tc_id)\n            if len(external_tool_id_results) == 0:\n                results_str = \"\"\n            elif len(external_tool_id_results) == 1:\n                results_str = external_tool_id_results[0][1]\n            else:\n                results_str = \"\\n\\n\".join(\n                    [\n                        f\"Result from tool/function \"\n                        f\"{parent_tool_id2name[id]}: {result}\"\n                        for id, result in external_tool_id_results\n                    ]\n                )\n\n            if len(id2result_) == 0:\n                id2result_ = None\n            elif len(id2result_) == 1 and len(external_tool_id_results) == 0:\n                results_str = list(id2result_.values())[0]\n                oai_tool_id = list(id2result_.keys())[0]\n                id2result_ = None\n\n    return results_str, id2result_, oai_tool_id\n</code></pre>"},{"location":"reference/#langroid.Agent.response_template","title":"<code>response_template(e, content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for response from entity <code>e</code>.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def response_template(\n    self,\n    e: Entity,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: Optional[List[OpenAIToolCall]] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for response from entity `e`.\"\"\"\n    return ChatDocument(\n        content=content or \"\",\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        oai_tool_choice=oai_tool_choice,\n        metadata=ChatDocMetaData(\n            source=e, sender=e, sender_name=self.config.name, recipient=recipient\n        ),\n    )\n</code></pre>"},{"location":"reference/#langroid.Agent.create_user_response","title":"<code>create_user_response(content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for user_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_user_response(\n    self,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: List[OpenAIToolCall] | None = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for user_response.\"\"\"\n    return self.response_template(\n        e=Entity.USER,\n        content=content,\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/#langroid.Agent.user_can_respond","title":"<code>user_can_respond(msg=None)</code>","text":"<p>Whether the user can respond to a message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_can_respond(self, msg: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the user can respond to a message.\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n\n    \"\"\"\n    # When msg explicitly addressed to user, this means an actual human response\n    # is being sought.\n    need_human_response = (\n        isinstance(msg, ChatDocument) and msg.metadata.recipient == Entity.USER\n    )\n\n    if not self.interactive and not need_human_response:\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/#langroid.Agent.user_response_async","title":"<code>user_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>user_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def user_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `user_response`. See there for details.\n    \"\"\"\n    if not self.user_can_respond(msg):\n        return None\n\n    if self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if (\n            self.callbacks.get_user_response_async is not None\n            and self.callbacks.get_user_response_async is not async_noop_fn\n        ):\n            user_msg = await self.callbacks.get_user_response_async(prompt=\"\")\n        elif self.callbacks.get_user_response is not None:\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}\"\n                + self.config.human_prompt\n                + f\"\\n{self.indent}\"\n            )\n\n    return self._user_response_final(msg, user_msg)\n</code></pre>"},{"location":"reference/#langroid.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n        (str) User response, packaged as a ChatDocument\n\n    \"\"\"\n\n    if not self.user_can_respond(msg):\n        return None\n\n    if self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if self.callbacks.get_user_response is not None:\n            # ask user with empty prompt: no need for prompt\n            # since user has seen the conversation so far.\n            # But non-empty prompt can be useful when Agent\n            # uses a tool that requires user input, or in other scenarios.\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}\"\n                + self.config.human_prompt\n                + f\"\\n{self.indent}\"\n            )\n\n    return self._user_response_final(msg, user_msg)\n</code></pre>"},{"location":"reference/#langroid.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n\n    Returns:\n\n    \"\"\"\n    if self.llm is None:\n        return False\n\n    if message is not None and len(self.try_get_tool_messages(message)) &gt; 0:\n        # if there is a valid \"tool\" message (either JSON or via `function_call`)\n        # then LLM cannot respond to it\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/#langroid.Agent.can_respond","title":"<code>can_respond(message=None)</code>","text":"<p>Whether the agent can respond to a message. Used in Task.py to skip a sub-task when we know it would not respond. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the agent can respond to a message.\n    Used in Task.py to skip a sub-task when we know it would not respond.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n    \"\"\"\n    tools = self.try_get_tool_messages(message)\n    if len(tools) == 0 and self.config.respond_tools_only:\n        return False\n    if message is not None and self.has_only_unhandled_tools(message):\n        # The message has tools that are NOT enabled to be handled by this agent,\n        # which means the agent cannot respond to it.\n        return False\n    return True\n</code></pre>"},{"location":"reference/#langroid.Agent.create_llm_response","title":"<code>create_llm_response(content=None, content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for llm_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_llm_response(\n    self,\n    content: str | None = None,\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: None | List[OpenAIToolCall] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for llm_response.\"\"\"\n    return self.response_template(\n        Entity.LLM,\n        content=content,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/#langroid.Agent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self,\n    message: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `llm_response`. See there for details.\n    \"\"\"\n    if message is None or not self.llm_can_respond(message):\n        return None\n\n    if isinstance(message, ChatDocument):\n        prompt = message.content\n    else:\n        prompt = message\n\n    output_len = self.config.llm.model_max_output_tokens\n    if self.num_tokens(prompt) + output_len &gt; self.llm.completion_context_length():\n        output_len = self.llm.completion_context_length() - self.num_tokens(prompt)\n        if output_len &lt; self.config.llm.min_output_tokens:\n            raise ValueError(\n                \"\"\"\n            Token-length of Prompt + Output is longer than the\n            completion context length of the LLM!\n            \"\"\"\n            )\n        else:\n            logger.warning(\n                f\"\"\"\n            Requested output length has been shortened to {output_len}\n            so that the total length of Prompt + Output is less than\n            the completion context length of the LLM.\n            \"\"\"\n            )\n\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm.agenerate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        print(cached + \"[green]\" + escape(response.message))\n    async with self.lock:\n        self.update_token_usage(\n            response,\n            prompt,\n            self.llm.get_stream(),\n            chat=False,  # i.e. it's a completion model not chat model\n            print_response_stats=self.config.show_stats and not settings.quiet,\n        )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = (\n        [] if isinstance(message, str) else message.metadata.tool_ids\n    )\n    return cdoc\n</code></pre>"},{"location":"reference/#langroid.Agent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>LLM response to a prompt. Args:     message (str|ChatDocument): prompt string, or ChatDocument object</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self,\n    message: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    LLM response to a prompt.\n    Args:\n        message (str|ChatDocument): prompt string, or ChatDocument object\n\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\n    if message is None or not self.llm_can_respond(message):\n        return None\n\n    if isinstance(message, ChatDocument):\n        prompt = message.content\n    else:\n        prompt = message\n\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream():\n            # show rich spinner only if not streaming!\n            cm = status(\"LLM responding to message...\")\n            stack.enter_context(cm)\n        output_len = self.config.llm.model_max_output_tokens\n        if (\n            self.num_tokens(prompt) + output_len\n            &gt; self.llm.completion_context_length()\n        ):\n            output_len = self.llm.completion_context_length() - self.num_tokens(\n                prompt\n            )\n            if output_len &lt; self.config.llm.min_output_tokens:\n                raise ValueError(\n                    \"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n                )\n            else:\n                logger.warning(\n                    f\"\"\"\n                Requested output length has been shortened to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM.\n                \"\"\"\n                )\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        response = self.llm.generate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # we would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response\n        # If we are here, it means the response has not yet been displayed.\n        cached = \"[red](cached)[/red]\" if response.cached else \"\"\n        console.print(f\"[green]{self.indent}\", end=\"\")\n        print(cached + \"[green]\" + escape(response.message))\n    self.update_token_usage(\n        response,\n        prompt,\n        self.llm.get_stream(),\n        chat=False,  # i.e. it's a completion model not chat model\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = (\n        [] if isinstance(message, str) else message.metadata.tool_ids\n    )\n    return cdoc\n</code></pre>"},{"location":"reference/#langroid.Agent.has_tool_message_attempt","title":"<code>has_tool_message_attempt(msg)</code>","text":"<p>Check whether msg contains a Tool/fn-call attempt (by the LLM).</p> <p>CAUTION: This uses self.get_tool_messages(msg) which as a side-effect may update msg.tool_messages when msg is a ChatDocument, if there are any tools in msg.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_tool_message_attempt(self, msg: str | ChatDocument | None) -&gt; bool:\n    \"\"\"\n    Check whether msg contains a Tool/fn-call attempt (by the LLM).\n\n    CAUTION: This uses self.get_tool_messages(msg) which as a side-effect\n    may update msg.tool_messages when msg is a ChatDocument, if there are\n    any tools in msg.\n    \"\"\"\n    if msg is None:\n        return False\n    if isinstance(msg, ChatDocument):\n        if len(msg.tool_messages) &gt; 0:\n            return True\n        if msg.metadata.sender != Entity.LLM:\n            return False\n    try:\n        tools = self.get_tool_messages(msg)\n        return len(tools) &gt; 0\n    except (ValidationError, XMLException):\n        # there is a tool/fn-call attempt but had a validation error,\n        # so we still consider this a tool message \"attempt\"\n        return True\n    return False\n</code></pre>"},{"location":"reference/#langroid.Agent.has_only_unhandled_tools","title":"<code>has_only_unhandled_tools(msg)</code>","text":"<p>Does the msg have at least one tool, and none of the tools in the msg are handleable by this agent?</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_only_unhandled_tools(self, msg: str | ChatDocument) -&gt; bool:\n    \"\"\"\n    Does the msg have at least one tool, and none of the tools in the msg are\n    handleable by this agent?\n    \"\"\"\n    if msg is None:\n        return False\n    tools = self.try_get_tool_messages(msg, all_tools=True)\n    if len(tools) == 0:\n        return False\n    return all(not self._tool_recipient_match(t) for t in tools)\n</code></pre>"},{"location":"reference/#langroid.Agent.get_tool_messages","title":"<code>get_tool_messages(msg, all_tools=False)</code>","text":"<p>Get ToolMessages recognized in msg, handle-able by this agent. NOTE: as a side-effect, this will update msg.tool_messages when msg is a ChatDocument and msg contains tool messages.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the message to extract tools from.</p> required <code>all_tools</code> <code>bool</code> <ul> <li>if True, return all tools,     i.e. any recognized tool in self.llm_tools_known,     whether it is handled by this agent or not;</li> <li>otherwise, return only the tools handled by this agent.</li> </ul> <code>False</code> <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_tool_messages(\n    self,\n    msg: str | ChatDocument | None,\n    all_tools: bool = False,\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Get ToolMessages recognized in msg, handle-able by this agent.\n    NOTE: as a side-effect, this will update msg.tool_messages\n    when msg is a ChatDocument and msg contains tool messages.\n\n    Args:\n        msg (str|ChatDocument): the message to extract tools from.\n        all_tools (bool):\n            - if True, return all tools,\n                i.e. any recognized tool in self.llm_tools_known,\n                whether it is handled by this agent or not;\n            - otherwise, return only the tools handled by this agent.\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n\n    if msg is None:\n        return []\n\n    if isinstance(msg, str):\n        json_tools = self.get_formatted_tool_messages(msg)\n        if all_tools:\n            return json_tools\n        else:\n            return [\n                t\n                for t in json_tools\n                if self._tool_recipient_match(t) and t.default_value(\"request\")\n            ]\n\n    if len(msg.tool_messages) &gt; 0:\n        # We've already found tool_messages,\n        # (either via OpenAI Fn-call or Langroid-native ToolMessage);\n        # or they were added by an agent_response.\n        # note these could be from a forwarded msg from another agent,\n        # so return ONLY the messages THIS agent to enabled to handle.\n        if all_tools:\n            return msg.tool_messages\n        return [t for t in msg.tool_messages if self._tool_recipient_match(t)]\n\n    if (\n        msg.all_tool_messages is not None\n        and msg.all_tool_messages_agent_id == self.id\n    ):\n        # We've already identified all_tool_messages in the msg by this same agent;\n        # so use them to return the corresponding ToolMessage objects\n        if all_tools:\n            return msg.all_tool_messages\n        msg.tool_messages = [\n            t for t in msg.all_tool_messages if self._tool_recipient_match(t)\n        ]\n        return msg.tool_messages\n\n    assert isinstance(msg, ChatDocument)\n    if (\n        SearchForTools.CONTENT.value in self.search_for_tools\n        and msg.content != \"\"\n        and msg.oai_tool_calls is None\n        and msg.function_call is None\n    ):\n\n        tools = self.get_formatted_tool_messages(\n            msg.content, from_llm=msg.metadata.sender == Entity.LLM\n        )\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        # filter for actually handle-able tools, and recipient is this agent\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n\n        if all_tools:\n            return tools\n        else:\n            return my_tools\n\n    # otherwise, we look for `tool_calls` (possibly multiple)\n    if SearchForTools.TOOLS.value in self.search_for_tools:\n        tools = self.get_oai_tool_calls_classes(msg)\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n    else:\n        tools = []\n        my_tools = []\n\n    if len(tools) == 0 and SearchForTools.FUNCTIONS.value in self.search_for_tools:\n        # otherwise, we look for a `function_call`\n        fun_call_cls = self.get_function_call_class(msg)\n        tools = [fun_call_cls] if fun_call_cls is not None else []\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n    if all_tools:\n        return tools\n    else:\n        return my_tools\n</code></pre>"},{"location":"reference/#langroid.Agent.get_formatted_tool_messages","title":"<code>get_formatted_tool_messages(input_str, from_llm=True)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to tool-formatted substrings, if any. ASSUMPTION - These tools are either ALL JSON-based, or ALL XML-based (i.e. not a mix of both). Terminology: a \"formatted tool msg\" is one which the LLM generates as     part of its raw string output, rather than within a JSON object     in the API response (i.e. this method does not extract tools/fns returned     by OpenAI's tools/fns API or similar APIs).</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <code>from_llm</code> <code>bool</code> <p>whether the input was generated by the LLM. If so, we track malformed tool calls.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_formatted_tool_messages(\n    self, input_str: str, from_llm: bool = True\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Returns ToolMessage objects (tools) corresponding to\n    tool-formatted substrings, if any.\n    ASSUMPTION - These tools are either ALL JSON-based, or ALL XML-based\n    (i.e. not a mix of both).\n    Terminology: a \"formatted tool msg\" is one which the LLM generates as\n        part of its raw string output, rather than within a JSON object\n        in the API response (i.e. this method does not extract tools/fns returned\n        by OpenAI's tools/fns API or similar APIs).\n\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n        from_llm (bool): whether the input was generated by the LLM. If so,\n            we track malformed tool calls.\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n    self.tool_error = False\n    substrings = XMLToolMessage.find_candidates(input_str)\n    is_json = False\n    if len(substrings) == 0:\n        substrings = extract_top_level_json(input_str)\n        is_json = len(substrings) &gt; 0\n        if not is_json:\n            return []\n\n    results = [self._get_one_tool_message(j, is_json, from_llm) for j in substrings]\n    valid_results = [r for r in results if r is not None]\n    # If any tool is correctly formed we do not set the flag\n    if len(valid_results) &gt; 0:\n        self.tool_error = False\n    return valid_results\n</code></pre>"},{"location":"reference/#langroid.Agent.get_function_call_class","title":"<code>get_function_call_class(msg)</code>","text":"<p>From ChatDocument (constructed from an LLM Response), get the <code>ToolMessage</code> corresponding to the <code>function_call</code> if it exists.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_function_call_class(self, msg: ChatDocument) -&gt; Optional[ToolMessage]:\n    \"\"\"\n    From ChatDocument (constructed from an LLM Response), get the `ToolMessage`\n    corresponding to the `function_call` if it exists.\n    \"\"\"\n    if msg.function_call is None:\n        return None\n    tool_name = msg.function_call.name\n    tool_msg = msg.function_call.arguments or {}\n    self.tool_error = False\n    if tool_name not in self.llm_tools_handled:\n        logger.warning(\n            f\"\"\"\n            The function_call '{tool_name}' is not handled\n            by the agent named '{self.config.name}'!\n            If you intended this agent to handle this function_call,\n            either the fn-call name is incorrectly generated by the LLM,\n            (in which case you may need to adjust your LLM instructions),\n            or you need to enable this agent to handle this fn-call.\n            \"\"\"\n        )\n        if (\n            tool_name not in self.all_llm_tools_known\n            and msg.metadata.sender == Entity.LLM\n        ):\n            self.tool_error = True\n        return None\n    tool_class = self.llm_tools_map[tool_name]\n    tool_msg.update(dict(request=tool_name))\n    try:\n        tool = tool_class.model_validate(tool_msg)\n    except ValidationError as ve:\n        # Store tool class as an attribute on the exception\n        ve.tool_class = tool_class  # type: ignore\n        raise ve\n    return tool\n</code></pre>"},{"location":"reference/#langroid.Agent.get_oai_tool_calls_classes","title":"<code>get_oai_tool_calls_classes(msg)</code>","text":"<p>From ChatDocument (constructed from an LLM Response), get  a list of ToolMessages corresponding to the <code>tool_calls</code>, if any.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_oai_tool_calls_classes(self, msg: ChatDocument) -&gt; List[ToolMessage]:\n    \"\"\"\n    From ChatDocument (constructed from an LLM Response), get\n     a list of ToolMessages corresponding to the `tool_calls`, if any.\n    \"\"\"\n\n    if msg.oai_tool_calls is None:\n        return []\n    tools = []\n    all_errors = True\n    for tc in msg.oai_tool_calls:\n        if tc.function is None:\n            continue\n        tool_name = tc.function.name\n        tool_msg = tc.function.arguments or {}\n        if tool_name not in self.llm_tools_handled:\n            logger.warning(\n                f\"\"\"\n                The tool_call '{tool_name}' is not handled\n                by the agent named '{self.config.name}'!\n                If you intended this agent to handle this function_call,\n                either the fn-call name is incorrectly generated by the LLM,\n                (in which case you may need to adjust your LLM instructions),\n                or you need to enable this agent to handle this fn-call.\n                \"\"\"\n            )\n            continue\n        all_errors = False\n        tool_class = self.llm_tools_map[tool_name]\n        tool_msg.update(dict(request=tool_name))\n        try:\n            tool = tool_class.model_validate(tool_msg)\n        except ValidationError as ve:\n            # Store tool class as an attribute on the exception\n            ve.tool_class = tool_class  # type: ignore\n            raise ve\n        tool.id = tc.id or \"\"\n        tools.append(tool)\n    # When no tool is valid and the message was produced\n    # by the LLM, set the recovery flag\n    self.tool_error = all_errors and msg.metadata.sender == Entity.LLM\n    return tools\n</code></pre>"},{"location":"reference/#langroid.Agent.tool_validation_error","title":"<code>tool_validation_error(ve, tool_class=None)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields. Args:     ve (ValidationError): The exception raised     tool_class (Optional[Type[ToolMessage]]): The tool class that         failed validation</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(\n    self, ve: ValidationError, tool_class: Optional[Type[ToolMessage]] = None\n) -&gt; str:\n    \"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        ve (ValidationError): The exception raised\n        tool_class (Optional[Type[ToolMessage]]): The tool class that\n            failed validation\n\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\n    # First try to get tool class from the exception itself\n    if hasattr(ve, \"tool_class\") and ve.tool_class:\n        tool_name = ve.tool_class.default_value(\"request\")  # type: ignore\n    elif tool_class is not None:\n        tool_name = tool_class.default_value(\"request\")\n    else:\n        # Fallback: try to extract from error context if available\n        tool_name = \"Unknown Tool\"\n    bad_field_errors = \"\\n\".join(\n        [f\"{e['loc']}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n    )\n    return f\"\"\"\n    There were one or more errors in your attempt to use the\n    TOOL or function_call named '{tool_name}':\n    {bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/#langroid.Agent.handle_message_async","title":"<code>handle_message_async(msg)</code>  <code>async</code>","text":"<p>Asynch version of <code>handle_message</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def handle_message_async(\n    self, msg: str | ChatDocument\n) -&gt; None | str | OrderedDict[str, str] | ChatDocument:\n    \"\"\"\n    Asynch version of `handle_message`. See there for details.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n        tools = [t for t in tools if self._tool_recipient_match(t)]\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except XMLException as xe:  # from XMLToolMessage parsing\n        return str(xe)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) &gt; 1 and not self.config.allow_multiple_tools:\n        return self.to_ChatDocument(\"ERROR: Use ONE tool at a time!\")\n    if len(tools) == 0:\n        fallback_result = self.handle_message_fallback(msg)\n        if fallback_result is None:\n            return None\n        return self.to_ChatDocument(\n            fallback_result,\n            chat_doc=msg if isinstance(msg, ChatDocument) else None,\n        )\n    chat_doc = msg if isinstance(msg, ChatDocument) else None\n\n    results = self._get_multiple_orch_tool_errs(tools)\n    if not results:\n        results = [\n            await self.handle_tool_message_async(t, chat_doc=chat_doc)\n            for t in tools\n        ]\n        # if there's a solitary ChatDocument|str result, return it as is\n        if len(results) == 1 and isinstance(results[0], (str, ChatDocument)):\n            return results[0]\n\n    return self._handle_message_final(tools, results)\n</code></pre>"},{"location":"reference/#langroid.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valid \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>None | str | OrderedDict[str, str] | ChatDocument</code> <p>The result of the handler method can be: - None if no tools successfully handled, or no tools present - str if langroid-native JSON tools were handled, and results concatenated,  OR there's a SINGLE OpenAI tool-call. (We do this so the common scenario of a single tool/fn-call  has a simple behavior). - Dict[str, str] if multiple OpenAI tool-calls were handled  (dict is an id-&gt;result map) - ChatDocument if a handler returned a ChatDocument, intended to be the  final response of the <code>agent_response</code> method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(\n    self, msg: str | ChatDocument\n) -&gt; None | str | OrderedDict[str, str] | ChatDocument:\n    \"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valid \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n\n    Returns:\n        The result of the handler method can be:\n         - None if no tools successfully handled, or no tools present\n         - str if langroid-native JSON tools were handled, and results concatenated,\n             OR there's a SINGLE OpenAI tool-call.\n            (We do this so the common scenario of a single tool/fn-call\n             has a simple behavior).\n         - Dict[str, str] if multiple OpenAI tool-calls were handled\n             (dict is an id-&gt;result map)\n         - ChatDocument if a handler returned a ChatDocument, intended to be the\n             final response of the `agent_response` method.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n        tools = [t for t in tools if self._tool_recipient_match(t)]\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except XMLException as xe:  # from XMLToolMessage parsing\n        return str(xe)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) == 0:\n        fallback_result = self.handle_message_fallback(msg)\n        if fallback_result is None:\n            return None\n        return self.to_ChatDocument(\n            fallback_result,\n            chat_doc=msg if isinstance(msg, ChatDocument) else None,\n        )\n\n    results: List[str | ChatDocument | None] = []\n    if len(tools) &gt; 1 and not self.config.allow_multiple_tools:\n        results = [\"ERROR: Use ONE tool at a time!\"] * len(tools)\n    if not results:\n        results = self._get_multiple_orch_tool_errs(tools)\n    if not results:\n        chat_doc = msg if isinstance(msg, ChatDocument) else None\n        results = [self.handle_tool_message(t, chat_doc=chat_doc) for t in tools]\n        # if there's a solitary ChatDocument|str result, return it as is\n        if len(results) == 1 and isinstance(results[0], (str, ChatDocument)):\n            return results[0]\n\n    return self._handle_message_final(tools, results)\n</code></pre>"},{"location":"reference/#langroid.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method for the case where the msg has no tools that can be handled by this agent. This method can be overridden by subclasses, e.g., to create a \"reminder\" message when a tool is expected but the LLM \"forgot\" to generate one.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     Any: The result of the handler method</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(self, msg: str | ChatDocument) -&gt; Any:\n    \"\"\"\n    Fallback method for the case where the msg has no tools that\n    can be handled by this agent.\n    This method can be overridden by subclasses, e.g.,\n    to create a \"reminder\" message when a tool is expected but the LLM \"forgot\"\n    to generate one.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        Any: The result of the handler method\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/#langroid.Agent.to_ChatDocument","title":"<code>to_ChatDocument(msg, orig_tool_name=None, chat_doc=None, author_entity=Entity.AGENT)</code>","text":"<p>Convert result of a responder (agent_response or llm_response, or task.run()), or tool handler, or handle_message_fallback, to a ChatDocument, to enable handling by other responders/tasks in a task loop possibly involving multiple agents.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Any</code> <p>The result of a responder or tool handler or task.run()</p> required <code>orig_tool_name</code> <code>str</code> <p>The original tool name that generated the response, if any.</p> <code>None</code> <code>chat_doc</code> <code>ChatDocument</code> <p>The original ChatDocument object that <code>msg</code> is a response to.</p> <code>None</code> <code>author_entity</code> <code>Entity</code> <p>The intended author of the result ChatDocument</p> <code>AGENT</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def to_ChatDocument(\n    self,\n    msg: Any,\n    orig_tool_name: str | None = None,\n    chat_doc: Optional[ChatDocument] = None,\n    author_entity: Entity = Entity.AGENT,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Convert result of a responder (agent_response or llm_response, or task.run()),\n    or tool handler, or handle_message_fallback,\n    to a ChatDocument, to enable handling by other\n    responders/tasks in a task loop possibly involving multiple agents.\n\n    Args:\n        msg (Any): The result of a responder or tool handler or task.run()\n        orig_tool_name (str): The original tool name that generated the response,\n            if any.\n        chat_doc (ChatDocument): The original ChatDocument object that `msg`\n            is a response to.\n        author_entity (Entity): The intended author of the result ChatDocument\n    \"\"\"\n    if msg is None or isinstance(msg, ChatDocument):\n        return msg\n\n    is_agent_author = author_entity == Entity.AGENT\n\n    if isinstance(msg, str):\n        return self.response_template(author_entity, content=msg, content_any=msg)\n    elif isinstance(msg, ToolMessage):\n        # result is a ToolMessage, so...\n        result_tool_name = msg.default_value(\"request\")\n        if (\n            is_agent_author\n            and result_tool_name in self.llm_tools_handled\n            and (orig_tool_name is None or orig_tool_name != result_tool_name)\n        ):\n            # TODO: do we need to remove the tool message from the chat_doc?\n            # if (chat_doc is not None and\n            #     msg in chat_doc.tool_messages):\n            #    chat_doc.tool_messages.remove(msg)\n            # if we can handle it, do so\n            result = self.handle_tool_message(msg, chat_doc=chat_doc)\n            if result is not None and isinstance(result, ChatDocument):\n                return result\n        else:\n            # else wrap it in an agent response and return it so\n            # orchestrator can find a respondent\n            return self.response_template(author_entity, tool_messages=[msg])\n    else:\n        result = to_string(msg)\n\n    return (\n        None\n        if result is None\n        else self.response_template(author_entity, content=result, content_any=msg)\n    )\n</code></pre>"},{"location":"reference/#langroid.Agent.from_ChatDocument","title":"<code>from_ChatDocument(msg, output_type)</code>","text":"<p>Extract a desired output_type from a ChatDocument object. We use this fallback order: - if <code>msg.content_any</code> exists and matches the output_type, return it - if <code>msg.content</code> exists and output_type is str return it - if output_type is a ToolMessage, return the first tool in <code>msg.tool_messages</code> - if output_type is a list of ToolMessage,     return all tools in <code>msg.tool_messages</code> - search for a tool in <code>msg.tool_messages</code> that has a field of output_type,      and if found, return that field value - return None if all the above fail</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def from_ChatDocument(self, msg: ChatDocument, output_type: Type[T]) -&gt; Optional[T]:\n    \"\"\"\n    Extract a desired output_type from a ChatDocument object.\n    We use this fallback order:\n    - if `msg.content_any` exists and matches the output_type, return it\n    - if `msg.content` exists and output_type is str return it\n    - if output_type is a ToolMessage, return the first tool in `msg.tool_messages`\n    - if output_type is a list of ToolMessage,\n        return all tools in `msg.tool_messages`\n    - search for a tool in `msg.tool_messages` that has a field of output_type,\n         and if found, return that field value\n    - return None if all the above fail\n    \"\"\"\n    content = msg.content\n    if output_type is str and content != \"\":\n        return cast(T, content)\n    content_any = msg.content_any\n    if content_any is not None and isinstance(content_any, output_type):\n        return cast(T, content_any)\n\n    tools = self.try_get_tool_messages(msg, all_tools=True)\n\n    if get_origin(output_type) is list:\n        list_element_type = get_args(output_type)[0]\n        if issubclass(list_element_type, ToolMessage):\n            # list_element_type is a subclass of ToolMessage:\n            # We output a list of objects derived from list_element_type\n            return cast(\n                T,\n                [t for t in tools if isinstance(t, list_element_type)],\n            )\n    elif get_origin(output_type) is None and issubclass(output_type, ToolMessage):\n        # output_type is a subclass of ToolMessage:\n        # return the first tool that has this specific output_type\n        for tool in tools:\n            if isinstance(tool, output_type):\n                return cast(T, tool)\n        return None\n    elif get_origin(output_type) is None and output_type in (str, int, float, bool):\n        # attempt to get the output_type from the content,\n        # if it's a primitive type\n        primitive_value = from_string(content, output_type)  # type: ignore\n        if primitive_value is not None:\n            return cast(T, primitive_value)\n\n    # then search for output_type as a field in a tool\n    for tool in tools:\n        value = tool.get_value_of_type(output_type)\n        if value is not None:\n            return cast(T, value)\n    return None\n</code></pre>"},{"location":"reference/#langroid.Agent.handle_tool_message_async","title":"<code>handle_tool_message_async(tool, chat_doc=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>handle_tool_message</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def handle_tool_message_async(\n    self,\n    tool: ToolMessage,\n    chat_doc: Optional[ChatDocument] = None,\n) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Asynch version of `handle_tool_message`. See there for details.\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    if hasattr(tool, \"_handler\"):\n        handler_name = getattr(tool, \"_handler\", tool_name)\n    else:\n        handler_name = tool_name\n    handler_method = getattr(self, handler_name + \"_async\", None)\n    if handler_method is None:\n        return self.handle_tool_message(tool, chat_doc=chat_doc)\n    has_chat_doc_arg = (\n        chat_doc is not None\n        and \"chat_doc\" in inspect.signature(handler_method).parameters\n    )\n    try:\n        if has_chat_doc_arg:\n            maybe_result = await handler_method(tool, chat_doc=chat_doc)\n        else:\n            maybe_result = await handler_method(tool)\n        result = self.to_ChatDocument(maybe_result, tool_name, chat_doc)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return self._maybe_truncate_result(\n        result, tool._max_result_tokens\n    )  # type: ignore\n</code></pre>"},{"location":"reference/#langroid.Agent.handle_tool_message","title":"<code>handle_tool_message(tool, chat_doc=None)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object. Args:     tool: ToolMessage object representing the tool request.     chat_doc: Optional ChatDocument object containing the tool request.         This is passed to the tool-handler method only if it has a <code>chat_doc</code>         argument.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(\n    self,\n    tool: ToolMessage,\n    chat_doc: Optional[ChatDocument] = None,\n) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n        chat_doc: Optional ChatDocument object containing the tool request.\n            This is passed to the tool-handler method only if it has a `chat_doc`\n            argument.\n\n    Returns:\n\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    if hasattr(tool, \"_handler\"):\n        handler_name = getattr(tool, \"_handler\", tool_name)\n    else:\n        handler_name = tool_name\n    handler_method = getattr(self, handler_name, None)\n    if handler_method is None:\n        return None\n    has_chat_doc_arg = (\n        chat_doc is not None\n        and \"chat_doc\" in inspect.signature(handler_method).parameters\n    )\n    try:\n        if has_chat_doc_arg:\n            maybe_result = handler_method(tool, chat_doc=chat_doc)\n        else:\n            maybe_result = handler_method(tool)\n        result = self.to_ChatDocument(maybe_result, tool_name, chat_doc)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return self._maybe_truncate_result(\n        result, tool._max_result_tokens\n    )  # type: ignore\n</code></pre>"},{"location":"reference/#langroid.Agent.update_token_usage","title":"<code>update_token_usage(response, prompt, stream, chat=True, print_response_stats=True)</code>","text":"<p>Updates <code>response.usage</code> obj (token usage and cost fields) if needed. An update is needed only if: - stream is True (i.e. streaming was enabled), and - the response was NOT obtained from cached, and - the API did NOT provide the usage/cost fields during streaming   (As of Sep 2024, the OpenAI API started providing these; for other APIs     this may not necessarily be the case).</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>LLMResponse</code> <p>LLMResponse object</p> required <code>prompt</code> <code>str | List[LLMMessage]</code> <p>prompt or list of LLMMessage objects</p> required <code>stream</code> <code>bool</code> <p>whether to update the usage in the response object if the response is not cached.</p> required <code>chat</code> <code>bool</code> <p>whether this is a chat model or a completion model</p> <code>True</code> <code>print_response_stats</code> <code>bool</code> <p>whether to print the response stats</p> <code>True</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def update_token_usage(\n    self,\n    response: LLMResponse,\n    prompt: str | List[LLMMessage],\n    stream: bool,\n    chat: bool = True,\n    print_response_stats: bool = True,\n) -&gt; None:\n    \"\"\"\n    Updates `response.usage` obj (token usage and cost fields) if needed.\n    An update is needed only if:\n    - stream is True (i.e. streaming was enabled), and\n    - the response was NOT obtained from cached, and\n    - the API did NOT provide the usage/cost fields during streaming\n      (As of Sep 2024, the OpenAI API started providing these; for other APIs\n        this may not necessarily be the case).\n\n    Args:\n        response (LLMResponse): LLMResponse object\n        prompt (str | List[LLMMessage]): prompt or list of LLMMessage objects\n        stream (bool): whether to update the usage in the response object\n            if the response is not cached.\n        chat (bool): whether this is a chat model or a completion model\n        print_response_stats (bool): whether to print the response stats\n    \"\"\"\n    if response is None or self.llm is None:\n        return\n\n    no_usage_info = response.usage is None or response.usage.prompt_tokens == 0\n    # Note: If response was not streamed, then\n    # `response.usage` would already have been set by the API,\n    # so we only need to update in the stream case.\n    if stream and no_usage_info:\n        # usage, cost = 0 when response is from cache\n        prompt_tokens = 0\n        completion_tokens = 0\n        cost = 0.0\n        if not response.cached:\n            prompt_tokens = self.num_tokens(prompt)\n            completion_tokens = self.num_tokens(response.message)\n            if response.function_call is not None:\n                completion_tokens += self.num_tokens(str(response.function_call))\n            cost = self.compute_token_cost(prompt_tokens, 0, completion_tokens)\n        response.usage = LLMTokenUsage(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            cost=cost,\n        )\n\n    # update total counters\n    if response.usage is not None:\n        self.total_llm_token_cost += response.usage.cost\n        self.total_llm_token_usage += response.usage.total_tokens\n        self.llm.update_usage_cost(\n            chat,\n            response.usage.prompt_tokens,\n            response.usage.completion_tokens,\n            response.usage.cost,\n        )\n        chat_length = 1 if isinstance(prompt, str) else len(prompt)\n        self.token_stats_str = self._get_response_stats(\n            chat_length, self.total_llm_token_cost, response\n        )\n        if print_response_stats:\n            print(self.indent + self.token_stats_str)\n</code></pre>"},{"location":"reference/#langroid.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and <code>RecipientTool</code> to address requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>user_confirm</code> <code>bool</code> <p>whether to gate the request with a human confirmation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\n    self,\n    agent: \"Agent\",\n    request: str,\n    no_answer: str = NO_ANSWER,\n    user_confirm: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and\n    `RecipientTool` to address requests to other agents. It is generally best to\n    avoid using this method.\n\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer (str): expected response when agent does not know the answer\n        user_confirm (bool): whether to gate the request with a human confirmation\n\n    Returns:\n        str: response from agent\n    \"\"\"\n    agent_type = type(agent).__name__\n    if user_confirm:\n        user_response = Prompt.ask(\n            f\"\"\"[magenta]Here is the request or message:\n            {request}\n            Should I forward this to {agent_type}?\"\"\",\n            default=\"y\",\n            choices=[\"y\", \"n\"],\n        )\n        if user_response not in [\"y\", \"yes\"]:\n            return None\n    answer = agent.llm_response(request)\n    if answer != no_answer:\n        return (f\"{agent_type} says: \" + str(answer)).strip()\n    return None\n</code></pre>"},{"location":"reference/#langroid.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components.</p>"},{"location":"reference/#langroid.StatusCode","title":"<code>StatusCode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Codes meant to be returned by task.run(). Some are not used yet.</p>"},{"location":"reference/#langroid.ChatDocument","title":"<code>ChatDocument(**data)</code>","text":"<p>               Bases: <code>Document</code></p> <p>Represents a message in a conversation among agents. All responders of an agent have signature ChatDocument -&gt; ChatDocument (modulo None, str, etc), and so does the Task.run() method.</p> <p>Attributes:</p> Name Type Description <code>oai_tool_calls</code> <code>Optional[List[OpenAIToolCall]]</code> <p>Tool-calls from an OpenAI-compatible API</p> <code>oai_tool_id2results</code> <code>Optional[OrderedDict[str, str]]</code> <p>Results of tool-calls from OpenAI (dict is a map of tool_id -&gt; result)</p> <code>oai_tool_choice</code> <code>ToolChoiceTypes | Dict[str, Dict[str, str] | str]</code> <p>ToolChoiceTypes | Dict[str, str]: Param controlling how the LLM should choose tool-use in its response (auto, none, required, or a specific tool)</p> <code>function_call</code> <code>Optional[LLMFunctionCall]</code> <p>Function-call from an OpenAI-compatible API     (deprecated by OpenAI, in favor of tool-calls)</p> <code>tool_messages</code> <code>List[ToolMessage]</code> <p>Langroid ToolMessages extracted from - <code>content</code> field (via JSON parsing), - <code>oai_tool_calls</code>, or - <code>function_call</code></p> <code>metadata</code> <code>ChatDocMetaData</code> <p>Metadata for the message, e.g. sender, recipient.</p> <code>attachment</code> <code>None | ChatDocAttachment</code> <p>Any additional data attached.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def __init__(self, **data: Any):\n    super().__init__(**data)\n    ObjectRegistry.register_object(self)\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.delete_id","title":"<code>delete_id(id)</code>  <code>staticmethod</code>","text":"<p>Remove ChatDocument with given id from ObjectRegistry, and all its descendants.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef delete_id(id: str) -&gt; None:\n    \"\"\"Remove ChatDocument with given id from ObjectRegistry,\n    and all its descendants.\n    \"\"\"\n    chat_doc = ChatDocument.from_id(id)\n    # first delete all descendants\n    while chat_doc is not None:\n        next_chat_doc = chat_doc.child\n        ObjectRegistry.remove(chat_doc.id())\n        chat_doc = next_chat_doc\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.get_tool_names","title":"<code>get_tool_names()</code>","text":"<p>Get names of attempted tool usages (JSON or non-JSON) in the content     of the message. Returns:     List[str]: list of attempted tool names     (We say \"attempted\" since we ONLY look at the <code>request</code> component of the     tool-call representation, and we're not fully parsing it into the     corresponding tool message class)</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_tool_names(self) -&gt; List[str]:\n    \"\"\"\n    Get names of attempted tool usages (JSON or non-JSON) in the content\n        of the message.\n    Returns:\n        List[str]: list of *attempted* tool names\n        (We say \"attempted\" since we ONLY look at the `request` component of the\n        tool-call representation, and we're not fully parsing it into the\n        corresponding tool message class)\n\n    \"\"\"\n    tool_candidates = XMLToolMessage.find_candidates(self.content)\n    if len(tool_candidates) == 0:\n        tool_candidates = extract_top_level_json(self.content)\n        if len(tool_candidates) == 0:\n            return []\n        tools = [json.loads(tc).get(\"request\") for tc in tool_candidates]\n    else:\n        tool_dicts = [\n            XMLToolMessage.extract_field_values(tc) for tc in tool_candidates\n        ]\n        tools = [td.get(\"request\") for td in tool_dicts if td is not None]\n    return [str(tool) for tool in tools if tool is not None]\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger Returns:     List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n    \"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\n    tool_type = \"\"  # FUNC or TOOL\n    tool = \"\"  # tool name or function name\n\n    # Skip tool detection for system messages - they contain tool instructions,\n    # not actual tool calls\n    if self.metadata.sender != Entity.SYSTEM:\n        oai_tools = (\n            []\n            if self.oai_tool_calls is None\n            else [t for t in self.oai_tool_calls if t.function is not None]\n        )\n        if self.function_call is not None:\n            tool_type = \"FUNC\"\n            tool = self.function_call.name\n        elif len(oai_tools) &gt; 0:\n            tool_type = \"OAI_TOOL\"\n            tool = \",\".join(t.function.name for t in oai_tools)  # type: ignore\n        else:\n            try:\n                json_tools = self.get_tool_names()\n            except Exception:\n                json_tools = []\n            if json_tools != []:\n                tool_type = \"TOOL\"\n                tool = json_tools[0]\n    recipient = self.metadata.recipient\n    content = self.content\n    sender_entity = self.metadata.sender\n    sender_name = self.metadata.sender_name\n    if tool_type == \"FUNC\":\n        content += str(self.function_call)\n    return ChatDocLoggerFields(\n        sender_entity=sender_entity,\n        sender_name=sender_name,\n        recipient=recipient,\n        block=self.metadata.block,\n        tool_type=tool_type,\n        tool=tool,\n        content=content,\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.pop_tool_ids","title":"<code>pop_tool_ids()</code>","text":"<p>Pop the last tool_id from the stack of tool_ids.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def pop_tool_ids(self) -&gt; None:\n    \"\"\"\n    Pop the last tool_id from the stack of tool_ids.\n    \"\"\"\n    if len(self.metadata.tool_ids) &gt; 0:\n        self.metadata.tool_ids.pop()\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.from_LLMResponse","title":"<code>from_LLMResponse(response, displayed=False, recognize_recipient_in_content=True)</code>  <code>staticmethod</code>","text":"<p>Convert LLMResponse to ChatDocument. Args:     response (LLMResponse): LLMResponse to convert.     displayed (bool): Whether this response was displayed to the user.     recognize_recipient_in_content (bool): Whether to parse message text         for recipient routing (<code>TO[&lt;recipient&gt;]:</code> and JSON         <code>{\"recipient\": ...}</code>). Default True. Returns:     ChatDocument: ChatDocument representation of this LLMResponse.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMResponse(\n    response: LLMResponse,\n    displayed: bool = False,\n    recognize_recipient_in_content: bool = True,\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMResponse to ChatDocument.\n    Args:\n        response (LLMResponse): LLMResponse to convert.\n        displayed (bool): Whether this response was displayed to the user.\n        recognize_recipient_in_content (bool): Whether to parse message text\n            for recipient routing (``TO[&lt;recipient&gt;]:`` and JSON\n            ``{\"recipient\": ...}``). Default True.\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMResponse.\n    \"\"\"\n    recipient, message = response.get_recipient_and_message(\n        recognize_recipient_in_content\n    )\n    message = message.strip()\n    if message in [\"''\", '\"\"']:\n        message = \"\"\n    if response.function_call is not None:\n        ChatDocument._clean_fn_call(response.function_call)\n    if response.oai_tool_calls is not None:\n        # there must be at least one if it's not None\n        for oai_tc in response.oai_tool_calls:\n            ChatDocument._clean_fn_call(oai_tc.function)\n    return ChatDocument(\n        content=message,\n        reasoning=response.reasoning,\n        content_with_reasoning=response.message_with_reasoning,\n        content_any=message,\n        oai_tool_calls=response.oai_tool_calls,\n        function_call=response.function_call,\n        metadata=ChatDocMetaData(\n            source=Entity.LLM,\n            sender=Entity.LLM,\n            usage=response.usage,\n            displayed=displayed,\n            cached=response.cached,\n            recipient=recipient,\n        ),\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.from_LLMMessage","title":"<code>from_LLMMessage(message, sender_name='', recipient='')</code>  <code>staticmethod</code>","text":"<p>Convert LLMMessage to ChatDocument.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>LLMMessage</code> <p>LLMMessage to convert.</p> required <code>sender_name</code> <code>str</code> <p>Name of the sender. Defaults to \"\".</p> <code>''</code> <code>recipient</code> <code>str</code> <p>Name of the recipient. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>ChatDocument</code> <code>'ChatDocument'</code> <p>ChatDocument representation of this LLMMessage.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMMessage(\n    message: LLMMessage,\n    sender_name: str = \"\",\n    recipient: str = \"\",\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMMessage to ChatDocument.\n\n    Args:\n        message (LLMMessage): LLMMessage to convert.\n        sender_name (str): Name of the sender. Defaults to \"\".\n        recipient (str): Name of the recipient. Defaults to \"\".\n\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMMessage.\n    \"\"\"\n    # Map LLMMessage Role to ChatDocument Entity\n    role_to_entity = {\n        Role.USER: Entity.USER,\n        Role.SYSTEM: Entity.SYSTEM,\n        Role.ASSISTANT: Entity.LLM,\n        Role.FUNCTION: Entity.LLM,\n        Role.TOOL: Entity.LLM,\n    }\n\n    sender_entity = role_to_entity.get(message.role, Entity.USER)\n\n    return ChatDocument(\n        content=message.content or \"\",\n        content_any=message.content,\n        files=message.files,\n        function_call=message.function_call,\n        oai_tool_calls=message.tool_calls,\n        metadata=ChatDocMetaData(\n            source=sender_entity,\n            sender=sender_entity,\n            sender_name=sender_name,\n            recipient=recipient,\n            oai_tool_id=message.tool_call_id,\n            tool_ids=[message.tool_id] if message.tool_id else [],\n        ),\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message, oai_tools=None)</code>  <code>staticmethod</code>","text":"<p>Convert to list of LLMMessage, to incorporate into msg-history sent to LLM API. Usually there will be just a single LLMMessage, but when the ChatDocument contains results from multiple OpenAI tool-calls, we would have a sequence LLMMessages, one per tool-call result.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <code>oai_tools</code> <code>Optional[List[OpenAIToolCall]]</code> <p>Tool-calls currently awaiting response, from the ChatAgent's latest message.</p> <code>None</code> <p>Returns:     List[LLMMessage]: list of LLMMessages corresponding to this ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(\n    message: Union[str, \"ChatDocument\"],\n    oai_tools: Optional[List[OpenAIToolCall]] = None,\n) -&gt; List[LLMMessage]:\n    \"\"\"\n    Convert to list of LLMMessage, to incorporate into msg-history sent to LLM API.\n    Usually there will be just a single LLMMessage, but when the ChatDocument\n    contains results from multiple OpenAI tool-calls, we would have a sequence\n    LLMMessages, one per tool-call result.\n\n    Args:\n        message (str|ChatDocument): Message to convert.\n        oai_tools (Optional[List[OpenAIToolCall]]): Tool-calls currently awaiting\n            response, from the ChatAgent's latest message.\n    Returns:\n        List[LLMMessage]: list of LLMMessages corresponding to this ChatDocument.\n    \"\"\"\n\n    sender_role = Role.USER\n    if isinstance(message, str):\n        message = ChatDocument.from_str(message)\n    # Prefer content_with_reasoning when available \u2014 this preserves\n    # inline thought signatures (e.g. &lt;thinking&gt;...&lt;/thinking&gt;) in\n    # message history, which certain models (Gemini 3 Flash, Amazon\n    # Nova) need to maintain reasoning across turns.\n    # content_with_reasoning is only set when inline tags were\n    # actually extracted, so this won't interfere with models that\n    # provide reasoning via a separate API field.\n    content = (\n        message.content_with_reasoning\n        or message.content\n        or to_string(message.content_any)\n        or \"\"\n    )\n    fun_call = message.function_call\n    oai_tool_calls = message.oai_tool_calls\n    if message.metadata.sender == Entity.USER and fun_call is not None:\n        # This may happen when a (parent agent's) LLM generates a\n        # a Function-call, and it ends up being sent to the current task's\n        # LLM (possibly because the function-call is mis-named or has other\n        # issues and couldn't be handled by handler methods).\n        # But a function-call can only be generated by an entity with\n        # Role.ASSISTANT, so we instead put the content of the function-call\n        # in the content of the message.\n        content += \" \" + str(fun_call)\n        fun_call = None\n    if message.metadata.sender == Entity.USER and oai_tool_calls is not None:\n        # same reasoning as for function-call above\n        content += \" \" + \"\\n\\n\".join(str(tc) for tc in oai_tool_calls)\n        oai_tool_calls = None\n    # some LLM APIs (e.g. gemini) don't like empty msg\n    content = content or \" \"\n    sender_name = message.metadata.sender_name\n    tool_ids = message.metadata.tool_ids\n    tool_id = tool_ids[-1] if len(tool_ids) &gt; 0 else \"\"\n    chat_document_id = message.id()\n    if message.metadata.sender == Entity.SYSTEM:\n        sender_role = Role.SYSTEM\n    if (\n        message.metadata.parent is not None\n        and message.metadata.parent.function_call is not None\n    ):\n        # This is a response to a function call, so set the role to FUNCTION.\n        sender_role = Role.FUNCTION\n        sender_name = message.metadata.parent.function_call.name\n    elif oai_tools is not None and len(oai_tools) &gt; 0:\n        pending_tool_ids = [tc.id for tc in oai_tools]\n        # The ChatAgent has pending OpenAI tool-call(s),\n        # so the current ChatDocument contains\n        # results for some/all/none of them.\n\n        if len(oai_tools) == 1:\n            # Case 1:\n            # There was exactly 1 pending tool-call, and in this case\n            # the result would be a plain string in `content`\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=oai_tools[0].id,\n                    content=content,\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n            ]\n\n        elif (\n            message.metadata.oai_tool_id is not None\n            and message.metadata.oai_tool_id in pending_tool_ids\n        ):\n            # Case 2:\n            # ChatDocument.content has result of a single tool-call\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=message.metadata.oai_tool_id,\n                    content=content,\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n            ]\n        elif message.oai_tool_id2result is not None:\n            # Case 2:\n            # There were &gt; 1 tool-calls awaiting response,\n            assert (\n                len(message.oai_tool_id2result) &gt; 1\n            ), \"oai_tool_id2result must have more than 1 item.\"\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=tool_id,\n                    content=result or \" \",\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n                for tool_id, result in message.oai_tool_id2result.items()\n            ]\n    elif message.metadata.sender == Entity.LLM:\n        sender_role = Role.ASSISTANT\n\n    return [\n        LLMMessage(\n            role=sender_role,\n            tool_id=tool_id,  # for OpenAI Assistant\n            content=content,\n            files=message.files,\n            function_call=fun_call,\n            tool_calls=oai_tool_calls,\n            name=sender_name,\n            chat_document_id=chat_document_id,\n        )\n    ]\n</code></pre>"},{"location":"reference/#langroid.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently these \"tools\" are handled by methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p>"},{"location":"reference/#langroid.ToolMessage.instructions","title":"<code>instructions()</code>  <code>classmethod</code>","text":"<p>Instructions on tool usage.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    \"\"\"\n    Instructions on tool usage.\n    \"\"\"\n    return \"\"\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.langroid_tools_instructions","title":"<code>langroid_tools_instructions()</code>  <code>classmethod</code>","text":"<p>Instructions on tool usage when <code>use_tools == True</code>, i.e. when using langroid built-in tools (as opposed to OpenAI-like function calls/tools).</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef langroid_tools_instructions(cls) -&gt; str:\n    \"\"\"\n    Instructions on tool usage when `use_tools == True`, i.e.\n    when using langroid built-in tools\n    (as opposed to OpenAI-like function calls/tools).\n    \"\"\"\n    return \"\"\"\n    IMPORTANT: When using this or any other tool/function, you MUST include a \n    `request` field and set it equal to the FUNCTION/TOOL NAME you intend to use.\n    \"\"\"\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Examples to use in few-shot demos with formatting instructions. Each example can be either: - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or - a tuple (description, ToolMessage instance), where the description is     a natural language \"thought\" that leads to the tool usage,     e.g. (\"I want to find the square of 5\",  SquareTool(num=5))     In some scenarios, including such a description can significantly     enhance reliability of tool use. Returns:</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[\"ToolMessage\" | Tuple[str, \"ToolMessage\"]]:\n    \"\"\"\n    Examples to use in few-shot demos with formatting instructions.\n    Each example can be either:\n    - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or\n    - a tuple (description, ToolMessage instance), where the description is\n        a natural language \"thought\" that leads to the tool usage,\n        e.g. (\"I want to find the square of 5\",  SquareTool(num=5))\n        In some scenarios, including such a description can significantly\n        enhance reliability of tool use.\n    Returns:\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.usage_examples","title":"<code>usage_examples(random=False)</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing examples of how to use the tool-message.</p> <p>Parameters:</p> Name Type Description Default <code>random</code> <code>bool</code> <p>whether to pick a random example from the list of examples. Set to <code>true</code> when using this to illustrate a dialog between LLM and user. (if false, use ALL examples)</p> <code>False</code> <p>Returns:     str: examples of how to use the tool/function-call</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_examples(cls, random: bool = False) -&gt; str:\n    \"\"\"\n    Instruction to the LLM showing examples of how to use the tool-message.\n\n    Args:\n        random (bool): whether to pick a random example from the list of examples.\n            Set to `true` when using this to illustrate a dialog between LLM and\n            user.\n            (if false, use ALL examples)\n    Returns:\n        str: examples of how to use the tool/function-call\n    \"\"\"\n    # pick a random example of the fields\n    if len(cls.examples()) == 0:\n        return \"\"\n    if random:\n        examples = [choice(cls.examples())]\n    else:\n        examples = cls.examples()\n    formatted_examples = [\n        (\n            f\"EXAMPLE {i}: (THOUGHT: {ex[0]}) =&gt; \\n{ex[1].format_example()}\"\n            if isinstance(ex, tuple)\n            else f\"EXAMPLE {i}:\\n {ex.format_example()}\"\n        )\n        for i, ex in enumerate(examples, 1)\n    ]\n    return \"\\n\\n\".join(formatted_examples)\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.get_value_of_type","title":"<code>get_value_of_type(target_type)</code>","text":"<p>Try to find a value of a desired type in the fields of the ToolMessage.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>def get_value_of_type(self, target_type: Type[Any]) -&gt; Any:\n    \"\"\"Try to find a value of a desired type in the fields of the ToolMessage.\"\"\"\n    ignore_fields = self._get_excluded_fields().union({\"request\"})\n    for field_name in set(self.model_dump().keys()) - ignore_fields:\n        value = getattr(self, field_name)\n        if is_instance_of(value, target_type):\n            return value\n    return None\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class Args:     f (str): field name</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>default value of the field, or None if not set or if the field does not exist.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n    \"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n\n    Returns:\n        Any: default value of the field, or None if not set or if the\n            field does not exist.\n    \"\"\"\n    schema = cls.model_json_schema()\n    properties = schema[\"properties\"]\n    return properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.format_instructions","title":"<code>format_instructions(tool=False)</code>  <code>classmethod</code>","text":"<p>Default Instructions to the LLM showing how to use the tool/function-call. Works for GPT4 but override this for weaker LLMs if needed.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>bool</code> <p>instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM) (or else it would be for OpenAI Function calls). Ignored in the default implementation, but can be used in subclasses.</p> <code>False</code> <p>Returns:     str: instructions on how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef format_instructions(cls, tool: bool = False) -&gt; str:\n    \"\"\"\n    Default Instructions to the LLM showing how to use the tool/function-call.\n    Works for GPT4 but override this for weaker LLMs if needed.\n\n    Args:\n        tool: instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM)\n            (or else it would be for OpenAI Function calls).\n            Ignored in the default implementation, but can be used in subclasses.\n    Returns:\n        str: instructions on how to use the message\n    \"\"\"\n    # TODO: when we attempt to use a \"simpler schema\"\n    # (i.e. all nested fields explicit without definitions),\n    # we seem to get worse results, so we turn it off for now\n    param_dict = (\n        # cls.simple_schema() if tool else\n        cls.llm_function_schema(request=True).parameters\n    )\n    examples_str = \"\"\n    if cls.examples():\n        examples_str = \"EXAMPLES:\\n\" + cls.usage_examples()\n    return textwrap.dedent(\n        f\"\"\"\n        TOOL: {cls.default_value(\"request\")}\n        PURPOSE: {cls.default_value(\"purpose\")} \n        JSON FORMAT: {\n            json.dumps(param_dict, indent=4)\n        }\n        {examples_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.group_format_instructions","title":"<code>group_format_instructions()</code>  <code>staticmethod</code>","text":"<p>Template for instructions for a group of tools. Works with GPT4 but override this for weaker LLMs if needed.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@staticmethod\ndef group_format_instructions() -&gt; str:\n    \"\"\"Template for instructions for a group of tools.\n    Works with GPT4 but override this for weaker LLMs if needed.\n    \"\"\"\n    return textwrap.dedent(\n        \"\"\"\n        === ALL AVAILABLE TOOLS and THEIR FORMAT INSTRUCTIONS ===\n        You have access to the following TOOLS to accomplish your task:\n\n        {format_instructions}\n\n        When one of the above TOOLs is applicable, you must express your \n        request as \"TOOL:\" followed by the request in the above format.\n        \"\"\"\n    )\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.llm_function_schema","title":"<code>llm_function_schema(request=False, defaults=True)</code>  <code>classmethod</code>","text":"<p>Clean up the schema of the Pydantic class (which can recursively contain other Pydantic classes), to create a version compatible with OpenAI Function-call API.</p> <p>Adapted from this excellent library: https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>bool</code> <p>whether to include the \"request\" field in the schema. (we set this to True when using Langroid-native TOOLs as opposed to OpenAI Function calls)</p> <code>False</code> <code>defaults</code> <code>bool</code> <p>whether to include fields with default values in the schema,     in the \"properties\" section.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>LLMFunctionSpec</code> <code>LLMFunctionSpec</code> <p>the schema as an LLMFunctionSpec</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(\n    cls,\n    request: bool = False,\n    defaults: bool = True,\n) -&gt; LLMFunctionSpec:\n    \"\"\"\n    Clean up the schema of the Pydantic class (which can recursively contain\n    other Pydantic classes), to create a version compatible with OpenAI\n    Function-call API.\n\n    Adapted from this excellent library:\n    https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py\n\n    Args:\n        request: whether to include the \"request\" field in the schema.\n            (we set this to True when using Langroid-native TOOLs as opposed to\n            OpenAI Function calls)\n        defaults: whether to include fields with default values in the schema,\n                in the \"properties\" section.\n\n    Returns:\n        LLMFunctionSpec: the schema as an LLMFunctionSpec\n\n    \"\"\"\n    schema = copy.deepcopy(cls.model_json_schema())\n    docstring = parse(cls.__doc__ or \"\")\n    parameters = {\n        k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n    }\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (\n            description := param.description\n        ):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    excludes = cls._get_excluded_fields().copy()\n    if not request:\n        excludes = excludes.union({\"request\"})\n    # exclude 'excludes' from parameters[\"properties\"]:\n    parameters[\"properties\"] = {\n        field: details\n        for field, details in parameters[\"properties\"].items()\n        if field not in excludes and (defaults or details.get(\"default\") is None)\n    }\n    parameters[\"required\"] = sorted(\n        k\n        for k, v in parameters[\"properties\"].items()\n        if (\"default\" not in v and k not in excludes)\n    )\n    if request:\n        parameters[\"required\"].append(\"request\")\n\n        # If request is present it must match the default value\n        # Similar to defining request as a literal type\n        parameters[\"request\"] = {\n            \"enum\": [cls.default_value(\"request\")],\n            \"type\": \"string\",\n        }\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            schema[\"description\"] = (\n                f\"Correctly extracted `{cls.__name__}` with all \"\n                f\"the required parameters with correct types\"\n            )\n\n    # Handle nested ToolMessage fields\n    if \"definitions\" in parameters:\n        for v in parameters[\"definitions\"].values():\n            if \"exclude\" in v:\n                v.pop(\"exclude\")\n\n                remove_if_exists(\"purpose\", v[\"properties\"])\n                remove_if_exists(\"id\", v[\"properties\"])\n                if (\n                    \"request\" in v[\"properties\"]\n                    and \"default\" in v[\"properties\"][\"request\"]\n                ):\n                    if \"required\" not in v:\n                        v[\"required\"] = []\n                    v[\"required\"].append(\"request\")\n                    v[\"properties\"][\"request\"] = {\n                        \"type\": \"string\",\n                        \"enum\": [v[\"properties\"][\"request\"][\"default\"]],\n                    }\n\n    parameters.pop(\"exclude\")\n    _recursive_purge_dict_key(parameters, \"title\")\n    _recursive_purge_dict_key(parameters, \"additionalProperties\")\n    return LLMFunctionSpec(\n        name=cls.default_value(\"request\"),\n        description=cls.default_value(\"purpose\")\n        or f\"Tool for {cls.default_value('request')}\",\n        parameters=parameters,\n    )\n</code></pre>"},{"location":"reference/#langroid.ToolMessage.simple_schema","title":"<code>simple_schema()</code>  <code>classmethod</code>","text":"<p>Return a simplified schema for the message, with only the request and required fields. Returns:     Dict[str, Any]: simplified schema</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef simple_schema(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a simplified schema for the message, with only the request and\n    required fields.\n    Returns:\n        Dict[str, Any]: simplified schema\n    \"\"\"\n    schema = generate_simple_schema(\n        cls,\n        exclude=list(cls._get_excluded_fields()),\n    )\n    return schema\n</code></pre>"},{"location":"reference/#langroid.ChatAgent","title":"<code>ChatAgent(config=ChatAgentConfig(), task=None)</code>","text":"<p>               Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with an optional \"Task Spec\", which is a sequence of <code>LLMMessage</code>s. These are used to initialize the <code>task_messages</code> of the agent. In most applications we will use a <code>ChatAgent</code> rather than a bare <code>Agent</code>. The <code>Agent</code> class mainly exists to hold various common methods and attributes. One difference between <code>ChatAgent</code> and <code>Agent</code> is that <code>ChatAgent</code>'s <code>llm_response</code> method uses \"chat mode\" API (i.e. one that takes a message sequence rather than a single message), whereas the same method in the <code>Agent</code> class uses \"completion mode\" API (i.e. one that takes a single message).</p> <pre><code>config: settings for the agent\n</code></pre> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: ChatAgentConfig = ChatAgentConfig(),\n    task: Optional[List[LLMMessage]] = None,\n):\n    \"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n\n    \"\"\"\n    super().__init__(config)\n    self.config: ChatAgentConfig = config\n    self.config._set_fn_or_tools()\n    self.message_history: List[LLMMessage] = []\n    self.init_state()\n    # An agent's \"task\" is defined by a system msg and an optional user msg;\n    # These are \"priming\" messages that kick off the agent's conversation.\n    self.system_message: str = self.config.system_message\n    self.user_message: str | None = self.config.user_message\n\n    if task is not None:\n        # if task contains a system msg, we override the config system msg\n        if len(task) &gt; 0 and task[0].role == Role.SYSTEM:\n            self.system_message = task[0].content\n        # if task contains a user msg, we override the config user msg\n        if len(task) &gt; 1 and task[1].role == Role.USER:\n            self.user_message = task[1].content\n\n    # system-level instructions for using tools/functions:\n    # We maintain these as tools/functions are enabled/disabled,\n    # and whenever an LLM response is sought, these are used to\n    # recreate the system message (via `_create_system_and_tools_message`)\n    # each time, so it reflects the current set of enabled tools/functions.\n    # (a) these are general instructions on using certain tools/functions,\n    #   if they are specified in a ToolMessage class as a classmethod `instructions`\n    self.system_tool_instructions: str = \"\"\n    # (b) these are only for the builtin in Langroid TOOLS mechanism:\n    self.system_tool_format_instructions: str = \"\"\n\n    self.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\n    self.llm_functions_handled: Set[str] = set()\n    self.llm_functions_usable: Set[str] = set()\n    self.llm_function_force: Optional[Dict[str, str]] = None\n\n    self.output_format: Optional[type[ToolMessage | BaseModel]] = None\n\n    self.saved_requests_and_tool_setings = self._requests_and_tool_settings()\n    # This variable is not None and equals a `ToolMessage` T, if and only if:\n    # (a) T has been set as the output_format of this agent, AND\n    # (b) T has been \"enabled for use\" ONLY for enforcing this output format, AND\n    # (c) T has NOT been explicitly \"enabled for use\" by this Agent.\n    self.enabled_use_output_format: Optional[type[ToolMessage]] = None\n    # As above but deals with \"enabled for handling\" instead of \"enabled for use\".\n    self.enabled_handling_output_format: Optional[type[ToolMessage]] = None\n    if config.output_format is not None:\n        self.set_output_format(config.output_format)\n    # instructions specifically related to enforcing `output_format`\n    self.output_format_instructions = \"\"\n\n    # controls whether to disable strict schemas for this agent if\n    # strict mode causes exception\n    self.disable_strict = False\n    # Tracks whether any strict tool is enabled; used to determine whether to set\n    # `self.disable_strict` on an exception\n    self.any_strict = False\n    # Tracks the set of tools on which we force-disable strict decoding\n    self.disable_strict_tools_set: set[str] = set()\n\n    # search for tools according to the agent configuration\n    if not config.search_for_tools_everywhere:\n        if config.use_functions_api:\n            if config.use_tools_api:\n                self.search_for_tools = {SearchForTools.TOOLS.value}\n            else:\n                self.search_for_tools = {SearchForTools.FUNCTIONS.value}\n        else:\n            self.search_for_tools = {SearchForTools.CONTENT.value}\n\n    if self.config.enable_orchestration_tool_handling:\n        # Only enable HANDLING by `agent_response`, NOT LLM generation of these.\n        # This is useful where tool-handlers or agent_response generate these\n        # tools, and need to be handled.\n        # We don't want enable orch tool GENERATION by default, since that\n        # might clutter-up the LLM system message unnecessarily.\n        from langroid.agent.tools.orchestration import (\n            AgentDoneTool,\n            AgentSendTool,\n            DonePassTool,\n            DoneTool,\n            ForwardTool,\n            PassTool,\n            ResultTool,\n            SendTool,\n        )\n\n        self.enable_message(ForwardTool, use=False, handle=True)\n        self.enable_message(DoneTool, use=False, handle=True)\n        self.enable_message(AgentDoneTool, use=False, handle=True)\n        self.enable_message(PassTool, use=False, handle=True)\n        self.enable_message(DonePassTool, use=False, handle=True)\n        self.enable_message(SendTool, use=False, handle=True)\n        self.enable_message(AgentSendTool, use=False, handle=True)\n        self.enable_message(ResultTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.task_messages","title":"<code>task_messages</code>  <code>property</code>","text":"<p>The task messages are the initial messages that define the task of the agent. There will be at least a system message plus possibly a user msg. Returns:     List[LLMMessage]: the task messages</p>"},{"location":"reference/#langroid.ChatAgent.all_llm_tools_known","title":"<code>all_llm_tools_known</code>  <code>property</code>","text":"<p>All known tools; we include <code>output_format</code> if it is a <code>ToolMessage</code>.</p>"},{"location":"reference/#langroid.ChatAgent.init_state","title":"<code>init_state()</code>","text":"<p>Initialize the state of the agent. Just conversation state here, but subclasses can override this to initialize other state.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_state(self) -&gt; None:\n    \"\"\"\n    Initialize the state of the agent. Just conversation state here,\n    but subclasses can override this to initialize other state.\n    \"\"\"\n    super().init_state()\n    self.clear_history(0)\n    self.clear_dialog()\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.from_id","title":"<code>from_id(id)</code>  <code>staticmethod</code>","text":"<p>Get an agent from its ID Args:     agent_id (str): ID of the agent Returns:     ChatAgent: The agent with the given ID</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@staticmethod\ndef from_id(id: str) -&gt; \"ChatAgent\":\n    \"\"\"\n    Get an agent from its ID\n    Args:\n        agent_id (str): ID of the agent\n    Returns:\n        ChatAgent: The agent with the given ID\n    \"\"\"\n    return cast(ChatAgent, Agent.from_id(id))\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.clone","title":"<code>clone(i=0)</code>","text":"<p>Create i'th clone of this agent, ensuring tool use/handling is cloned. Important: We assume all member variables are in the init method here and in the Agent class. TODO: We are attempting to clone an agent after its state has been changed in possibly many ways. Below is an imperfect solution. Caution advised. Revisit later.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clone(self, i: int = 0) -&gt; \"ChatAgent\":\n    \"\"\"Create i'th clone of this agent, ensuring tool use/handling is cloned.\n    Important: We assume all member variables are in the __init__ method here\n    and in the Agent class.\n    TODO: We are attempting to clone an agent after its state has been\n    changed in possibly many ways. Below is an imperfect solution. Caution advised.\n    Revisit later.\n    \"\"\"\n    agent_cls = type(self)\n    # Use model_copy to preserve Pydantic subclass types (like MockLMConfig)\n    # instead of deepcopy which loses subclass information\n    config_copy = self.config.model_copy(deep=True)\n    config_copy.name = f\"{config_copy.name}-{i}\"\n    new_agent = agent_cls(config_copy)\n    new_agent.system_tool_instructions = self.system_tool_instructions\n    new_agent.system_tool_format_instructions = self.system_tool_format_instructions\n    new_agent.llm_tools_map = self.llm_tools_map\n    new_agent.llm_tools_known = self.llm_tools_known\n    new_agent.llm_tools_handled = self.llm_tools_handled\n    new_agent.llm_tools_usable = self.llm_tools_usable\n    new_agent.llm_functions_map = self.llm_functions_map\n    new_agent.llm_functions_handled = self.llm_functions_handled\n    new_agent.llm_functions_usable = self.llm_functions_usable\n    new_agent.llm_function_force = self.llm_function_force\n    # Ensure each clone gets its own vecdb client when supported.\n    new_agent.vecdb = None if self.vecdb is None else self.vecdb.clone()\n    self._clone_extra_state(new_agent)\n    new_agent.id = ObjectRegistry.new_id()\n    if self.config.add_to_registry:\n        ObjectRegistry.register_object(new_agent)\n    return new_agent\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.clear_history","title":"<code>clear_history(start=-2, end=-1)</code>","text":"<p>Clear the message history, deleting  messages from index <code>start</code>, up to index <code>end</code>.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> <code>end</code> <code>int</code> <p>index of last message to delete; Default = -1     (i.e. delete all messages up to the last one)</p> <code>-1</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2, end: int = -1) -&gt; None:\n    \"\"\"\n    Clear the message history, deleting  messages from index `start`,\n    up to index `end`.\n\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n        end (int): index of last message to delete; Default = -1\n                (i.e. delete all messages up to the last one)\n    \"\"\"\n    n = len(self.message_history)\n    if start &lt; 0:\n        start = max(0, n + start)\n    end_ = n if end == -1 else end + 1\n    dropped = self.message_history[start:end_]\n    # consider the dropped msgs in REVERSE order, so we are\n    # carefully updating self.oai_tool_calls\n    for msg in reversed(dropped):\n        self._drop_msg_update_tool_calls(msg)\n        # clear out the chat document from the ObjectRegistry\n        ChatDocument.delete_id(msg.chat_document_id)\n    del self.message_history[start:end_]\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response. Args:     message (str): user message     response: (str): LLM response</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n    \"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\n    self.message_history.extend(\n        [\n            LLMMessage(role=Role.USER, content=message),\n            LLMMessage(role=Role.ASSISTANT, content=response),\n        ]\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.tool_format_rules","title":"<code>tool_format_rules()</code>","text":"<p>Specification of tool formatting rules (typically JSON-based but can be non-JSON, e.g. XMLToolMessage), based on the currently enabled usable <code>ToolMessage</code>s</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_format_rules(self) -&gt; str:\n    \"\"\"\n    Specification of tool formatting rules\n    (typically JSON-based but can be non-JSON, e.g. XMLToolMessage),\n    based on the currently enabled usable `ToolMessage`s\n\n    Returns:\n        str: formatting rules\n    \"\"\"\n    # ONLY Usable tools (i.e. LLM-generation allowed),\n    usable_tool_classes: List[Type[ToolMessage]] = [\n        t\n        for t in list(self.llm_tools_map.values())\n        if t.default_value(\"request\") in self.llm_tools_usable\n    ]\n\n    if len(usable_tool_classes) == 0:\n        return \"\"\n    format_instructions = \"\\n\\n\".join(\n        [\n            msg_cls.format_instructions(tool=self.config.use_tools)\n            for msg_cls in usable_tool_classes\n        ]\n    )\n    # if any of the enabled classes has json_group_instructions, then use that,\n    # else fall back to ToolMessage.json_group_instructions\n    for msg_cls in usable_tool_classes:\n        if hasattr(msg_cls, \"json_group_instructions\") and callable(\n            getattr(msg_cls, \"json_group_instructions\")\n        ):\n            return msg_cls.group_format_instructions().format(\n                format_instructions=format_instructions\n            )\n    return ToolMessage.group_format_instructions().format(\n        format_instructions=format_instructions\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.tool_instructions","title":"<code>tool_instructions()</code>","text":"<p>Instructions for tools or function-calls, for enabled and usable Tools. These are inserted into system prompt regardless of whether we are using our own ToolMessage mechanism or the LLM's function-call mechanism.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>concatenation of instructions for all usable tools</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_instructions(self) -&gt; str:\n    \"\"\"\n    Instructions for tools or function-calls, for enabled and usable Tools.\n    These are inserted into system prompt regardless of whether we are using\n    our own ToolMessage mechanism or the LLM's function-call mechanism.\n\n    Returns:\n        str: concatenation of instructions for all usable tools\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"\"\n    instructions = []\n    for msg_cls in enabled_classes:\n        if msg_cls.default_value(\"request\") in self.llm_tools_usable:\n            class_instructions = \"\"\n            if hasattr(msg_cls, \"instructions\") and inspect.ismethod(\n                msg_cls.instructions\n            ):\n                class_instructions = msg_cls.instructions()\n            if (\n                self.config.use_tools\n                and hasattr(msg_cls, \"langroid_tools_instructions\")\n                and inspect.ismethod(msg_cls.langroid_tools_instructions)\n            ):\n                class_instructions += msg_cls.langroid_tools_instructions()\n            # example will be shown in tool_format_rules() when using TOOLs,\n            # so we don't need to show it here.\n            example = \"\" if self.config.use_tools else (msg_cls.usage_examples())\n            if example != \"\":\n                example = \"EXAMPLES:\\n\" + example\n            guidance = (\n                \"\"\n                if class_instructions == \"\"\n                else (\"GUIDANCE: \" + class_instructions)\n            )\n            if guidance == \"\" and example == \"\":\n                continue\n            instructions.append(\n                textwrap.dedent(\n                    f\"\"\"\n                    TOOL: {msg_cls.default_value(\"request\")}:\n                    {guidance}\n                    {example}\n                    \"\"\".lstrip()\n                )\n            )\n    if len(instructions) == 0:\n        return \"\"\n    instructions_str = \"\\n\\n\".join(instructions)\n    return textwrap.dedent(\n        f\"\"\"\n        === GUIDELINES ON SOME TOOLS/FUNCTIONS USAGE ===\n        {instructions_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.augment_system_message","title":"<code>augment_system_message(message)</code>","text":"<p>Augment the system message with the given message. Args:     message (str): system message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def augment_system_message(self, message: str) -&gt; None:\n    \"\"\"\n    Augment the system message with the given message.\n    Args:\n        message (str): system message\n    \"\"\"\n    self.system_message += \"\\n\\n\" + message\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.last_message_with_role","title":"<code>last_message_with_role(role)</code>","text":"<p>from <code>message_history</code>, return the last message with role <code>role</code></p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_with_role(self, role: Role) -&gt; LLMMessage | None:\n    \"\"\"from `message_history`, return the last message with role `role`\"\"\"\n    n_role_msgs = len([m for m in self.message_history if m.role == role])\n    if n_role_msgs == 0:\n        return None\n    idx = self.nth_message_idx_with_role(role, n_role_msgs)\n    return self.message_history[idx]\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.last_message_idx_with_role","title":"<code>last_message_idx_with_role(role)</code>","text":"<p>Index of last message in message_history, with specified role. Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_idx_with_role(self, role: Role) -&gt; int:\n    \"\"\"Index of last message in message_history, with specified role.\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n    if len(indices_with_role) == 0:\n        return -1\n    return indices_with_role[-1]\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.nth_message_idx_with_role","title":"<code>nth_message_idx_with_role(role, n)</code>","text":"<p>Index of <code>n</code>th message in message_history, with specified role. (n is assumed to be 1-based, i.e. 1 is the first message with that role). Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def nth_message_idx_with_role(self, role: Role, n: int) -&gt; int:\n    \"\"\"Index of `n`th message in message_history, with specified role.\n    (n is assumed to be 1-based, i.e. 1 is the first message with that role).\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n\n    if len(indices_with_role) &lt; n:\n        return -1\n    return indices_with_role[n - 1]\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message that has role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question. Args:     message (str): new message to replace with     role (str): role of message to replace</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Update the last message that has role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): new message to replace with\n        role (str): role of message to replace\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history[i].content = message\n            break\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.delete_last_message","title":"<code>delete_last_message(role=Role.USER)</code>","text":"<p>Delete the last message that has role <code>role</code> from the message history. Args:     role (str): role of message to delete</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def delete_last_message(self, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Delete the last message that has role `role` from the message history.\n    Args:\n        role (str): role of message to delete\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history.pop(i)\n            break\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method for the \"no-tools\" scenario, i.e., the current <code>msg</code> (presumably emitted by the LLM) does not have any tool that the agent can handle. NOTE: The <code>msg</code> may contain tools but either (a) the agent is not enabled to handle them, or (b) there's an explicit <code>recipient</code> field in the tool that doesn't match the agent's name.</p> <p>Uses the self.config.non_tool_routing to determine the action to take.</p> <p>This method can be overridden by subclasses, e.g., to create a \"reminder\" message when a tool is expected but the LLM \"forgot\" to generate one.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     Any: The result of the handler method</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def handle_message_fallback(self, msg: str | ChatDocument) -&gt; Any:\n    \"\"\"\n    Fallback method for the \"no-tools\" scenario, i.e., the current `msg`\n    (presumably emitted by the LLM) does not have any tool that the agent\n    can handle.\n    NOTE: The `msg` may contain tools but either (a) the agent is not\n    enabled to handle them, or (b) there's an explicit `recipient` field\n    in the tool that doesn't match the agent's name.\n\n    Uses the self.config.non_tool_routing to determine the action to take.\n\n    This method can be overridden by subclasses, e.g.,\n    to create a \"reminder\" message when a tool is expected but the LLM \"forgot\"\n    to generate one.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        Any: The result of the handler method\n    \"\"\"\n    if (\n        isinstance(msg, str)\n        or msg.metadata.sender != Entity.LLM\n        or self.config.handle_llm_no_tool is None\n        or self.has_only_unhandled_tools(msg)\n    ):\n        return None\n    # we ONLY use the `handle_llm_no_tool` config option when\n    # the msg is from LLM and does not contain ANY tools at all.\n    from langroid.agent.tools.orchestration import AgentDoneTool, ForwardTool\n\n    no_tool_option = self.config.handle_llm_no_tool\n    if no_tool_option in list(NonToolAction):\n        # in case the `no_tool_option` is one of the special NonToolAction vals\n        match self.config.handle_llm_no_tool:\n            case NonToolAction.FORWARD_USER:\n                return ForwardTool(agent=\"User\")\n            case NonToolAction.DONE:\n                return AgentDoneTool(content=msg.content, tools=msg.tool_messages)\n    elif is_callable(no_tool_option):\n        return no_tool_option(msg)\n    # Otherwise just return `no_tool_option` as is:\n    # This can be any string, such as a specific nudge/reminder to the LLM,\n    # or even something like ResultTool etc.\n    return no_tool_option\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.unhandled_tools","title":"<code>unhandled_tools()</code>","text":"<p>The set of tools that are known but not handled. Useful in task flow: an agent can refuse to accept an incoming msg when it only has unhandled tools.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def unhandled_tools(self) -&gt; set[str]:\n    \"\"\"The set of tools that are known but not handled.\n    Useful in task flow: an agent can refuse to accept an incoming msg\n    when it only has unhandled tools.\n    \"\"\"\n    return self.llm_tools_known - self.llm_tools_handled\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False, include_defaults=True)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage] | List[Type[ToolMessage]]]</code> <p>The ToolMessage class OR List of such classes to enable, for USE, or HANDLING, or both. If this is a list of ToolMessage classes, then the remain args are applied to all classes. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> <code>require_recipient</code> <code>bool</code> <p>whether to require that recipient be specified when using the tool message (only applies if <code>use</code> is True).</p> <code>False</code> <code>include_defaults</code> <code>bool</code> <p>whether to include fields that have default values, in the \"properties\" section of the JSON format instructions. (Normally the OpenAI completion API ignores these fields, but the Assistant fn-calling seems to pay attn to these, and if we don't want this, we should set this to False.)</p> <code>True</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage] | List[Type[ToolMessage]]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n    include_defaults: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n\n    Args:\n        message_class: The ToolMessage class OR List of such classes to enable,\n            for USE, or HANDLING, or both.\n            If this is a list of ToolMessage classes, then the remain args are\n            applied to all classes.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n        require_recipient: whether to require that recipient be specified\n            when using the tool message (only applies if `use` is True).\n        include_defaults: whether to include fields that have default values,\n            in the \"properties\" section of the JSON format instructions.\n            (Normally the OpenAI completion API ignores these fields,\n            but the Assistant fn-calling seems to pay attn to these,\n            and if we don't want this, we should set this to False.)\n    \"\"\"\n    if message_class is not None and isinstance(message_class, list):\n        for mc in message_class:\n            self.enable_message(\n                mc,\n                use=use,\n                handle=handle,\n                force=force,\n                require_recipient=require_recipient,\n                include_defaults=include_defaults,\n            )\n        return None\n\n    # Validate that use/handle are booleans, not accidentally passed tool classes\n    if isclass(use) or isclass(handle):\n        param = \"use\" if isclass(use) else \"handle\"\n        raise TypeError(\n            textwrap.dedent(\n                f\"\"\"\n                Invalid arguments to enable_message().\n                It appears you passed multiple ToolMessage classes as separate\n                arguments instead of as a list.\n\n                Correct usage:\n                    agent.enable_message([Tool1, Tool2, Tool3])\n\n                Incorrect usage:\n                    agent.enable_message(Tool1, Tool2, Tool3)\n\n                The '{param}' parameter must be a boolean, not a class.\n                \"\"\"\n            )\n        )\n\n    if require_recipient and message_class is not None:\n        message_class = message_class.require_recipient()\n    if isinstance(message_class, XMLToolMessage):\n        # XMLToolMessage is not compatible with OpenAI's Tools/functions API,\n        # so we disable use of functions API, enable langroid-native Tools,\n        # which are prompt-based.\n        self.config.use_functions_api = False\n        self.config.use_tools = True\n    super().enable_message_handling(message_class)  # enables handling only\n    tools = self._get_tool_list(message_class)\n    if message_class is not None:\n        request = message_class.default_value(\"request\")\n        if request == \"\":\n            raise ValueError(\n                f\"\"\"\n                ToolMessage class {message_class} must have a non-empty\n                'request' field if it is to be enabled as a tool.\n                \"\"\"\n            )\n        llm_function = message_class.llm_function_schema(defaults=include_defaults)\n        self.llm_functions_map[request] = llm_function\n        if force:\n            self.llm_function_force = dict(name=request)\n        else:\n            self.llm_function_force = None\n\n    for t in tools:\n        self.llm_tools_known.add(t)\n\n        if handle:\n            self.llm_tools_handled.add(t)\n            self.llm_functions_handled.add(t)\n\n            if (\n                self.enabled_handling_output_format is not None\n                and self.enabled_handling_output_format.name() == t\n            ):\n                # `t` was designated as \"enabled for handling\" ONLY for\n                # output_format enforcement, but we are explicitly ]\n                # enabling it for handling here, so we set the variable to None.\n                self.enabled_handling_output_format = None\n        else:\n            self.llm_tools_handled.discard(t)\n            self.llm_functions_handled.discard(t)\n\n        if use:\n            tool_class = self.llm_tools_map[t]\n            allow_llm_use = tool_class._allow_llm_use\n            if isinstance(allow_llm_use, ModelPrivateAttr):\n                allow_llm_use = allow_llm_use.default\n            if allow_llm_use:\n                self.llm_tools_usable.add(t)\n                self.llm_functions_usable.add(t)\n            else:\n                logger.warning(\n                    f\"\"\"\n                    ToolMessage class {tool_class} does not allow LLM use,\n                    because `_allow_llm_use=False` either in the Tool or a\n                    parent class of this tool;\n                    so not enabling LLM use for this tool!\n                    If you intended an LLM to use this tool,\n                    set `_allow_llm_use=True` when you define the tool.\n                    \"\"\"\n                )\n            if (\n                self.enabled_use_output_format is not None\n                and self.enabled_use_output_format.default_value(\"request\") == t\n            ):\n                # `t` was designated as \"enabled for use\" ONLY for output_format\n                # enforcement, but we are explicitly enabling it for use here,\n                # so we set the variable to None.\n                self.enabled_use_output_format = None\n        else:\n            self.llm_tools_usable.discard(t)\n            self.llm_functions_usable.discard(t)\n\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.set_output_format","title":"<code>set_output_format(output_type, force_tools=None, use=None, handle=None, instructions=None, is_copy=False)</code>","text":"<p>Sets <code>output_format</code> to <code>output_type</code> and, if <code>force_tools</code> is enabled, switches to the native Langroid tools mechanism to ensure that no tool calls not of <code>output_type</code> are generated. By default, <code>force_tools</code> follows the <code>use_tools_on_output_format</code> parameter in the config.</p> <p>If <code>output_type</code> is None, restores to the state prior to setting <code>output_format</code>.</p> <p>If <code>use</code>, we enable use of <code>output_type</code> when it is a subclass of <code>ToolMesage</code>. Note that this primarily controls instruction generation: the model will always generate <code>output_type</code> regardless of whether <code>use</code> is set. Defaults to the <code>use_output_format</code> parameter in the config. Similarly, handling of <code>output_type</code> is controlled by <code>handle</code>, which defaults to the <code>handle_output_format</code> parameter in the config.</p> <p><code>instructions</code> controls whether we generate instructions specifying the output format schema. Defaults to the <code>instructions_output_format</code> parameter in the config.</p> <p><code>is_copy</code> is set when called via <code>__getitem__</code>. In that case, we must copy certain fields to ensure that we do not overwrite the main agent's setings.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def set_output_format(\n    self,\n    output_type: Optional[type],\n    force_tools: Optional[bool] = None,\n    use: Optional[bool] = None,\n    handle: Optional[bool] = None,\n    instructions: Optional[bool] = None,\n    is_copy: bool = False,\n) -&gt; None:\n    \"\"\"\n    Sets `output_format` to `output_type` and, if `force_tools` is enabled,\n    switches to the native Langroid tools mechanism to ensure that no tool\n    calls not of `output_type` are generated. By default, `force_tools`\n    follows the `use_tools_on_output_format` parameter in the config.\n\n    If `output_type` is None, restores to the state prior to setting\n    `output_format`.\n\n    If `use`, we enable use of `output_type` when it is a subclass\n    of `ToolMesage`. Note that this primarily controls instruction\n    generation: the model will always generate `output_type` regardless\n    of whether `use` is set. Defaults to the `use_output_format`\n    parameter in the config. Similarly, handling of `output_type` is\n    controlled by `handle`, which defaults to the\n    `handle_output_format` parameter in the config.\n\n    `instructions` controls whether we generate instructions specifying\n    the output format schema. Defaults to the `instructions_output_format`\n    parameter in the config.\n\n    `is_copy` is set when called via `__getitem__`. In that case, we must\n    copy certain fields to ensure that we do not overwrite the main agent's\n    setings.\n    \"\"\"\n    # Disable usage of an output format which was not specifically enabled\n    # by `enable_message`\n    if self.enabled_use_output_format is not None:\n        self.disable_message_use(self.enabled_use_output_format)\n        self.enabled_use_output_format = None\n\n    # Disable handling of an output format which did not specifically have\n    # handling enabled via `enable_message`\n    if self.enabled_handling_output_format is not None:\n        self.disable_message_handling(self.enabled_handling_output_format)\n        self.enabled_handling_output_format = None\n\n    # Reset any previous instructions\n    self.output_format_instructions = \"\"\n\n    if output_type is None:\n        self.output_format = None\n        (\n            requests_for_inference,\n            use_functions_api,\n            use_tools,\n        ) = self.saved_requests_and_tool_setings\n        self.config = self.config.model_copy()\n        self.enabled_requests_for_inference = requests_for_inference\n        self.config.use_functions_api = use_functions_api\n        self.config.use_tools = use_tools\n    else:\n        if force_tools is None:\n            force_tools = self.config.use_tools_on_output_format\n\n        if not any(\n            (isclass(output_type) and issubclass(output_type, t))\n            for t in [ToolMessage, BaseModel]\n        ):\n            output_type = get_pydantic_wrapper(output_type)\n\n        if self.output_format is None and force_tools:\n            self.saved_requests_and_tool_setings = (\n                self._requests_and_tool_settings()\n            )\n\n        self.output_format = output_type\n        if issubclass(output_type, ToolMessage):\n            name = output_type.default_value(\"request\")\n            if use is None:\n                use = self.config.use_output_format\n\n            if handle is None:\n                handle = self.config.handle_output_format\n\n            if use or handle:\n                is_usable = name in self.llm_tools_usable.union(\n                    self.llm_functions_usable\n                )\n                is_handled = name in self.llm_tools_handled.union(\n                    self.llm_functions_handled\n                )\n\n                if is_copy:\n                    if use:\n                        # We must copy `llm_tools_usable` so the base agent\n                        # is unmodified\n                        self.llm_tools_usable = self.llm_tools_usable.copy()\n                        self.llm_functions_usable = self.llm_functions_usable.copy()\n                    if handle:\n                        # If handling the tool, do the same for `llm_tools_handled`\n                        self.llm_tools_handled = self.llm_tools_handled.copy()\n                        self.llm_functions_handled = (\n                            self.llm_functions_handled.copy()\n                        )\n                # Enable `output_type`\n                self.enable_message(\n                    output_type,\n                    # Do not override existing settings\n                    use=use or is_usable,\n                    handle=handle or is_handled,\n                )\n\n                # If the `output_type` ToilMessage was not already enabled for\n                # use, this means we are ONLY enabling it for use specifically\n                # for enforcing this output format, so we set the\n                # `enabled_use_output_forma  to this output_type, to\n                # record that it should be disabled when `output_format` is changed\n                if not is_usable:\n                    self.enabled_use_output_format = output_type\n\n                # (same reasoning as for use-enabling)\n                if not is_handled:\n                    self.enabled_handling_output_format = output_type\n\n            generated_tool_instructions = name in self.llm_tools_usable.union(\n                self.llm_functions_usable\n            )\n        else:\n            generated_tool_instructions = False\n\n        if instructions is None:\n            instructions = self.config.instructions_output_format\n        if issubclass(output_type, BaseModel) and instructions:\n            if generated_tool_instructions:\n                # Already generated tool instructions as part of \"enabling for use\",\n                # so only need to generate a reminder to use this tool.\n                name = cast(ToolMessage, output_type).default_value(\"request\")\n                self.output_format_instructions = textwrap.dedent(\n                    f\"\"\"\n                    === OUTPUT FORMAT INSTRUCTIONS ===\n\n                    Please provide output using the `{name}` tool/function.\n                    \"\"\"\n                )\n            else:\n                if issubclass(output_type, ToolMessage):\n                    output_format_schema = output_type.llm_function_schema(\n                        request=True,\n                        defaults=self.config.output_format_include_defaults,\n                    ).parameters\n                else:\n                    output_format_schema = output_type.model_json_schema()\n\n                format_schema_for_strict(output_format_schema)\n\n                self.output_format_instructions = textwrap.dedent(\n                    f\"\"\"\n                    === OUTPUT FORMAT INSTRUCTIONS ===\n                    Please provide output as JSON with the following schema:\n\n                    {output_format_schema}\n                    \"\"\"\n                )\n\n        if force_tools:\n            if issubclass(output_type, ToolMessage):\n                self.enabled_requests_for_inference = {\n                    output_type.default_value(\"request\")\n                }\n            if self.config.use_functions_api:\n                self.config = self.config.model_copy()\n                self.config.use_functions_api = False\n                self.config.use_tools = True\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL. Args:     message_class: The ToolMessage class to disable; Optional.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\n    super().disable_message_handling(message_class)\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n        self.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools. Args:     message_class: The ToolMessage class to disable.         If None, disable all.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n    \"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_usable.discard(t)\n        self.llm_functions_usable.discard(t)\n\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool) Args:     message_class: The only ToolMessage class to allow</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n    \"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\n    request = message_class.model_fields[\"request\"].default\n    to_remove = [r for r in self.llm_tools_usable if r != request]\n    for r in to_remove:\n        self.llm_tools_usable.discard(r)\n        self.llm_functions_usable.discard(r)\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.get_tool_messages","title":"<code>get_tool_messages(msg, all_tools=False)</code>","text":"<p>Extracts messages and tracks whether any errors occurred. If strict mode was enabled, disables it for the tool, else triggers strict recovery.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def get_tool_messages(\n    self,\n    msg: str | ChatDocument | None,\n    all_tools: bool = False,\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Extracts messages and tracks whether any errors occurred. If strict mode\n    was enabled, disables it for the tool, else triggers strict recovery.\n    \"\"\"\n    self.tool_error = False\n    most_recent_sent_by_llm = (\n        len(self.message_history) &gt; 0\n        and self.message_history[-1].role == Role.ASSISTANT\n    )\n    was_llm = most_recent_sent_by_llm or (\n        isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM\n    )\n    try:\n        tools = super().get_tool_messages(msg, all_tools)\n    except ValidationError as ve:\n        # Check if tool class was attached to the exception\n        if hasattr(ve, \"tool_class\") and ve.tool_class:\n            tool_class = ve.tool_class  # type: ignore\n            if issubclass(tool_class, ToolMessage):\n                was_strict = (\n                    self.config.use_functions_api\n                    and self.config.use_tools_api\n                    and self._strict_mode_for_tool(tool_class)\n                )\n                # If the result of strict output for a tool using the\n                # OpenAI tools API fails to parse, we infer that the\n                # schema edits necessary for compatibility prevented\n                # adherence to the underlying `ToolMessage` schema and\n                # disable strict output for the tool\n                if was_strict:\n                    name = tool_class.default_value(\"request\")\n                    self.disable_strict_tools_set.add(name)\n                    logging.warning(\n                        f\"\"\"\n                        Validation error occured with strict tool format.\n                        Disabling strict mode for the {name} tool.\n                        \"\"\"\n                    )\n                else:\n                    # We will trigger the strict recovery mechanism to force\n                    # the LLM to correct its output, allowing us to parse\n                    if isinstance(msg, ChatDocument):\n                        self.tool_error = msg.metadata.sender == Entity.LLM\n                    else:\n                        self.tool_error = most_recent_sent_by_llm\n\n        if was_llm:\n            raise ve\n        else:\n            self.tool_error = False\n            return []\n\n    if not was_llm:\n        self.tool_error = False\n\n    return tools\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.truncate_message","title":"<code>truncate_message(idx, tokens=5, warning='...[Contents truncated!]', inplace=True)</code>","text":"<p>Truncate message at idx in msg history to <code>tokens</code> tokens.</p> <p>If inplace is True, the message is truncated in place, else it LEAVES the original message INTACT and returns a new message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def truncate_message(\n    self,\n    idx: int,\n    tokens: int = 5,\n    warning: str = \"...[Contents truncated!]\",\n    inplace: bool = True,\n) -&gt; LLMMessage:\n    \"\"\"\n    Truncate message at idx in msg history to `tokens` tokens.\n\n    If inplace is True, the message is truncated in place, else\n    it LEAVES the original message INTACT and returns a new message\n    \"\"\"\n    if inplace:\n        llm_msg = self.message_history[idx]\n    else:\n        llm_msg = copy.deepcopy(self.message_history[idx])\n    orig_content = llm_msg.content\n    new_content = (\n        self.parser.truncate_tokens(orig_content, tokens)\n        if self.parser is not None\n        else orig_content[: tokens * 4]  # approx truncation\n    )\n    llm_msg.content = new_content + \"\\n\" + warning\n    return llm_msg\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode Args:     message (str|ChatDocument): message or ChatDocument object to respond to.         If None, use the self.task_messages Returns:     LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    # If enabled and a tool error occurred, we recover by generating the tool in\n    # strict json mode\n    if (\n        self.tool_error\n        and self.output_format is None\n        and self._json_schema_available()\n        and self.config.strict_recovery\n    ):\n        self.tool_error = False\n        AnyTool = self._get_any_tool_message()\n        if AnyTool is None:\n            return None\n        self.set_output_format(\n            AnyTool,\n            force_tools=True,\n            use=True,\n            handle=True,\n            instructions=True,\n        )\n        recovery_message = self._strict_recovery_instructions(AnyTool)\n        augmented_message = message\n        if augmented_message is None:\n            augmented_message = recovery_message\n        elif isinstance(augmented_message, str):\n            augmented_message = augmented_message + recovery_message\n        else:\n            augmented_message.content = augmented_message.content + recovery_message\n\n        # only use the augmented message for this one response...\n        result = self.llm_response(augmented_message)\n        # ... restore the original user message so that the AnyTool recover\n        # instructions don't persist in the message history\n        # (this can cause the LLM to use the AnyTool directly as a tool)\n        if message is None:\n            self.delete_last_message(role=Role.USER)\n        else:\n            msg = message if isinstance(message, str) else message.content\n            self.update_last_message(msg, role=Role.USER)\n        return result\n\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    tool_choice = (\n        \"auto\"\n        if isinstance(message, str)\n        else (message.oai_tool_choice if message is not None else \"auto\")\n    )\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        try:\n            response = self.llm_response_messages(hist, output_len, tool_choice)\n        except openai.BadRequestError as e:\n            if self.any_strict:\n                self.disable_strict = True\n                self.set_output_format(None)\n                logging.warning(\n                    f\"\"\"\n                    OpenAI BadRequestError raised with strict mode enabled.\n                    Message: {e.message}\n                    Disabling strict mode and retrying.\n                    \"\"\"\n                )\n                return self.llm_response(message)\n            else:\n                raise e\n    self.message_history.extend(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    if isinstance(message, ChatDocument):\n        self._reduce_raw_tool_results(message)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n\n    return response\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of `llm_response`. See there for details.\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    # If enabled and a tool error occurred, we recover by generating the tool in\n    # strict json mode\n    if (\n        self.tool_error\n        and self.output_format is None\n        and self._json_schema_available()\n        and self.config.strict_recovery\n    ):\n        self.tool_error = False\n        AnyTool = self._get_any_tool_message()\n        self.set_output_format(\n            AnyTool,\n            force_tools=True,\n            use=True,\n            handle=True,\n            instructions=True,\n        )\n        recovery_message = self._strict_recovery_instructions(AnyTool)\n        augmented_message = message\n        if augmented_message is None:\n            augmented_message = recovery_message\n        elif isinstance(augmented_message, str):\n            augmented_message = augmented_message + recovery_message\n        else:\n            augmented_message.content = augmented_message.content + recovery_message\n\n        # only use the augmented message for this one response...\n        result = self.llm_response(augmented_message)\n        # ... restore the original user message so that the AnyTool recover\n        # instructions don't persist in the message history\n        # (this can cause the LLM to use the AnyTool directly as a tool)\n        if message is None:\n            self.delete_last_message(role=Role.USER)\n        else:\n            msg = message if isinstance(message, str) else message.content\n            self.update_last_message(msg, role=Role.USER)\n        return result\n\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    tool_choice = (\n        \"auto\"\n        if isinstance(message, str)\n        else (message.oai_tool_choice if message is not None else \"auto\")\n    )\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        try:\n            response = await self.llm_response_messages_async(\n                hist, output_len, tool_choice\n            )\n        except openai.BadRequestError as e:\n            if self.any_strict:\n                self.disable_strict = True\n                self.set_output_format(None)\n                logging.warning(\n                    f\"\"\"\n                    OpenAI BadRequestError raised with strict mode enabled.\n                    Message: {e.message}\n                    Disabling strict mode and retrying.\n                    \"\"\"\n                )\n                return await self.llm_response_async(message)\n            else:\n                raise e\n    self.message_history.extend(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    if isinstance(message, ChatDocument):\n        self._reduce_raw_tool_results(message)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n\n    return response\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.init_message_history","title":"<code>init_message_history()</code>","text":"<p>Initialize the message history with the system message and user message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_message_history(self) -&gt; None:\n    \"\"\"\n    Initialize the message history with the system message and user message\n    \"\"\"\n    self.message_history = [self._create_system_and_tools_message()]\n    if self.user_message:\n        self.message_history.append(\n            LLMMessage(role=Role.USER, content=self.user_message)\n        )\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None, tool_choice='auto')</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion Args:     messages: seq of messages (with role, content fields) sent to LLM     output_len: max number of tokens expected in response.             If None, use the LLM's default model_max_output_tokens. Returns:     Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\n    self,\n    messages: List[LLMMessage],\n    output_len: Optional[int] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n) -&gt; ChatDocument:\n    \"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n        output_len: max number of tokens expected in response.\n                If None, use the LLM's default model_max_output_tokens.\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.model_max_output_tokens\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream() and not settings.quiet:\n            # show rich spinner only if not streaming!\n            # (Why? b/c the intent of showing a spinner is to \"show progress\",\n            # and we don't need to do that when streaming, since\n            # streaming output already shows progress.)\n            cm = status(\n                \"LLM responding to messages...\",\n                log_if_quiet=False,\n            )\n            stack.enter_context(cm)\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        functions, fun_call, tools, force_tool, output_format = (\n            self._function_args()\n        )\n        assert self.llm is not None\n        response = self.llm.chat(\n            messages,\n            output_len,\n            tools=tools,\n            tool_choice=force_tool or tool_choice,\n            functions=functions,\n            function_call=fun_call,\n            response_format=output_format,\n        )\n    if self.llm.get_stream():\n        # Create temp ChatDocument for tool check, then clean up to avoid\n        # polluting ObjectRegistry (see PR #939 discussion)\n        temp_doc = ChatDocument.from_LLMResponse(\n            response,\n            displayed=True,\n            recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n        )\n        self._call_callback_with_reasoning(\n            \"finish_llm_stream\",\n            reasoning=response.reasoning,\n            content=response.message,\n            tools_content=response.tools_content(),\n            is_tool=self.has_tool_message_attempt(temp_doc),\n        )\n        ObjectRegistry.remove(temp_doc.id())\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(\n        response,\n        displayed=True,\n        recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n    )\n    self.oai_tool_calls = response.oai_tool_calls or []\n    self.oai_tool_id2call.update(\n        {t.id: t for t in self.oai_tool_calls if t.id is not None}\n    )\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(chat_doc)\n\n    return chat_doc\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_messages_async","title":"<code>llm_response_messages_async(messages, output_len=None, tool_choice='auto')</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_messages</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_messages_async(\n    self,\n    messages: List[LLMMessage],\n    output_len: Optional[int] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_messages`. See there for details.\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.model_max_output_tokens\n    functions, fun_call, tools, force_tool, output_format = self._function_args()\n    assert self.llm is not None\n\n    streamer_async = async_noop_fn\n    if self.llm.get_stream():\n        streamer_async = await self.callbacks.start_llm_stream_async()\n    self.llm.config.streamer_async = streamer_async\n\n    response = await self.llm.achat(\n        messages,\n        output_len,\n        tools=tools,\n        tool_choice=force_tool or tool_choice,\n        functions=functions,\n        function_call=fun_call,\n        response_format=output_format,\n    )\n    if self.llm.get_stream():\n        # Create temp ChatDocument for tool check, then clean up to avoid\n        # polluting ObjectRegistry (see PR #939 discussion)\n        temp_doc = ChatDocument.from_LLMResponse(\n            response,\n            displayed=True,\n            recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n        )\n        self._call_callback_with_reasoning(\n            \"finish_llm_stream\",\n            reasoning=response.reasoning,\n            content=response.message,\n            tools_content=response.tools_content(),\n            is_tool=self.has_tool_message_attempt(temp_doc),\n        )\n        ObjectRegistry.remove(temp_doc.id())\n    self.llm.config.streamer_async = async_noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(\n        response,\n        displayed=True,\n        recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n    )\n    self.oai_tool_calls = response.oai_tool_calls or []\n    self.oai_tool_id2call.update(\n        {t.id: t for t in self.oai_tool_calls if t.id is not None}\n    )\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(chat_doc)\n\n    return chat_doc\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message=None)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>message to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n\n    Args:\n        message (str|ChatDocument): message to respond to.\n\n    Returns:\n        A Document object with the response.\n\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(ChatDocument, ChatAgent.llm_response(self, message))\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(response)\n\n    return response\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.llm_response_forget_async","title":"<code>llm_response_forget_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_forget</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_forget_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_forget`. See there for details.\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(\n            ChatDocument, await ChatAgent.llm_response_async(self, message)\n        )\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n    return response\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:     int: number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n    \"\"\"\n    Total number of tokens in the message history so far.\n\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\n            \"ChatAgent.parser is None. \"\n            \"You must set ChatAgent.parser \"\n            \"before calling chat_num_tokens().\"\n        )\n    hist = messages if messages is not None else self.message_history\n    return sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/#langroid.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history Args:     i: if provided, return only the i-th message when i is postive,         or last k messages when i = -k. Returns:</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n    \"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\n    if i is None:\n        return \"\\n\".join([str(m) for m in self.message_history])\n    elif i &gt; 0:\n        return str(self.message_history[i])\n    else:\n        return \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/#langroid.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>               Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent</p> <p>Attributes:</p> Name Type Description <code>system_message</code> <code>str</code> <p>system message to include in message sequence  (typically defines role and task of agent).  Used only if <code>task</code> is not specified in the constructor.</p> <code>user_message</code> <code>Optional[str]</code> <p>user message to include in message sequence.  Used only if <code>task</code> is not specified in the constructor.</p> <code>use_tools</code> <code>bool</code> <p>whether to use our own ToolMessages mechanism</p> <code>handle_llm_no_tool</code> <code>Any</code> <p>desired agent_response when LLM generates non-tool msg.</p> <code>use_functions_api</code> <code>bool</code> <p>whether to use functions/tools native to the LLM API     (e.g. OpenAI's <code>function_call</code> or <code>tool_call</code> mechanism)</p> <code>use_tools_api</code> <code>bool</code> <p>When <code>use_functions_api</code> is True, if this is also True, the OpenAI tool-call API is used, rather than the older/deprecated function-call API. However the tool-call API has some tricky aspects, hence we set this to False by default.</p> <code>strict_recovery</code> <code>bool</code> <p>whether to enable strict schema recovery when there is a tool-generation error.</p> <code>enable_orchestration_tool_handling</code> <code>bool</code> <p>whether to enable handling of orchestration tools, e.g. ForwardTool, DoneTool, PassTool, etc.</p> <code>output_format</code> <code>Optional[type]</code> <p>When supported by the LLM (certain OpenAI LLMs and local LLMs served by providers such as vLLM), ensures that the output is a JSON matching the corresponding schema via grammar-based decoding</p> <code>handle_output_format</code> <code>bool</code> <p>When <code>output_format</code> is a <code>ToolMessage</code> T, controls whether T is \"enabled for handling\".</p> <code>use_output_format</code> <code>bool</code> <p>When <code>output_format</code> is a <code>ToolMessage</code> T, controls whether T is \"enabled for use\" (by LLM) and instructions on using T are added to the system message.</p> <code>instructions_output_format</code> <code>bool</code> <p>Controls whether we generate instructions for <code>output_format</code> in the system message.</p> <code>use_tools_on_output_format</code> <code>bool</code> <p>Controls whether to automatically switch to the Langroid-native tools mechanism when <code>output_format</code> is set. Note that LLMs may generate tool calls which do not belong to <code>output_format</code> even when strict JSON mode is enabled, so this should be enabled when such tool calls are not desired.</p> <code>output_format_include_defaults</code> <code>bool</code> <p>Whether to include fields with default arguments in the output schema</p> <code>full_citations</code> <code>bool</code> <p>Whether to show source reference citation + content for each citation, or just the main reference citation.</p> <code>search_for_tools_everywhere</code> <code>bool</code> <p>Whether to search for tools everywhere, or only in specific LLM response elements based on use_tools / use_functions_api / use_tools_api config settings.</p> <code>recognize_recipient_in_content</code> <code>bool</code> <p>Whether to parse LLM response text content for recipient routing patterns, specifically: - <code>TO[&lt;recipient&gt;]:&lt;content&gt;</code> addressing format, and - JSON <code>{\"recipient\": \"&lt;name&gt;\"}</code> at the top level of the message. When False, only structured routing via function_call/tool_call <code>recipient</code> fields is recognized. Default is True. Note: this is distinct from <code>TaskConfig.recognize_string_signals</code>, which controls Task-level signals like DONE, PASS, and SEND_TO. To fully disable all text-based routing, set both to False.</p> <code>context_overflow_strategy</code> <code>Literal['truncate', 'drop_turns']</code> <p>Strategy for handling context overflow when message history exceeds model context length. Options: - \"truncate\": Truncate content of early messages (preserves all messages   but with shortened content). This maintains the message sequence. - \"drop_turns\": Drop complete conversation turns (USER + all responses   until next USER). More aggressive but cleaner for voice agents. Default is \"truncate\" for backward compatibility.</p>"},{"location":"reference/#langroid.Task","title":"<code>Task(agent=None, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=True, default_human_response=None, interactive=True, only_user_quits_root=True, erase_substeps=False, allow_null_result=False, max_stalled_steps=5, default_return_type=None, done_if_no_response=[], done_if_response=[], config=TaskConfig(), **kwargs)</code>","text":"<p>A <code>Task</code> wraps an <code>Agent</code> object, and sets up the <code>Agent</code>'s goals and instructions. A <code>Task</code> maintains two key variables:</p> <ul> <li><code>self.pending_message</code>, which is the message awaiting a response, and</li> <li><code>self.pending_sender</code>, which is the entity that sent the pending message.</li> </ul> <p>The possible responders to <code>self.pending_message</code> are the <code>Agent</code>'s own \"native\" responders (<code>agent_response</code>, <code>llm_response</code>, and <code>user_response</code>), and the <code>run()</code> methods of any sub-tasks. All responders have the same type-signature (somewhat simplified): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> Responders may or may not specify an intended recipient of their generated response.</p> <p>The main top-level method in the <code>Task</code> class is <code>run()</code>, which repeatedly calls <code>step()</code> until <code>done()</code> returns true. The <code>step()</code> represents a \"turn\" in the conversation: this method sequentially (in round-robin fashion) calls the responders until it finds one that generates a valid response to the <code>pending_message</code> (as determined by the <code>valid()</code> method). Once a valid response is found, <code>step()</code> updates the <code>pending_message</code> and <code>pending_sender</code> variables, and on the next iteration, <code>step()</code> re-starts its search for a valid response from the beginning of the list of responders (the exception being that the human user always gets a chance to respond after each non-human valid response). This process repeats until <code>done()</code> returns true, at which point <code>run()</code> returns the value of <code>result()</code>, which is the final result of the task.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent associated with the task</p> <code>None</code> <code>name</code> <code>str</code> <p>name of the task</p> <code>''</code> <code>llm_delegate</code> <code>bool</code> <p>Whether to delegate \"control\" to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve, and decides when a task is done. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity). See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by \"controller\" (i.e. LLM if <code>llm_delegate</code> is true, otherwise USER) and subsequent response by non-controller [When a tool is involved, this will not give intended results. See <code>done_if_response</code>, <code>done_if_no_response</code> below]. termination]. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\". See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's system_message</p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's user_message</p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true (default), resets the agent's message history at every run when it is the top-level task. Ignored when the task is a subtask of another task. Restart behavior of a subtask's <code>run()</code> can be controlled via the <code>TaskConfig.restart_as_subtask</code> setting.</p> <code>True</code> <code>default_human_response</code> <code>str | None</code> <p>default response from user; useful for testing, to avoid interactive input from user. [Instead of this, setting <code>interactive</code> usually suffices]</p> <code>None</code> <code>default_return_type</code> <code>Optional[type]</code> <p>if not None, extracts a value of this type from the result of self.run()</p> <code>None</code> <code>interactive</code> <code>bool</code> <p>if true, wait for human input after each non-human response (prevents infinite loop of non-human responses). Default is true. If false, then <code>default_human_response</code> is set to \"\" Note: When interactive = False, the one exception is when the user is explicitly addressed, via \"@user\" or using RecipientTool, in which case the system will wait for a user response. In other words, use <code>interactive=False</code> when you want a \"largely non-interactive\" run, with the exception of explicit user addressing.</p> <code>True</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, when interactive=True, only user can quit the root task (Ignored when interactive=False).</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> <code>allow_null_result</code> <code>bool</code> <p>If true, create dummy NO_ANSWER response when no valid response is found in a step. Optional, default is False. Note: In non-interactive mode, when this is set to True, you can have a situation where an LLM generates (non-tool) text, and no other responders have valid responses, and a \"Null result\" is inserted as a dummy response from the User entity, so the LLM will now respond to this Null result, and this will continue until the LLM emits a DONE signal (if instructed to do so), otherwise langroid detects a potential infinite loop after a certain number of such steps (= <code>TaskConfig.inf_loop_wait_factor</code>) and will raise an InfiniteLoopException.</p> <code>False</code> <code>max_stalled_steps</code> <code>int</code> <p>task considered done after this many consecutive steps with no progress. Default is 3.</p> <code>5</code> <code>done_if_no_response</code> <code>List[Responder]</code> <p>consider task done if NULL response from any of these responders. Default is empty list.</p> <code>[]</code> <code>done_if_response</code> <code>List[Responder]</code> <p>consider task done if NON-NULL response from any of these responders. Default is empty list.</p> <code>[]</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\n    self,\n    agent: Optional[Agent] = None,\n    name: str = \"\",\n    llm_delegate: bool = False,\n    single_round: bool = False,\n    system_message: str = \"\",\n    user_message: str | None = \"\",\n    restart: bool = True,\n    default_human_response: Optional[str] = None,\n    interactive: bool = True,\n    only_user_quits_root: bool = True,\n    erase_substeps: bool = False,\n    allow_null_result: bool = False,\n    max_stalled_steps: int = 5,\n    default_return_type: Optional[type] = None,\n    done_if_no_response: List[Responder] = [],\n    done_if_response: List[Responder] = [],\n    config: TaskConfig = TaskConfig(),\n    **kwargs: Any,  # catch-all for any legacy params, for backwards compatibility\n):\n    \"\"\"\n    A task to be performed by an agent.\n\n    Args:\n        agent (Agent): agent associated with the task\n        name (str): name of the task\n        llm_delegate (bool):\n            Whether to delegate \"control\" to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve, and decides when a task is done.\n            The \"controlling entity\" is either the LLM or the USER.\n            (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        single_round (bool):\n            If true, task runs until one message by \"controller\"\n            (i.e. LLM if `llm_delegate` is true, otherwise USER)\n            and subsequent response by non-controller [When a tool is involved,\n            this will not give intended results. See `done_if_response`,\n            `done_if_no_response` below].\n            termination]. If false, runs for the specified number of turns in\n            `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        system_message (str): if not empty, overrides agent's system_message\n        user_message (str): if not empty, overrides agent's user_message\n        restart (bool): if true (default), resets the agent's message history\n            *at every run* when it is the top-level task. Ignored when\n            the task is a subtask of another task. Restart behavior of a subtask's\n            `run()` can be controlled via the `TaskConfig.restart_as_subtask`\n            setting.\n        default_human_response (str|None): default response from user; useful for\n            testing, to avoid interactive input from user.\n            [Instead of this, setting `interactive` usually suffices]\n        default_return_type: if not None, extracts a value of this type from the\n            result of self.run()\n        interactive (bool): if true, wait for human input after each non-human\n            response (prevents infinite loop of non-human responses).\n            Default is true. If false, then `default_human_response` is set to \"\"\n            Note: When interactive = False, the one exception is when the user\n            is explicitly addressed, via \"@user\" or using RecipientTool, in which\n            case the system will wait for a user response. In other words, use\n            `interactive=False` when you want a \"largely non-interactive\"\n            run, with the exception of explicit user addressing.\n        only_user_quits_root (bool): if true, when interactive=True, only user can\n            quit the root task (Ignored when interactive=False).\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n        allow_null_result (bool):\n            If true, create dummy NO_ANSWER response when no valid response is found\n            in a step.\n            Optional, default is False.\n            *Note:* In non-interactive mode, when this is set to True,\n            you can have a situation where an LLM generates (non-tool) text,\n            and no other responders have valid responses, and a \"Null result\"\n            is inserted as a dummy response from the User entity, so the LLM\n            will now respond to this Null result, and this will continue\n            until the LLM emits a DONE signal (if instructed to do so),\n            otherwise langroid detects a potential infinite loop after\n            a certain number of such steps (= `TaskConfig.inf_loop_wait_factor`)\n            and will raise an InfiniteLoopException.\n        max_stalled_steps (int): task considered done after this many consecutive\n            steps with no progress. Default is 3.\n        done_if_no_response (List[Responder]): consider task done if NULL\n            response from any of these responders. Default is empty list.\n        done_if_response (List[Responder]): consider task done if NON-NULL\n            response from any of these responders. Default is empty list.\n    \"\"\"\n    if agent is None:\n        agent = ChatAgent()\n    self.callbacks = SimpleNamespace(\n        show_subtask_response=noop_fn,\n        set_parent_agent=noop_fn,\n    )\n    self.config = config\n    # Store parsed done sequences (will be initialized after agent assignment)\n    self._parsed_done_sequences: Optional[List[DoneSequence]] = None\n    # how to behave as a sub-task; can be overridden by `add_sub_task()`\n    self.config_sub_task = copy.deepcopy(config)\n    # counts of distinct pending messages in history,\n    # to help detect (exact) infinite loops\n    self.message_counter: Counter[str] = Counter()\n    self._init_message_counter()\n\n    self.history: Deque[str] = deque(\n        maxlen=self.config.inf_loop_cycle_len * self.config.inf_loop_wait_factor\n    )\n    # copy the agent's config, so that we don't modify the original agent's config,\n    # which may be shared by other agents.\n    try:\n        config_copy = copy.deepcopy(agent.config)\n        agent.config = config_copy\n    except Exception:\n        logger.warning(\n            \"\"\"\n            Failed to deep-copy Agent config during task creation, \n            proceeding with original config. Be aware that changes to \n            the config may affect other agents using the same config.\n            \"\"\"\n        )\n    self.restart = restart\n    agent = cast(ChatAgent, agent)\n    self.agent: ChatAgent = agent\n    if isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\n        self.agent.init_state()\n        # possibly change the system and user messages\n        if system_message:\n            # we always have at least 1 task_message\n            self.agent.set_system_message(system_message)\n        if user_message:\n            self.agent.set_user_message(user_message)\n\n    # Initialize parsed done sequences now that self.agent is available\n    if self.config.done_sequences:\n        from .done_sequence_parser import parse_done_sequences\n\n        # Pass agent's llm_tools_map directly\n        tools_map = (\n            self.agent.llm_tools_map\n            if hasattr(self.agent, \"llm_tools_map\")\n            else None\n        )\n        self._parsed_done_sequences = parse_done_sequences(\n            self.config.done_sequences, tools_map\n        )\n\n    self.max_cost: float = 0\n    self.max_tokens: int = 0\n    self.session_id: str = \"\"\n    self.logger: None | RichFileLogger = None\n    self.tsv_logger: None | logging.Logger = None\n    self.html_logger: Optional[HTMLLogger] = None\n    self.color_log: bool = False if settings.notebook else True\n\n    self.n_stalled_steps = 0  # how many consecutive steps with no progress?\n    # how many 2-step-apart alternations of no_answer step-result have we had,\n    # i.e. x1, N/A, x2, N/A, x3, N/A ...\n    self.n_no_answer_alternations = 0\n    self._no_answer_step: int = -5\n    self._step_idx = -1  # current step index\n    self.max_stalled_steps = max_stalled_steps\n    self.done_if_response = [r.value for r in done_if_response]\n    self.done_if_no_response = [r.value for r in done_if_no_response]\n    self.is_done = False  # is task done (based on response)?\n    self.is_pass_thru = False  # is current response a pass-thru?\n    if name:\n        # task name overrides name in agent config\n        agent.config.name = name\n    self.name = name or agent.config.name\n    self.value: str = self.name\n\n    self.default_human_response = default_human_response\n    if default_human_response is not None:\n        # only override agent's default_human_response if it is explicitly set\n        self.agent.default_human_response = default_human_response\n    self.interactive = interactive\n    self.agent.interactive = interactive\n    self.only_user_quits_root = only_user_quits_root\n    self.message_history_idx = -1\n    self.default_return_type = default_return_type\n\n    # set to True if we want to collapse multi-turn conversation with sub-tasks into\n    # just the first outgoing message and last incoming message.\n    # Note this also completely erases sub-task agents' message_history.\n    self.erase_substeps = erase_substeps\n    self.allow_null_result = allow_null_result\n\n    agent_entity_responders = agent.entity_responders()\n    agent_entity_responders_async = agent.entity_responders_async()\n    self.responders: List[Responder] = [e for e, _ in agent_entity_responders]\n    self.responders_async: List[Responder] = [\n        e for e, _ in agent_entity_responders_async\n    ]\n    self.non_human_responders: List[Responder] = [\n        r for r in self.responders if r != Entity.USER\n    ]\n    self.non_human_responders_async: List[Responder] = [\n        r for r in self.responders_async if r != Entity.USER\n    ]\n\n    self.human_tried = False  # did human get a chance to respond in last step?\n    self._entity_responder_map: Dict[\n        Entity, Callable[..., Optional[ChatDocument]]\n    ] = dict(agent_entity_responders)\n\n    self._entity_responder_async_map: Dict[\n        Entity, Callable[..., Coroutine[Any, Any, Optional[ChatDocument]]]\n    ] = dict(agent_entity_responders_async)\n\n    self.name_sub_task_map: Dict[str, Task] = {}\n    # latest message in a conversation among entities and agents.\n    self.pending_message: Optional[ChatDocument] = None\n    self.pending_sender: Responder = Entity.USER\n    self.single_round = single_round\n    self.turns = -1  # no limit\n    self.llm_delegate = llm_delegate\n    # Track last responder for done sequence checking\n    self._last_responder: Optional[Responder] = None\n    # Track response sequence for message chain\n    self.response_sequence: List[ChatDocument] = []\n    if llm_delegate:\n        if self.single_round:\n            # 0: User instructs (delegating to LLM);\n            # 1: LLM (as the Controller) asks;\n            # 2: user replies.\n            self.turns = 2\n    else:\n        if self.single_round:\n            # 0: User (as Controller) asks,\n            # 1: LLM replies.\n            self.turns = 1\n    # other sub_tasks this task can delegate to\n    self.sub_tasks: List[Task] = []\n    self.caller: Task | None = None  # which task called this task's `run` method\n</code></pre>"},{"location":"reference/#langroid.Task.clone","title":"<code>clone(i)</code>","text":"<p>Returns a copy of this task, with a new agent.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def clone(self, i: int) -&gt; \"Task\":\n    \"\"\"\n    Returns a copy of this task, with a new agent.\n    \"\"\"\n    assert isinstance(self.agent, ChatAgent), \"Task clone only works for ChatAgent\"\n    agent: ChatAgent = self.agent.clone(i)\n    return Task(\n        agent,\n        name=self.name + f\"-{i}\",\n        llm_delegate=self.llm_delegate,\n        single_round=self.single_round,\n        system_message=self.agent.system_message,\n        user_message=self.agent.user_message,\n        restart=self.restart,\n        default_human_response=self.default_human_response,\n        interactive=self.interactive,\n        erase_substeps=self.erase_substeps,\n        allow_null_result=self.allow_null_result,\n        max_stalled_steps=self.max_stalled_steps,\n        done_if_no_response=[Entity(s) for s in self.done_if_no_response],\n        done_if_response=[Entity(s) for s in self.done_if_response],\n        default_return_type=self.default_return_type,\n        config=self.config,\n    )\n</code></pre>"},{"location":"reference/#langroid.Task.kill_session","title":"<code>kill_session(session_id='')</code>  <code>classmethod</code>","text":"<p>Kill the session with the given session_id.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>@classmethod\ndef kill_session(cls, session_id: str = \"\") -&gt; None:\n    \"\"\"\n    Kill the session with the given session_id.\n    \"\"\"\n    session_id_kill_key = f\"{session_id}:kill\"\n    cls.cache().store(session_id_kill_key, \"1\")\n</code></pre>"},{"location":"reference/#langroid.Task.kill","title":"<code>kill()</code>","text":"<p>Kill the task run associated with the current session.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def kill(self) -&gt; None:\n    \"\"\"\n    Kill the task run associated with the current session.\n    \"\"\"\n    self._cache_session_store(\"kill\", \"1\")\n</code></pre>"},{"location":"reference/#langroid.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response (unless a sub-task is explicitly addressed).</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]</code> <p>A task, or list of tasks, or a tuple of task and task config, or a list of tuples of task and task config. These tasks are added as sub-tasks of the current task. The task configs (if any) dictate how the tasks are run when invoked as sub-tasks of other tasks. This allows users to specify behavior applicable only in the context of a particular task-subtask combination.</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(\n    self,\n    task: (\n        Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response (unless a sub-task is explicitly addressed).\n\n    Args:\n        task: A task, or list of tasks, or a tuple of task and task config,\n            or a list of tuples of task and task config.\n            These tasks are added as sub-tasks of the current task.\n            The task configs (if any) dictate how the tasks are run when\n            invoked as sub-tasks of other tasks. This allows users to specify\n            behavior applicable only in the context of a particular task-subtask\n            combination.\n    \"\"\"\n    if isinstance(task, list):\n        for t in task:\n            self.add_sub_task(t)\n        return\n\n    if isinstance(task, tuple):\n        task, config = task\n    else:\n        config = TaskConfig()\n    task.config_sub_task = config\n    self.sub_tasks.append(task)\n    self.name_sub_task_map[task.name] = task\n    self.responders.append(cast(Responder, task))\n    self.responders_async.append(cast(Responder, task))\n    self.non_human_responders.append(cast(Responder, task))\n    self.non_human_responders_async.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/#langroid.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>. Args:     msg (str|ChatDocument): optional message to start the conversation.</p> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\n    self.pending_sender = Entity.USER\n    if isinstance(msg, str):\n        self.pending_message = ChatDocument(\n            content=msg,\n            metadata=ChatDocMetaData(\n                sender=Entity.USER,\n            ),\n        )\n    elif msg is None and len(self.agent.message_history) &gt; 1:\n        # if agent has a history beyond system msg, set the\n        # pending message to the ChatDocument linked from\n        # last message in the history\n        last_agent_msg = self.agent.message_history[-1]\n        self.pending_message = ChatDocument.from_id(last_agent_msg.chat_document_id)\n        if self.pending_message is not None:\n            self.pending_sender = self.pending_message.metadata.sender\n    else:\n        if isinstance(msg, ChatDocument):\n            # carefully deep-copy: fresh metadata.id, register\n            # as new obj in registry\n            original_parent_id = msg.metadata.parent_id\n            self.pending_message = ChatDocument.deepcopy(msg)\n            # Preserve the parent pointer from the original message\n            self.pending_message.metadata.parent_id = original_parent_id\n        if self.pending_message is not None and self.caller is not None:\n            # msg may have come from `caller`, so we pretend this is from\n            # the CURRENT task's USER entity\n            self.pending_message.metadata.sender = Entity.USER\n            # update parent, child, agent pointers\n            if msg is not None:\n                msg.metadata.child_id = self.pending_message.metadata.id\n                # Only override parent_id if it wasn't already set in the\n                # original message. This preserves parent chains from TaskTool\n                if not msg.metadata.parent_id:\n                    self.pending_message.metadata.parent_id = msg.metadata.id\n        if self.pending_message is not None:\n            self.pending_message.metadata.agent_id = self.agent.id\n\n    self._show_pending_message_if_debug()\n    self.init_loggers()\n    # Log system message if it exists\n    if (\n        hasattr(self.agent, \"_create_system_and_tools_message\")\n        and hasattr(self.agent, \"system_message\")\n        and self.agent.system_message\n    ):\n        system_msg = self.agent._create_system_and_tools_message()\n        system_message_chat_doc = ChatDocument.from_LLMMessage(\n            system_msg,\n            sender_name=self.name or \"system\",\n        )\n        # log the system message\n        self.log_message(Entity.SYSTEM, system_message_chat_doc, mark=True)\n    self.log_message(Entity.USER, self.pending_message, mark=True)\n    return self.pending_message\n</code></pre>"},{"location":"reference/#langroid.Task.init_loggers","title":"<code>init_loggers()</code>","text":"<p>Initialise per-task Rich and TSV loggers.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init_loggers(self) -&gt; None:\n    \"\"\"Initialise per-task Rich and TSV loggers.\"\"\"\n    from langroid.utils.logging import RichFileLogger\n\n    if not self.config.enable_loggers:\n        return\n\n    if self.caller is not None and self.caller.logger is not None:\n        self.logger = self.caller.logger\n    elif self.logger is None:\n        self.logger = RichFileLogger(\n            str(Path(self.config.logs_dir) / f\"{self.name}.log\"),\n            append=True,\n            color=self.color_log,\n        )\n\n    if self.caller is not None and self.caller.tsv_logger is not None:\n        self.tsv_logger = self.caller.tsv_logger\n    elif self.tsv_logger is None:\n        # unique logger name ensures a distinct `logging.Logger` object\n        self.tsv_logger = setup_file_logger(\n            f\"tsv_logger.{self.name}.{id(self)}\",\n            str(Path(self.config.logs_dir) / f\"{self.name}.tsv\"),\n        )\n        header = ChatDocLoggerFields().tsv_header()\n        self.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\n\n    # HTML logger\n    if self.config.enable_html_logging:\n        if (\n            self.caller is not None\n            and hasattr(self.caller, \"html_logger\")\n            and self.caller.html_logger is not None\n        ):\n            self.html_logger = self.caller.html_logger\n        elif not hasattr(self, \"html_logger\") or self.html_logger is None:\n            from langroid.utils.html_logger import HTMLLogger\n\n            model_info = \"\"\n            if (\n                hasattr(self, \"agent\")\n                and hasattr(self.agent, \"config\")\n                and hasattr(self.agent.config, \"llm\")\n            ):\n                model_info = getattr(self.agent.config.llm, \"chat_model\", \"\")\n            self.html_logger = HTMLLogger(\n                filename=self.name,\n                log_dir=self.config.logs_dir,\n                model_info=model_info,\n                append=False,\n            )\n            # Log clickable file:// link to the HTML log\n            html_log_path = self.html_logger.file_path.resolve()\n            logger.warning(f\"\ud83d\udcca HTML Log: file://{html_log_path}\")\n</code></pre>"},{"location":"reference/#langroid.Task.reset_all_sub_tasks","title":"<code>reset_all_sub_tasks()</code>","text":"<p>Recursively reset message history &amp; state of own agent and those of all sub-tasks.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def reset_all_sub_tasks(self) -&gt; None:\n    \"\"\"\n    Recursively reset message history &amp; state of own agent and\n    those of all sub-tasks.\n    \"\"\"\n    self.agent.init_state()\n    for t in self.sub_tasks:\n        t.reset_all_sub_tasks()\n</code></pre>"},{"location":"reference/#langroid.Task.run","title":"<code>run(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='', allow_restart=True, return_type=None)</code>","text":"<pre><code>run(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True\n) -&gt; Optional[ChatDocument]\n</code></pre><pre><code>run(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Type[T]\n) -&gt; Optional[T]\n</code></pre> <p>Synchronous version of <code>run_async()</code>. See <code>run_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\n    self,\n    msg: Any = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Optional[Type[T]] = None,\n) -&gt; Optional[ChatDocument | T]:\n    \"\"\"Synchronous version of `run_async()`.\n    See `run_async()` for details.\"\"\"\n    if allow_restart and (\n        (self.restart and caller is None)\n        or (self.config_sub_task.restart_as_subtask and caller is not None)\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    msg_input = self.agent.to_ChatDocument(msg, author_entity=Entity.USER)\n\n    if (\n        isinstance(msg_input, ChatDocument)\n        and msg_input.metadata.recipient != \"\"\n        and msg_input.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n\n    self._pre_run_loop(\n        msg=msg_input,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        self.step()\n        # Track pending message in response sequence\n        if self.pending_message is not None:\n            if (\n                not self.response_sequence\n                or self.pending_message.id() != self.response_sequence[-1].id()\n            ):\n                self.response_sequence.append(self.pending_message)\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    if final_result is None:\n        return None\n\n    if return_type is None:\n        return_type = self.default_return_type\n\n    # If possible, take a final strict decoding step\n    # when the output does not match `return_type`\n    if return_type is not None and return_type != ChatDocument:\n        parsed_result = self.agent.from_ChatDocument(final_result, return_type)\n\n        if (\n            parsed_result is None\n            and isinstance(self.agent, ChatAgent)\n            and self.agent._json_schema_available()\n        ):\n            strict_agent = self.agent[return_type]\n            output_args = strict_agent._function_args()[-1]\n            if output_args is not None:\n                schema = output_args.function.parameters\n                strict_result = strict_agent.llm_response(\n                    f\"\"\"\n                    A response adhering to the following JSON schema was expected:\n                    {schema}\n\n                    Please resubmit with the correct schema. \n                    \"\"\"\n                )\n\n                if strict_result is not None:\n                    return cast(\n                        Optional[T],\n                        strict_agent.from_ChatDocument(strict_result, return_type),\n                    )\n\n        return parsed_result\n\n    return final_result\n</code></pre>"},{"location":"reference/#langroid.Task.run_async","title":"<code>run_async(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='', allow_restart=True, return_type=None)</code>  <code>async</code>","text":"<pre><code>run_async(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True\n) -&gt; Optional[ChatDocument]\n</code></pre><pre><code>run_async(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Type[T]\n) -&gt; Optional[T]\n</code></pre> <p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached. Runs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Any</code> <p>initial user-role message to process; if None, the LLM will respond to its initial <code>self.task_messages</code> which set up and kick off the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User). Note that <code>msg</code>, if passed, is treated as message with role <code>user</code>; a \"system\" role message should not be passed here.</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <code>caller</code> <code>Task | None</code> <p>the calling task, if any</p> <code>None</code> <code>max_cost</code> <code>float</code> <p>max cost allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>max_tokens</code> <code>int</code> <p>max tokens allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>session_id</code> <code>str</code> <p>session id for the task</p> <code>''</code> <code>allow_restart</code> <code>bool</code> <p>whether to allow restarting the task</p> <code>True</code> <code>return_type</code> <code>Optional[Type[T]]</code> <p>desired final result type</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument | T]</code> <p>Optional[ChatDocument]: valid result of the task.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def run_async(\n    self,\n    msg: Any = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Optional[Type[T]] = None,\n) -&gt; Optional[ChatDocument | T]:\n    \"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Runs asynchronously.\n\n    Args:\n        msg (Any): initial *user-role* message to process; if None,\n            the LLM will respond to its initial `self.task_messages`\n            which set up and kick off the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User). Note that `msg`, if passed, is treated as\n            message with role `user`; a \"system\" role message should not be\n            passed here.\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n        caller (Task|None): the calling task, if any\n        max_cost (float): max cost allowed for the task (default 0 -&gt; no limit)\n        max_tokens (int): max tokens allowed for the task (default 0 -&gt; no limit)\n        session_id (str): session id for the task\n        allow_restart (bool): whether to allow restarting the task\n        return_type (Optional[Type[T]]): desired final result type\n\n    Returns:\n        Optional[ChatDocument]: valid result of the task.\n    \"\"\"\n\n    # Even if the initial \"sender\" is not literally the USER (since the task could\n    # have come from another LLM), as far as this agent is concerned, the initial\n    # message can be considered to be from the USER\n    # (from the POV of this agent's LLM).\n\n    if allow_restart and (\n        (self.restart and caller is None)\n        or (self.config_sub_task.restart_as_subtask and caller is not None)\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    msg_input = self.agent.to_ChatDocument(msg, author_entity=Entity.USER)\n\n    if (\n        isinstance(msg_input, ChatDocument)\n        and msg_input.metadata.recipient != \"\"\n        and msg_input.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n\n    self._pre_run_loop(\n        msg=msg_input,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        await self.step_async()\n        await asyncio.sleep(0.01)  # temp yield to avoid blocking\n        # Track pending message in response sequence\n        if self.pending_message is not None:\n            if (\n                not self.response_sequence\n                or self.pending_message.id() != self.response_sequence[-1].id()\n            ):\n                self.response_sequence.append(self.pending_message)\n\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    if final_result is None:\n        return None\n\n    if return_type is None:\n        return_type = self.default_return_type\n\n    # If possible, take a final strict decoding step\n    # when the output does not match `return_type`\n    if return_type is not None and return_type != ChatDocument:\n        parsed_result = self.agent.from_ChatDocument(final_result, return_type)\n\n        if (\n            parsed_result is None\n            and isinstance(self.agent, ChatAgent)\n            and self.agent._json_schema_available()\n        ):\n            strict_agent = self.agent[return_type]\n            output_args = strict_agent._function_args()[-1]\n            if output_args is not None:\n                schema = output_args.function.parameters\n                strict_result = await strict_agent.llm_response_async(\n                    f\"\"\"\n                    A response adhering to the following JSON schema was expected:\n                    {schema}\n\n                    Please resubmit with the correct schema. \n                    \"\"\"\n                )\n\n                if strict_result is not None:\n                    return cast(\n                        Optional[T],\n                        strict_agent.from_ChatDocument(strict_result, return_type),\n                    )\n\n        return parsed_result\n\n    return final_result\n</code></pre>"},{"location":"reference/#langroid.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>Synchronous version of <code>step_async()</code>. See <code>step_async()</code> for details. TODO: Except for the self.response() calls, this fn should be identical to <code>step_async()</code>. Consider refactoring to avoid duplication.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    Synchronous version of `step_async()`. See `step_async()` for details.\n    TODO: Except for the self.response() calls, this fn should be identical to\n    `step_async()`. Consider refactoring to avoid duplication.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # (When `interactive=False`, human is only allowed to respond only if\n        #  if explicitly addressed)\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    n_non_responders = 0\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            n_non_responders += 1\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            if n_non_responders == len(responders):\n                # don't stay in this \"non-response\" loop forever\n                break\n            continue\n        self.human_tried = r == Entity.USER\n        result = self.response(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:  # did not find a valid response\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/#langroid.Task.step_async","title":"<code>step_async(turns=-1)</code>  <code>async</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def step_async(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders_async.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = await self.response_async(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/#langroid.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Sync version of <code>response_async()</code>. See <code>response_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Sync version of `response_async()`. See `response_async()` for details.\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        pending_tools = self.agent.try_get_tool_messages(self.pending_message)\n        # TODO disable this\n        if (\n            len(pending_tools) &gt; 1\n            and len(self.agent.oai_tool_calls) &gt; 1\n            and not self.config.allow_subtask_multi_oai_tools\n        ):\n            result = self._forbid_multi_oai_tools(e)\n        else:\n            result = e.run(\n                self.pending_message,\n                turns=actual_turns,\n                caller=self,\n                max_cost=self.max_cost,\n                max_tokens=self.max_tokens,\n            )\n            # update result.tool_messages if any\n            if isinstance(result, ChatDocument):\n                self.agent.try_get_tool_messages(result)\n            if result is not None:\n                content, id2result, oai_tool_id = self.agent.process_tool_results(\n                    result.content,\n                    result.oai_tool_id2result,\n                    (\n                        self.pending_message.oai_tool_calls\n                        if isinstance(self.pending_message, ChatDocument)\n                        else None\n                    ),\n                )\n                result.content = content\n                result.oai_tool_id2result = id2result\n                result.metadata.oai_tool_id = oai_tool_id\n\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\"\n            if result is None\n            else \"\\n\\n\".join(str(m) for m in ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_map[cast(Entity, e)]\n        result = response_fn(self.pending_message)\n        # update result.tool_messages if any.\n        # Do this only if sender is LLM, since this could be\n        # a tool-call result from the Agent responder, which may\n        # contain strings that look like tools, and we don't want to\n        # trigger strict tool recovery due to that.\n        if (\n            isinstance(result, ChatDocument)\n            and result.metadata.sender == Entity.LLM\n        ):\n            self.agent.try_get_tool_messages(result)\n\n    result_chat_doc = self.agent.to_ChatDocument(\n        result,\n        chat_doc=self.pending_message,\n        author_entity=e if isinstance(e, Entity) else Entity.USER,\n    )\n    return self._process_result_routing(result_chat_doc, e)\n</code></pre>"},{"location":"reference/#langroid.Task.response_async","title":"<code>response_async(e, turns=-1)</code>  <code>async</code>","text":"<p>Get response to <code>self.pending_message</code> from a responder. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None. Args:     e (Responder): responder to get response from.     turns (int): number of turns to run the task for.         Default is -1, which means run until task is done.</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def response_async(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get response to `self.pending_message` from a responder.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Responder): responder to get response from.\n        turns (int): number of turns to run the task for.\n            Default is -1, which means run until task is done.\n\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        pending_tools = self.agent.try_get_tool_messages(self.pending_message)\n        # TODO disable this\n        if (\n            len(pending_tools) &gt; 1\n            and len(self.agent.oai_tool_calls) &gt; 1\n            and not self.config.allow_subtask_multi_oai_tools\n        ):\n            result = self._forbid_multi_oai_tools(e)\n        else:\n            # e.callbacks.set_parent_agent(self.agent)\n            result = await e.run_async(\n                self.pending_message,\n                turns=actual_turns,\n                caller=self,\n                max_cost=self.max_cost,\n                max_tokens=self.max_tokens,\n            )\n            # update result.tool_messages if any\n            if isinstance(result, ChatDocument):\n                self.agent.try_get_tool_messages(result)\n            if result is not None:\n                content, id2result, oai_tool_id = self.agent.process_tool_results(\n                    result.content,\n                    result.oai_tool_id2result,\n                    (\n                        self.pending_message.oai_tool_calls\n                        if isinstance(self.pending_message, ChatDocument)\n                        else None\n                    ),\n                )\n                result.content = content\n                result.oai_tool_id2result = id2result\n                result.metadata.oai_tool_id = oai_tool_id\n\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\"\n            if result is None\n            else \"\\n\\n\".join(str(m) for m in ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_async_map[cast(Entity, e)]\n        result = await response_fn(self.pending_message)\n        # update result.tool_messages if any\n        if (\n            isinstance(result, ChatDocument)\n            and result.metadata.sender == Entity.LLM\n        ):\n            self.agent.try_get_tool_messages(result)\n\n    result_chat_doc = self.agent.to_ChatDocument(\n        result,\n        chat_doc=self.pending_message,\n        author_entity=e if isinstance(e, Entity) else Entity.USER,\n    )\n    return self._process_result_routing(result_chat_doc, e)\n</code></pre>"},{"location":"reference/#langroid.Task.result","title":"<code>result(status=None)</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this.</p> <p>Note the result of a task is returned as if it is from the User entity.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>StatusCode</code> <p>status of the task when it ended</p> <code>None</code> <p>Returns:     ChatDocument: result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self, status: StatusCode | None = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n\n    Note the result of a task is returned as if it is from the User entity.\n\n    Args:\n        status (StatusCode): status of the task when it ended\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\n    if status in [StatusCode.STALLED, StatusCode.MAX_TURNS, StatusCode.INF_LOOP]:\n        # In these case we don't know (and don't want to try to guess)\n        # what the task result should be, so we return None\n        return None\n\n    result_msg = self.pending_message\n\n    content = result_msg.content if result_msg else \"\"\n    content_any = result_msg.content_any if result_msg else None\n    if DONE in content and self.config.recognize_string_signals:\n        # assuming it is of the form \"DONE: &lt;content&gt;\"\n        content = content.replace(DONE, \"\").strip()\n    oai_tool_calls = result_msg.oai_tool_calls if result_msg else None\n    oai_tool_id2result = result_msg.oai_tool_id2result if result_msg else None\n    fun_call = result_msg.function_call if result_msg else None\n    tool_messages = result_msg.tool_messages if result_msg else []\n    # if there is a DoneTool or AgentDoneTool among these,\n    # we extract content and tools from here, and ignore all others\n    for t in tool_messages:\n        if isinstance(t, FinalResultTool):\n            content = \"\"\n            content_any = None\n            tool_messages = [t]  # pass it on to parent so it also quits\n            break\n        elif isinstance(t, (AgentDoneTool, DoneTool)):\n            # there shouldn't be multiple tools like this; just take the first\n            content = to_string(t.content)\n            content_any = t.content\n            fun_call = None\n            oai_tool_calls = None\n            if isinstance(t, AgentDoneTool):\n                # AgentDoneTool may have tools, unlike DoneTool\n                tool_messages = t.tools\n            break\n    # drop the \"Done\" tools since they should not be part of the task result,\n    # or else they would cause the parent task to get unintentionally done!\n    tool_messages = [\n        t for t in tool_messages if not isinstance(t, (DoneTool, AgentDoneTool))\n    ]\n    block = result_msg.metadata.block if result_msg else None\n    recipient = result_msg.metadata.recipient if result_msg else \"\"\n    tool_ids = result_msg.metadata.tool_ids if result_msg else []\n\n    # regardless of which entity actually produced the result,\n    # when we return the result, we set entity to USER\n    # since to the \"parent\" task, this result is equivalent to a response from USER\n    result_doc = ChatDocument(\n        content=content,\n        content_any=content_any,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=fun_call,\n        tool_messages=tool_messages,\n        metadata=ChatDocMetaData(\n            source=Entity.USER,\n            sender=Entity.USER,\n            block=block,\n            status=status or (result_msg.metadata.status if result_msg else None),\n            sender_name=self.name,\n            recipient=recipient,\n            tool_ids=tool_ids,\n            parent_id=result_msg.id() if result_msg else \"\",\n            agent_id=str(self.agent.id),\n        ),\n    )\n    if self.pending_message is not None:\n        self.pending_message.metadata.child_id = result_doc.id()\n\n    return result_doc\n</code></pre>"},{"location":"reference/#langroid.Task.done","title":"<code>done(result=None, r=None)</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this. Args:     result (ChatDocument|None): result from a responder     r (Responder|None): responder that produced the result         Not used here, but could be used by derived classes. Returns:     bool: True if task is done, False otherwise     StatusCode: status code indicating why task is done</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(\n    self, result: ChatDocument | None = None, r: Responder | None = None\n) -&gt; Tuple[bool, StatusCode]:\n    \"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Args:\n        result (ChatDocument|None): result from a responder\n        r (Responder|None): responder that produced the result\n            Not used here, but could be used by derived classes.\n    Returns:\n        bool: True if task is done, False otherwise\n        StatusCode: status code indicating why task is done\n    \"\"\"\n    if self._is_kill():\n        return (True, StatusCode.KILL)\n    result = result or self.pending_message\n\n    # Check if task should be done if message contains a tool\n    if self.config.done_if_tool and result is not None:\n        if isinstance(result, ChatDocument) and self.agent.try_get_tool_messages(\n            result, all_tools=True\n        ):\n            return (True, StatusCode.DONE)\n\n    # Check done sequences\n    if self._parsed_done_sequences and result is not None:\n        # Get the message chain from the current result\n        msg_chain = self._get_message_chain(result)\n\n        # Use last responder if r not provided\n        responder = r if r is not None else self._last_responder\n\n        # Check each sequence\n        for sequence in self._parsed_done_sequences:\n            if self._matches_sequence_with_current(\n                msg_chain, sequence, result, responder\n            ):\n                seq_name = sequence.name or \"unnamed\"\n                logger.info(f\"Task {self.name} done: matched sequence '{seq_name}'\")\n                return (True, StatusCode.DONE)\n\n    allow_done_string = self.config.recognize_string_signals\n    # An entity decided task is done, either via DoneTool,\n    # or by explicitly saying DONE\n    done_result = result is not None and (\n        (\n            DONE in (result.content if isinstance(result, str) else result.content)\n            and allow_done_string\n        )\n        or any(\n            isinstance(t, (DoneTool, AgentDoneTool, FinalResultTool))\n            for t in result.tool_messages\n        )\n    )\n\n    user_quit = (\n        result is not None\n        and (result.content in USER_QUIT_STRINGS or done_result)\n        and result.metadata.sender == Entity.USER\n    )\n\n    if self.n_stalled_steps &gt;= self.max_stalled_steps:\n        # we are stuck, so bail to avoid infinite loop\n        logger.warning(\n            f\"Task {self.name} stuck for {self.max_stalled_steps} steps; exiting.\"\n        )\n        return (True, StatusCode.STALLED)\n\n    if self.max_cost &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[1] &gt; self.max_cost:\n                logger.warning(\n                    f\"Task {self.name} cost exceeded {self.max_cost}; exiting.\"\n                )\n                return (True, StatusCode.MAX_COST)\n        except Exception:\n            pass\n\n    if self.max_tokens &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[0] &gt; self.max_tokens:\n                logger.warning(\n                    f\"Task {self.name} uses &gt; {self.max_tokens} tokens; exiting.\"\n                )\n                return (True, StatusCode.MAX_TOKENS)\n        except Exception:\n            pass\n\n    if self._level == 0 and self._user_can_respond() and self.only_user_quits_root:\n        # for top-level task, only user can quit out\n        return (user_quit, StatusCode.USER_QUIT if user_quit else StatusCode.OK)\n\n    if self.is_done:\n        return (True, StatusCode.DONE)\n\n    final = (\n        # no valid response from any entity/agent in current turn\n        result is None\n        or done_result\n        or (  # current task is addressing message to caller task\n            self.caller is not None\n            and self.caller.name != \"\"\n            and result.metadata.recipient == self.caller.name\n        )\n        or user_quit\n    )\n    return (final, StatusCode.OK)\n</code></pre>"},{"location":"reference/#langroid.Task.valid","title":"<code>valid(result, r)</code>","text":"<p>Is the result from a Responder (i.e. an entity or sub-task) such that we can stop searching for responses in this step?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(\n    self,\n    result: Optional[ChatDocument],\n    r: Responder,\n) -&gt; bool:\n    \"\"\"\n    Is the result from a Responder (i.e. an entity or sub-task)\n    such that we can stop searching for responses in this step?\n    \"\"\"\n    # TODO caution we should ensure that no handler method (tool) returns simply\n    # an empty string (e.g when showing contents of an empty file), since that\n    # would be considered an invalid response, and other responders will wrongly\n    # be given a chance to respond.\n\n    # if task would be considered done given responder r's `result`,\n    # then consider the result valid.\n    if result is not None and self.done(result, r)[0]:\n        return True\n    return (\n        result is not None\n        and not self._is_empty_message(result)\n        # some weaker LLMs, including even GPT-4o, may say \"DO-NOT-KNOW.\"\n        # (with a punctuation at the end), so need to strip out punctuation\n        and re.sub(r\"[,.!?:]\", \"\", result.content.strip()) != NO_ANSWER\n    )\n</code></pre>"},{"location":"reference/#langroid.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\n    self,\n    resp: Responder,\n    msg: ChatDocument | None = None,\n    mark: bool = False,\n) -&gt; None:\n    \"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\n    from langroid.agent.chat_document import ChatDocLoggerFields\n\n    default_values = ChatDocLoggerFields().model_dump().values()\n    msg_str_tsv = \"\\t\".join(str(v) for v in default_values)\n    if msg is not None:\n        msg_str_tsv = msg.tsv_str()\n\n    mark_str = \"*\" if mark else \" \"\n    task_name = self.name if self.name != \"\" else \"root\"\n    resp_color = \"white\" if mark else \"red\"\n    resp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\n\n    if msg is None:\n        msg_str = f\"{mark_str}({task_name}) {resp_str}\"\n    else:\n        color = {\n            Entity.LLM: \"green\",\n            Entity.USER: \"blue\",\n            Entity.AGENT: \"red\",\n            Entity.SYSTEM: \"magenta\",\n        }[msg.metadata.sender]\n        f = msg.log_fields()\n        tool_type = f.tool_type.rjust(6)\n        tool_name = f.tool.rjust(10)\n        tool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\n        sender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\n        sender_name = f.sender_name.rjust(10)\n        recipient = \"=&gt;\" + str(f.recipient).rjust(10)\n        block = \"X \" + str(f.block or \"\").rjust(10)\n        content = f\"[{color}]{f.content}[/{color}]\"\n        msg_str = (\n            f\"{mark_str}({task_name}) \"\n            f\"{resp_str} {sender}({sender_name}) \"\n            f\"({recipient}) ({block}) {tool_str} {content}\"\n        )\n\n    if self.logger is not None:\n        self.logger.log(msg_str)\n    if self.tsv_logger is not None:\n        resp_str = str(resp)\n        self.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n\n    # HTML logger\n    if self.html_logger is not None:\n        if msg is None:\n            # Create a minimal fields object for None messages\n            from langroid.agent.chat_document import ChatDocLoggerFields\n\n            fields_dict = {\n                \"responder\": str(resp),\n                \"mark\": \"*\" if mark else \"\",\n                \"task_name\": self.name or \"root\",\n                \"content\": \"\",\n                \"sender_entity\": str(resp),\n                \"sender_name\": \"\",\n                \"recipient\": \"\",\n                \"block\": None,\n                \"tool_type\": \"\",\n                \"tool\": \"\",\n            }\n        else:\n            # Get fields from the message\n            fields = msg.log_fields()\n            fields_dict = fields.model_dump()\n            fields_dict.update(\n                {\n                    \"responder\": str(resp),\n                    \"mark\": \"*\" if mark else \"\",\n                    \"task_name\": self.name or \"root\",\n                }\n            )\n\n        # Create a ChatDocLoggerFields-like object for the HTML logger\n        # Create a simple BaseModel subclass dynamically\n        from pydantic import BaseModel\n\n        class LogFields(BaseModel):\n            model_config = ConfigDict(extra=\"allow\")  # Allow extra fields\n\n        log_obj = LogFields(**fields_dict)\n        self.html_logger.log(log_obj)\n</code></pre>"},{"location":"reference/#langroid.Task.set_color_log","title":"<code>set_color_log(enable=True)</code>","text":"<p>Flag to enable/disable color logging using rich.console. In some contexts, such as Colab notebooks, we may want to disable color logging using rich.console, since those logs show up in the cell output rather than in the log file. Turning off this feature will still create logs, but without the color formatting from rich.console Args:     enable (bool): value of <code>self.color_log</code> to set to,         which will enable/diable rich logging</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def set_color_log(self, enable: bool = True) -&gt; None:\n    \"\"\"\n    Flag to enable/disable color logging using rich.console.\n    In some contexts, such as Colab notebooks, we may want to disable color logging\n    using rich.console, since those logs show up in the cell output rather than\n    in the log file. Turning off this feature will still create logs, but without\n    the color formatting from rich.console\n    Args:\n        enable (bool): value of `self.color_log` to set to,\n            which will enable/diable rich logging\n    \"\"\"\n    self.color_log = enable\n</code></pre>"},{"location":"reference/#langroid.Task.close_loggers","title":"<code>close_loggers()</code>","text":"<p>Close all loggers to ensure clean shutdown.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def close_loggers(self) -&gt; None:\n    \"\"\"Close all loggers to ensure clean shutdown.\"\"\"\n    if hasattr(self, \"logger\") and self.logger is not None:\n        self.logger.close()\n    if hasattr(self, \"html_logger\") and self.html_logger is not None:\n        self.html_logger.close()\n</code></pre>"},{"location":"reference/#langroid.TaskConfig","title":"<code>TaskConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a Task. This is a container for any params that we didn't include in the task <code>__init__</code> method. We may eventually move all the task init params to this class, analogous to how we have config classes for <code>Agent</code>, <code>ChatAgent</code>, <code>LanguageModel</code>, etc.</p> <p>Attributes:</p> Name Type Description <code>inf_loop_cycle_len</code> <code>int</code> <p>max exact-loop cycle length: 0 =&gt; no inf loop test</p> <code>inf_loop_dominance_factor</code> <code>float</code> <p>dominance factor for exact-loop detection</p> <code>inf_loop_wait_factor</code> <code>int</code> <p>wait this * cycle_len msgs before loop-check</p> <code>restart_as_subtask</code> <code>bool</code> <p>whether to restart every run of this task when run as a subtask.</p> <code>addressing_prefix</code> <code>str</code> <p>\"@\"-like prefix an agent can use to address other agents, or entities of the agent. E.g., if this is \"@\", the addressing string would be \"@Alice\", or \"@user\", \"@llm\", \"@agent\", etc. If this is an empty string, then addressing is disabled. Default is empty string \"\". CAUTION: this is a deprecated practice, since normal prompts can accidentally contain such addressing prefixes, and will break your runs. This could happen especially when your prompt/context contains code, but of course could occur in normal text as well. Instead, use the <code>RecipientTool</code> to have agents address other agents or entities. If you do choose to use <code>addressing_prefix</code>, the recommended setting is to use <code>langroid.utils.constants.AT</code>, which currently is \"|@|\". Note that this setting does NOT affect the use of <code>constants.SEND_TO</code> -- this is always enabled since this is a critical way for responders to indicate that the message should be sent to a specific entity/agent. (Search for \"SEND_TO\" in the examples/ dir to see how this is used.)</p> <code>allow_subtask_multi_oai_tools</code> <code>bool</code> <p>whether to allow multiple OpenAI tool-calls to be sent to a sub-task.</p> <code>recognize_string_signals</code> <code>bool</code> <p>whether to recognize string-based signaling like DONE, SEND_TO, PASS, etc. Default is True, but note that we don't need to use string-based signaling, and it is recommended to use the new Orchestration tools instead (see agent/tools/orchestration.py), e.g. DoneTool, SendTool, etc. Note: this is distinct from <code>ChatAgentConfig.recognize_recipient_in_content</code>, which controls whether LLM response text is parsed for <code>TO[&lt;recipient&gt;]:</code> and JSON <code>{\"recipient\": ...}</code> patterns at the Agent level. To fully disable all text-based routing, set both to False.</p> <code>done_if_tool</code> <code>bool</code> <p>whether to consider the task done if the pending message contains a Tool attempt by the LLM (including tools not handled by the agent). Default is False.</p> <code>done_sequences</code> <code>List[DoneSequence]</code> <p>List of event sequences that trigger task completion. Task is done if ANY sequence matches the recent event history. Each sequence is checked against the message parent chain. Tool classes can be referenced in sequences like \"T[MyToolClass]\".</p>"},{"location":"reference/#langroid.DocMetaData","title":"<code>DocMetaData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a document.</p>"},{"location":"reference/#langroid.DocMetaData.convert_id_to_string","title":"<code>convert_id_to_string(v)</code>  <code>classmethod</code>","text":"<p>Convert id to string if it's not already.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>@field_validator(\"id\", mode=\"before\")\n@classmethod\ndef convert_id_to_string(cls, v: Any) -&gt; str:\n    \"\"\"Convert id to string if it's not already.\"\"\"\n    if v is None:\n        return str(uuid4())\n    return str(v)\n</code></pre>"},{"location":"reference/#langroid.DocMetaData.dict_bool_int","title":"<code>dict_bool_int(*args, **kwargs)</code>","text":"<p>Special dict method to convert bool fields to int, to appease some downstream libraries,  e.g. Chroma which complains about bool fields in metadata.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>def dict_bool_int(self, *args: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Special dict method to convert bool fields to int, to appease some\n    downstream libraries,  e.g. Chroma which complains about bool fields in\n    metadata.\n    \"\"\"\n    original_dict = super().model_dump(*args, **kwargs)\n\n    for key, value in original_dict.items():\n        if isinstance(value, bool):\n            original_dict[key] = 1 * value\n\n    return original_dict\n</code></pre>"},{"location":"reference/#langroid.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Interface for interacting with a document.</p>"},{"location":"reference/#langroid.Entity","title":"<code>Entity</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the different types of entities that can respond to the current message.</p>"},{"location":"reference/#langroid.LangroidImportError","title":"<code>LangroidImportError(package=None, extra=None, error='', *args)</code>","text":"<p>               Bases: <code>ImportError</code></p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The name of the package to import.</p> <code>None</code> <code>extra</code> <code>str</code> <p>The name of the extras package required for this import.</p> <code>None</code> <code>error</code> <code>str</code> <p>The error message to display. Depending on context, we can set this by capturing the ImportError message.</p> <code>''</code> Source code in <code>langroid/exceptions.py</code> <pre><code>def __init__(\n    self,\n    package: Optional[str] = None,\n    extra: Optional[str | List[str]] = None,\n    error: str = \"\",\n    *args: object,\n) -&gt; None:\n    \"\"\"\n    Generate helpful warning when attempting to import package or module.\n\n    Args:\n        package (str): The name of the package to import.\n        extra (str): The name of the extras package required for this import.\n        error (str): The error message to display. Depending on context, we\n            can set this by capturing the ImportError message.\n\n    \"\"\"\n    if error == \"\" and package is not None:\n        error = f\"{package} is not installed by default with Langroid.\\n\"\n\n    if extra:\n        if isinstance(extra, list):\n            help_preamble = f\"\"\"\n            If you want to use it, please install langroid with one of these \n            extras: {', '.join(extra)}. The examples below use the first one, \n            i.e. {extra[0]}.\n            \"\"\"\n            extra = extra[0]\n        else:\n            help_preamble = f\"\"\"\n            If you want to use it, please install langroid with the\n            `{extra}` extra.\n            \"\"\"\n\n        install_help = f\"\"\"\n            {help_preamble}\n\n            If you are using pip:\n            pip install \"langroid[{extra}]\"\n\n            For multiple extras, you can separate them with commas:\n            pip install \"langroid[{extra},another-extra]\"\n\n            If you are using Poetry:\n            poetry add langroid --extras \"{extra}\"\n\n            For multiple extras with Poetry, list them with spaces:\n            poetry add langroid --extras \"{extra} another-extra\"\n\n            If you are using uv:\n            uv add \"langroid[{extra}]\"\n\n            For multiple extras with uv, you can separate them with commas: \n            uv add \"langroid[{extra},another-extra]\"\n\n            If you are working within the langroid dev env (which uses uv),\n            you can do:\n            uv sync --dev --extra \"{extra}\"\n            or if you want to include multiple extras:\n            uv sync --dev --extra \"{extra}\" --extra \"another-extra\"\n            \"\"\"\n    else:\n        install_help = \"\"\"\n            If you want to use it, please install it in the same\n            virtual environment as langroid.\n            \"\"\"\n    msg = error + install_help\n\n    super().__init__(msg, *args)\n</code></pre>"},{"location":"reference/#langroid.run_batch_tasks","title":"<code>run_batch_tasks(task, items, input_map=lambda x: str(x), output_map=lambda x: x, stop_on_first_result=False, sequential=True, batch_size=None, turns=-1, max_cost=0.0, max_tokens=0)</code>","text":"<p>Run copies of <code>task</code> async/concurrently one per item in <code>items</code> list. For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result. Args:     task (Task): task to run     items (list[T]): list of items to process     input_map (Callable[[T], str|ChatDocument]): function to map item to         initial message to process     output_map (Callable[[ChatDocument|str], U]): function to map result         to final result     sequential (bool): whether to run sequentially         (e.g. some APIs such as ooba don't support concurrent requests)     batch_size (Optional[int]): The number of tasks to run at a time,         if None, unbatched     turns (int): number of turns to run, -1 for infinite     max_cost: float: maximum cost to run the task (default 0.0 for unlimited)     max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)</p> <p>Returns:</p> Type Description <code>List[Optional[U]]</code> <p>list[Optional[U]]: list of final results. Always list[U] if</p> <code>List[Optional[U]]</code> <p><code>stop_on_first_result</code> is disabled</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_tasks(\n    task: Task,\n    items: list[T],\n    input_map: Callable[[T], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], U] = lambda x: x,  # type: ignore\n    stop_on_first_result: bool = False,\n    sequential: bool = True,\n    batch_size: Optional[int] = None,\n    turns: int = -1,\n    max_cost: float = 0.0,\n    max_tokens: int = 0,\n) -&gt; List[Optional[U]]:\n    \"\"\"\n    Run copies of `task` async/concurrently one per item in `items` list.\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n    Args:\n        task (Task): task to run\n        items (list[T]): list of items to process\n        input_map (Callable[[T], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], U]): function to map result\n            to final result\n        sequential (bool): whether to run sequentially\n            (e.g. some APIs such as ooba don't support concurrent requests)\n        batch_size (Optional[int]): The number of tasks to run at a time,\n            if None, unbatched\n        turns (int): number of turns to run, -1 for infinite\n        max_cost: float: maximum cost to run the task (default 0.0 for unlimited)\n        max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)\n\n    Returns:\n        list[Optional[U]]: list of final results. Always list[U] if\n        `stop_on_first_result` is disabled\n    \"\"\"\n    message = f\"[bold green]Running {len(items)} copies of {task.name}...\"\n    return run_batch_task_gen(\n        lambda i: task.clone(i),\n        items,\n        input_map,\n        output_map,\n        stop_on_first_result,\n        sequential,\n        batch_size,\n        turns,\n        message,\n        max_cost=max_cost,\n        max_tokens=max_tokens,\n    )\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>langroid<ul> <li>agent<ul> <li>base</li> <li>batch</li> <li>callbacks<ul> <li>chainlit</li> </ul> </li> <li>chat_agent</li> <li>chat_document</li> <li>done_sequence_parser</li> <li>openai_assistant</li> <li>special<ul> <li>arangodb<ul> <li>arangodb_agent</li> <li>system_messages</li> <li>tools</li> <li>utils</li> </ul> </li> <li>doc_chat_agent</li> <li>lance_doc_chat_agent</li> <li>lance_rag<ul> <li>critic_agent</li> <li>lance_rag_task</li> <li>query_planner_agent</li> </ul> </li> <li>lance_tools</li> <li>neo4j<ul> <li>csv_kg_chat</li> <li>neo4j_chat_agent</li> <li>system_messages</li> <li>tools</li> </ul> </li> <li>relevance_extractor_agent</li> <li>retriever_agent</li> <li>sql<ul> <li>sql_chat_agent</li> <li>utils<ul> <li>description_extractors</li> <li>populate_metadata</li> <li>system_message</li> <li>tools</li> </ul> </li> </ul> </li> <li>table_chat_agent</li> </ul> </li> <li>task</li> <li>tool_message</li> <li>tools<ul> <li>duckduckgo_search_tool</li> <li>exa_search_tool</li> <li>file_tools</li> <li>google_search_tool</li> <li>mcp<ul> <li>decorators</li> <li>fastmcp_client</li> </ul> </li> <li>metaphor_search_tool</li> <li>orchestration</li> <li>recipient_tool</li> <li>retrieval_tool</li> <li>rewind_tool</li> <li>segment_extract_tool</li> <li>task_tool</li> <li>tavily_search_tool</li> </ul> </li> <li>xml_tool_message</li> </ul> </li> <li>cachedb<ul> <li>base</li> <li>redis_cachedb</li> </ul> </li> <li>embedding_models<ul> <li>base</li> <li>models</li> <li>protoc<ul> <li>embeddings_pb2</li> <li>embeddings_pb2_grpc</li> </ul> </li> <li>remote_embeds</li> </ul> </li> <li>exceptions</li> <li>language_models<ul> <li>azure_openai</li> <li>base</li> <li>client_cache</li> <li>config</li> <li>mock_lm</li> <li>model_info</li> <li>openai_gpt</li> <li>prompt_formatter<ul> <li>base</li> <li>hf_formatter</li> <li>llama2_formatter</li> </ul> </li> <li>provider_params</li> <li>utils</li> </ul> </li> <li>mytypes</li> <li>parsing<ul> <li>agent_chats</li> <li>code_parser</li> <li>document_parser</li> <li>file_attachment</li> <li>md_parser</li> <li>para_sentence_split</li> <li>parse_json</li> <li>parser</li> <li>pdf_utils</li> <li>repo_loader</li> <li>routing</li> <li>search</li> <li>spider</li> <li>table_loader</li> <li>url_loader</li> <li>urls</li> <li>utils</li> <li>web_search</li> </ul> </li> <li>prompts<ul> <li>dialog</li> <li>prompts_config</li> <li>templates</li> </ul> </li> <li>pydantic_v1<ul> <li>main</li> </ul> </li> <li>utils<ul> <li>algorithms<ul> <li>graph</li> </ul> </li> <li>configuration</li> <li>constants</li> <li>git_utils</li> <li>globals</li> <li>html_logger</li> <li>logging</li> <li>object_registry</li> <li>output<ul> <li>citations</li> <li>printing</li> <li>status</li> </ul> </li> <li>pandas_utils</li> <li>pydantic_utils</li> <li>system</li> <li>types</li> </ul> </li> <li>vector_store<ul> <li>base</li> <li>chromadb</li> <li>lancedb</li> <li>meilisearch</li> <li>pineconedb</li> <li>postgres</li> <li>qdrantdb</li> <li>weaviatedb</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/exceptions/","title":"exceptions","text":"<p>langroid/exceptions.py </p>"},{"location":"reference/exceptions/#langroid.exceptions.LangroidImportError","title":"<code>LangroidImportError(package=None, extra=None, error='', *args)</code>","text":"<p>               Bases: <code>ImportError</code></p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The name of the package to import.</p> <code>None</code> <code>extra</code> <code>str</code> <p>The name of the extras package required for this import.</p> <code>None</code> <code>error</code> <code>str</code> <p>The error message to display. Depending on context, we can set this by capturing the ImportError message.</p> <code>''</code> Source code in <code>langroid/exceptions.py</code> <pre><code>def __init__(\n    self,\n    package: Optional[str] = None,\n    extra: Optional[str | List[str]] = None,\n    error: str = \"\",\n    *args: object,\n) -&gt; None:\n    \"\"\"\n    Generate helpful warning when attempting to import package or module.\n\n    Args:\n        package (str): The name of the package to import.\n        extra (str): The name of the extras package required for this import.\n        error (str): The error message to display. Depending on context, we\n            can set this by capturing the ImportError message.\n\n    \"\"\"\n    if error == \"\" and package is not None:\n        error = f\"{package} is not installed by default with Langroid.\\n\"\n\n    if extra:\n        if isinstance(extra, list):\n            help_preamble = f\"\"\"\n            If you want to use it, please install langroid with one of these \n            extras: {', '.join(extra)}. The examples below use the first one, \n            i.e. {extra[0]}.\n            \"\"\"\n            extra = extra[0]\n        else:\n            help_preamble = f\"\"\"\n            If you want to use it, please install langroid with the\n            `{extra}` extra.\n            \"\"\"\n\n        install_help = f\"\"\"\n            {help_preamble}\n\n            If you are using pip:\n            pip install \"langroid[{extra}]\"\n\n            For multiple extras, you can separate them with commas:\n            pip install \"langroid[{extra},another-extra]\"\n\n            If you are using Poetry:\n            poetry add langroid --extras \"{extra}\"\n\n            For multiple extras with Poetry, list them with spaces:\n            poetry add langroid --extras \"{extra} another-extra\"\n\n            If you are using uv:\n            uv add \"langroid[{extra}]\"\n\n            For multiple extras with uv, you can separate them with commas: \n            uv add \"langroid[{extra},another-extra]\"\n\n            If you are working within the langroid dev env (which uses uv),\n            you can do:\n            uv sync --dev --extra \"{extra}\"\n            or if you want to include multiple extras:\n            uv sync --dev --extra \"{extra}\" --extra \"another-extra\"\n            \"\"\"\n    else:\n        install_help = \"\"\"\n            If you want to use it, please install it in the same\n            virtual environment as langroid.\n            \"\"\"\n    msg = error + install_help\n\n    super().__init__(msg, *args)\n</code></pre>"},{"location":"reference/mytypes/","title":"mytypes","text":"<p>langroid/mytypes.py </p>"},{"location":"reference/mytypes/#langroid.mytypes.Entity","title":"<code>Entity</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the different types of entities that can respond to the current message.</p>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData","title":"<code>DocMetaData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a document.</p>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData.convert_id_to_string","title":"<code>convert_id_to_string(v)</code>  <code>classmethod</code>","text":"<p>Convert id to string if it's not already.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>@field_validator(\"id\", mode=\"before\")\n@classmethod\ndef convert_id_to_string(cls, v: Any) -&gt; str:\n    \"\"\"Convert id to string if it's not already.\"\"\"\n    if v is None:\n        return str(uuid4())\n    return str(v)\n</code></pre>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData.dict_bool_int","title":"<code>dict_bool_int(*args, **kwargs)</code>","text":"<p>Special dict method to convert bool fields to int, to appease some downstream libraries,  e.g. Chroma which complains about bool fields in metadata.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>def dict_bool_int(self, *args: Any, **kwargs: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    Special dict method to convert bool fields to int, to appease some\n    downstream libraries,  e.g. Chroma which complains about bool fields in\n    metadata.\n    \"\"\"\n    original_dict = super().model_dump(*args, **kwargs)\n\n    for key, value in original_dict.items():\n        if isinstance(value, bool):\n            original_dict[key] = 1 * value\n\n    return original_dict\n</code></pre>"},{"location":"reference/mytypes/#langroid.mytypes.Document","title":"<code>Document</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Interface for interacting with a document.</p>"},{"location":"reference/mytypes/#langroid.mytypes.NonToolAction","title":"<code>NonToolAction</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Possible options to handle non-tool msgs from LLM.</p>"},{"location":"reference/agent/","title":"agent","text":"<p>langroid/agent/init.py </p>"},{"location":"reference/agent/#langroid.agent.Agent","title":"<code>Agent(config=AgentConfig())</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An Agent is an abstraction that typically (but not necessarily) encapsulates an LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def __init__(self, config: AgentConfig = AgentConfig()):\n    self.config = config\n    self.id = ObjectRegistry.new_id()  # Initialize agent ID\n    self.lock = asyncio.Lock()  # for async access to update self.llm.usage_cost\n    self.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\n    self.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\n    self.llm_tools_handled: Set[str] = set()\n    self.llm_tools_usable: Set[str] = set()\n    self.llm_tools_known: Set[str] = set()  # all known tools, handled/used or not\n    # Indicates which tool-names are allowed to be inferred when\n    # the LLM \"forgets\" to include the request field in its tool-call.\n    self.enabled_requests_for_inference: Optional[Set[str]] = (\n        None  # If None, we allow all\n    )\n    self.interactive: bool = True  # may be modified by Task wrapper\n    self.token_stats_str = \"\"\n    self.default_human_response: Optional[str] = None\n    self._indent = \"\"\n    self.llm = LanguageModel.create(config.llm)\n    self.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\n    self.tool_error = False\n    self.search_for_tools = {\n        SearchForTools.CONTENT.value,\n        SearchForTools.TOOLS.value,\n        SearchForTools.FUNCTIONS.value,\n    }\n    if config.parsing is not None and self.config.llm is not None:\n        # token_encoding_model is used to obtain the tokenizer,\n        # so in case it's an OpenAI model, we ensure that the tokenizer\n        # corresponding to the model is used.\n        if isinstance(self.llm, OpenAIGPT) and self.llm.is_openai_chat_model():\n            config.parsing.token_encoding_model = self.llm.config.chat_model\n    self.parser: Optional[Parser] = (\n        Parser(config.parsing) if config.parsing else None\n    )\n    if config.add_to_registry:\n        ObjectRegistry.register_object(self)\n\n    self.callbacks = SimpleNamespace(\n        start_llm_stream=lambda: noop_fn,\n        start_llm_stream_async=async_lambda_noop_fn,\n        cancel_llm_stream=noop_fn,\n        finish_llm_stream=noop_fn,\n        show_llm_response=noop_fn,\n        show_agent_response=noop_fn,\n        get_user_response=None,\n        get_user_response_async=None,\n        get_last_step=noop_fn,\n        set_parent_agent=noop_fn,\n        show_error_message=noop_fn,\n        show_start_response=noop_fn,\n    )\n    Agent.init_state(self)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.indent","title":"<code>indent</code>  <code>property</code> <code>writable</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/agent/#langroid.agent.Agent.all_llm_tools_known","title":"<code>all_llm_tools_known</code>  <code>property</code>","text":"<p>All known tools; this may extend self.llm_tools_known.</p>"},{"location":"reference/agent/#langroid.agent.Agent.init_state","title":"<code>init_state()</code>","text":"<p>Initialize all state vars. Called by Task.run() if restart is True</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def init_state(self) -&gt; None:\n    \"\"\"Initialize all state vars. Called by Task.run() if restart is True\"\"\"\n    self.total_llm_token_cost = 0.0\n    self.total_llm_token_usage = 0\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details. Returns:     Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\n    self,\n) -&gt; List[\n    Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n    \"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response),\n        (Entity.LLM, self.llm_response),\n        (Entity.USER, self.user_response),\n    ]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.entity_responders_async","title":"<code>entity_responders_async()</code>","text":"<p>Async version of <code>entity_responders</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders_async(\n    self,\n) -&gt; List[\n    Tuple[\n        Entity,\n        Callable[\n            [None | str | ChatDocument], Coroutine[Any, Any, None | ChatDocument]\n        ],\n    ]\n]:\n    \"\"\"\n    Async version of `entity_responders`. See there for details.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response_async),\n        (Entity.LLM, self.llm_response_async),\n        (Entity.USER, self.user_response_async),\n    ]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\n    self, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n    \"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable a message class from being handled by this Agent.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes. Returns:     str: The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n    \"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    # use at most 2 sample conversations, no need to be exhaustive;\n    sample_convo = [\n        msg_cls().usage_examples(random=True)  # type: ignore\n        for i, msg_cls in enumerate(enabled_classes)\n        if i &lt; 2\n    ]\n    return \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.create_agent_response","title":"<code>create_agent_response(content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for agent_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_agent_response(\n    self,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: Optional[List[OpenAIToolCall]] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for agent_response.\"\"\"\n    return self.response_template(\n        Entity.AGENT,\n        content=content,\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.render_agent_response","title":"<code>render_agent_response(results)</code>","text":"<p>Render the response from the agent, typically from tool-handling. Args:     results: results from tool-handling, which may be a string,         a dict of tool results, or a ChatDocument.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def render_agent_response(\n    self,\n    results: Optional[str | OrderedDict[str, str] | ChatDocument],\n) -&gt; None:\n    \"\"\"\n    Render the response from the agent, typically from tool-handling.\n    Args:\n        results: results from tool-handling, which may be a string,\n            a dict of tool results, or a ChatDocument.\n    \"\"\"\n    if self.config.hide_agent_response or results is None:\n        return\n    if isinstance(results, str):\n        results_str = results\n    elif isinstance(results, ChatDocument):\n        results_str = results.content\n    elif isinstance(results, dict):\n        results_str = json.dumps(results, indent=2)\n    if not settings.quiet:\n        console.print(f\"[red]{self.indent}\", end=\"\")\n        print(f\"[red]Agent: {escape(results_str)}\")\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.agent_response_async","title":"<code>agent_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>agent_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def agent_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `agent_response`. See there for details.\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = await self.handle_message_async(msg)\n\n    return self._agent_response_final(msg, results)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\", typically (but not only) used to handle LLM's \"tool message\" or <code>function_call</code> (e.g. OpenAI <code>function_call</code>). Args:     msg (str|ChatDocument): the input to respond to: if msg is a string,         and it contains a valid JSON-structured \"tool message\", or         if msg is a ChatDocument, and it contains a <code>function_call</code>. Returns:     Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Response from the \"agent itself\", typically (but not only)\n    used to handle LLM's \"tool message\" or `function_call`\n    (e.g. OpenAI `function_call`).\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = self.handle_message(msg)\n\n    return self._agent_response_final(msg, results)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.process_tool_results","title":"<code>process_tool_results(results, id2result, tool_calls=None)</code>","text":"<p>Process results from a response, based on whether they are results of OpenAI tool-calls from THIS agent, so that we can construct an appropriate LLMMessage that contains tool results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>str</code> <p>A possible string result from handling tool(s)</p> required <code>id2result</code> <code>OrderedDict[str, str] | None</code> <p>A dict of OpenAI tool id -&gt; result, if there are multiple tool results.</p> required <code>tool_calls</code> <code>List[OpenAIToolCall] | None</code> <p>List of OpenAI tool-calls that the results are a response to.</p> <code>None</code> Return <ul> <li>str: The response string</li> <li>Dict[str,str]|None: A dict of OpenAI tool id -&gt; result, if there are     multiple tool results.</li> <li>str|None: tool_id if there was a single tool result</li> </ul> Source code in <code>langroid/agent/base.py</code> <pre><code>def process_tool_results(\n    self,\n    results: str,\n    id2result: OrderedDict[str, str] | None,\n    tool_calls: List[OpenAIToolCall] | None = None,\n) -&gt; Tuple[str, Dict[str, str] | None, str | None]:\n    \"\"\"\n    Process results from a response, based on whether\n    they are results of OpenAI tool-calls from THIS agent, so that\n    we can construct an appropriate LLMMessage that contains tool results.\n\n    Args:\n        results (str): A possible string result from handling tool(s)\n        id2result (OrderedDict[str,str]|None): A dict of OpenAI tool id -&gt; result,\n            if there are multiple tool results.\n        tool_calls (List[OpenAIToolCall]|None): List of OpenAI tool-calls that the\n            results are a response to.\n\n    Return:\n        - str: The response string\n        - Dict[str,str]|None: A dict of OpenAI tool id -&gt; result, if there are\n            multiple tool results.\n        - str|None: tool_id if there was a single tool result\n\n    \"\"\"\n    id2result_ = copy.deepcopy(id2result) if id2result is not None else None\n    results_str = \"\"\n    oai_tool_id = None\n\n    if results != \"\":\n        # in this case ignore id2result\n        assert (\n            id2result is None\n        ), \"id2result should be None when results string is non-empty!\"\n        results_str = results\n        if len(self.oai_tool_calls) &gt; 0:\n            # We only have one result, so in case there is a\n            # \"pending\" OpenAI tool-call, we expect no more than 1 such.\n            assert (\n                len(self.oai_tool_calls) == 1\n            ), \"There are multiple pending tool-calls, but only one result!\"\n            # We record the tool_id of the tool-call that\n            # the result is a response to, so that ChatDocument.to_LLMMessage\n            # can properly set the `tool_call_id` field of the LLMMessage.\n            oai_tool_id = self.oai_tool_calls[0].id\n    elif id2result is not None and id2result_ is not None:  # appease mypy\n        if len(id2result_) == len(self.oai_tool_calls):\n            # if the number of pending tool calls equals the number of results,\n            # then ignore the ids in id2result, and use the results in order,\n            # which is preserved since id2result is an OrderedDict.\n            assert len(id2result_) &gt; 1, \"Expected to see &gt; 1 result in id2result!\"\n            results_str = \"\"\n            id2result_ = OrderedDict(\n                zip(\n                    [tc.id or \"\" for tc in self.oai_tool_calls], id2result_.values()\n                )\n            )\n        else:\n            assert (\n                tool_calls is not None\n            ), \"tool_calls cannot be None when id2result is not None!\"\n            # This must be an OpenAI tool id -&gt; result map;\n            # However some ids may not correspond to the tool-calls in the list of\n            # pending tool-calls (self.oai_tool_calls).\n            # Such results are concatenated into a simple string, to store in the\n            # ChatDocument.content, and the rest\n            # (i.e. those that DO correspond to tools in self.oai_tool_calls)\n            # are stored as a dict in ChatDocument.oai_tool_id2result.\n\n            # OAI tools from THIS agent, awaiting response\n            pending_tool_ids = [tc.id for tc in self.oai_tool_calls]\n            # tool_calls that the results are a response to\n            # (but these may have been sent from another agent, hence may not be in\n            # self.oai_tool_calls)\n            parent_tool_id2name = {\n                tc.id: tc.function.name\n                for tc in tool_calls or []\n                if tc.function is not None\n            }\n\n            # (id, result) for result NOT corresponding to self.oai_tool_calls,\n            # i.e. these are results of EXTERNAL tool-calls from another agent.\n            external_tool_id_results = []\n\n            for tc_id, result in id2result.items():\n                if tc_id not in pending_tool_ids:\n                    external_tool_id_results.append((tc_id, result))\n                    id2result_.pop(tc_id)\n            if len(external_tool_id_results) == 0:\n                results_str = \"\"\n            elif len(external_tool_id_results) == 1:\n                results_str = external_tool_id_results[0][1]\n            else:\n                results_str = \"\\n\\n\".join(\n                    [\n                        f\"Result from tool/function \"\n                        f\"{parent_tool_id2name[id]}: {result}\"\n                        for id, result in external_tool_id_results\n                    ]\n                )\n\n            if len(id2result_) == 0:\n                id2result_ = None\n            elif len(id2result_) == 1 and len(external_tool_id_results) == 0:\n                results_str = list(id2result_.values())[0]\n                oai_tool_id = list(id2result_.keys())[0]\n                id2result_ = None\n\n    return results_str, id2result_, oai_tool_id\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.response_template","title":"<code>response_template(e, content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for response from entity <code>e</code>.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def response_template(\n    self,\n    e: Entity,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: Optional[List[OpenAIToolCall]] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for response from entity `e`.\"\"\"\n    return ChatDocument(\n        content=content or \"\",\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        oai_tool_choice=oai_tool_choice,\n        metadata=ChatDocMetaData(\n            source=e, sender=e, sender_name=self.config.name, recipient=recipient\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.create_user_response","title":"<code>create_user_response(content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for user_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_user_response(\n    self,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: List[OpenAIToolCall] | None = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for user_response.\"\"\"\n    return self.response_template(\n        e=Entity.USER,\n        content=content,\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.user_can_respond","title":"<code>user_can_respond(msg=None)</code>","text":"<p>Whether the user can respond to a message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_can_respond(self, msg: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the user can respond to a message.\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n\n    \"\"\"\n    # When msg explicitly addressed to user, this means an actual human response\n    # is being sought.\n    need_human_response = (\n        isinstance(msg, ChatDocument) and msg.metadata.recipient == Entity.USER\n    )\n\n    if not self.interactive and not need_human_response:\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.user_response_async","title":"<code>user_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>user_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def user_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `user_response`. See there for details.\n    \"\"\"\n    if not self.user_can_respond(msg):\n        return None\n\n    if self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if (\n            self.callbacks.get_user_response_async is not None\n            and self.callbacks.get_user_response_async is not async_noop_fn\n        ):\n            user_msg = await self.callbacks.get_user_response_async(prompt=\"\")\n        elif self.callbacks.get_user_response is not None:\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}\"\n                + self.config.human_prompt\n                + f\"\\n{self.indent}\"\n            )\n\n    return self._user_response_final(msg, user_msg)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n        (str) User response, packaged as a ChatDocument\n\n    \"\"\"\n\n    if not self.user_can_respond(msg):\n        return None\n\n    if self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if self.callbacks.get_user_response is not None:\n            # ask user with empty prompt: no need for prompt\n            # since user has seen the conversation so far.\n            # But non-empty prompt can be useful when Agent\n            # uses a tool that requires user input, or in other scenarios.\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}\"\n                + self.config.human_prompt\n                + f\"\\n{self.indent}\"\n            )\n\n    return self._user_response_final(msg, user_msg)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n\n    Returns:\n\n    \"\"\"\n    if self.llm is None:\n        return False\n\n    if message is not None and len(self.try_get_tool_messages(message)) &gt; 0:\n        # if there is a valid \"tool\" message (either JSON or via `function_call`)\n        # then LLM cannot respond to it\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.can_respond","title":"<code>can_respond(message=None)</code>","text":"<p>Whether the agent can respond to a message. Used in Task.py to skip a sub-task when we know it would not respond. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the agent can respond to a message.\n    Used in Task.py to skip a sub-task when we know it would not respond.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n    \"\"\"\n    tools = self.try_get_tool_messages(message)\n    if len(tools) == 0 and self.config.respond_tools_only:\n        return False\n    if message is not None and self.has_only_unhandled_tools(message):\n        # The message has tools that are NOT enabled to be handled by this agent,\n        # which means the agent cannot respond to it.\n        return False\n    return True\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.create_llm_response","title":"<code>create_llm_response(content=None, content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for llm_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_llm_response(\n    self,\n    content: str | None = None,\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: None | List[OpenAIToolCall] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for llm_response.\"\"\"\n    return self.response_template(\n        Entity.LLM,\n        content=content,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self,\n    message: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `llm_response`. See there for details.\n    \"\"\"\n    if message is None or not self.llm_can_respond(message):\n        return None\n\n    if isinstance(message, ChatDocument):\n        prompt = message.content\n    else:\n        prompt = message\n\n    output_len = self.config.llm.model_max_output_tokens\n    if self.num_tokens(prompt) + output_len &gt; self.llm.completion_context_length():\n        output_len = self.llm.completion_context_length() - self.num_tokens(prompt)\n        if output_len &lt; self.config.llm.min_output_tokens:\n            raise ValueError(\n                \"\"\"\n            Token-length of Prompt + Output is longer than the\n            completion context length of the LLM!\n            \"\"\"\n            )\n        else:\n            logger.warning(\n                f\"\"\"\n            Requested output length has been shortened to {output_len}\n            so that the total length of Prompt + Output is less than\n            the completion context length of the LLM.\n            \"\"\"\n            )\n\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm.agenerate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        print(cached + \"[green]\" + escape(response.message))\n    async with self.lock:\n        self.update_token_usage(\n            response,\n            prompt,\n            self.llm.get_stream(),\n            chat=False,  # i.e. it's a completion model not chat model\n            print_response_stats=self.config.show_stats and not settings.quiet,\n        )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = (\n        [] if isinstance(message, str) else message.metadata.tool_ids\n    )\n    return cdoc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>LLM response to a prompt. Args:     message (str|ChatDocument): prompt string, or ChatDocument object</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self,\n    message: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    LLM response to a prompt.\n    Args:\n        message (str|ChatDocument): prompt string, or ChatDocument object\n\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\n    if message is None or not self.llm_can_respond(message):\n        return None\n\n    if isinstance(message, ChatDocument):\n        prompt = message.content\n    else:\n        prompt = message\n\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream():\n            # show rich spinner only if not streaming!\n            cm = status(\"LLM responding to message...\")\n            stack.enter_context(cm)\n        output_len = self.config.llm.model_max_output_tokens\n        if (\n            self.num_tokens(prompt) + output_len\n            &gt; self.llm.completion_context_length()\n        ):\n            output_len = self.llm.completion_context_length() - self.num_tokens(\n                prompt\n            )\n            if output_len &lt; self.config.llm.min_output_tokens:\n                raise ValueError(\n                    \"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n                )\n            else:\n                logger.warning(\n                    f\"\"\"\n                Requested output length has been shortened to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM.\n                \"\"\"\n                )\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        response = self.llm.generate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # we would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response\n        # If we are here, it means the response has not yet been displayed.\n        cached = \"[red](cached)[/red]\" if response.cached else \"\"\n        console.print(f\"[green]{self.indent}\", end=\"\")\n        print(cached + \"[green]\" + escape(response.message))\n    self.update_token_usage(\n        response,\n        prompt,\n        self.llm.get_stream(),\n        chat=False,  # i.e. it's a completion model not chat model\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = (\n        [] if isinstance(message, str) else message.metadata.tool_ids\n    )\n    return cdoc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.has_tool_message_attempt","title":"<code>has_tool_message_attempt(msg)</code>","text":"<p>Check whether msg contains a Tool/fn-call attempt (by the LLM).</p> <p>CAUTION: This uses self.get_tool_messages(msg) which as a side-effect may update msg.tool_messages when msg is a ChatDocument, if there are any tools in msg.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_tool_message_attempt(self, msg: str | ChatDocument | None) -&gt; bool:\n    \"\"\"\n    Check whether msg contains a Tool/fn-call attempt (by the LLM).\n\n    CAUTION: This uses self.get_tool_messages(msg) which as a side-effect\n    may update msg.tool_messages when msg is a ChatDocument, if there are\n    any tools in msg.\n    \"\"\"\n    if msg is None:\n        return False\n    if isinstance(msg, ChatDocument):\n        if len(msg.tool_messages) &gt; 0:\n            return True\n        if msg.metadata.sender != Entity.LLM:\n            return False\n    try:\n        tools = self.get_tool_messages(msg)\n        return len(tools) &gt; 0\n    except (ValidationError, XMLException):\n        # there is a tool/fn-call attempt but had a validation error,\n        # so we still consider this a tool message \"attempt\"\n        return True\n    return False\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.has_only_unhandled_tools","title":"<code>has_only_unhandled_tools(msg)</code>","text":"<p>Does the msg have at least one tool, and none of the tools in the msg are handleable by this agent?</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_only_unhandled_tools(self, msg: str | ChatDocument) -&gt; bool:\n    \"\"\"\n    Does the msg have at least one tool, and none of the tools in the msg are\n    handleable by this agent?\n    \"\"\"\n    if msg is None:\n        return False\n    tools = self.try_get_tool_messages(msg, all_tools=True)\n    if len(tools) == 0:\n        return False\n    return all(not self._tool_recipient_match(t) for t in tools)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.get_tool_messages","title":"<code>get_tool_messages(msg, all_tools=False)</code>","text":"<p>Get ToolMessages recognized in msg, handle-able by this agent. NOTE: as a side-effect, this will update msg.tool_messages when msg is a ChatDocument and msg contains tool messages.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the message to extract tools from.</p> required <code>all_tools</code> <code>bool</code> <ul> <li>if True, return all tools,     i.e. any recognized tool in self.llm_tools_known,     whether it is handled by this agent or not;</li> <li>otherwise, return only the tools handled by this agent.</li> </ul> <code>False</code> <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_tool_messages(\n    self,\n    msg: str | ChatDocument | None,\n    all_tools: bool = False,\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Get ToolMessages recognized in msg, handle-able by this agent.\n    NOTE: as a side-effect, this will update msg.tool_messages\n    when msg is a ChatDocument and msg contains tool messages.\n\n    Args:\n        msg (str|ChatDocument): the message to extract tools from.\n        all_tools (bool):\n            - if True, return all tools,\n                i.e. any recognized tool in self.llm_tools_known,\n                whether it is handled by this agent or not;\n            - otherwise, return only the tools handled by this agent.\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n\n    if msg is None:\n        return []\n\n    if isinstance(msg, str):\n        json_tools = self.get_formatted_tool_messages(msg)\n        if all_tools:\n            return json_tools\n        else:\n            return [\n                t\n                for t in json_tools\n                if self._tool_recipient_match(t) and t.default_value(\"request\")\n            ]\n\n    if len(msg.tool_messages) &gt; 0:\n        # We've already found tool_messages,\n        # (either via OpenAI Fn-call or Langroid-native ToolMessage);\n        # or they were added by an agent_response.\n        # note these could be from a forwarded msg from another agent,\n        # so return ONLY the messages THIS agent to enabled to handle.\n        if all_tools:\n            return msg.tool_messages\n        return [t for t in msg.tool_messages if self._tool_recipient_match(t)]\n\n    if (\n        msg.all_tool_messages is not None\n        and msg.all_tool_messages_agent_id == self.id\n    ):\n        # We've already identified all_tool_messages in the msg by this same agent;\n        # so use them to return the corresponding ToolMessage objects\n        if all_tools:\n            return msg.all_tool_messages\n        msg.tool_messages = [\n            t for t in msg.all_tool_messages if self._tool_recipient_match(t)\n        ]\n        return msg.tool_messages\n\n    assert isinstance(msg, ChatDocument)\n    if (\n        SearchForTools.CONTENT.value in self.search_for_tools\n        and msg.content != \"\"\n        and msg.oai_tool_calls is None\n        and msg.function_call is None\n    ):\n\n        tools = self.get_formatted_tool_messages(\n            msg.content, from_llm=msg.metadata.sender == Entity.LLM\n        )\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        # filter for actually handle-able tools, and recipient is this agent\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n\n        if all_tools:\n            return tools\n        else:\n            return my_tools\n\n    # otherwise, we look for `tool_calls` (possibly multiple)\n    if SearchForTools.TOOLS.value in self.search_for_tools:\n        tools = self.get_oai_tool_calls_classes(msg)\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n    else:\n        tools = []\n        my_tools = []\n\n    if len(tools) == 0 and SearchForTools.FUNCTIONS.value in self.search_for_tools:\n        # otherwise, we look for a `function_call`\n        fun_call_cls = self.get_function_call_class(msg)\n        tools = [fun_call_cls] if fun_call_cls is not None else []\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n    if all_tools:\n        return tools\n    else:\n        return my_tools\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.get_formatted_tool_messages","title":"<code>get_formatted_tool_messages(input_str, from_llm=True)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to tool-formatted substrings, if any. ASSUMPTION - These tools are either ALL JSON-based, or ALL XML-based (i.e. not a mix of both). Terminology: a \"formatted tool msg\" is one which the LLM generates as     part of its raw string output, rather than within a JSON object     in the API response (i.e. this method does not extract tools/fns returned     by OpenAI's tools/fns API or similar APIs).</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <code>from_llm</code> <code>bool</code> <p>whether the input was generated by the LLM. If so, we track malformed tool calls.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_formatted_tool_messages(\n    self, input_str: str, from_llm: bool = True\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Returns ToolMessage objects (tools) corresponding to\n    tool-formatted substrings, if any.\n    ASSUMPTION - These tools are either ALL JSON-based, or ALL XML-based\n    (i.e. not a mix of both).\n    Terminology: a \"formatted tool msg\" is one which the LLM generates as\n        part of its raw string output, rather than within a JSON object\n        in the API response (i.e. this method does not extract tools/fns returned\n        by OpenAI's tools/fns API or similar APIs).\n\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n        from_llm (bool): whether the input was generated by the LLM. If so,\n            we track malformed tool calls.\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n    self.tool_error = False\n    substrings = XMLToolMessage.find_candidates(input_str)\n    is_json = False\n    if len(substrings) == 0:\n        substrings = extract_top_level_json(input_str)\n        is_json = len(substrings) &gt; 0\n        if not is_json:\n            return []\n\n    results = [self._get_one_tool_message(j, is_json, from_llm) for j in substrings]\n    valid_results = [r for r in results if r is not None]\n    # If any tool is correctly formed we do not set the flag\n    if len(valid_results) &gt; 0:\n        self.tool_error = False\n    return valid_results\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.get_function_call_class","title":"<code>get_function_call_class(msg)</code>","text":"<p>From ChatDocument (constructed from an LLM Response), get the <code>ToolMessage</code> corresponding to the <code>function_call</code> if it exists.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_function_call_class(self, msg: ChatDocument) -&gt; Optional[ToolMessage]:\n    \"\"\"\n    From ChatDocument (constructed from an LLM Response), get the `ToolMessage`\n    corresponding to the `function_call` if it exists.\n    \"\"\"\n    if msg.function_call is None:\n        return None\n    tool_name = msg.function_call.name\n    tool_msg = msg.function_call.arguments or {}\n    self.tool_error = False\n    if tool_name not in self.llm_tools_handled:\n        logger.warning(\n            f\"\"\"\n            The function_call '{tool_name}' is not handled\n            by the agent named '{self.config.name}'!\n            If you intended this agent to handle this function_call,\n            either the fn-call name is incorrectly generated by the LLM,\n            (in which case you may need to adjust your LLM instructions),\n            or you need to enable this agent to handle this fn-call.\n            \"\"\"\n        )\n        if (\n            tool_name not in self.all_llm_tools_known\n            and msg.metadata.sender == Entity.LLM\n        ):\n            self.tool_error = True\n        return None\n    tool_class = self.llm_tools_map[tool_name]\n    tool_msg.update(dict(request=tool_name))\n    try:\n        tool = tool_class.model_validate(tool_msg)\n    except ValidationError as ve:\n        # Store tool class as an attribute on the exception\n        ve.tool_class = tool_class  # type: ignore\n        raise ve\n    return tool\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.get_oai_tool_calls_classes","title":"<code>get_oai_tool_calls_classes(msg)</code>","text":"<p>From ChatDocument (constructed from an LLM Response), get  a list of ToolMessages corresponding to the <code>tool_calls</code>, if any.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_oai_tool_calls_classes(self, msg: ChatDocument) -&gt; List[ToolMessage]:\n    \"\"\"\n    From ChatDocument (constructed from an LLM Response), get\n     a list of ToolMessages corresponding to the `tool_calls`, if any.\n    \"\"\"\n\n    if msg.oai_tool_calls is None:\n        return []\n    tools = []\n    all_errors = True\n    for tc in msg.oai_tool_calls:\n        if tc.function is None:\n            continue\n        tool_name = tc.function.name\n        tool_msg = tc.function.arguments or {}\n        if tool_name not in self.llm_tools_handled:\n            logger.warning(\n                f\"\"\"\n                The tool_call '{tool_name}' is not handled\n                by the agent named '{self.config.name}'!\n                If you intended this agent to handle this function_call,\n                either the fn-call name is incorrectly generated by the LLM,\n                (in which case you may need to adjust your LLM instructions),\n                or you need to enable this agent to handle this fn-call.\n                \"\"\"\n            )\n            continue\n        all_errors = False\n        tool_class = self.llm_tools_map[tool_name]\n        tool_msg.update(dict(request=tool_name))\n        try:\n            tool = tool_class.model_validate(tool_msg)\n        except ValidationError as ve:\n            # Store tool class as an attribute on the exception\n            ve.tool_class = tool_class  # type: ignore\n            raise ve\n        tool.id = tc.id or \"\"\n        tools.append(tool)\n    # When no tool is valid and the message was produced\n    # by the LLM, set the recovery flag\n    self.tool_error = all_errors and msg.metadata.sender == Entity.LLM\n    return tools\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.tool_validation_error","title":"<code>tool_validation_error(ve, tool_class=None)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields. Args:     ve (ValidationError): The exception raised     tool_class (Optional[Type[ToolMessage]]): The tool class that         failed validation</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(\n    self, ve: ValidationError, tool_class: Optional[Type[ToolMessage]] = None\n) -&gt; str:\n    \"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        ve (ValidationError): The exception raised\n        tool_class (Optional[Type[ToolMessage]]): The tool class that\n            failed validation\n\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\n    # First try to get tool class from the exception itself\n    if hasattr(ve, \"tool_class\") and ve.tool_class:\n        tool_name = ve.tool_class.default_value(\"request\")  # type: ignore\n    elif tool_class is not None:\n        tool_name = tool_class.default_value(\"request\")\n    else:\n        # Fallback: try to extract from error context if available\n        tool_name = \"Unknown Tool\"\n    bad_field_errors = \"\\n\".join(\n        [f\"{e['loc']}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n    )\n    return f\"\"\"\n    There were one or more errors in your attempt to use the\n    TOOL or function_call named '{tool_name}':\n    {bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.handle_message_async","title":"<code>handle_message_async(msg)</code>  <code>async</code>","text":"<p>Asynch version of <code>handle_message</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def handle_message_async(\n    self, msg: str | ChatDocument\n) -&gt; None | str | OrderedDict[str, str] | ChatDocument:\n    \"\"\"\n    Asynch version of `handle_message`. See there for details.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n        tools = [t for t in tools if self._tool_recipient_match(t)]\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except XMLException as xe:  # from XMLToolMessage parsing\n        return str(xe)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) &gt; 1 and not self.config.allow_multiple_tools:\n        return self.to_ChatDocument(\"ERROR: Use ONE tool at a time!\")\n    if len(tools) == 0:\n        fallback_result = self.handle_message_fallback(msg)\n        if fallback_result is None:\n            return None\n        return self.to_ChatDocument(\n            fallback_result,\n            chat_doc=msg if isinstance(msg, ChatDocument) else None,\n        )\n    chat_doc = msg if isinstance(msg, ChatDocument) else None\n\n    results = self._get_multiple_orch_tool_errs(tools)\n    if not results:\n        results = [\n            await self.handle_tool_message_async(t, chat_doc=chat_doc)\n            for t in tools\n        ]\n        # if there's a solitary ChatDocument|str result, return it as is\n        if len(results) == 1 and isinstance(results[0], (str, ChatDocument)):\n            return results[0]\n\n    return self._handle_message_final(tools, results)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valid \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>None | str | OrderedDict[str, str] | ChatDocument</code> <p>The result of the handler method can be: - None if no tools successfully handled, or no tools present - str if langroid-native JSON tools were handled, and results concatenated,  OR there's a SINGLE OpenAI tool-call. (We do this so the common scenario of a single tool/fn-call  has a simple behavior). - Dict[str, str] if multiple OpenAI tool-calls were handled  (dict is an id-&gt;result map) - ChatDocument if a handler returned a ChatDocument, intended to be the  final response of the <code>agent_response</code> method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(\n    self, msg: str | ChatDocument\n) -&gt; None | str | OrderedDict[str, str] | ChatDocument:\n    \"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valid \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n\n    Returns:\n        The result of the handler method can be:\n         - None if no tools successfully handled, or no tools present\n         - str if langroid-native JSON tools were handled, and results concatenated,\n             OR there's a SINGLE OpenAI tool-call.\n            (We do this so the common scenario of a single tool/fn-call\n             has a simple behavior).\n         - Dict[str, str] if multiple OpenAI tool-calls were handled\n             (dict is an id-&gt;result map)\n         - ChatDocument if a handler returned a ChatDocument, intended to be the\n             final response of the `agent_response` method.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n        tools = [t for t in tools if self._tool_recipient_match(t)]\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except XMLException as xe:  # from XMLToolMessage parsing\n        return str(xe)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) == 0:\n        fallback_result = self.handle_message_fallback(msg)\n        if fallback_result is None:\n            return None\n        return self.to_ChatDocument(\n            fallback_result,\n            chat_doc=msg if isinstance(msg, ChatDocument) else None,\n        )\n\n    results: List[str | ChatDocument | None] = []\n    if len(tools) &gt; 1 and not self.config.allow_multiple_tools:\n        results = [\"ERROR: Use ONE tool at a time!\"] * len(tools)\n    if not results:\n        results = self._get_multiple_orch_tool_errs(tools)\n    if not results:\n        chat_doc = msg if isinstance(msg, ChatDocument) else None\n        results = [self.handle_tool_message(t, chat_doc=chat_doc) for t in tools]\n        # if there's a solitary ChatDocument|str result, return it as is\n        if len(results) == 1 and isinstance(results[0], (str, ChatDocument)):\n            return results[0]\n\n    return self._handle_message_final(tools, results)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method for the case where the msg has no tools that can be handled by this agent. This method can be overridden by subclasses, e.g., to create a \"reminder\" message when a tool is expected but the LLM \"forgot\" to generate one.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     Any: The result of the handler method</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(self, msg: str | ChatDocument) -&gt; Any:\n    \"\"\"\n    Fallback method for the case where the msg has no tools that\n    can be handled by this agent.\n    This method can be overridden by subclasses, e.g.,\n    to create a \"reminder\" message when a tool is expected but the LLM \"forgot\"\n    to generate one.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        Any: The result of the handler method\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.to_ChatDocument","title":"<code>to_ChatDocument(msg, orig_tool_name=None, chat_doc=None, author_entity=Entity.AGENT)</code>","text":"<p>Convert result of a responder (agent_response or llm_response, or task.run()), or tool handler, or handle_message_fallback, to a ChatDocument, to enable handling by other responders/tasks in a task loop possibly involving multiple agents.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Any</code> <p>The result of a responder or tool handler or task.run()</p> required <code>orig_tool_name</code> <code>str</code> <p>The original tool name that generated the response, if any.</p> <code>None</code> <code>chat_doc</code> <code>ChatDocument</code> <p>The original ChatDocument object that <code>msg</code> is a response to.</p> <code>None</code> <code>author_entity</code> <code>Entity</code> <p>The intended author of the result ChatDocument</p> <code>AGENT</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def to_ChatDocument(\n    self,\n    msg: Any,\n    orig_tool_name: str | None = None,\n    chat_doc: Optional[ChatDocument] = None,\n    author_entity: Entity = Entity.AGENT,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Convert result of a responder (agent_response or llm_response, or task.run()),\n    or tool handler, or handle_message_fallback,\n    to a ChatDocument, to enable handling by other\n    responders/tasks in a task loop possibly involving multiple agents.\n\n    Args:\n        msg (Any): The result of a responder or tool handler or task.run()\n        orig_tool_name (str): The original tool name that generated the response,\n            if any.\n        chat_doc (ChatDocument): The original ChatDocument object that `msg`\n            is a response to.\n        author_entity (Entity): The intended author of the result ChatDocument\n    \"\"\"\n    if msg is None or isinstance(msg, ChatDocument):\n        return msg\n\n    is_agent_author = author_entity == Entity.AGENT\n\n    if isinstance(msg, str):\n        return self.response_template(author_entity, content=msg, content_any=msg)\n    elif isinstance(msg, ToolMessage):\n        # result is a ToolMessage, so...\n        result_tool_name = msg.default_value(\"request\")\n        if (\n            is_agent_author\n            and result_tool_name in self.llm_tools_handled\n            and (orig_tool_name is None or orig_tool_name != result_tool_name)\n        ):\n            # TODO: do we need to remove the tool message from the chat_doc?\n            # if (chat_doc is not None and\n            #     msg in chat_doc.tool_messages):\n            #    chat_doc.tool_messages.remove(msg)\n            # if we can handle it, do so\n            result = self.handle_tool_message(msg, chat_doc=chat_doc)\n            if result is not None and isinstance(result, ChatDocument):\n                return result\n        else:\n            # else wrap it in an agent response and return it so\n            # orchestrator can find a respondent\n            return self.response_template(author_entity, tool_messages=[msg])\n    else:\n        result = to_string(msg)\n\n    return (\n        None\n        if result is None\n        else self.response_template(author_entity, content=result, content_any=msg)\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.from_ChatDocument","title":"<code>from_ChatDocument(msg, output_type)</code>","text":"<p>Extract a desired output_type from a ChatDocument object. We use this fallback order: - if <code>msg.content_any</code> exists and matches the output_type, return it - if <code>msg.content</code> exists and output_type is str return it - if output_type is a ToolMessage, return the first tool in <code>msg.tool_messages</code> - if output_type is a list of ToolMessage,     return all tools in <code>msg.tool_messages</code> - search for a tool in <code>msg.tool_messages</code> that has a field of output_type,      and if found, return that field value - return None if all the above fail</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def from_ChatDocument(self, msg: ChatDocument, output_type: Type[T]) -&gt; Optional[T]:\n    \"\"\"\n    Extract a desired output_type from a ChatDocument object.\n    We use this fallback order:\n    - if `msg.content_any` exists and matches the output_type, return it\n    - if `msg.content` exists and output_type is str return it\n    - if output_type is a ToolMessage, return the first tool in `msg.tool_messages`\n    - if output_type is a list of ToolMessage,\n        return all tools in `msg.tool_messages`\n    - search for a tool in `msg.tool_messages` that has a field of output_type,\n         and if found, return that field value\n    - return None if all the above fail\n    \"\"\"\n    content = msg.content\n    if output_type is str and content != \"\":\n        return cast(T, content)\n    content_any = msg.content_any\n    if content_any is not None and isinstance(content_any, output_type):\n        return cast(T, content_any)\n\n    tools = self.try_get_tool_messages(msg, all_tools=True)\n\n    if get_origin(output_type) is list:\n        list_element_type = get_args(output_type)[0]\n        if issubclass(list_element_type, ToolMessage):\n            # list_element_type is a subclass of ToolMessage:\n            # We output a list of objects derived from list_element_type\n            return cast(\n                T,\n                [t for t in tools if isinstance(t, list_element_type)],\n            )\n    elif get_origin(output_type) is None and issubclass(output_type, ToolMessage):\n        # output_type is a subclass of ToolMessage:\n        # return the first tool that has this specific output_type\n        for tool in tools:\n            if isinstance(tool, output_type):\n                return cast(T, tool)\n        return None\n    elif get_origin(output_type) is None and output_type in (str, int, float, bool):\n        # attempt to get the output_type from the content,\n        # if it's a primitive type\n        primitive_value = from_string(content, output_type)  # type: ignore\n        if primitive_value is not None:\n            return cast(T, primitive_value)\n\n    # then search for output_type as a field in a tool\n    for tool in tools:\n        value = tool.get_value_of_type(output_type)\n        if value is not None:\n            return cast(T, value)\n    return None\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.handle_tool_message_async","title":"<code>handle_tool_message_async(tool, chat_doc=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>handle_tool_message</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def handle_tool_message_async(\n    self,\n    tool: ToolMessage,\n    chat_doc: Optional[ChatDocument] = None,\n) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Asynch version of `handle_tool_message`. See there for details.\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    if hasattr(tool, \"_handler\"):\n        handler_name = getattr(tool, \"_handler\", tool_name)\n    else:\n        handler_name = tool_name\n    handler_method = getattr(self, handler_name + \"_async\", None)\n    if handler_method is None:\n        return self.handle_tool_message(tool, chat_doc=chat_doc)\n    has_chat_doc_arg = (\n        chat_doc is not None\n        and \"chat_doc\" in inspect.signature(handler_method).parameters\n    )\n    try:\n        if has_chat_doc_arg:\n            maybe_result = await handler_method(tool, chat_doc=chat_doc)\n        else:\n            maybe_result = await handler_method(tool)\n        result = self.to_ChatDocument(maybe_result, tool_name, chat_doc)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return self._maybe_truncate_result(\n        result, tool._max_result_tokens\n    )  # type: ignore\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.handle_tool_message","title":"<code>handle_tool_message(tool, chat_doc=None)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object. Args:     tool: ToolMessage object representing the tool request.     chat_doc: Optional ChatDocument object containing the tool request.         This is passed to the tool-handler method only if it has a <code>chat_doc</code>         argument.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(\n    self,\n    tool: ToolMessage,\n    chat_doc: Optional[ChatDocument] = None,\n) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n        chat_doc: Optional ChatDocument object containing the tool request.\n            This is passed to the tool-handler method only if it has a `chat_doc`\n            argument.\n\n    Returns:\n\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    if hasattr(tool, \"_handler\"):\n        handler_name = getattr(tool, \"_handler\", tool_name)\n    else:\n        handler_name = tool_name\n    handler_method = getattr(self, handler_name, None)\n    if handler_method is None:\n        return None\n    has_chat_doc_arg = (\n        chat_doc is not None\n        and \"chat_doc\" in inspect.signature(handler_method).parameters\n    )\n    try:\n        if has_chat_doc_arg:\n            maybe_result = handler_method(tool, chat_doc=chat_doc)\n        else:\n            maybe_result = handler_method(tool)\n        result = self.to_ChatDocument(maybe_result, tool_name, chat_doc)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return self._maybe_truncate_result(\n        result, tool._max_result_tokens\n    )  # type: ignore\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.update_token_usage","title":"<code>update_token_usage(response, prompt, stream, chat=True, print_response_stats=True)</code>","text":"<p>Updates <code>response.usage</code> obj (token usage and cost fields) if needed. An update is needed only if: - stream is True (i.e. streaming was enabled), and - the response was NOT obtained from cached, and - the API did NOT provide the usage/cost fields during streaming   (As of Sep 2024, the OpenAI API started providing these; for other APIs     this may not necessarily be the case).</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>LLMResponse</code> <p>LLMResponse object</p> required <code>prompt</code> <code>str | List[LLMMessage]</code> <p>prompt or list of LLMMessage objects</p> required <code>stream</code> <code>bool</code> <p>whether to update the usage in the response object if the response is not cached.</p> required <code>chat</code> <code>bool</code> <p>whether this is a chat model or a completion model</p> <code>True</code> <code>print_response_stats</code> <code>bool</code> <p>whether to print the response stats</p> <code>True</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def update_token_usage(\n    self,\n    response: LLMResponse,\n    prompt: str | List[LLMMessage],\n    stream: bool,\n    chat: bool = True,\n    print_response_stats: bool = True,\n) -&gt; None:\n    \"\"\"\n    Updates `response.usage` obj (token usage and cost fields) if needed.\n    An update is needed only if:\n    - stream is True (i.e. streaming was enabled), and\n    - the response was NOT obtained from cached, and\n    - the API did NOT provide the usage/cost fields during streaming\n      (As of Sep 2024, the OpenAI API started providing these; for other APIs\n        this may not necessarily be the case).\n\n    Args:\n        response (LLMResponse): LLMResponse object\n        prompt (str | List[LLMMessage]): prompt or list of LLMMessage objects\n        stream (bool): whether to update the usage in the response object\n            if the response is not cached.\n        chat (bool): whether this is a chat model or a completion model\n        print_response_stats (bool): whether to print the response stats\n    \"\"\"\n    if response is None or self.llm is None:\n        return\n\n    no_usage_info = response.usage is None or response.usage.prompt_tokens == 0\n    # Note: If response was not streamed, then\n    # `response.usage` would already have been set by the API,\n    # so we only need to update in the stream case.\n    if stream and no_usage_info:\n        # usage, cost = 0 when response is from cache\n        prompt_tokens = 0\n        completion_tokens = 0\n        cost = 0.0\n        if not response.cached:\n            prompt_tokens = self.num_tokens(prompt)\n            completion_tokens = self.num_tokens(response.message)\n            if response.function_call is not None:\n                completion_tokens += self.num_tokens(str(response.function_call))\n            cost = self.compute_token_cost(prompt_tokens, 0, completion_tokens)\n        response.usage = LLMTokenUsage(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            cost=cost,\n        )\n\n    # update total counters\n    if response.usage is not None:\n        self.total_llm_token_cost += response.usage.cost\n        self.total_llm_token_usage += response.usage.total_tokens\n        self.llm.update_usage_cost(\n            chat,\n            response.usage.prompt_tokens,\n            response.usage.completion_tokens,\n            response.usage.cost,\n        )\n        chat_length = 1 if isinstance(prompt, str) else len(prompt)\n        self.token_stats_str = self._get_response_stats(\n            chat_length, self.total_llm_token_cost, response\n        )\n        if print_response_stats:\n            print(self.indent + self.token_stats_str)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and <code>RecipientTool</code> to address requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>user_confirm</code> <code>bool</code> <p>whether to gate the request with a human confirmation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\n    self,\n    agent: \"Agent\",\n    request: str,\n    no_answer: str = NO_ANSWER,\n    user_confirm: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and\n    `RecipientTool` to address requests to other agents. It is generally best to\n    avoid using this method.\n\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer (str): expected response when agent does not know the answer\n        user_confirm (bool): whether to gate the request with a human confirmation\n\n    Returns:\n        str: response from agent\n    \"\"\"\n    agent_type = type(agent).__name__\n    if user_confirm:\n        user_response = Prompt.ask(\n            f\"\"\"[magenta]Here is the request or message:\n            {request}\n            Should I forward this to {agent_type}?\"\"\",\n            default=\"y\",\n            choices=[\"y\", \"n\"],\n        )\n        if user_response not in [\"y\", \"yes\"]:\n            return None\n    answer = agent.llm_response(request)\n    if answer != no_answer:\n        return (f\"{agent_type} says: \" + str(answer)).strip()\n    return None\n</code></pre>"},{"location":"reference/agent/#langroid.agent.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components.</p>"},{"location":"reference/agent/#langroid.agent.ChatDocument","title":"<code>ChatDocument(**data)</code>","text":"<p>               Bases: <code>Document</code></p> <p>Represents a message in a conversation among agents. All responders of an agent have signature ChatDocument -&gt; ChatDocument (modulo None, str, etc), and so does the Task.run() method.</p> <p>Attributes:</p> Name Type Description <code>oai_tool_calls</code> <code>Optional[List[OpenAIToolCall]]</code> <p>Tool-calls from an OpenAI-compatible API</p> <code>oai_tool_id2results</code> <code>Optional[OrderedDict[str, str]]</code> <p>Results of tool-calls from OpenAI (dict is a map of tool_id -&gt; result)</p> <code>oai_tool_choice</code> <code>ToolChoiceTypes | Dict[str, Dict[str, str] | str]</code> <p>ToolChoiceTypes | Dict[str, str]: Param controlling how the LLM should choose tool-use in its response (auto, none, required, or a specific tool)</p> <code>function_call</code> <code>Optional[LLMFunctionCall]</code> <p>Function-call from an OpenAI-compatible API     (deprecated by OpenAI, in favor of tool-calls)</p> <code>tool_messages</code> <code>List[ToolMessage]</code> <p>Langroid ToolMessages extracted from - <code>content</code> field (via JSON parsing), - <code>oai_tool_calls</code>, or - <code>function_call</code></p> <code>metadata</code> <code>ChatDocMetaData</code> <p>Metadata for the message, e.g. sender, recipient.</p> <code>attachment</code> <code>None | ChatDocAttachment</code> <p>Any additional data attached.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def __init__(self, **data: Any):\n    super().__init__(**data)\n    ObjectRegistry.register_object(self)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.delete_id","title":"<code>delete_id(id)</code>  <code>staticmethod</code>","text":"<p>Remove ChatDocument with given id from ObjectRegistry, and all its descendants.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef delete_id(id: str) -&gt; None:\n    \"\"\"Remove ChatDocument with given id from ObjectRegistry,\n    and all its descendants.\n    \"\"\"\n    chat_doc = ChatDocument.from_id(id)\n    # first delete all descendants\n    while chat_doc is not None:\n        next_chat_doc = chat_doc.child\n        ObjectRegistry.remove(chat_doc.id())\n        chat_doc = next_chat_doc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.get_tool_names","title":"<code>get_tool_names()</code>","text":"<p>Get names of attempted tool usages (JSON or non-JSON) in the content     of the message. Returns:     List[str]: list of attempted tool names     (We say \"attempted\" since we ONLY look at the <code>request</code> component of the     tool-call representation, and we're not fully parsing it into the     corresponding tool message class)</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_tool_names(self) -&gt; List[str]:\n    \"\"\"\n    Get names of attempted tool usages (JSON or non-JSON) in the content\n        of the message.\n    Returns:\n        List[str]: list of *attempted* tool names\n        (We say \"attempted\" since we ONLY look at the `request` component of the\n        tool-call representation, and we're not fully parsing it into the\n        corresponding tool message class)\n\n    \"\"\"\n    tool_candidates = XMLToolMessage.find_candidates(self.content)\n    if len(tool_candidates) == 0:\n        tool_candidates = extract_top_level_json(self.content)\n        if len(tool_candidates) == 0:\n            return []\n        tools = [json.loads(tc).get(\"request\") for tc in tool_candidates]\n    else:\n        tool_dicts = [\n            XMLToolMessage.extract_field_values(tc) for tc in tool_candidates\n        ]\n        tools = [td.get(\"request\") for td in tool_dicts if td is not None]\n    return [str(tool) for tool in tools if tool is not None]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger Returns:     List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n    \"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\n    tool_type = \"\"  # FUNC or TOOL\n    tool = \"\"  # tool name or function name\n\n    # Skip tool detection for system messages - they contain tool instructions,\n    # not actual tool calls\n    if self.metadata.sender != Entity.SYSTEM:\n        oai_tools = (\n            []\n            if self.oai_tool_calls is None\n            else [t for t in self.oai_tool_calls if t.function is not None]\n        )\n        if self.function_call is not None:\n            tool_type = \"FUNC\"\n            tool = self.function_call.name\n        elif len(oai_tools) &gt; 0:\n            tool_type = \"OAI_TOOL\"\n            tool = \",\".join(t.function.name for t in oai_tools)  # type: ignore\n        else:\n            try:\n                json_tools = self.get_tool_names()\n            except Exception:\n                json_tools = []\n            if json_tools != []:\n                tool_type = \"TOOL\"\n                tool = json_tools[0]\n    recipient = self.metadata.recipient\n    content = self.content\n    sender_entity = self.metadata.sender\n    sender_name = self.metadata.sender_name\n    if tool_type == \"FUNC\":\n        content += str(self.function_call)\n    return ChatDocLoggerFields(\n        sender_entity=sender_entity,\n        sender_name=sender_name,\n        recipient=recipient,\n        block=self.metadata.block,\n        tool_type=tool_type,\n        tool=tool,\n        content=content,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.pop_tool_ids","title":"<code>pop_tool_ids()</code>","text":"<p>Pop the last tool_id from the stack of tool_ids.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def pop_tool_ids(self) -&gt; None:\n    \"\"\"\n    Pop the last tool_id from the stack of tool_ids.\n    \"\"\"\n    if len(self.metadata.tool_ids) &gt; 0:\n        self.metadata.tool_ids.pop()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.from_LLMResponse","title":"<code>from_LLMResponse(response, displayed=False, recognize_recipient_in_content=True)</code>  <code>staticmethod</code>","text":"<p>Convert LLMResponse to ChatDocument. Args:     response (LLMResponse): LLMResponse to convert.     displayed (bool): Whether this response was displayed to the user.     recognize_recipient_in_content (bool): Whether to parse message text         for recipient routing (<code>TO[&lt;recipient&gt;]:</code> and JSON         <code>{\"recipient\": ...}</code>). Default True. Returns:     ChatDocument: ChatDocument representation of this LLMResponse.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMResponse(\n    response: LLMResponse,\n    displayed: bool = False,\n    recognize_recipient_in_content: bool = True,\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMResponse to ChatDocument.\n    Args:\n        response (LLMResponse): LLMResponse to convert.\n        displayed (bool): Whether this response was displayed to the user.\n        recognize_recipient_in_content (bool): Whether to parse message text\n            for recipient routing (``TO[&lt;recipient&gt;]:`` and JSON\n            ``{\"recipient\": ...}``). Default True.\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMResponse.\n    \"\"\"\n    recipient, message = response.get_recipient_and_message(\n        recognize_recipient_in_content\n    )\n    message = message.strip()\n    if message in [\"''\", '\"\"']:\n        message = \"\"\n    if response.function_call is not None:\n        ChatDocument._clean_fn_call(response.function_call)\n    if response.oai_tool_calls is not None:\n        # there must be at least one if it's not None\n        for oai_tc in response.oai_tool_calls:\n            ChatDocument._clean_fn_call(oai_tc.function)\n    return ChatDocument(\n        content=message,\n        reasoning=response.reasoning,\n        content_with_reasoning=response.message_with_reasoning,\n        content_any=message,\n        oai_tool_calls=response.oai_tool_calls,\n        function_call=response.function_call,\n        metadata=ChatDocMetaData(\n            source=Entity.LLM,\n            sender=Entity.LLM,\n            usage=response.usage,\n            displayed=displayed,\n            cached=response.cached,\n            recipient=recipient,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.from_LLMMessage","title":"<code>from_LLMMessage(message, sender_name='', recipient='')</code>  <code>staticmethod</code>","text":"<p>Convert LLMMessage to ChatDocument.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>LLMMessage</code> <p>LLMMessage to convert.</p> required <code>sender_name</code> <code>str</code> <p>Name of the sender. Defaults to \"\".</p> <code>''</code> <code>recipient</code> <code>str</code> <p>Name of the recipient. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>ChatDocument</code> <code>'ChatDocument'</code> <p>ChatDocument representation of this LLMMessage.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMMessage(\n    message: LLMMessage,\n    sender_name: str = \"\",\n    recipient: str = \"\",\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMMessage to ChatDocument.\n\n    Args:\n        message (LLMMessage): LLMMessage to convert.\n        sender_name (str): Name of the sender. Defaults to \"\".\n        recipient (str): Name of the recipient. Defaults to \"\".\n\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMMessage.\n    \"\"\"\n    # Map LLMMessage Role to ChatDocument Entity\n    role_to_entity = {\n        Role.USER: Entity.USER,\n        Role.SYSTEM: Entity.SYSTEM,\n        Role.ASSISTANT: Entity.LLM,\n        Role.FUNCTION: Entity.LLM,\n        Role.TOOL: Entity.LLM,\n    }\n\n    sender_entity = role_to_entity.get(message.role, Entity.USER)\n\n    return ChatDocument(\n        content=message.content or \"\",\n        content_any=message.content,\n        files=message.files,\n        function_call=message.function_call,\n        oai_tool_calls=message.tool_calls,\n        metadata=ChatDocMetaData(\n            source=sender_entity,\n            sender=sender_entity,\n            sender_name=sender_name,\n            recipient=recipient,\n            oai_tool_id=message.tool_call_id,\n            tool_ids=[message.tool_id] if message.tool_id else [],\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message, oai_tools=None)</code>  <code>staticmethod</code>","text":"<p>Convert to list of LLMMessage, to incorporate into msg-history sent to LLM API. Usually there will be just a single LLMMessage, but when the ChatDocument contains results from multiple OpenAI tool-calls, we would have a sequence LLMMessages, one per tool-call result.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <code>oai_tools</code> <code>Optional[List[OpenAIToolCall]]</code> <p>Tool-calls currently awaiting response, from the ChatAgent's latest message.</p> <code>None</code> <p>Returns:     List[LLMMessage]: list of LLMMessages corresponding to this ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(\n    message: Union[str, \"ChatDocument\"],\n    oai_tools: Optional[List[OpenAIToolCall]] = None,\n) -&gt; List[LLMMessage]:\n    \"\"\"\n    Convert to list of LLMMessage, to incorporate into msg-history sent to LLM API.\n    Usually there will be just a single LLMMessage, but when the ChatDocument\n    contains results from multiple OpenAI tool-calls, we would have a sequence\n    LLMMessages, one per tool-call result.\n\n    Args:\n        message (str|ChatDocument): Message to convert.\n        oai_tools (Optional[List[OpenAIToolCall]]): Tool-calls currently awaiting\n            response, from the ChatAgent's latest message.\n    Returns:\n        List[LLMMessage]: list of LLMMessages corresponding to this ChatDocument.\n    \"\"\"\n\n    sender_role = Role.USER\n    if isinstance(message, str):\n        message = ChatDocument.from_str(message)\n    # Prefer content_with_reasoning when available \u2014 this preserves\n    # inline thought signatures (e.g. &lt;thinking&gt;...&lt;/thinking&gt;) in\n    # message history, which certain models (Gemini 3 Flash, Amazon\n    # Nova) need to maintain reasoning across turns.\n    # content_with_reasoning is only set when inline tags were\n    # actually extracted, so this won't interfere with models that\n    # provide reasoning via a separate API field.\n    content = (\n        message.content_with_reasoning\n        or message.content\n        or to_string(message.content_any)\n        or \"\"\n    )\n    fun_call = message.function_call\n    oai_tool_calls = message.oai_tool_calls\n    if message.metadata.sender == Entity.USER and fun_call is not None:\n        # This may happen when a (parent agent's) LLM generates a\n        # a Function-call, and it ends up being sent to the current task's\n        # LLM (possibly because the function-call is mis-named or has other\n        # issues and couldn't be handled by handler methods).\n        # But a function-call can only be generated by an entity with\n        # Role.ASSISTANT, so we instead put the content of the function-call\n        # in the content of the message.\n        content += \" \" + str(fun_call)\n        fun_call = None\n    if message.metadata.sender == Entity.USER and oai_tool_calls is not None:\n        # same reasoning as for function-call above\n        content += \" \" + \"\\n\\n\".join(str(tc) for tc in oai_tool_calls)\n        oai_tool_calls = None\n    # some LLM APIs (e.g. gemini) don't like empty msg\n    content = content or \" \"\n    sender_name = message.metadata.sender_name\n    tool_ids = message.metadata.tool_ids\n    tool_id = tool_ids[-1] if len(tool_ids) &gt; 0 else \"\"\n    chat_document_id = message.id()\n    if message.metadata.sender == Entity.SYSTEM:\n        sender_role = Role.SYSTEM\n    if (\n        message.metadata.parent is not None\n        and message.metadata.parent.function_call is not None\n    ):\n        # This is a response to a function call, so set the role to FUNCTION.\n        sender_role = Role.FUNCTION\n        sender_name = message.metadata.parent.function_call.name\n    elif oai_tools is not None and len(oai_tools) &gt; 0:\n        pending_tool_ids = [tc.id for tc in oai_tools]\n        # The ChatAgent has pending OpenAI tool-call(s),\n        # so the current ChatDocument contains\n        # results for some/all/none of them.\n\n        if len(oai_tools) == 1:\n            # Case 1:\n            # There was exactly 1 pending tool-call, and in this case\n            # the result would be a plain string in `content`\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=oai_tools[0].id,\n                    content=content,\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n            ]\n\n        elif (\n            message.metadata.oai_tool_id is not None\n            and message.metadata.oai_tool_id in pending_tool_ids\n        ):\n            # Case 2:\n            # ChatDocument.content has result of a single tool-call\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=message.metadata.oai_tool_id,\n                    content=content,\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n            ]\n        elif message.oai_tool_id2result is not None:\n            # Case 2:\n            # There were &gt; 1 tool-calls awaiting response,\n            assert (\n                len(message.oai_tool_id2result) &gt; 1\n            ), \"oai_tool_id2result must have more than 1 item.\"\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=tool_id,\n                    content=result or \" \",\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n                for tool_id, result in message.oai_tool_id2result.items()\n            ]\n    elif message.metadata.sender == Entity.LLM:\n        sender_role = Role.ASSISTANT\n\n    return [\n        LLMMessage(\n            role=sender_role,\n            tool_id=tool_id,  # for OpenAI Assistant\n            content=content,\n            files=message.files,\n            function_call=fun_call,\n            tool_calls=oai_tool_calls,\n            name=sender_name,\n            chat_document_id=chat_document_id,\n        )\n    ]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>               Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent</p> <p>Attributes:</p> Name Type Description <code>system_message</code> <code>str</code> <p>system message to include in message sequence  (typically defines role and task of agent).  Used only if <code>task</code> is not specified in the constructor.</p> <code>user_message</code> <code>Optional[str]</code> <p>user message to include in message sequence.  Used only if <code>task</code> is not specified in the constructor.</p> <code>use_tools</code> <code>bool</code> <p>whether to use our own ToolMessages mechanism</p> <code>handle_llm_no_tool</code> <code>Any</code> <p>desired agent_response when LLM generates non-tool msg.</p> <code>use_functions_api</code> <code>bool</code> <p>whether to use functions/tools native to the LLM API     (e.g. OpenAI's <code>function_call</code> or <code>tool_call</code> mechanism)</p> <code>use_tools_api</code> <code>bool</code> <p>When <code>use_functions_api</code> is True, if this is also True, the OpenAI tool-call API is used, rather than the older/deprecated function-call API. However the tool-call API has some tricky aspects, hence we set this to False by default.</p> <code>strict_recovery</code> <code>bool</code> <p>whether to enable strict schema recovery when there is a tool-generation error.</p> <code>enable_orchestration_tool_handling</code> <code>bool</code> <p>whether to enable handling of orchestration tools, e.g. ForwardTool, DoneTool, PassTool, etc.</p> <code>output_format</code> <code>Optional[type]</code> <p>When supported by the LLM (certain OpenAI LLMs and local LLMs served by providers such as vLLM), ensures that the output is a JSON matching the corresponding schema via grammar-based decoding</p> <code>handle_output_format</code> <code>bool</code> <p>When <code>output_format</code> is a <code>ToolMessage</code> T, controls whether T is \"enabled for handling\".</p> <code>use_output_format</code> <code>bool</code> <p>When <code>output_format</code> is a <code>ToolMessage</code> T, controls whether T is \"enabled for use\" (by LLM) and instructions on using T are added to the system message.</p> <code>instructions_output_format</code> <code>bool</code> <p>Controls whether we generate instructions for <code>output_format</code> in the system message.</p> <code>use_tools_on_output_format</code> <code>bool</code> <p>Controls whether to automatically switch to the Langroid-native tools mechanism when <code>output_format</code> is set. Note that LLMs may generate tool calls which do not belong to <code>output_format</code> even when strict JSON mode is enabled, so this should be enabled when such tool calls are not desired.</p> <code>output_format_include_defaults</code> <code>bool</code> <p>Whether to include fields with default arguments in the output schema</p> <code>full_citations</code> <code>bool</code> <p>Whether to show source reference citation + content for each citation, or just the main reference citation.</p> <code>search_for_tools_everywhere</code> <code>bool</code> <p>Whether to search for tools everywhere, or only in specific LLM response elements based on use_tools / use_functions_api / use_tools_api config settings.</p> <code>recognize_recipient_in_content</code> <code>bool</code> <p>Whether to parse LLM response text content for recipient routing patterns, specifically: - <code>TO[&lt;recipient&gt;]:&lt;content&gt;</code> addressing format, and - JSON <code>{\"recipient\": \"&lt;name&gt;\"}</code> at the top level of the message. When False, only structured routing via function_call/tool_call <code>recipient</code> fields is recognized. Default is True. Note: this is distinct from <code>TaskConfig.recognize_string_signals</code>, which controls Task-level signals like DONE, PASS, and SEND_TO. To fully disable all text-based routing, set both to False.</p> <code>context_overflow_strategy</code> <code>Literal['truncate', 'drop_turns']</code> <p>Strategy for handling context overflow when message history exceeds model context length. Options: - \"truncate\": Truncate content of early messages (preserves all messages   but with shortened content). This maintains the message sequence. - \"drop_turns\": Drop complete conversation turns (USER + all responses   until next USER). More aggressive but cleaner for voice agents. Default is \"truncate\" for backward compatibility.</p>"},{"location":"reference/agent/#langroid.agent.ChatAgent","title":"<code>ChatAgent(config=ChatAgentConfig(), task=None)</code>","text":"<p>               Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with an optional \"Task Spec\", which is a sequence of <code>LLMMessage</code>s. These are used to initialize the <code>task_messages</code> of the agent. In most applications we will use a <code>ChatAgent</code> rather than a bare <code>Agent</code>. The <code>Agent</code> class mainly exists to hold various common methods and attributes. One difference between <code>ChatAgent</code> and <code>Agent</code> is that <code>ChatAgent</code>'s <code>llm_response</code> method uses \"chat mode\" API (i.e. one that takes a message sequence rather than a single message), whereas the same method in the <code>Agent</code> class uses \"completion mode\" API (i.e. one that takes a single message).</p> <pre><code>config: settings for the agent\n</code></pre> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: ChatAgentConfig = ChatAgentConfig(),\n    task: Optional[List[LLMMessage]] = None,\n):\n    \"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n\n    \"\"\"\n    super().__init__(config)\n    self.config: ChatAgentConfig = config\n    self.config._set_fn_or_tools()\n    self.message_history: List[LLMMessage] = []\n    self.init_state()\n    # An agent's \"task\" is defined by a system msg and an optional user msg;\n    # These are \"priming\" messages that kick off the agent's conversation.\n    self.system_message: str = self.config.system_message\n    self.user_message: str | None = self.config.user_message\n\n    if task is not None:\n        # if task contains a system msg, we override the config system msg\n        if len(task) &gt; 0 and task[0].role == Role.SYSTEM:\n            self.system_message = task[0].content\n        # if task contains a user msg, we override the config user msg\n        if len(task) &gt; 1 and task[1].role == Role.USER:\n            self.user_message = task[1].content\n\n    # system-level instructions for using tools/functions:\n    # We maintain these as tools/functions are enabled/disabled,\n    # and whenever an LLM response is sought, these are used to\n    # recreate the system message (via `_create_system_and_tools_message`)\n    # each time, so it reflects the current set of enabled tools/functions.\n    # (a) these are general instructions on using certain tools/functions,\n    #   if they are specified in a ToolMessage class as a classmethod `instructions`\n    self.system_tool_instructions: str = \"\"\n    # (b) these are only for the builtin in Langroid TOOLS mechanism:\n    self.system_tool_format_instructions: str = \"\"\n\n    self.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\n    self.llm_functions_handled: Set[str] = set()\n    self.llm_functions_usable: Set[str] = set()\n    self.llm_function_force: Optional[Dict[str, str]] = None\n\n    self.output_format: Optional[type[ToolMessage | BaseModel]] = None\n\n    self.saved_requests_and_tool_setings = self._requests_and_tool_settings()\n    # This variable is not None and equals a `ToolMessage` T, if and only if:\n    # (a) T has been set as the output_format of this agent, AND\n    # (b) T has been \"enabled for use\" ONLY for enforcing this output format, AND\n    # (c) T has NOT been explicitly \"enabled for use\" by this Agent.\n    self.enabled_use_output_format: Optional[type[ToolMessage]] = None\n    # As above but deals with \"enabled for handling\" instead of \"enabled for use\".\n    self.enabled_handling_output_format: Optional[type[ToolMessage]] = None\n    if config.output_format is not None:\n        self.set_output_format(config.output_format)\n    # instructions specifically related to enforcing `output_format`\n    self.output_format_instructions = \"\"\n\n    # controls whether to disable strict schemas for this agent if\n    # strict mode causes exception\n    self.disable_strict = False\n    # Tracks whether any strict tool is enabled; used to determine whether to set\n    # `self.disable_strict` on an exception\n    self.any_strict = False\n    # Tracks the set of tools on which we force-disable strict decoding\n    self.disable_strict_tools_set: set[str] = set()\n\n    # search for tools according to the agent configuration\n    if not config.search_for_tools_everywhere:\n        if config.use_functions_api:\n            if config.use_tools_api:\n                self.search_for_tools = {SearchForTools.TOOLS.value}\n            else:\n                self.search_for_tools = {SearchForTools.FUNCTIONS.value}\n        else:\n            self.search_for_tools = {SearchForTools.CONTENT.value}\n\n    if self.config.enable_orchestration_tool_handling:\n        # Only enable HANDLING by `agent_response`, NOT LLM generation of these.\n        # This is useful where tool-handlers or agent_response generate these\n        # tools, and need to be handled.\n        # We don't want enable orch tool GENERATION by default, since that\n        # might clutter-up the LLM system message unnecessarily.\n        from langroid.agent.tools.orchestration import (\n            AgentDoneTool,\n            AgentSendTool,\n            DonePassTool,\n            DoneTool,\n            ForwardTool,\n            PassTool,\n            ResultTool,\n            SendTool,\n        )\n\n        self.enable_message(ForwardTool, use=False, handle=True)\n        self.enable_message(DoneTool, use=False, handle=True)\n        self.enable_message(AgentDoneTool, use=False, handle=True)\n        self.enable_message(PassTool, use=False, handle=True)\n        self.enable_message(DonePassTool, use=False, handle=True)\n        self.enable_message(SendTool, use=False, handle=True)\n        self.enable_message(AgentSendTool, use=False, handle=True)\n        self.enable_message(ResultTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.task_messages","title":"<code>task_messages</code>  <code>property</code>","text":"<p>The task messages are the initial messages that define the task of the agent. There will be at least a system message plus possibly a user msg. Returns:     List[LLMMessage]: the task messages</p>"},{"location":"reference/agent/#langroid.agent.ChatAgent.all_llm_tools_known","title":"<code>all_llm_tools_known</code>  <code>property</code>","text":"<p>All known tools; we include <code>output_format</code> if it is a <code>ToolMessage</code>.</p>"},{"location":"reference/agent/#langroid.agent.ChatAgent.init_state","title":"<code>init_state()</code>","text":"<p>Initialize the state of the agent. Just conversation state here, but subclasses can override this to initialize other state.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_state(self) -&gt; None:\n    \"\"\"\n    Initialize the state of the agent. Just conversation state here,\n    but subclasses can override this to initialize other state.\n    \"\"\"\n    super().init_state()\n    self.clear_history(0)\n    self.clear_dialog()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.from_id","title":"<code>from_id(id)</code>  <code>staticmethod</code>","text":"<p>Get an agent from its ID Args:     agent_id (str): ID of the agent Returns:     ChatAgent: The agent with the given ID</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@staticmethod\ndef from_id(id: str) -&gt; \"ChatAgent\":\n    \"\"\"\n    Get an agent from its ID\n    Args:\n        agent_id (str): ID of the agent\n    Returns:\n        ChatAgent: The agent with the given ID\n    \"\"\"\n    return cast(ChatAgent, Agent.from_id(id))\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.clone","title":"<code>clone(i=0)</code>","text":"<p>Create i'th clone of this agent, ensuring tool use/handling is cloned. Important: We assume all member variables are in the init method here and in the Agent class. TODO: We are attempting to clone an agent after its state has been changed in possibly many ways. Below is an imperfect solution. Caution advised. Revisit later.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clone(self, i: int = 0) -&gt; \"ChatAgent\":\n    \"\"\"Create i'th clone of this agent, ensuring tool use/handling is cloned.\n    Important: We assume all member variables are in the __init__ method here\n    and in the Agent class.\n    TODO: We are attempting to clone an agent after its state has been\n    changed in possibly many ways. Below is an imperfect solution. Caution advised.\n    Revisit later.\n    \"\"\"\n    agent_cls = type(self)\n    # Use model_copy to preserve Pydantic subclass types (like MockLMConfig)\n    # instead of deepcopy which loses subclass information\n    config_copy = self.config.model_copy(deep=True)\n    config_copy.name = f\"{config_copy.name}-{i}\"\n    new_agent = agent_cls(config_copy)\n    new_agent.system_tool_instructions = self.system_tool_instructions\n    new_agent.system_tool_format_instructions = self.system_tool_format_instructions\n    new_agent.llm_tools_map = self.llm_tools_map\n    new_agent.llm_tools_known = self.llm_tools_known\n    new_agent.llm_tools_handled = self.llm_tools_handled\n    new_agent.llm_tools_usable = self.llm_tools_usable\n    new_agent.llm_functions_map = self.llm_functions_map\n    new_agent.llm_functions_handled = self.llm_functions_handled\n    new_agent.llm_functions_usable = self.llm_functions_usable\n    new_agent.llm_function_force = self.llm_function_force\n    # Ensure each clone gets its own vecdb client when supported.\n    new_agent.vecdb = None if self.vecdb is None else self.vecdb.clone()\n    self._clone_extra_state(new_agent)\n    new_agent.id = ObjectRegistry.new_id()\n    if self.config.add_to_registry:\n        ObjectRegistry.register_object(new_agent)\n    return new_agent\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.clear_history","title":"<code>clear_history(start=-2, end=-1)</code>","text":"<p>Clear the message history, deleting  messages from index <code>start</code>, up to index <code>end</code>.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> <code>end</code> <code>int</code> <p>index of last message to delete; Default = -1     (i.e. delete all messages up to the last one)</p> <code>-1</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2, end: int = -1) -&gt; None:\n    \"\"\"\n    Clear the message history, deleting  messages from index `start`,\n    up to index `end`.\n\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n        end (int): index of last message to delete; Default = -1\n                (i.e. delete all messages up to the last one)\n    \"\"\"\n    n = len(self.message_history)\n    if start &lt; 0:\n        start = max(0, n + start)\n    end_ = n if end == -1 else end + 1\n    dropped = self.message_history[start:end_]\n    # consider the dropped msgs in REVERSE order, so we are\n    # carefully updating self.oai_tool_calls\n    for msg in reversed(dropped):\n        self._drop_msg_update_tool_calls(msg)\n        # clear out the chat document from the ObjectRegistry\n        ChatDocument.delete_id(msg.chat_document_id)\n    del self.message_history[start:end_]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response. Args:     message (str): user message     response: (str): LLM response</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n    \"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\n    self.message_history.extend(\n        [\n            LLMMessage(role=Role.USER, content=message),\n            LLMMessage(role=Role.ASSISTANT, content=response),\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.tool_format_rules","title":"<code>tool_format_rules()</code>","text":"<p>Specification of tool formatting rules (typically JSON-based but can be non-JSON, e.g. XMLToolMessage), based on the currently enabled usable <code>ToolMessage</code>s</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_format_rules(self) -&gt; str:\n    \"\"\"\n    Specification of tool formatting rules\n    (typically JSON-based but can be non-JSON, e.g. XMLToolMessage),\n    based on the currently enabled usable `ToolMessage`s\n\n    Returns:\n        str: formatting rules\n    \"\"\"\n    # ONLY Usable tools (i.e. LLM-generation allowed),\n    usable_tool_classes: List[Type[ToolMessage]] = [\n        t\n        for t in list(self.llm_tools_map.values())\n        if t.default_value(\"request\") in self.llm_tools_usable\n    ]\n\n    if len(usable_tool_classes) == 0:\n        return \"\"\n    format_instructions = \"\\n\\n\".join(\n        [\n            msg_cls.format_instructions(tool=self.config.use_tools)\n            for msg_cls in usable_tool_classes\n        ]\n    )\n    # if any of the enabled classes has json_group_instructions, then use that,\n    # else fall back to ToolMessage.json_group_instructions\n    for msg_cls in usable_tool_classes:\n        if hasattr(msg_cls, \"json_group_instructions\") and callable(\n            getattr(msg_cls, \"json_group_instructions\")\n        ):\n            return msg_cls.group_format_instructions().format(\n                format_instructions=format_instructions\n            )\n    return ToolMessage.group_format_instructions().format(\n        format_instructions=format_instructions\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.tool_instructions","title":"<code>tool_instructions()</code>","text":"<p>Instructions for tools or function-calls, for enabled and usable Tools. These are inserted into system prompt regardless of whether we are using our own ToolMessage mechanism or the LLM's function-call mechanism.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>concatenation of instructions for all usable tools</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_instructions(self) -&gt; str:\n    \"\"\"\n    Instructions for tools or function-calls, for enabled and usable Tools.\n    These are inserted into system prompt regardless of whether we are using\n    our own ToolMessage mechanism or the LLM's function-call mechanism.\n\n    Returns:\n        str: concatenation of instructions for all usable tools\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"\"\n    instructions = []\n    for msg_cls in enabled_classes:\n        if msg_cls.default_value(\"request\") in self.llm_tools_usable:\n            class_instructions = \"\"\n            if hasattr(msg_cls, \"instructions\") and inspect.ismethod(\n                msg_cls.instructions\n            ):\n                class_instructions = msg_cls.instructions()\n            if (\n                self.config.use_tools\n                and hasattr(msg_cls, \"langroid_tools_instructions\")\n                and inspect.ismethod(msg_cls.langroid_tools_instructions)\n            ):\n                class_instructions += msg_cls.langroid_tools_instructions()\n            # example will be shown in tool_format_rules() when using TOOLs,\n            # so we don't need to show it here.\n            example = \"\" if self.config.use_tools else (msg_cls.usage_examples())\n            if example != \"\":\n                example = \"EXAMPLES:\\n\" + example\n            guidance = (\n                \"\"\n                if class_instructions == \"\"\n                else (\"GUIDANCE: \" + class_instructions)\n            )\n            if guidance == \"\" and example == \"\":\n                continue\n            instructions.append(\n                textwrap.dedent(\n                    f\"\"\"\n                    TOOL: {msg_cls.default_value(\"request\")}:\n                    {guidance}\n                    {example}\n                    \"\"\".lstrip()\n                )\n            )\n    if len(instructions) == 0:\n        return \"\"\n    instructions_str = \"\\n\\n\".join(instructions)\n    return textwrap.dedent(\n        f\"\"\"\n        === GUIDELINES ON SOME TOOLS/FUNCTIONS USAGE ===\n        {instructions_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.augment_system_message","title":"<code>augment_system_message(message)</code>","text":"<p>Augment the system message with the given message. Args:     message (str): system message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def augment_system_message(self, message: str) -&gt; None:\n    \"\"\"\n    Augment the system message with the given message.\n    Args:\n        message (str): system message\n    \"\"\"\n    self.system_message += \"\\n\\n\" + message\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.last_message_with_role","title":"<code>last_message_with_role(role)</code>","text":"<p>from <code>message_history</code>, return the last message with role <code>role</code></p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_with_role(self, role: Role) -&gt; LLMMessage | None:\n    \"\"\"from `message_history`, return the last message with role `role`\"\"\"\n    n_role_msgs = len([m for m in self.message_history if m.role == role])\n    if n_role_msgs == 0:\n        return None\n    idx = self.nth_message_idx_with_role(role, n_role_msgs)\n    return self.message_history[idx]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.last_message_idx_with_role","title":"<code>last_message_idx_with_role(role)</code>","text":"<p>Index of last message in message_history, with specified role. Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_idx_with_role(self, role: Role) -&gt; int:\n    \"\"\"Index of last message in message_history, with specified role.\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n    if len(indices_with_role) == 0:\n        return -1\n    return indices_with_role[-1]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.nth_message_idx_with_role","title":"<code>nth_message_idx_with_role(role, n)</code>","text":"<p>Index of <code>n</code>th message in message_history, with specified role. (n is assumed to be 1-based, i.e. 1 is the first message with that role). Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def nth_message_idx_with_role(self, role: Role, n: int) -&gt; int:\n    \"\"\"Index of `n`th message in message_history, with specified role.\n    (n is assumed to be 1-based, i.e. 1 is the first message with that role).\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n\n    if len(indices_with_role) &lt; n:\n        return -1\n    return indices_with_role[n - 1]\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message that has role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question. Args:     message (str): new message to replace with     role (str): role of message to replace</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Update the last message that has role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): new message to replace with\n        role (str): role of message to replace\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history[i].content = message\n            break\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.delete_last_message","title":"<code>delete_last_message(role=Role.USER)</code>","text":"<p>Delete the last message that has role <code>role</code> from the message history. Args:     role (str): role of message to delete</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def delete_last_message(self, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Delete the last message that has role `role` from the message history.\n    Args:\n        role (str): role of message to delete\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history.pop(i)\n            break\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method for the \"no-tools\" scenario, i.e., the current <code>msg</code> (presumably emitted by the LLM) does not have any tool that the agent can handle. NOTE: The <code>msg</code> may contain tools but either (a) the agent is not enabled to handle them, or (b) there's an explicit <code>recipient</code> field in the tool that doesn't match the agent's name.</p> <p>Uses the self.config.non_tool_routing to determine the action to take.</p> <p>This method can be overridden by subclasses, e.g., to create a \"reminder\" message when a tool is expected but the LLM \"forgot\" to generate one.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     Any: The result of the handler method</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def handle_message_fallback(self, msg: str | ChatDocument) -&gt; Any:\n    \"\"\"\n    Fallback method for the \"no-tools\" scenario, i.e., the current `msg`\n    (presumably emitted by the LLM) does not have any tool that the agent\n    can handle.\n    NOTE: The `msg` may contain tools but either (a) the agent is not\n    enabled to handle them, or (b) there's an explicit `recipient` field\n    in the tool that doesn't match the agent's name.\n\n    Uses the self.config.non_tool_routing to determine the action to take.\n\n    This method can be overridden by subclasses, e.g.,\n    to create a \"reminder\" message when a tool is expected but the LLM \"forgot\"\n    to generate one.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        Any: The result of the handler method\n    \"\"\"\n    if (\n        isinstance(msg, str)\n        or msg.metadata.sender != Entity.LLM\n        or self.config.handle_llm_no_tool is None\n        or self.has_only_unhandled_tools(msg)\n    ):\n        return None\n    # we ONLY use the `handle_llm_no_tool` config option when\n    # the msg is from LLM and does not contain ANY tools at all.\n    from langroid.agent.tools.orchestration import AgentDoneTool, ForwardTool\n\n    no_tool_option = self.config.handle_llm_no_tool\n    if no_tool_option in list(NonToolAction):\n        # in case the `no_tool_option` is one of the special NonToolAction vals\n        match self.config.handle_llm_no_tool:\n            case NonToolAction.FORWARD_USER:\n                return ForwardTool(agent=\"User\")\n            case NonToolAction.DONE:\n                return AgentDoneTool(content=msg.content, tools=msg.tool_messages)\n    elif is_callable(no_tool_option):\n        return no_tool_option(msg)\n    # Otherwise just return `no_tool_option` as is:\n    # This can be any string, such as a specific nudge/reminder to the LLM,\n    # or even something like ResultTool etc.\n    return no_tool_option\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.unhandled_tools","title":"<code>unhandled_tools()</code>","text":"<p>The set of tools that are known but not handled. Useful in task flow: an agent can refuse to accept an incoming msg when it only has unhandled tools.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def unhandled_tools(self) -&gt; set[str]:\n    \"\"\"The set of tools that are known but not handled.\n    Useful in task flow: an agent can refuse to accept an incoming msg\n    when it only has unhandled tools.\n    \"\"\"\n    return self.llm_tools_known - self.llm_tools_handled\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False, include_defaults=True)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage] | List[Type[ToolMessage]]]</code> <p>The ToolMessage class OR List of such classes to enable, for USE, or HANDLING, or both. If this is a list of ToolMessage classes, then the remain args are applied to all classes. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> <code>require_recipient</code> <code>bool</code> <p>whether to require that recipient be specified when using the tool message (only applies if <code>use</code> is True).</p> <code>False</code> <code>include_defaults</code> <code>bool</code> <p>whether to include fields that have default values, in the \"properties\" section of the JSON format instructions. (Normally the OpenAI completion API ignores these fields, but the Assistant fn-calling seems to pay attn to these, and if we don't want this, we should set this to False.)</p> <code>True</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage] | List[Type[ToolMessage]]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n    include_defaults: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n\n    Args:\n        message_class: The ToolMessage class OR List of such classes to enable,\n            for USE, or HANDLING, or both.\n            If this is a list of ToolMessage classes, then the remain args are\n            applied to all classes.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n        require_recipient: whether to require that recipient be specified\n            when using the tool message (only applies if `use` is True).\n        include_defaults: whether to include fields that have default values,\n            in the \"properties\" section of the JSON format instructions.\n            (Normally the OpenAI completion API ignores these fields,\n            but the Assistant fn-calling seems to pay attn to these,\n            and if we don't want this, we should set this to False.)\n    \"\"\"\n    if message_class is not None and isinstance(message_class, list):\n        for mc in message_class:\n            self.enable_message(\n                mc,\n                use=use,\n                handle=handle,\n                force=force,\n                require_recipient=require_recipient,\n                include_defaults=include_defaults,\n            )\n        return None\n\n    # Validate that use/handle are booleans, not accidentally passed tool classes\n    if isclass(use) or isclass(handle):\n        param = \"use\" if isclass(use) else \"handle\"\n        raise TypeError(\n            textwrap.dedent(\n                f\"\"\"\n                Invalid arguments to enable_message().\n                It appears you passed multiple ToolMessage classes as separate\n                arguments instead of as a list.\n\n                Correct usage:\n                    agent.enable_message([Tool1, Tool2, Tool3])\n\n                Incorrect usage:\n                    agent.enable_message(Tool1, Tool2, Tool3)\n\n                The '{param}' parameter must be a boolean, not a class.\n                \"\"\"\n            )\n        )\n\n    if require_recipient and message_class is not None:\n        message_class = message_class.require_recipient()\n    if isinstance(message_class, XMLToolMessage):\n        # XMLToolMessage is not compatible with OpenAI's Tools/functions API,\n        # so we disable use of functions API, enable langroid-native Tools,\n        # which are prompt-based.\n        self.config.use_functions_api = False\n        self.config.use_tools = True\n    super().enable_message_handling(message_class)  # enables handling only\n    tools = self._get_tool_list(message_class)\n    if message_class is not None:\n        request = message_class.default_value(\"request\")\n        if request == \"\":\n            raise ValueError(\n                f\"\"\"\n                ToolMessage class {message_class} must have a non-empty\n                'request' field if it is to be enabled as a tool.\n                \"\"\"\n            )\n        llm_function = message_class.llm_function_schema(defaults=include_defaults)\n        self.llm_functions_map[request] = llm_function\n        if force:\n            self.llm_function_force = dict(name=request)\n        else:\n            self.llm_function_force = None\n\n    for t in tools:\n        self.llm_tools_known.add(t)\n\n        if handle:\n            self.llm_tools_handled.add(t)\n            self.llm_functions_handled.add(t)\n\n            if (\n                self.enabled_handling_output_format is not None\n                and self.enabled_handling_output_format.name() == t\n            ):\n                # `t` was designated as \"enabled for handling\" ONLY for\n                # output_format enforcement, but we are explicitly ]\n                # enabling it for handling here, so we set the variable to None.\n                self.enabled_handling_output_format = None\n        else:\n            self.llm_tools_handled.discard(t)\n            self.llm_functions_handled.discard(t)\n\n        if use:\n            tool_class = self.llm_tools_map[t]\n            allow_llm_use = tool_class._allow_llm_use\n            if isinstance(allow_llm_use, ModelPrivateAttr):\n                allow_llm_use = allow_llm_use.default\n            if allow_llm_use:\n                self.llm_tools_usable.add(t)\n                self.llm_functions_usable.add(t)\n            else:\n                logger.warning(\n                    f\"\"\"\n                    ToolMessage class {tool_class} does not allow LLM use,\n                    because `_allow_llm_use=False` either in the Tool or a\n                    parent class of this tool;\n                    so not enabling LLM use for this tool!\n                    If you intended an LLM to use this tool,\n                    set `_allow_llm_use=True` when you define the tool.\n                    \"\"\"\n                )\n            if (\n                self.enabled_use_output_format is not None\n                and self.enabled_use_output_format.default_value(\"request\") == t\n            ):\n                # `t` was designated as \"enabled for use\" ONLY for output_format\n                # enforcement, but we are explicitly enabling it for use here,\n                # so we set the variable to None.\n                self.enabled_use_output_format = None\n        else:\n            self.llm_tools_usable.discard(t)\n            self.llm_functions_usable.discard(t)\n\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.set_output_format","title":"<code>set_output_format(output_type, force_tools=None, use=None, handle=None, instructions=None, is_copy=False)</code>","text":"<p>Sets <code>output_format</code> to <code>output_type</code> and, if <code>force_tools</code> is enabled, switches to the native Langroid tools mechanism to ensure that no tool calls not of <code>output_type</code> are generated. By default, <code>force_tools</code> follows the <code>use_tools_on_output_format</code> parameter in the config.</p> <p>If <code>output_type</code> is None, restores to the state prior to setting <code>output_format</code>.</p> <p>If <code>use</code>, we enable use of <code>output_type</code> when it is a subclass of <code>ToolMesage</code>. Note that this primarily controls instruction generation: the model will always generate <code>output_type</code> regardless of whether <code>use</code> is set. Defaults to the <code>use_output_format</code> parameter in the config. Similarly, handling of <code>output_type</code> is controlled by <code>handle</code>, which defaults to the <code>handle_output_format</code> parameter in the config.</p> <p><code>instructions</code> controls whether we generate instructions specifying the output format schema. Defaults to the <code>instructions_output_format</code> parameter in the config.</p> <p><code>is_copy</code> is set when called via <code>__getitem__</code>. In that case, we must copy certain fields to ensure that we do not overwrite the main agent's setings.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def set_output_format(\n    self,\n    output_type: Optional[type],\n    force_tools: Optional[bool] = None,\n    use: Optional[bool] = None,\n    handle: Optional[bool] = None,\n    instructions: Optional[bool] = None,\n    is_copy: bool = False,\n) -&gt; None:\n    \"\"\"\n    Sets `output_format` to `output_type` and, if `force_tools` is enabled,\n    switches to the native Langroid tools mechanism to ensure that no tool\n    calls not of `output_type` are generated. By default, `force_tools`\n    follows the `use_tools_on_output_format` parameter in the config.\n\n    If `output_type` is None, restores to the state prior to setting\n    `output_format`.\n\n    If `use`, we enable use of `output_type` when it is a subclass\n    of `ToolMesage`. Note that this primarily controls instruction\n    generation: the model will always generate `output_type` regardless\n    of whether `use` is set. Defaults to the `use_output_format`\n    parameter in the config. Similarly, handling of `output_type` is\n    controlled by `handle`, which defaults to the\n    `handle_output_format` parameter in the config.\n\n    `instructions` controls whether we generate instructions specifying\n    the output format schema. Defaults to the `instructions_output_format`\n    parameter in the config.\n\n    `is_copy` is set when called via `__getitem__`. In that case, we must\n    copy certain fields to ensure that we do not overwrite the main agent's\n    setings.\n    \"\"\"\n    # Disable usage of an output format which was not specifically enabled\n    # by `enable_message`\n    if self.enabled_use_output_format is not None:\n        self.disable_message_use(self.enabled_use_output_format)\n        self.enabled_use_output_format = None\n\n    # Disable handling of an output format which did not specifically have\n    # handling enabled via `enable_message`\n    if self.enabled_handling_output_format is not None:\n        self.disable_message_handling(self.enabled_handling_output_format)\n        self.enabled_handling_output_format = None\n\n    # Reset any previous instructions\n    self.output_format_instructions = \"\"\n\n    if output_type is None:\n        self.output_format = None\n        (\n            requests_for_inference,\n            use_functions_api,\n            use_tools,\n        ) = self.saved_requests_and_tool_setings\n        self.config = self.config.model_copy()\n        self.enabled_requests_for_inference = requests_for_inference\n        self.config.use_functions_api = use_functions_api\n        self.config.use_tools = use_tools\n    else:\n        if force_tools is None:\n            force_tools = self.config.use_tools_on_output_format\n\n        if not any(\n            (isclass(output_type) and issubclass(output_type, t))\n            for t in [ToolMessage, BaseModel]\n        ):\n            output_type = get_pydantic_wrapper(output_type)\n\n        if self.output_format is None and force_tools:\n            self.saved_requests_and_tool_setings = (\n                self._requests_and_tool_settings()\n            )\n\n        self.output_format = output_type\n        if issubclass(output_type, ToolMessage):\n            name = output_type.default_value(\"request\")\n            if use is None:\n                use = self.config.use_output_format\n\n            if handle is None:\n                handle = self.config.handle_output_format\n\n            if use or handle:\n                is_usable = name in self.llm_tools_usable.union(\n                    self.llm_functions_usable\n                )\n                is_handled = name in self.llm_tools_handled.union(\n                    self.llm_functions_handled\n                )\n\n                if is_copy:\n                    if use:\n                        # We must copy `llm_tools_usable` so the base agent\n                        # is unmodified\n                        self.llm_tools_usable = self.llm_tools_usable.copy()\n                        self.llm_functions_usable = self.llm_functions_usable.copy()\n                    if handle:\n                        # If handling the tool, do the same for `llm_tools_handled`\n                        self.llm_tools_handled = self.llm_tools_handled.copy()\n                        self.llm_functions_handled = (\n                            self.llm_functions_handled.copy()\n                        )\n                # Enable `output_type`\n                self.enable_message(\n                    output_type,\n                    # Do not override existing settings\n                    use=use or is_usable,\n                    handle=handle or is_handled,\n                )\n\n                # If the `output_type` ToilMessage was not already enabled for\n                # use, this means we are ONLY enabling it for use specifically\n                # for enforcing this output format, so we set the\n                # `enabled_use_output_forma  to this output_type, to\n                # record that it should be disabled when `output_format` is changed\n                if not is_usable:\n                    self.enabled_use_output_format = output_type\n\n                # (same reasoning as for use-enabling)\n                if not is_handled:\n                    self.enabled_handling_output_format = output_type\n\n            generated_tool_instructions = name in self.llm_tools_usable.union(\n                self.llm_functions_usable\n            )\n        else:\n            generated_tool_instructions = False\n\n        if instructions is None:\n            instructions = self.config.instructions_output_format\n        if issubclass(output_type, BaseModel) and instructions:\n            if generated_tool_instructions:\n                # Already generated tool instructions as part of \"enabling for use\",\n                # so only need to generate a reminder to use this tool.\n                name = cast(ToolMessage, output_type).default_value(\"request\")\n                self.output_format_instructions = textwrap.dedent(\n                    f\"\"\"\n                    === OUTPUT FORMAT INSTRUCTIONS ===\n\n                    Please provide output using the `{name}` tool/function.\n                    \"\"\"\n                )\n            else:\n                if issubclass(output_type, ToolMessage):\n                    output_format_schema = output_type.llm_function_schema(\n                        request=True,\n                        defaults=self.config.output_format_include_defaults,\n                    ).parameters\n                else:\n                    output_format_schema = output_type.model_json_schema()\n\n                format_schema_for_strict(output_format_schema)\n\n                self.output_format_instructions = textwrap.dedent(\n                    f\"\"\"\n                    === OUTPUT FORMAT INSTRUCTIONS ===\n                    Please provide output as JSON with the following schema:\n\n                    {output_format_schema}\n                    \"\"\"\n                )\n\n        if force_tools:\n            if issubclass(output_type, ToolMessage):\n                self.enabled_requests_for_inference = {\n                    output_type.default_value(\"request\")\n                }\n            if self.config.use_functions_api:\n                self.config = self.config.model_copy()\n                self.config.use_functions_api = False\n                self.config.use_tools = True\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL. Args:     message_class: The ToolMessage class to disable; Optional.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\n    super().disable_message_handling(message_class)\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n        self.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools. Args:     message_class: The ToolMessage class to disable.         If None, disable all.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n    \"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_usable.discard(t)\n        self.llm_functions_usable.discard(t)\n\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool) Args:     message_class: The only ToolMessage class to allow</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n    \"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\n    request = message_class.model_fields[\"request\"].default\n    to_remove = [r for r in self.llm_tools_usable if r != request]\n    for r in to_remove:\n        self.llm_tools_usable.discard(r)\n        self.llm_functions_usable.discard(r)\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.get_tool_messages","title":"<code>get_tool_messages(msg, all_tools=False)</code>","text":"<p>Extracts messages and tracks whether any errors occurred. If strict mode was enabled, disables it for the tool, else triggers strict recovery.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def get_tool_messages(\n    self,\n    msg: str | ChatDocument | None,\n    all_tools: bool = False,\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Extracts messages and tracks whether any errors occurred. If strict mode\n    was enabled, disables it for the tool, else triggers strict recovery.\n    \"\"\"\n    self.tool_error = False\n    most_recent_sent_by_llm = (\n        len(self.message_history) &gt; 0\n        and self.message_history[-1].role == Role.ASSISTANT\n    )\n    was_llm = most_recent_sent_by_llm or (\n        isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM\n    )\n    try:\n        tools = super().get_tool_messages(msg, all_tools)\n    except ValidationError as ve:\n        # Check if tool class was attached to the exception\n        if hasattr(ve, \"tool_class\") and ve.tool_class:\n            tool_class = ve.tool_class  # type: ignore\n            if issubclass(tool_class, ToolMessage):\n                was_strict = (\n                    self.config.use_functions_api\n                    and self.config.use_tools_api\n                    and self._strict_mode_for_tool(tool_class)\n                )\n                # If the result of strict output for a tool using the\n                # OpenAI tools API fails to parse, we infer that the\n                # schema edits necessary for compatibility prevented\n                # adherence to the underlying `ToolMessage` schema and\n                # disable strict output for the tool\n                if was_strict:\n                    name = tool_class.default_value(\"request\")\n                    self.disable_strict_tools_set.add(name)\n                    logging.warning(\n                        f\"\"\"\n                        Validation error occured with strict tool format.\n                        Disabling strict mode for the {name} tool.\n                        \"\"\"\n                    )\n                else:\n                    # We will trigger the strict recovery mechanism to force\n                    # the LLM to correct its output, allowing us to parse\n                    if isinstance(msg, ChatDocument):\n                        self.tool_error = msg.metadata.sender == Entity.LLM\n                    else:\n                        self.tool_error = most_recent_sent_by_llm\n\n        if was_llm:\n            raise ve\n        else:\n            self.tool_error = False\n            return []\n\n    if not was_llm:\n        self.tool_error = False\n\n    return tools\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.truncate_message","title":"<code>truncate_message(idx, tokens=5, warning='...[Contents truncated!]', inplace=True)</code>","text":"<p>Truncate message at idx in msg history to <code>tokens</code> tokens.</p> <p>If inplace is True, the message is truncated in place, else it LEAVES the original message INTACT and returns a new message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def truncate_message(\n    self,\n    idx: int,\n    tokens: int = 5,\n    warning: str = \"...[Contents truncated!]\",\n    inplace: bool = True,\n) -&gt; LLMMessage:\n    \"\"\"\n    Truncate message at idx in msg history to `tokens` tokens.\n\n    If inplace is True, the message is truncated in place, else\n    it LEAVES the original message INTACT and returns a new message\n    \"\"\"\n    if inplace:\n        llm_msg = self.message_history[idx]\n    else:\n        llm_msg = copy.deepcopy(self.message_history[idx])\n    orig_content = llm_msg.content\n    new_content = (\n        self.parser.truncate_tokens(orig_content, tokens)\n        if self.parser is not None\n        else orig_content[: tokens * 4]  # approx truncation\n    )\n    llm_msg.content = new_content + \"\\n\" + warning\n    return llm_msg\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode Args:     message (str|ChatDocument): message or ChatDocument object to respond to.         If None, use the self.task_messages Returns:     LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    # If enabled and a tool error occurred, we recover by generating the tool in\n    # strict json mode\n    if (\n        self.tool_error\n        and self.output_format is None\n        and self._json_schema_available()\n        and self.config.strict_recovery\n    ):\n        self.tool_error = False\n        AnyTool = self._get_any_tool_message()\n        if AnyTool is None:\n            return None\n        self.set_output_format(\n            AnyTool,\n            force_tools=True,\n            use=True,\n            handle=True,\n            instructions=True,\n        )\n        recovery_message = self._strict_recovery_instructions(AnyTool)\n        augmented_message = message\n        if augmented_message is None:\n            augmented_message = recovery_message\n        elif isinstance(augmented_message, str):\n            augmented_message = augmented_message + recovery_message\n        else:\n            augmented_message.content = augmented_message.content + recovery_message\n\n        # only use the augmented message for this one response...\n        result = self.llm_response(augmented_message)\n        # ... restore the original user message so that the AnyTool recover\n        # instructions don't persist in the message history\n        # (this can cause the LLM to use the AnyTool directly as a tool)\n        if message is None:\n            self.delete_last_message(role=Role.USER)\n        else:\n            msg = message if isinstance(message, str) else message.content\n            self.update_last_message(msg, role=Role.USER)\n        return result\n\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    tool_choice = (\n        \"auto\"\n        if isinstance(message, str)\n        else (message.oai_tool_choice if message is not None else \"auto\")\n    )\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        try:\n            response = self.llm_response_messages(hist, output_len, tool_choice)\n        except openai.BadRequestError as e:\n            if self.any_strict:\n                self.disable_strict = True\n                self.set_output_format(None)\n                logging.warning(\n                    f\"\"\"\n                    OpenAI BadRequestError raised with strict mode enabled.\n                    Message: {e.message}\n                    Disabling strict mode and retrying.\n                    \"\"\"\n                )\n                return self.llm_response(message)\n            else:\n                raise e\n    self.message_history.extend(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    if isinstance(message, ChatDocument):\n        self._reduce_raw_tool_results(message)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n\n    return response\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of `llm_response`. See there for details.\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    # If enabled and a tool error occurred, we recover by generating the tool in\n    # strict json mode\n    if (\n        self.tool_error\n        and self.output_format is None\n        and self._json_schema_available()\n        and self.config.strict_recovery\n    ):\n        self.tool_error = False\n        AnyTool = self._get_any_tool_message()\n        self.set_output_format(\n            AnyTool,\n            force_tools=True,\n            use=True,\n            handle=True,\n            instructions=True,\n        )\n        recovery_message = self._strict_recovery_instructions(AnyTool)\n        augmented_message = message\n        if augmented_message is None:\n            augmented_message = recovery_message\n        elif isinstance(augmented_message, str):\n            augmented_message = augmented_message + recovery_message\n        else:\n            augmented_message.content = augmented_message.content + recovery_message\n\n        # only use the augmented message for this one response...\n        result = self.llm_response(augmented_message)\n        # ... restore the original user message so that the AnyTool recover\n        # instructions don't persist in the message history\n        # (this can cause the LLM to use the AnyTool directly as a tool)\n        if message is None:\n            self.delete_last_message(role=Role.USER)\n        else:\n            msg = message if isinstance(message, str) else message.content\n            self.update_last_message(msg, role=Role.USER)\n        return result\n\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    tool_choice = (\n        \"auto\"\n        if isinstance(message, str)\n        else (message.oai_tool_choice if message is not None else \"auto\")\n    )\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        try:\n            response = await self.llm_response_messages_async(\n                hist, output_len, tool_choice\n            )\n        except openai.BadRequestError as e:\n            if self.any_strict:\n                self.disable_strict = True\n                self.set_output_format(None)\n                logging.warning(\n                    f\"\"\"\n                    OpenAI BadRequestError raised with strict mode enabled.\n                    Message: {e.message}\n                    Disabling strict mode and retrying.\n                    \"\"\"\n                )\n                return await self.llm_response_async(message)\n            else:\n                raise e\n    self.message_history.extend(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    if isinstance(message, ChatDocument):\n        self._reduce_raw_tool_results(message)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n\n    return response\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.init_message_history","title":"<code>init_message_history()</code>","text":"<p>Initialize the message history with the system message and user message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_message_history(self) -&gt; None:\n    \"\"\"\n    Initialize the message history with the system message and user message\n    \"\"\"\n    self.message_history = [self._create_system_and_tools_message()]\n    if self.user_message:\n        self.message_history.append(\n            LLMMessage(role=Role.USER, content=self.user_message)\n        )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None, tool_choice='auto')</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion Args:     messages: seq of messages (with role, content fields) sent to LLM     output_len: max number of tokens expected in response.             If None, use the LLM's default model_max_output_tokens. Returns:     Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\n    self,\n    messages: List[LLMMessage],\n    output_len: Optional[int] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n) -&gt; ChatDocument:\n    \"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n        output_len: max number of tokens expected in response.\n                If None, use the LLM's default model_max_output_tokens.\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.model_max_output_tokens\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream() and not settings.quiet:\n            # show rich spinner only if not streaming!\n            # (Why? b/c the intent of showing a spinner is to \"show progress\",\n            # and we don't need to do that when streaming, since\n            # streaming output already shows progress.)\n            cm = status(\n                \"LLM responding to messages...\",\n                log_if_quiet=False,\n            )\n            stack.enter_context(cm)\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        functions, fun_call, tools, force_tool, output_format = (\n            self._function_args()\n        )\n        assert self.llm is not None\n        response = self.llm.chat(\n            messages,\n            output_len,\n            tools=tools,\n            tool_choice=force_tool or tool_choice,\n            functions=functions,\n            function_call=fun_call,\n            response_format=output_format,\n        )\n    if self.llm.get_stream():\n        # Create temp ChatDocument for tool check, then clean up to avoid\n        # polluting ObjectRegistry (see PR #939 discussion)\n        temp_doc = ChatDocument.from_LLMResponse(\n            response,\n            displayed=True,\n            recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n        )\n        self._call_callback_with_reasoning(\n            \"finish_llm_stream\",\n            reasoning=response.reasoning,\n            content=response.message,\n            tools_content=response.tools_content(),\n            is_tool=self.has_tool_message_attempt(temp_doc),\n        )\n        ObjectRegistry.remove(temp_doc.id())\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(\n        response,\n        displayed=True,\n        recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n    )\n    self.oai_tool_calls = response.oai_tool_calls or []\n    self.oai_tool_id2call.update(\n        {t.id: t for t in self.oai_tool_calls if t.id is not None}\n    )\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(chat_doc)\n\n    return chat_doc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_messages_async","title":"<code>llm_response_messages_async(messages, output_len=None, tool_choice='auto')</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_messages</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_messages_async(\n    self,\n    messages: List[LLMMessage],\n    output_len: Optional[int] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_messages`. See there for details.\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.model_max_output_tokens\n    functions, fun_call, tools, force_tool, output_format = self._function_args()\n    assert self.llm is not None\n\n    streamer_async = async_noop_fn\n    if self.llm.get_stream():\n        streamer_async = await self.callbacks.start_llm_stream_async()\n    self.llm.config.streamer_async = streamer_async\n\n    response = await self.llm.achat(\n        messages,\n        output_len,\n        tools=tools,\n        tool_choice=force_tool or tool_choice,\n        functions=functions,\n        function_call=fun_call,\n        response_format=output_format,\n    )\n    if self.llm.get_stream():\n        # Create temp ChatDocument for tool check, then clean up to avoid\n        # polluting ObjectRegistry (see PR #939 discussion)\n        temp_doc = ChatDocument.from_LLMResponse(\n            response,\n            displayed=True,\n            recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n        )\n        self._call_callback_with_reasoning(\n            \"finish_llm_stream\",\n            reasoning=response.reasoning,\n            content=response.message,\n            tools_content=response.tools_content(),\n            is_tool=self.has_tool_message_attempt(temp_doc),\n        )\n        ObjectRegistry.remove(temp_doc.id())\n    self.llm.config.streamer_async = async_noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(\n        response,\n        displayed=True,\n        recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n    )\n    self.oai_tool_calls = response.oai_tool_calls or []\n    self.oai_tool_id2call.update(\n        {t.id: t for t in self.oai_tool_calls if t.id is not None}\n    )\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(chat_doc)\n\n    return chat_doc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message=None)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>message to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n\n    Args:\n        message (str|ChatDocument): message to respond to.\n\n    Returns:\n        A Document object with the response.\n\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(ChatDocument, ChatAgent.llm_response(self, message))\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(response)\n\n    return response\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.llm_response_forget_async","title":"<code>llm_response_forget_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_forget</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_forget_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_forget`. See there for details.\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(\n            ChatDocument, await ChatAgent.llm_response_async(self, message)\n        )\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n    return response\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:     int: number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n    \"\"\"\n    Total number of tokens in the message history so far.\n\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\n            \"ChatAgent.parser is None. \"\n            \"You must set ChatAgent.parser \"\n            \"before calling chat_num_tokens().\"\n        )\n    hist = messages if messages is not None else self.message_history\n    return sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history Args:     i: if provided, return only the i-th message when i is postive,         or last k messages when i = -k. Returns:</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n    \"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\n    if i is None:\n        return \"\\n\".join([str(m) for m in self.message_history])\n    elif i &gt; 0:\n        return str(self.message_history[i])\n    else:\n        return \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently these \"tools\" are handled by methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p>"},{"location":"reference/agent/#langroid.agent.ToolMessage.instructions","title":"<code>instructions()</code>  <code>classmethod</code>","text":"<p>Instructions on tool usage.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    \"\"\"\n    Instructions on tool usage.\n    \"\"\"\n    return \"\"\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.langroid_tools_instructions","title":"<code>langroid_tools_instructions()</code>  <code>classmethod</code>","text":"<p>Instructions on tool usage when <code>use_tools == True</code>, i.e. when using langroid built-in tools (as opposed to OpenAI-like function calls/tools).</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef langroid_tools_instructions(cls) -&gt; str:\n    \"\"\"\n    Instructions on tool usage when `use_tools == True`, i.e.\n    when using langroid built-in tools\n    (as opposed to OpenAI-like function calls/tools).\n    \"\"\"\n    return \"\"\"\n    IMPORTANT: When using this or any other tool/function, you MUST include a \n    `request` field and set it equal to the FUNCTION/TOOL NAME you intend to use.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Examples to use in few-shot demos with formatting instructions. Each example can be either: - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or - a tuple (description, ToolMessage instance), where the description is     a natural language \"thought\" that leads to the tool usage,     e.g. (\"I want to find the square of 5\",  SquareTool(num=5))     In some scenarios, including such a description can significantly     enhance reliability of tool use. Returns:</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[\"ToolMessage\" | Tuple[str, \"ToolMessage\"]]:\n    \"\"\"\n    Examples to use in few-shot demos with formatting instructions.\n    Each example can be either:\n    - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or\n    - a tuple (description, ToolMessage instance), where the description is\n        a natural language \"thought\" that leads to the tool usage,\n        e.g. (\"I want to find the square of 5\",  SquareTool(num=5))\n        In some scenarios, including such a description can significantly\n        enhance reliability of tool use.\n    Returns:\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.usage_examples","title":"<code>usage_examples(random=False)</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing examples of how to use the tool-message.</p> <p>Parameters:</p> Name Type Description Default <code>random</code> <code>bool</code> <p>whether to pick a random example from the list of examples. Set to <code>true</code> when using this to illustrate a dialog between LLM and user. (if false, use ALL examples)</p> <code>False</code> <p>Returns:     str: examples of how to use the tool/function-call</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_examples(cls, random: bool = False) -&gt; str:\n    \"\"\"\n    Instruction to the LLM showing examples of how to use the tool-message.\n\n    Args:\n        random (bool): whether to pick a random example from the list of examples.\n            Set to `true` when using this to illustrate a dialog between LLM and\n            user.\n            (if false, use ALL examples)\n    Returns:\n        str: examples of how to use the tool/function-call\n    \"\"\"\n    # pick a random example of the fields\n    if len(cls.examples()) == 0:\n        return \"\"\n    if random:\n        examples = [choice(cls.examples())]\n    else:\n        examples = cls.examples()\n    formatted_examples = [\n        (\n            f\"EXAMPLE {i}: (THOUGHT: {ex[0]}) =&gt; \\n{ex[1].format_example()}\"\n            if isinstance(ex, tuple)\n            else f\"EXAMPLE {i}:\\n {ex.format_example()}\"\n        )\n        for i, ex in enumerate(examples, 1)\n    ]\n    return \"\\n\\n\".join(formatted_examples)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.get_value_of_type","title":"<code>get_value_of_type(target_type)</code>","text":"<p>Try to find a value of a desired type in the fields of the ToolMessage.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>def get_value_of_type(self, target_type: Type[Any]) -&gt; Any:\n    \"\"\"Try to find a value of a desired type in the fields of the ToolMessage.\"\"\"\n    ignore_fields = self._get_excluded_fields().union({\"request\"})\n    for field_name in set(self.model_dump().keys()) - ignore_fields:\n        value = getattr(self, field_name)\n        if is_instance_of(value, target_type):\n            return value\n    return None\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class Args:     f (str): field name</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>default value of the field, or None if not set or if the field does not exist.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n    \"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n\n    Returns:\n        Any: default value of the field, or None if not set or if the\n            field does not exist.\n    \"\"\"\n    schema = cls.model_json_schema()\n    properties = schema[\"properties\"]\n    return properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.format_instructions","title":"<code>format_instructions(tool=False)</code>  <code>classmethod</code>","text":"<p>Default Instructions to the LLM showing how to use the tool/function-call. Works for GPT4 but override this for weaker LLMs if needed.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>bool</code> <p>instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM) (or else it would be for OpenAI Function calls). Ignored in the default implementation, but can be used in subclasses.</p> <code>False</code> <p>Returns:     str: instructions on how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef format_instructions(cls, tool: bool = False) -&gt; str:\n    \"\"\"\n    Default Instructions to the LLM showing how to use the tool/function-call.\n    Works for GPT4 but override this for weaker LLMs if needed.\n\n    Args:\n        tool: instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM)\n            (or else it would be for OpenAI Function calls).\n            Ignored in the default implementation, but can be used in subclasses.\n    Returns:\n        str: instructions on how to use the message\n    \"\"\"\n    # TODO: when we attempt to use a \"simpler schema\"\n    # (i.e. all nested fields explicit without definitions),\n    # we seem to get worse results, so we turn it off for now\n    param_dict = (\n        # cls.simple_schema() if tool else\n        cls.llm_function_schema(request=True).parameters\n    )\n    examples_str = \"\"\n    if cls.examples():\n        examples_str = \"EXAMPLES:\\n\" + cls.usage_examples()\n    return textwrap.dedent(\n        f\"\"\"\n        TOOL: {cls.default_value(\"request\")}\n        PURPOSE: {cls.default_value(\"purpose\")} \n        JSON FORMAT: {\n            json.dumps(param_dict, indent=4)\n        }\n        {examples_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.group_format_instructions","title":"<code>group_format_instructions()</code>  <code>staticmethod</code>","text":"<p>Template for instructions for a group of tools. Works with GPT4 but override this for weaker LLMs if needed.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@staticmethod\ndef group_format_instructions() -&gt; str:\n    \"\"\"Template for instructions for a group of tools.\n    Works with GPT4 but override this for weaker LLMs if needed.\n    \"\"\"\n    return textwrap.dedent(\n        \"\"\"\n        === ALL AVAILABLE TOOLS and THEIR FORMAT INSTRUCTIONS ===\n        You have access to the following TOOLS to accomplish your task:\n\n        {format_instructions}\n\n        When one of the above TOOLs is applicable, you must express your \n        request as \"TOOL:\" followed by the request in the above format.\n        \"\"\"\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.llm_function_schema","title":"<code>llm_function_schema(request=False, defaults=True)</code>  <code>classmethod</code>","text":"<p>Clean up the schema of the Pydantic class (which can recursively contain other Pydantic classes), to create a version compatible with OpenAI Function-call API.</p> <p>Adapted from this excellent library: https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>bool</code> <p>whether to include the \"request\" field in the schema. (we set this to True when using Langroid-native TOOLs as opposed to OpenAI Function calls)</p> <code>False</code> <code>defaults</code> <code>bool</code> <p>whether to include fields with default values in the schema,     in the \"properties\" section.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>LLMFunctionSpec</code> <code>LLMFunctionSpec</code> <p>the schema as an LLMFunctionSpec</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(\n    cls,\n    request: bool = False,\n    defaults: bool = True,\n) -&gt; LLMFunctionSpec:\n    \"\"\"\n    Clean up the schema of the Pydantic class (which can recursively contain\n    other Pydantic classes), to create a version compatible with OpenAI\n    Function-call API.\n\n    Adapted from this excellent library:\n    https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py\n\n    Args:\n        request: whether to include the \"request\" field in the schema.\n            (we set this to True when using Langroid-native TOOLs as opposed to\n            OpenAI Function calls)\n        defaults: whether to include fields with default values in the schema,\n                in the \"properties\" section.\n\n    Returns:\n        LLMFunctionSpec: the schema as an LLMFunctionSpec\n\n    \"\"\"\n    schema = copy.deepcopy(cls.model_json_schema())\n    docstring = parse(cls.__doc__ or \"\")\n    parameters = {\n        k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n    }\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (\n            description := param.description\n        ):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    excludes = cls._get_excluded_fields().copy()\n    if not request:\n        excludes = excludes.union({\"request\"})\n    # exclude 'excludes' from parameters[\"properties\"]:\n    parameters[\"properties\"] = {\n        field: details\n        for field, details in parameters[\"properties\"].items()\n        if field not in excludes and (defaults or details.get(\"default\") is None)\n    }\n    parameters[\"required\"] = sorted(\n        k\n        for k, v in parameters[\"properties\"].items()\n        if (\"default\" not in v and k not in excludes)\n    )\n    if request:\n        parameters[\"required\"].append(\"request\")\n\n        # If request is present it must match the default value\n        # Similar to defining request as a literal type\n        parameters[\"request\"] = {\n            \"enum\": [cls.default_value(\"request\")],\n            \"type\": \"string\",\n        }\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            schema[\"description\"] = (\n                f\"Correctly extracted `{cls.__name__}` with all \"\n                f\"the required parameters with correct types\"\n            )\n\n    # Handle nested ToolMessage fields\n    if \"definitions\" in parameters:\n        for v in parameters[\"definitions\"].values():\n            if \"exclude\" in v:\n                v.pop(\"exclude\")\n\n                remove_if_exists(\"purpose\", v[\"properties\"])\n                remove_if_exists(\"id\", v[\"properties\"])\n                if (\n                    \"request\" in v[\"properties\"]\n                    and \"default\" in v[\"properties\"][\"request\"]\n                ):\n                    if \"required\" not in v:\n                        v[\"required\"] = []\n                    v[\"required\"].append(\"request\")\n                    v[\"properties\"][\"request\"] = {\n                        \"type\": \"string\",\n                        \"enum\": [v[\"properties\"][\"request\"][\"default\"]],\n                    }\n\n    parameters.pop(\"exclude\")\n    _recursive_purge_dict_key(parameters, \"title\")\n    _recursive_purge_dict_key(parameters, \"additionalProperties\")\n    return LLMFunctionSpec(\n        name=cls.default_value(\"request\"),\n        description=cls.default_value(\"purpose\")\n        or f\"Tool for {cls.default_value('request')}\",\n        parameters=parameters,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.ToolMessage.simple_schema","title":"<code>simple_schema()</code>  <code>classmethod</code>","text":"<p>Return a simplified schema for the message, with only the request and required fields. Returns:     Dict[str, Any]: simplified schema</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef simple_schema(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a simplified schema for the message, with only the request and\n    required fields.\n    Returns:\n        Dict[str, Any]: simplified schema\n    \"\"\"\n    schema = generate_simple_schema(\n        cls,\n        exclude=list(cls._get_excluded_fields()),\n    )\n    return schema\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task","title":"<code>Task(agent=None, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=True, default_human_response=None, interactive=True, only_user_quits_root=True, erase_substeps=False, allow_null_result=False, max_stalled_steps=5, default_return_type=None, done_if_no_response=[], done_if_response=[], config=TaskConfig(), **kwargs)</code>","text":"<p>A <code>Task</code> wraps an <code>Agent</code> object, and sets up the <code>Agent</code>'s goals and instructions. A <code>Task</code> maintains two key variables:</p> <ul> <li><code>self.pending_message</code>, which is the message awaiting a response, and</li> <li><code>self.pending_sender</code>, which is the entity that sent the pending message.</li> </ul> <p>The possible responders to <code>self.pending_message</code> are the <code>Agent</code>'s own \"native\" responders (<code>agent_response</code>, <code>llm_response</code>, and <code>user_response</code>), and the <code>run()</code> methods of any sub-tasks. All responders have the same type-signature (somewhat simplified): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> Responders may or may not specify an intended recipient of their generated response.</p> <p>The main top-level method in the <code>Task</code> class is <code>run()</code>, which repeatedly calls <code>step()</code> until <code>done()</code> returns true. The <code>step()</code> represents a \"turn\" in the conversation: this method sequentially (in round-robin fashion) calls the responders until it finds one that generates a valid response to the <code>pending_message</code> (as determined by the <code>valid()</code> method). Once a valid response is found, <code>step()</code> updates the <code>pending_message</code> and <code>pending_sender</code> variables, and on the next iteration, <code>step()</code> re-starts its search for a valid response from the beginning of the list of responders (the exception being that the human user always gets a chance to respond after each non-human valid response). This process repeats until <code>done()</code> returns true, at which point <code>run()</code> returns the value of <code>result()</code>, which is the final result of the task.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent associated with the task</p> <code>None</code> <code>name</code> <code>str</code> <p>name of the task</p> <code>''</code> <code>llm_delegate</code> <code>bool</code> <p>Whether to delegate \"control\" to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve, and decides when a task is done. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity). See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by \"controller\" (i.e. LLM if <code>llm_delegate</code> is true, otherwise USER) and subsequent response by non-controller [When a tool is involved, this will not give intended results. See <code>done_if_response</code>, <code>done_if_no_response</code> below]. termination]. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\". See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's system_message</p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's user_message</p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true (default), resets the agent's message history at every run when it is the top-level task. Ignored when the task is a subtask of another task. Restart behavior of a subtask's <code>run()</code> can be controlled via the <code>TaskConfig.restart_as_subtask</code> setting.</p> <code>True</code> <code>default_human_response</code> <code>str | None</code> <p>default response from user; useful for testing, to avoid interactive input from user. [Instead of this, setting <code>interactive</code> usually suffices]</p> <code>None</code> <code>default_return_type</code> <code>Optional[type]</code> <p>if not None, extracts a value of this type from the result of self.run()</p> <code>None</code> <code>interactive</code> <code>bool</code> <p>if true, wait for human input after each non-human response (prevents infinite loop of non-human responses). Default is true. If false, then <code>default_human_response</code> is set to \"\" Note: When interactive = False, the one exception is when the user is explicitly addressed, via \"@user\" or using RecipientTool, in which case the system will wait for a user response. In other words, use <code>interactive=False</code> when you want a \"largely non-interactive\" run, with the exception of explicit user addressing.</p> <code>True</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, when interactive=True, only user can quit the root task (Ignored when interactive=False).</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> <code>allow_null_result</code> <code>bool</code> <p>If true, create dummy NO_ANSWER response when no valid response is found in a step. Optional, default is False. Note: In non-interactive mode, when this is set to True, you can have a situation where an LLM generates (non-tool) text, and no other responders have valid responses, and a \"Null result\" is inserted as a dummy response from the User entity, so the LLM will now respond to this Null result, and this will continue until the LLM emits a DONE signal (if instructed to do so), otherwise langroid detects a potential infinite loop after a certain number of such steps (= <code>TaskConfig.inf_loop_wait_factor</code>) and will raise an InfiniteLoopException.</p> <code>False</code> <code>max_stalled_steps</code> <code>int</code> <p>task considered done after this many consecutive steps with no progress. Default is 3.</p> <code>5</code> <code>done_if_no_response</code> <code>List[Responder]</code> <p>consider task done if NULL response from any of these responders. Default is empty list.</p> <code>[]</code> <code>done_if_response</code> <code>List[Responder]</code> <p>consider task done if NON-NULL response from any of these responders. Default is empty list.</p> <code>[]</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\n    self,\n    agent: Optional[Agent] = None,\n    name: str = \"\",\n    llm_delegate: bool = False,\n    single_round: bool = False,\n    system_message: str = \"\",\n    user_message: str | None = \"\",\n    restart: bool = True,\n    default_human_response: Optional[str] = None,\n    interactive: bool = True,\n    only_user_quits_root: bool = True,\n    erase_substeps: bool = False,\n    allow_null_result: bool = False,\n    max_stalled_steps: int = 5,\n    default_return_type: Optional[type] = None,\n    done_if_no_response: List[Responder] = [],\n    done_if_response: List[Responder] = [],\n    config: TaskConfig = TaskConfig(),\n    **kwargs: Any,  # catch-all for any legacy params, for backwards compatibility\n):\n    \"\"\"\n    A task to be performed by an agent.\n\n    Args:\n        agent (Agent): agent associated with the task\n        name (str): name of the task\n        llm_delegate (bool):\n            Whether to delegate \"control\" to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve, and decides when a task is done.\n            The \"controlling entity\" is either the LLM or the USER.\n            (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        single_round (bool):\n            If true, task runs until one message by \"controller\"\n            (i.e. LLM if `llm_delegate` is true, otherwise USER)\n            and subsequent response by non-controller [When a tool is involved,\n            this will not give intended results. See `done_if_response`,\n            `done_if_no_response` below].\n            termination]. If false, runs for the specified number of turns in\n            `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        system_message (str): if not empty, overrides agent's system_message\n        user_message (str): if not empty, overrides agent's user_message\n        restart (bool): if true (default), resets the agent's message history\n            *at every run* when it is the top-level task. Ignored when\n            the task is a subtask of another task. Restart behavior of a subtask's\n            `run()` can be controlled via the `TaskConfig.restart_as_subtask`\n            setting.\n        default_human_response (str|None): default response from user; useful for\n            testing, to avoid interactive input from user.\n            [Instead of this, setting `interactive` usually suffices]\n        default_return_type: if not None, extracts a value of this type from the\n            result of self.run()\n        interactive (bool): if true, wait for human input after each non-human\n            response (prevents infinite loop of non-human responses).\n            Default is true. If false, then `default_human_response` is set to \"\"\n            Note: When interactive = False, the one exception is when the user\n            is explicitly addressed, via \"@user\" or using RecipientTool, in which\n            case the system will wait for a user response. In other words, use\n            `interactive=False` when you want a \"largely non-interactive\"\n            run, with the exception of explicit user addressing.\n        only_user_quits_root (bool): if true, when interactive=True, only user can\n            quit the root task (Ignored when interactive=False).\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n        allow_null_result (bool):\n            If true, create dummy NO_ANSWER response when no valid response is found\n            in a step.\n            Optional, default is False.\n            *Note:* In non-interactive mode, when this is set to True,\n            you can have a situation where an LLM generates (non-tool) text,\n            and no other responders have valid responses, and a \"Null result\"\n            is inserted as a dummy response from the User entity, so the LLM\n            will now respond to this Null result, and this will continue\n            until the LLM emits a DONE signal (if instructed to do so),\n            otherwise langroid detects a potential infinite loop after\n            a certain number of such steps (= `TaskConfig.inf_loop_wait_factor`)\n            and will raise an InfiniteLoopException.\n        max_stalled_steps (int): task considered done after this many consecutive\n            steps with no progress. Default is 3.\n        done_if_no_response (List[Responder]): consider task done if NULL\n            response from any of these responders. Default is empty list.\n        done_if_response (List[Responder]): consider task done if NON-NULL\n            response from any of these responders. Default is empty list.\n    \"\"\"\n    if agent is None:\n        agent = ChatAgent()\n    self.callbacks = SimpleNamespace(\n        show_subtask_response=noop_fn,\n        set_parent_agent=noop_fn,\n    )\n    self.config = config\n    # Store parsed done sequences (will be initialized after agent assignment)\n    self._parsed_done_sequences: Optional[List[DoneSequence]] = None\n    # how to behave as a sub-task; can be overridden by `add_sub_task()`\n    self.config_sub_task = copy.deepcopy(config)\n    # counts of distinct pending messages in history,\n    # to help detect (exact) infinite loops\n    self.message_counter: Counter[str] = Counter()\n    self._init_message_counter()\n\n    self.history: Deque[str] = deque(\n        maxlen=self.config.inf_loop_cycle_len * self.config.inf_loop_wait_factor\n    )\n    # copy the agent's config, so that we don't modify the original agent's config,\n    # which may be shared by other agents.\n    try:\n        config_copy = copy.deepcopy(agent.config)\n        agent.config = config_copy\n    except Exception:\n        logger.warning(\n            \"\"\"\n            Failed to deep-copy Agent config during task creation, \n            proceeding with original config. Be aware that changes to \n            the config may affect other agents using the same config.\n            \"\"\"\n        )\n    self.restart = restart\n    agent = cast(ChatAgent, agent)\n    self.agent: ChatAgent = agent\n    if isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\n        self.agent.init_state()\n        # possibly change the system and user messages\n        if system_message:\n            # we always have at least 1 task_message\n            self.agent.set_system_message(system_message)\n        if user_message:\n            self.agent.set_user_message(user_message)\n\n    # Initialize parsed done sequences now that self.agent is available\n    if self.config.done_sequences:\n        from .done_sequence_parser import parse_done_sequences\n\n        # Pass agent's llm_tools_map directly\n        tools_map = (\n            self.agent.llm_tools_map\n            if hasattr(self.agent, \"llm_tools_map\")\n            else None\n        )\n        self._parsed_done_sequences = parse_done_sequences(\n            self.config.done_sequences, tools_map\n        )\n\n    self.max_cost: float = 0\n    self.max_tokens: int = 0\n    self.session_id: str = \"\"\n    self.logger: None | RichFileLogger = None\n    self.tsv_logger: None | logging.Logger = None\n    self.html_logger: Optional[HTMLLogger] = None\n    self.color_log: bool = False if settings.notebook else True\n\n    self.n_stalled_steps = 0  # how many consecutive steps with no progress?\n    # how many 2-step-apart alternations of no_answer step-result have we had,\n    # i.e. x1, N/A, x2, N/A, x3, N/A ...\n    self.n_no_answer_alternations = 0\n    self._no_answer_step: int = -5\n    self._step_idx = -1  # current step index\n    self.max_stalled_steps = max_stalled_steps\n    self.done_if_response = [r.value for r in done_if_response]\n    self.done_if_no_response = [r.value for r in done_if_no_response]\n    self.is_done = False  # is task done (based on response)?\n    self.is_pass_thru = False  # is current response a pass-thru?\n    if name:\n        # task name overrides name in agent config\n        agent.config.name = name\n    self.name = name or agent.config.name\n    self.value: str = self.name\n\n    self.default_human_response = default_human_response\n    if default_human_response is not None:\n        # only override agent's default_human_response if it is explicitly set\n        self.agent.default_human_response = default_human_response\n    self.interactive = interactive\n    self.agent.interactive = interactive\n    self.only_user_quits_root = only_user_quits_root\n    self.message_history_idx = -1\n    self.default_return_type = default_return_type\n\n    # set to True if we want to collapse multi-turn conversation with sub-tasks into\n    # just the first outgoing message and last incoming message.\n    # Note this also completely erases sub-task agents' message_history.\n    self.erase_substeps = erase_substeps\n    self.allow_null_result = allow_null_result\n\n    agent_entity_responders = agent.entity_responders()\n    agent_entity_responders_async = agent.entity_responders_async()\n    self.responders: List[Responder] = [e for e, _ in agent_entity_responders]\n    self.responders_async: List[Responder] = [\n        e for e, _ in agent_entity_responders_async\n    ]\n    self.non_human_responders: List[Responder] = [\n        r for r in self.responders if r != Entity.USER\n    ]\n    self.non_human_responders_async: List[Responder] = [\n        r for r in self.responders_async if r != Entity.USER\n    ]\n\n    self.human_tried = False  # did human get a chance to respond in last step?\n    self._entity_responder_map: Dict[\n        Entity, Callable[..., Optional[ChatDocument]]\n    ] = dict(agent_entity_responders)\n\n    self._entity_responder_async_map: Dict[\n        Entity, Callable[..., Coroutine[Any, Any, Optional[ChatDocument]]]\n    ] = dict(agent_entity_responders_async)\n\n    self.name_sub_task_map: Dict[str, Task] = {}\n    # latest message in a conversation among entities and agents.\n    self.pending_message: Optional[ChatDocument] = None\n    self.pending_sender: Responder = Entity.USER\n    self.single_round = single_round\n    self.turns = -1  # no limit\n    self.llm_delegate = llm_delegate\n    # Track last responder for done sequence checking\n    self._last_responder: Optional[Responder] = None\n    # Track response sequence for message chain\n    self.response_sequence: List[ChatDocument] = []\n    if llm_delegate:\n        if self.single_round:\n            # 0: User instructs (delegating to LLM);\n            # 1: LLM (as the Controller) asks;\n            # 2: user replies.\n            self.turns = 2\n    else:\n        if self.single_round:\n            # 0: User (as Controller) asks,\n            # 1: LLM replies.\n            self.turns = 1\n    # other sub_tasks this task can delegate to\n    self.sub_tasks: List[Task] = []\n    self.caller: Task | None = None  # which task called this task's `run` method\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.clone","title":"<code>clone(i)</code>","text":"<p>Returns a copy of this task, with a new agent.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def clone(self, i: int) -&gt; \"Task\":\n    \"\"\"\n    Returns a copy of this task, with a new agent.\n    \"\"\"\n    assert isinstance(self.agent, ChatAgent), \"Task clone only works for ChatAgent\"\n    agent: ChatAgent = self.agent.clone(i)\n    return Task(\n        agent,\n        name=self.name + f\"-{i}\",\n        llm_delegate=self.llm_delegate,\n        single_round=self.single_round,\n        system_message=self.agent.system_message,\n        user_message=self.agent.user_message,\n        restart=self.restart,\n        default_human_response=self.default_human_response,\n        interactive=self.interactive,\n        erase_substeps=self.erase_substeps,\n        allow_null_result=self.allow_null_result,\n        max_stalled_steps=self.max_stalled_steps,\n        done_if_no_response=[Entity(s) for s in self.done_if_no_response],\n        done_if_response=[Entity(s) for s in self.done_if_response],\n        default_return_type=self.default_return_type,\n        config=self.config,\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.kill_session","title":"<code>kill_session(session_id='')</code>  <code>classmethod</code>","text":"<p>Kill the session with the given session_id.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>@classmethod\ndef kill_session(cls, session_id: str = \"\") -&gt; None:\n    \"\"\"\n    Kill the session with the given session_id.\n    \"\"\"\n    session_id_kill_key = f\"{session_id}:kill\"\n    cls.cache().store(session_id_kill_key, \"1\")\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.kill","title":"<code>kill()</code>","text":"<p>Kill the task run associated with the current session.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def kill(self) -&gt; None:\n    \"\"\"\n    Kill the task run associated with the current session.\n    \"\"\"\n    self._cache_session_store(\"kill\", \"1\")\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response (unless a sub-task is explicitly addressed).</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]</code> <p>A task, or list of tasks, or a tuple of task and task config, or a list of tuples of task and task config. These tasks are added as sub-tasks of the current task. The task configs (if any) dictate how the tasks are run when invoked as sub-tasks of other tasks. This allows users to specify behavior applicable only in the context of a particular task-subtask combination.</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(\n    self,\n    task: (\n        Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response (unless a sub-task is explicitly addressed).\n\n    Args:\n        task: A task, or list of tasks, or a tuple of task and task config,\n            or a list of tuples of task and task config.\n            These tasks are added as sub-tasks of the current task.\n            The task configs (if any) dictate how the tasks are run when\n            invoked as sub-tasks of other tasks. This allows users to specify\n            behavior applicable only in the context of a particular task-subtask\n            combination.\n    \"\"\"\n    if isinstance(task, list):\n        for t in task:\n            self.add_sub_task(t)\n        return\n\n    if isinstance(task, tuple):\n        task, config = task\n    else:\n        config = TaskConfig()\n    task.config_sub_task = config\n    self.sub_tasks.append(task)\n    self.name_sub_task_map[task.name] = task\n    self.responders.append(cast(Responder, task))\n    self.responders_async.append(cast(Responder, task))\n    self.non_human_responders.append(cast(Responder, task))\n    self.non_human_responders_async.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>. Args:     msg (str|ChatDocument): optional message to start the conversation.</p> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\n    self.pending_sender = Entity.USER\n    if isinstance(msg, str):\n        self.pending_message = ChatDocument(\n            content=msg,\n            metadata=ChatDocMetaData(\n                sender=Entity.USER,\n            ),\n        )\n    elif msg is None and len(self.agent.message_history) &gt; 1:\n        # if agent has a history beyond system msg, set the\n        # pending message to the ChatDocument linked from\n        # last message in the history\n        last_agent_msg = self.agent.message_history[-1]\n        self.pending_message = ChatDocument.from_id(last_agent_msg.chat_document_id)\n        if self.pending_message is not None:\n            self.pending_sender = self.pending_message.metadata.sender\n    else:\n        if isinstance(msg, ChatDocument):\n            # carefully deep-copy: fresh metadata.id, register\n            # as new obj in registry\n            original_parent_id = msg.metadata.parent_id\n            self.pending_message = ChatDocument.deepcopy(msg)\n            # Preserve the parent pointer from the original message\n            self.pending_message.metadata.parent_id = original_parent_id\n        if self.pending_message is not None and self.caller is not None:\n            # msg may have come from `caller`, so we pretend this is from\n            # the CURRENT task's USER entity\n            self.pending_message.metadata.sender = Entity.USER\n            # update parent, child, agent pointers\n            if msg is not None:\n                msg.metadata.child_id = self.pending_message.metadata.id\n                # Only override parent_id if it wasn't already set in the\n                # original message. This preserves parent chains from TaskTool\n                if not msg.metadata.parent_id:\n                    self.pending_message.metadata.parent_id = msg.metadata.id\n        if self.pending_message is not None:\n            self.pending_message.metadata.agent_id = self.agent.id\n\n    self._show_pending_message_if_debug()\n    self.init_loggers()\n    # Log system message if it exists\n    if (\n        hasattr(self.agent, \"_create_system_and_tools_message\")\n        and hasattr(self.agent, \"system_message\")\n        and self.agent.system_message\n    ):\n        system_msg = self.agent._create_system_and_tools_message()\n        system_message_chat_doc = ChatDocument.from_LLMMessage(\n            system_msg,\n            sender_name=self.name or \"system\",\n        )\n        # log the system message\n        self.log_message(Entity.SYSTEM, system_message_chat_doc, mark=True)\n    self.log_message(Entity.USER, self.pending_message, mark=True)\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.init_loggers","title":"<code>init_loggers()</code>","text":"<p>Initialise per-task Rich and TSV loggers.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init_loggers(self) -&gt; None:\n    \"\"\"Initialise per-task Rich and TSV loggers.\"\"\"\n    from langroid.utils.logging import RichFileLogger\n\n    if not self.config.enable_loggers:\n        return\n\n    if self.caller is not None and self.caller.logger is not None:\n        self.logger = self.caller.logger\n    elif self.logger is None:\n        self.logger = RichFileLogger(\n            str(Path(self.config.logs_dir) / f\"{self.name}.log\"),\n            append=True,\n            color=self.color_log,\n        )\n\n    if self.caller is not None and self.caller.tsv_logger is not None:\n        self.tsv_logger = self.caller.tsv_logger\n    elif self.tsv_logger is None:\n        # unique logger name ensures a distinct `logging.Logger` object\n        self.tsv_logger = setup_file_logger(\n            f\"tsv_logger.{self.name}.{id(self)}\",\n            str(Path(self.config.logs_dir) / f\"{self.name}.tsv\"),\n        )\n        header = ChatDocLoggerFields().tsv_header()\n        self.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\n\n    # HTML logger\n    if self.config.enable_html_logging:\n        if (\n            self.caller is not None\n            and hasattr(self.caller, \"html_logger\")\n            and self.caller.html_logger is not None\n        ):\n            self.html_logger = self.caller.html_logger\n        elif not hasattr(self, \"html_logger\") or self.html_logger is None:\n            from langroid.utils.html_logger import HTMLLogger\n\n            model_info = \"\"\n            if (\n                hasattr(self, \"agent\")\n                and hasattr(self.agent, \"config\")\n                and hasattr(self.agent.config, \"llm\")\n            ):\n                model_info = getattr(self.agent.config.llm, \"chat_model\", \"\")\n            self.html_logger = HTMLLogger(\n                filename=self.name,\n                log_dir=self.config.logs_dir,\n                model_info=model_info,\n                append=False,\n            )\n            # Log clickable file:// link to the HTML log\n            html_log_path = self.html_logger.file_path.resolve()\n            logger.warning(f\"\ud83d\udcca HTML Log: file://{html_log_path}\")\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.reset_all_sub_tasks","title":"<code>reset_all_sub_tasks()</code>","text":"<p>Recursively reset message history &amp; state of own agent and those of all sub-tasks.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def reset_all_sub_tasks(self) -&gt; None:\n    \"\"\"\n    Recursively reset message history &amp; state of own agent and\n    those of all sub-tasks.\n    \"\"\"\n    self.agent.init_state()\n    for t in self.sub_tasks:\n        t.reset_all_sub_tasks()\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.run","title":"<code>run(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='', allow_restart=True, return_type=None)</code>","text":"<pre><code>run(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True\n) -&gt; Optional[ChatDocument]\n</code></pre><pre><code>run(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Type[T]\n) -&gt; Optional[T]\n</code></pre> <p>Synchronous version of <code>run_async()</code>. See <code>run_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\n    self,\n    msg: Any = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Optional[Type[T]] = None,\n) -&gt; Optional[ChatDocument | T]:\n    \"\"\"Synchronous version of `run_async()`.\n    See `run_async()` for details.\"\"\"\n    if allow_restart and (\n        (self.restart and caller is None)\n        or (self.config_sub_task.restart_as_subtask and caller is not None)\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    msg_input = self.agent.to_ChatDocument(msg, author_entity=Entity.USER)\n\n    if (\n        isinstance(msg_input, ChatDocument)\n        and msg_input.metadata.recipient != \"\"\n        and msg_input.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n\n    self._pre_run_loop(\n        msg=msg_input,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        self.step()\n        # Track pending message in response sequence\n        if self.pending_message is not None:\n            if (\n                not self.response_sequence\n                or self.pending_message.id() != self.response_sequence[-1].id()\n            ):\n                self.response_sequence.append(self.pending_message)\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    if final_result is None:\n        return None\n\n    if return_type is None:\n        return_type = self.default_return_type\n\n    # If possible, take a final strict decoding step\n    # when the output does not match `return_type`\n    if return_type is not None and return_type != ChatDocument:\n        parsed_result = self.agent.from_ChatDocument(final_result, return_type)\n\n        if (\n            parsed_result is None\n            and isinstance(self.agent, ChatAgent)\n            and self.agent._json_schema_available()\n        ):\n            strict_agent = self.agent[return_type]\n            output_args = strict_agent._function_args()[-1]\n            if output_args is not None:\n                schema = output_args.function.parameters\n                strict_result = strict_agent.llm_response(\n                    f\"\"\"\n                    A response adhering to the following JSON schema was expected:\n                    {schema}\n\n                    Please resubmit with the correct schema. \n                    \"\"\"\n                )\n\n                if strict_result is not None:\n                    return cast(\n                        Optional[T],\n                        strict_agent.from_ChatDocument(strict_result, return_type),\n                    )\n\n        return parsed_result\n\n    return final_result\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.run_async","title":"<code>run_async(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='', allow_restart=True, return_type=None)</code>  <code>async</code>","text":"<pre><code>run_async(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True\n) -&gt; Optional[ChatDocument]\n</code></pre><pre><code>run_async(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Type[T]\n) -&gt; Optional[T]\n</code></pre> <p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached. Runs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Any</code> <p>initial user-role message to process; if None, the LLM will respond to its initial <code>self.task_messages</code> which set up and kick off the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User). Note that <code>msg</code>, if passed, is treated as message with role <code>user</code>; a \"system\" role message should not be passed here.</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <code>caller</code> <code>Task | None</code> <p>the calling task, if any</p> <code>None</code> <code>max_cost</code> <code>float</code> <p>max cost allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>max_tokens</code> <code>int</code> <p>max tokens allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>session_id</code> <code>str</code> <p>session id for the task</p> <code>''</code> <code>allow_restart</code> <code>bool</code> <p>whether to allow restarting the task</p> <code>True</code> <code>return_type</code> <code>Optional[Type[T]]</code> <p>desired final result type</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument | T]</code> <p>Optional[ChatDocument]: valid result of the task.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def run_async(\n    self,\n    msg: Any = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Optional[Type[T]] = None,\n) -&gt; Optional[ChatDocument | T]:\n    \"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Runs asynchronously.\n\n    Args:\n        msg (Any): initial *user-role* message to process; if None,\n            the LLM will respond to its initial `self.task_messages`\n            which set up and kick off the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User). Note that `msg`, if passed, is treated as\n            message with role `user`; a \"system\" role message should not be\n            passed here.\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n        caller (Task|None): the calling task, if any\n        max_cost (float): max cost allowed for the task (default 0 -&gt; no limit)\n        max_tokens (int): max tokens allowed for the task (default 0 -&gt; no limit)\n        session_id (str): session id for the task\n        allow_restart (bool): whether to allow restarting the task\n        return_type (Optional[Type[T]]): desired final result type\n\n    Returns:\n        Optional[ChatDocument]: valid result of the task.\n    \"\"\"\n\n    # Even if the initial \"sender\" is not literally the USER (since the task could\n    # have come from another LLM), as far as this agent is concerned, the initial\n    # message can be considered to be from the USER\n    # (from the POV of this agent's LLM).\n\n    if allow_restart and (\n        (self.restart and caller is None)\n        or (self.config_sub_task.restart_as_subtask and caller is not None)\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    msg_input = self.agent.to_ChatDocument(msg, author_entity=Entity.USER)\n\n    if (\n        isinstance(msg_input, ChatDocument)\n        and msg_input.metadata.recipient != \"\"\n        and msg_input.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n\n    self._pre_run_loop(\n        msg=msg_input,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        await self.step_async()\n        await asyncio.sleep(0.01)  # temp yield to avoid blocking\n        # Track pending message in response sequence\n        if self.pending_message is not None:\n            if (\n                not self.response_sequence\n                or self.pending_message.id() != self.response_sequence[-1].id()\n            ):\n                self.response_sequence.append(self.pending_message)\n\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    if final_result is None:\n        return None\n\n    if return_type is None:\n        return_type = self.default_return_type\n\n    # If possible, take a final strict decoding step\n    # when the output does not match `return_type`\n    if return_type is not None and return_type != ChatDocument:\n        parsed_result = self.agent.from_ChatDocument(final_result, return_type)\n\n        if (\n            parsed_result is None\n            and isinstance(self.agent, ChatAgent)\n            and self.agent._json_schema_available()\n        ):\n            strict_agent = self.agent[return_type]\n            output_args = strict_agent._function_args()[-1]\n            if output_args is not None:\n                schema = output_args.function.parameters\n                strict_result = await strict_agent.llm_response_async(\n                    f\"\"\"\n                    A response adhering to the following JSON schema was expected:\n                    {schema}\n\n                    Please resubmit with the correct schema. \n                    \"\"\"\n                )\n\n                if strict_result is not None:\n                    return cast(\n                        Optional[T],\n                        strict_agent.from_ChatDocument(strict_result, return_type),\n                    )\n\n        return parsed_result\n\n    return final_result\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>Synchronous version of <code>step_async()</code>. See <code>step_async()</code> for details. TODO: Except for the self.response() calls, this fn should be identical to <code>step_async()</code>. Consider refactoring to avoid duplication.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    Synchronous version of `step_async()`. See `step_async()` for details.\n    TODO: Except for the self.response() calls, this fn should be identical to\n    `step_async()`. Consider refactoring to avoid duplication.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # (When `interactive=False`, human is only allowed to respond only if\n        #  if explicitly addressed)\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    n_non_responders = 0\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            n_non_responders += 1\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            if n_non_responders == len(responders):\n                # don't stay in this \"non-response\" loop forever\n                break\n            continue\n        self.human_tried = r == Entity.USER\n        result = self.response(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:  # did not find a valid response\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.step_async","title":"<code>step_async(turns=-1)</code>  <code>async</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def step_async(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders_async.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = await self.response_async(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Sync version of <code>response_async()</code>. See <code>response_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Sync version of `response_async()`. See `response_async()` for details.\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        pending_tools = self.agent.try_get_tool_messages(self.pending_message)\n        # TODO disable this\n        if (\n            len(pending_tools) &gt; 1\n            and len(self.agent.oai_tool_calls) &gt; 1\n            and not self.config.allow_subtask_multi_oai_tools\n        ):\n            result = self._forbid_multi_oai_tools(e)\n        else:\n            result = e.run(\n                self.pending_message,\n                turns=actual_turns,\n                caller=self,\n                max_cost=self.max_cost,\n                max_tokens=self.max_tokens,\n            )\n            # update result.tool_messages if any\n            if isinstance(result, ChatDocument):\n                self.agent.try_get_tool_messages(result)\n            if result is not None:\n                content, id2result, oai_tool_id = self.agent.process_tool_results(\n                    result.content,\n                    result.oai_tool_id2result,\n                    (\n                        self.pending_message.oai_tool_calls\n                        if isinstance(self.pending_message, ChatDocument)\n                        else None\n                    ),\n                )\n                result.content = content\n                result.oai_tool_id2result = id2result\n                result.metadata.oai_tool_id = oai_tool_id\n\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\"\n            if result is None\n            else \"\\n\\n\".join(str(m) for m in ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_map[cast(Entity, e)]\n        result = response_fn(self.pending_message)\n        # update result.tool_messages if any.\n        # Do this only if sender is LLM, since this could be\n        # a tool-call result from the Agent responder, which may\n        # contain strings that look like tools, and we don't want to\n        # trigger strict tool recovery due to that.\n        if (\n            isinstance(result, ChatDocument)\n            and result.metadata.sender == Entity.LLM\n        ):\n            self.agent.try_get_tool_messages(result)\n\n    result_chat_doc = self.agent.to_ChatDocument(\n        result,\n        chat_doc=self.pending_message,\n        author_entity=e if isinstance(e, Entity) else Entity.USER,\n    )\n    return self._process_result_routing(result_chat_doc, e)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.response_async","title":"<code>response_async(e, turns=-1)</code>  <code>async</code>","text":"<p>Get response to <code>self.pending_message</code> from a responder. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None. Args:     e (Responder): responder to get response from.     turns (int): number of turns to run the task for.         Default is -1, which means run until task is done.</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def response_async(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get response to `self.pending_message` from a responder.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Responder): responder to get response from.\n        turns (int): number of turns to run the task for.\n            Default is -1, which means run until task is done.\n\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        pending_tools = self.agent.try_get_tool_messages(self.pending_message)\n        # TODO disable this\n        if (\n            len(pending_tools) &gt; 1\n            and len(self.agent.oai_tool_calls) &gt; 1\n            and not self.config.allow_subtask_multi_oai_tools\n        ):\n            result = self._forbid_multi_oai_tools(e)\n        else:\n            # e.callbacks.set_parent_agent(self.agent)\n            result = await e.run_async(\n                self.pending_message,\n                turns=actual_turns,\n                caller=self,\n                max_cost=self.max_cost,\n                max_tokens=self.max_tokens,\n            )\n            # update result.tool_messages if any\n            if isinstance(result, ChatDocument):\n                self.agent.try_get_tool_messages(result)\n            if result is not None:\n                content, id2result, oai_tool_id = self.agent.process_tool_results(\n                    result.content,\n                    result.oai_tool_id2result,\n                    (\n                        self.pending_message.oai_tool_calls\n                        if isinstance(self.pending_message, ChatDocument)\n                        else None\n                    ),\n                )\n                result.content = content\n                result.oai_tool_id2result = id2result\n                result.metadata.oai_tool_id = oai_tool_id\n\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\"\n            if result is None\n            else \"\\n\\n\".join(str(m) for m in ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_async_map[cast(Entity, e)]\n        result = await response_fn(self.pending_message)\n        # update result.tool_messages if any\n        if (\n            isinstance(result, ChatDocument)\n            and result.metadata.sender == Entity.LLM\n        ):\n            self.agent.try_get_tool_messages(result)\n\n    result_chat_doc = self.agent.to_ChatDocument(\n        result,\n        chat_doc=self.pending_message,\n        author_entity=e if isinstance(e, Entity) else Entity.USER,\n    )\n    return self._process_result_routing(result_chat_doc, e)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.result","title":"<code>result(status=None)</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this.</p> <p>Note the result of a task is returned as if it is from the User entity.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>StatusCode</code> <p>status of the task when it ended</p> <code>None</code> <p>Returns:     ChatDocument: result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self, status: StatusCode | None = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n\n    Note the result of a task is returned as if it is from the User entity.\n\n    Args:\n        status (StatusCode): status of the task when it ended\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\n    if status in [StatusCode.STALLED, StatusCode.MAX_TURNS, StatusCode.INF_LOOP]:\n        # In these case we don't know (and don't want to try to guess)\n        # what the task result should be, so we return None\n        return None\n\n    result_msg = self.pending_message\n\n    content = result_msg.content if result_msg else \"\"\n    content_any = result_msg.content_any if result_msg else None\n    if DONE in content and self.config.recognize_string_signals:\n        # assuming it is of the form \"DONE: &lt;content&gt;\"\n        content = content.replace(DONE, \"\").strip()\n    oai_tool_calls = result_msg.oai_tool_calls if result_msg else None\n    oai_tool_id2result = result_msg.oai_tool_id2result if result_msg else None\n    fun_call = result_msg.function_call if result_msg else None\n    tool_messages = result_msg.tool_messages if result_msg else []\n    # if there is a DoneTool or AgentDoneTool among these,\n    # we extract content and tools from here, and ignore all others\n    for t in tool_messages:\n        if isinstance(t, FinalResultTool):\n            content = \"\"\n            content_any = None\n            tool_messages = [t]  # pass it on to parent so it also quits\n            break\n        elif isinstance(t, (AgentDoneTool, DoneTool)):\n            # there shouldn't be multiple tools like this; just take the first\n            content = to_string(t.content)\n            content_any = t.content\n            fun_call = None\n            oai_tool_calls = None\n            if isinstance(t, AgentDoneTool):\n                # AgentDoneTool may have tools, unlike DoneTool\n                tool_messages = t.tools\n            break\n    # drop the \"Done\" tools since they should not be part of the task result,\n    # or else they would cause the parent task to get unintentionally done!\n    tool_messages = [\n        t for t in tool_messages if not isinstance(t, (DoneTool, AgentDoneTool))\n    ]\n    block = result_msg.metadata.block if result_msg else None\n    recipient = result_msg.metadata.recipient if result_msg else \"\"\n    tool_ids = result_msg.metadata.tool_ids if result_msg else []\n\n    # regardless of which entity actually produced the result,\n    # when we return the result, we set entity to USER\n    # since to the \"parent\" task, this result is equivalent to a response from USER\n    result_doc = ChatDocument(\n        content=content,\n        content_any=content_any,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=fun_call,\n        tool_messages=tool_messages,\n        metadata=ChatDocMetaData(\n            source=Entity.USER,\n            sender=Entity.USER,\n            block=block,\n            status=status or (result_msg.metadata.status if result_msg else None),\n            sender_name=self.name,\n            recipient=recipient,\n            tool_ids=tool_ids,\n            parent_id=result_msg.id() if result_msg else \"\",\n            agent_id=str(self.agent.id),\n        ),\n    )\n    if self.pending_message is not None:\n        self.pending_message.metadata.child_id = result_doc.id()\n\n    return result_doc\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.done","title":"<code>done(result=None, r=None)</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this. Args:     result (ChatDocument|None): result from a responder     r (Responder|None): responder that produced the result         Not used here, but could be used by derived classes. Returns:     bool: True if task is done, False otherwise     StatusCode: status code indicating why task is done</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(\n    self, result: ChatDocument | None = None, r: Responder | None = None\n) -&gt; Tuple[bool, StatusCode]:\n    \"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Args:\n        result (ChatDocument|None): result from a responder\n        r (Responder|None): responder that produced the result\n            Not used here, but could be used by derived classes.\n    Returns:\n        bool: True if task is done, False otherwise\n        StatusCode: status code indicating why task is done\n    \"\"\"\n    if self._is_kill():\n        return (True, StatusCode.KILL)\n    result = result or self.pending_message\n\n    # Check if task should be done if message contains a tool\n    if self.config.done_if_tool and result is not None:\n        if isinstance(result, ChatDocument) and self.agent.try_get_tool_messages(\n            result, all_tools=True\n        ):\n            return (True, StatusCode.DONE)\n\n    # Check done sequences\n    if self._parsed_done_sequences and result is not None:\n        # Get the message chain from the current result\n        msg_chain = self._get_message_chain(result)\n\n        # Use last responder if r not provided\n        responder = r if r is not None else self._last_responder\n\n        # Check each sequence\n        for sequence in self._parsed_done_sequences:\n            if self._matches_sequence_with_current(\n                msg_chain, sequence, result, responder\n            ):\n                seq_name = sequence.name or \"unnamed\"\n                logger.info(f\"Task {self.name} done: matched sequence '{seq_name}'\")\n                return (True, StatusCode.DONE)\n\n    allow_done_string = self.config.recognize_string_signals\n    # An entity decided task is done, either via DoneTool,\n    # or by explicitly saying DONE\n    done_result = result is not None and (\n        (\n            DONE in (result.content if isinstance(result, str) else result.content)\n            and allow_done_string\n        )\n        or any(\n            isinstance(t, (DoneTool, AgentDoneTool, FinalResultTool))\n            for t in result.tool_messages\n        )\n    )\n\n    user_quit = (\n        result is not None\n        and (result.content in USER_QUIT_STRINGS or done_result)\n        and result.metadata.sender == Entity.USER\n    )\n\n    if self.n_stalled_steps &gt;= self.max_stalled_steps:\n        # we are stuck, so bail to avoid infinite loop\n        logger.warning(\n            f\"Task {self.name} stuck for {self.max_stalled_steps} steps; exiting.\"\n        )\n        return (True, StatusCode.STALLED)\n\n    if self.max_cost &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[1] &gt; self.max_cost:\n                logger.warning(\n                    f\"Task {self.name} cost exceeded {self.max_cost}; exiting.\"\n                )\n                return (True, StatusCode.MAX_COST)\n        except Exception:\n            pass\n\n    if self.max_tokens &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[0] &gt; self.max_tokens:\n                logger.warning(\n                    f\"Task {self.name} uses &gt; {self.max_tokens} tokens; exiting.\"\n                )\n                return (True, StatusCode.MAX_TOKENS)\n        except Exception:\n            pass\n\n    if self._level == 0 and self._user_can_respond() and self.only_user_quits_root:\n        # for top-level task, only user can quit out\n        return (user_quit, StatusCode.USER_QUIT if user_quit else StatusCode.OK)\n\n    if self.is_done:\n        return (True, StatusCode.DONE)\n\n    final = (\n        # no valid response from any entity/agent in current turn\n        result is None\n        or done_result\n        or (  # current task is addressing message to caller task\n            self.caller is not None\n            and self.caller.name != \"\"\n            and result.metadata.recipient == self.caller.name\n        )\n        or user_quit\n    )\n    return (final, StatusCode.OK)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.valid","title":"<code>valid(result, r)</code>","text":"<p>Is the result from a Responder (i.e. an entity or sub-task) such that we can stop searching for responses in this step?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(\n    self,\n    result: Optional[ChatDocument],\n    r: Responder,\n) -&gt; bool:\n    \"\"\"\n    Is the result from a Responder (i.e. an entity or sub-task)\n    such that we can stop searching for responses in this step?\n    \"\"\"\n    # TODO caution we should ensure that no handler method (tool) returns simply\n    # an empty string (e.g when showing contents of an empty file), since that\n    # would be considered an invalid response, and other responders will wrongly\n    # be given a chance to respond.\n\n    # if task would be considered done given responder r's `result`,\n    # then consider the result valid.\n    if result is not None and self.done(result, r)[0]:\n        return True\n    return (\n        result is not None\n        and not self._is_empty_message(result)\n        # some weaker LLMs, including even GPT-4o, may say \"DO-NOT-KNOW.\"\n        # (with a punctuation at the end), so need to strip out punctuation\n        and re.sub(r\"[,.!?:]\", \"\", result.content.strip()) != NO_ANSWER\n    )\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\n    self,\n    resp: Responder,\n    msg: ChatDocument | None = None,\n    mark: bool = False,\n) -&gt; None:\n    \"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\n    from langroid.agent.chat_document import ChatDocLoggerFields\n\n    default_values = ChatDocLoggerFields().model_dump().values()\n    msg_str_tsv = \"\\t\".join(str(v) for v in default_values)\n    if msg is not None:\n        msg_str_tsv = msg.tsv_str()\n\n    mark_str = \"*\" if mark else \" \"\n    task_name = self.name if self.name != \"\" else \"root\"\n    resp_color = \"white\" if mark else \"red\"\n    resp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\n\n    if msg is None:\n        msg_str = f\"{mark_str}({task_name}) {resp_str}\"\n    else:\n        color = {\n            Entity.LLM: \"green\",\n            Entity.USER: \"blue\",\n            Entity.AGENT: \"red\",\n            Entity.SYSTEM: \"magenta\",\n        }[msg.metadata.sender]\n        f = msg.log_fields()\n        tool_type = f.tool_type.rjust(6)\n        tool_name = f.tool.rjust(10)\n        tool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\n        sender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\n        sender_name = f.sender_name.rjust(10)\n        recipient = \"=&gt;\" + str(f.recipient).rjust(10)\n        block = \"X \" + str(f.block or \"\").rjust(10)\n        content = f\"[{color}]{f.content}[/{color}]\"\n        msg_str = (\n            f\"{mark_str}({task_name}) \"\n            f\"{resp_str} {sender}({sender_name}) \"\n            f\"({recipient}) ({block}) {tool_str} {content}\"\n        )\n\n    if self.logger is not None:\n        self.logger.log(msg_str)\n    if self.tsv_logger is not None:\n        resp_str = str(resp)\n        self.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n\n    # HTML logger\n    if self.html_logger is not None:\n        if msg is None:\n            # Create a minimal fields object for None messages\n            from langroid.agent.chat_document import ChatDocLoggerFields\n\n            fields_dict = {\n                \"responder\": str(resp),\n                \"mark\": \"*\" if mark else \"\",\n                \"task_name\": self.name or \"root\",\n                \"content\": \"\",\n                \"sender_entity\": str(resp),\n                \"sender_name\": \"\",\n                \"recipient\": \"\",\n                \"block\": None,\n                \"tool_type\": \"\",\n                \"tool\": \"\",\n            }\n        else:\n            # Get fields from the message\n            fields = msg.log_fields()\n            fields_dict = fields.model_dump()\n            fields_dict.update(\n                {\n                    \"responder\": str(resp),\n                    \"mark\": \"*\" if mark else \"\",\n                    \"task_name\": self.name or \"root\",\n                }\n            )\n\n        # Create a ChatDocLoggerFields-like object for the HTML logger\n        # Create a simple BaseModel subclass dynamically\n        from pydantic import BaseModel\n\n        class LogFields(BaseModel):\n            model_config = ConfigDict(extra=\"allow\")  # Allow extra fields\n\n        log_obj = LogFields(**fields_dict)\n        self.html_logger.log(log_obj)\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.set_color_log","title":"<code>set_color_log(enable=True)</code>","text":"<p>Flag to enable/disable color logging using rich.console. In some contexts, such as Colab notebooks, we may want to disable color logging using rich.console, since those logs show up in the cell output rather than in the log file. Turning off this feature will still create logs, but without the color formatting from rich.console Args:     enable (bool): value of <code>self.color_log</code> to set to,         which will enable/diable rich logging</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def set_color_log(self, enable: bool = True) -&gt; None:\n    \"\"\"\n    Flag to enable/disable color logging using rich.console.\n    In some contexts, such as Colab notebooks, we may want to disable color logging\n    using rich.console, since those logs show up in the cell output rather than\n    in the log file. Turning off this feature will still create logs, but without\n    the color formatting from rich.console\n    Args:\n        enable (bool): value of `self.color_log` to set to,\n            which will enable/diable rich logging\n    \"\"\"\n    self.color_log = enable\n</code></pre>"},{"location":"reference/agent/#langroid.agent.Task.close_loggers","title":"<code>close_loggers()</code>","text":"<p>Close all loggers to ensure clean shutdown.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def close_loggers(self) -&gt; None:\n    \"\"\"Close all loggers to ensure clean shutdown.\"\"\"\n    if hasattr(self, \"logger\") and self.logger is not None:\n        self.logger.close()\n    if hasattr(self, \"html_logger\") and self.html_logger is not None:\n        self.html_logger.close()\n</code></pre>"},{"location":"reference/agent/base/","title":"base","text":"<p>langroid/agent/base.py </p>"},{"location":"reference/agent/base/#langroid.agent.base.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent","title":"<code>Agent(config=AgentConfig())</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An Agent is an abstraction that typically (but not necessarily) encapsulates an LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def __init__(self, config: AgentConfig = AgentConfig()):\n    self.config = config\n    self.id = ObjectRegistry.new_id()  # Initialize agent ID\n    self.lock = asyncio.Lock()  # for async access to update self.llm.usage_cost\n    self.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\n    self.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\n    self.llm_tools_handled: Set[str] = set()\n    self.llm_tools_usable: Set[str] = set()\n    self.llm_tools_known: Set[str] = set()  # all known tools, handled/used or not\n    # Indicates which tool-names are allowed to be inferred when\n    # the LLM \"forgets\" to include the request field in its tool-call.\n    self.enabled_requests_for_inference: Optional[Set[str]] = (\n        None  # If None, we allow all\n    )\n    self.interactive: bool = True  # may be modified by Task wrapper\n    self.token_stats_str = \"\"\n    self.default_human_response: Optional[str] = None\n    self._indent = \"\"\n    self.llm = LanguageModel.create(config.llm)\n    self.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\n    self.tool_error = False\n    self.search_for_tools = {\n        SearchForTools.CONTENT.value,\n        SearchForTools.TOOLS.value,\n        SearchForTools.FUNCTIONS.value,\n    }\n    if config.parsing is not None and self.config.llm is not None:\n        # token_encoding_model is used to obtain the tokenizer,\n        # so in case it's an OpenAI model, we ensure that the tokenizer\n        # corresponding to the model is used.\n        if isinstance(self.llm, OpenAIGPT) and self.llm.is_openai_chat_model():\n            config.parsing.token_encoding_model = self.llm.config.chat_model\n    self.parser: Optional[Parser] = (\n        Parser(config.parsing) if config.parsing else None\n    )\n    if config.add_to_registry:\n        ObjectRegistry.register_object(self)\n\n    self.callbacks = SimpleNamespace(\n        start_llm_stream=lambda: noop_fn,\n        start_llm_stream_async=async_lambda_noop_fn,\n        cancel_llm_stream=noop_fn,\n        finish_llm_stream=noop_fn,\n        show_llm_response=noop_fn,\n        show_agent_response=noop_fn,\n        get_user_response=None,\n        get_user_response_async=None,\n        get_last_step=noop_fn,\n        set_parent_agent=noop_fn,\n        show_error_message=noop_fn,\n        show_start_response=noop_fn,\n    )\n    Agent.init_state(self)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.indent","title":"<code>indent</code>  <code>property</code> <code>writable</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.all_llm_tools_known","title":"<code>all_llm_tools_known</code>  <code>property</code>","text":"<p>All known tools; this may extend self.llm_tools_known.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.init_state","title":"<code>init_state()</code>","text":"<p>Initialize all state vars. Called by Task.run() if restart is True</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def init_state(self) -&gt; None:\n    \"\"\"Initialize all state vars. Called by Task.run() if restart is True\"\"\"\n    self.total_llm_token_cost = 0.0\n    self.total_llm_token_usage = 0\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details. Returns:     Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\n    self,\n) -&gt; List[\n    Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n    \"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response),\n        (Entity.LLM, self.llm_response),\n        (Entity.USER, self.user_response),\n    ]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.entity_responders_async","title":"<code>entity_responders_async()</code>","text":"<p>Async version of <code>entity_responders</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders_async(\n    self,\n) -&gt; List[\n    Tuple[\n        Entity,\n        Callable[\n            [None | str | ChatDocument], Coroutine[Any, Any, None | ChatDocument]\n        ],\n    ]\n]:\n    \"\"\"\n    Async version of `entity_responders`. See there for details.\n    \"\"\"\n    return [\n        (Entity.AGENT, self.agent_response_async),\n        (Entity.LLM, self.llm_response_async),\n        (Entity.USER, self.user_response_async),\n    ]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\n    self, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n    \"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable a message class from being handled by this Agent.\n\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes. Returns:     str: The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n    \"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    # use at most 2 sample conversations, no need to be exhaustive;\n    sample_convo = [\n        msg_cls().usage_examples(random=True)  # type: ignore\n        for i, msg_cls in enumerate(enabled_classes)\n        if i &lt; 2\n    ]\n    return \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.create_agent_response","title":"<code>create_agent_response(content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for agent_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_agent_response(\n    self,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: Optional[List[OpenAIToolCall]] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for agent_response.\"\"\"\n    return self.response_template(\n        Entity.AGENT,\n        content=content,\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.render_agent_response","title":"<code>render_agent_response(results)</code>","text":"<p>Render the response from the agent, typically from tool-handling. Args:     results: results from tool-handling, which may be a string,         a dict of tool results, or a ChatDocument.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def render_agent_response(\n    self,\n    results: Optional[str | OrderedDict[str, str] | ChatDocument],\n) -&gt; None:\n    \"\"\"\n    Render the response from the agent, typically from tool-handling.\n    Args:\n        results: results from tool-handling, which may be a string,\n            a dict of tool results, or a ChatDocument.\n    \"\"\"\n    if self.config.hide_agent_response or results is None:\n        return\n    if isinstance(results, str):\n        results_str = results\n    elif isinstance(results, ChatDocument):\n        results_str = results.content\n    elif isinstance(results, dict):\n        results_str = json.dumps(results, indent=2)\n    if not settings.quiet:\n        console.print(f\"[red]{self.indent}\", end=\"\")\n        print(f\"[red]Agent: {escape(results_str)}\")\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.agent_response_async","title":"<code>agent_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>agent_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def agent_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `agent_response`. See there for details.\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = await self.handle_message_async(msg)\n\n    return self._agent_response_final(msg, results)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\", typically (but not only) used to handle LLM's \"tool message\" or <code>function_call</code> (e.g. OpenAI <code>function_call</code>). Args:     msg (str|ChatDocument): the input to respond to: if msg is a string,         and it contains a valid JSON-structured \"tool message\", or         if msg is a ChatDocument, and it contains a <code>function_call</code>. Returns:     Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Response from the \"agent itself\", typically (but not only)\n    used to handle LLM's \"tool message\" or `function_call`\n    (e.g. OpenAI `function_call`).\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n\n    \"\"\"\n    if msg is None:\n        return None\n\n    results = self.handle_message(msg)\n\n    return self._agent_response_final(msg, results)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.process_tool_results","title":"<code>process_tool_results(results, id2result, tool_calls=None)</code>","text":"<p>Process results from a response, based on whether they are results of OpenAI tool-calls from THIS agent, so that we can construct an appropriate LLMMessage that contains tool results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>str</code> <p>A possible string result from handling tool(s)</p> required <code>id2result</code> <code>OrderedDict[str, str] | None</code> <p>A dict of OpenAI tool id -&gt; result, if there are multiple tool results.</p> required <code>tool_calls</code> <code>List[OpenAIToolCall] | None</code> <p>List of OpenAI tool-calls that the results are a response to.</p> <code>None</code> Return <ul> <li>str: The response string</li> <li>Dict[str,str]|None: A dict of OpenAI tool id -&gt; result, if there are     multiple tool results.</li> <li>str|None: tool_id if there was a single tool result</li> </ul> Source code in <code>langroid/agent/base.py</code> <pre><code>def process_tool_results(\n    self,\n    results: str,\n    id2result: OrderedDict[str, str] | None,\n    tool_calls: List[OpenAIToolCall] | None = None,\n) -&gt; Tuple[str, Dict[str, str] | None, str | None]:\n    \"\"\"\n    Process results from a response, based on whether\n    they are results of OpenAI tool-calls from THIS agent, so that\n    we can construct an appropriate LLMMessage that contains tool results.\n\n    Args:\n        results (str): A possible string result from handling tool(s)\n        id2result (OrderedDict[str,str]|None): A dict of OpenAI tool id -&gt; result,\n            if there are multiple tool results.\n        tool_calls (List[OpenAIToolCall]|None): List of OpenAI tool-calls that the\n            results are a response to.\n\n    Return:\n        - str: The response string\n        - Dict[str,str]|None: A dict of OpenAI tool id -&gt; result, if there are\n            multiple tool results.\n        - str|None: tool_id if there was a single tool result\n\n    \"\"\"\n    id2result_ = copy.deepcopy(id2result) if id2result is not None else None\n    results_str = \"\"\n    oai_tool_id = None\n\n    if results != \"\":\n        # in this case ignore id2result\n        assert (\n            id2result is None\n        ), \"id2result should be None when results string is non-empty!\"\n        results_str = results\n        if len(self.oai_tool_calls) &gt; 0:\n            # We only have one result, so in case there is a\n            # \"pending\" OpenAI tool-call, we expect no more than 1 such.\n            assert (\n                len(self.oai_tool_calls) == 1\n            ), \"There are multiple pending tool-calls, but only one result!\"\n            # We record the tool_id of the tool-call that\n            # the result is a response to, so that ChatDocument.to_LLMMessage\n            # can properly set the `tool_call_id` field of the LLMMessage.\n            oai_tool_id = self.oai_tool_calls[0].id\n    elif id2result is not None and id2result_ is not None:  # appease mypy\n        if len(id2result_) == len(self.oai_tool_calls):\n            # if the number of pending tool calls equals the number of results,\n            # then ignore the ids in id2result, and use the results in order,\n            # which is preserved since id2result is an OrderedDict.\n            assert len(id2result_) &gt; 1, \"Expected to see &gt; 1 result in id2result!\"\n            results_str = \"\"\n            id2result_ = OrderedDict(\n                zip(\n                    [tc.id or \"\" for tc in self.oai_tool_calls], id2result_.values()\n                )\n            )\n        else:\n            assert (\n                tool_calls is not None\n            ), \"tool_calls cannot be None when id2result is not None!\"\n            # This must be an OpenAI tool id -&gt; result map;\n            # However some ids may not correspond to the tool-calls in the list of\n            # pending tool-calls (self.oai_tool_calls).\n            # Such results are concatenated into a simple string, to store in the\n            # ChatDocument.content, and the rest\n            # (i.e. those that DO correspond to tools in self.oai_tool_calls)\n            # are stored as a dict in ChatDocument.oai_tool_id2result.\n\n            # OAI tools from THIS agent, awaiting response\n            pending_tool_ids = [tc.id for tc in self.oai_tool_calls]\n            # tool_calls that the results are a response to\n            # (but these may have been sent from another agent, hence may not be in\n            # self.oai_tool_calls)\n            parent_tool_id2name = {\n                tc.id: tc.function.name\n                for tc in tool_calls or []\n                if tc.function is not None\n            }\n\n            # (id, result) for result NOT corresponding to self.oai_tool_calls,\n            # i.e. these are results of EXTERNAL tool-calls from another agent.\n            external_tool_id_results = []\n\n            for tc_id, result in id2result.items():\n                if tc_id not in pending_tool_ids:\n                    external_tool_id_results.append((tc_id, result))\n                    id2result_.pop(tc_id)\n            if len(external_tool_id_results) == 0:\n                results_str = \"\"\n            elif len(external_tool_id_results) == 1:\n                results_str = external_tool_id_results[0][1]\n            else:\n                results_str = \"\\n\\n\".join(\n                    [\n                        f\"Result from tool/function \"\n                        f\"{parent_tool_id2name[id]}: {result}\"\n                        for id, result in external_tool_id_results\n                    ]\n                )\n\n            if len(id2result_) == 0:\n                id2result_ = None\n            elif len(id2result_) == 1 and len(external_tool_id_results) == 0:\n                results_str = list(id2result_.values())[0]\n                oai_tool_id = list(id2result_.keys())[0]\n                id2result_ = None\n\n    return results_str, id2result_, oai_tool_id\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.response_template","title":"<code>response_template(e, content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for response from entity <code>e</code>.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def response_template(\n    self,\n    e: Entity,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: Optional[List[OpenAIToolCall]] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for response from entity `e`.\"\"\"\n    return ChatDocument(\n        content=content or \"\",\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        oai_tool_choice=oai_tool_choice,\n        metadata=ChatDocMetaData(\n            source=e, sender=e, sender_name=self.config.name, recipient=recipient\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.create_user_response","title":"<code>create_user_response(content=None, files=[], content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for user_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_user_response(\n    self,\n    content: str | None = None,\n    files: List[FileAttachment] = [],\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: List[OpenAIToolCall] | None = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for user_response.\"\"\"\n    return self.response_template(\n        e=Entity.USER,\n        content=content,\n        files=files,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.user_can_respond","title":"<code>user_can_respond(msg=None)</code>","text":"<p>Whether the user can respond to a message.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_can_respond(self, msg: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the user can respond to a message.\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n\n    \"\"\"\n    # When msg explicitly addressed to user, this means an actual human response\n    # is being sought.\n    need_human_response = (\n        isinstance(msg, ChatDocument) and msg.metadata.recipient == Entity.USER\n    )\n\n    if not self.interactive and not need_human_response:\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.user_response_async","title":"<code>user_response_async(msg=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>user_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def user_response_async(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `user_response`. See there for details.\n    \"\"\"\n    if not self.user_can_respond(msg):\n        return None\n\n    if self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if (\n            self.callbacks.get_user_response_async is not None\n            and self.callbacks.get_user_response_async is not async_noop_fn\n        ):\n            user_msg = await self.callbacks.get_user_response_async(prompt=\"\")\n        elif self.callbacks.get_user_response is not None:\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}\"\n                + self.config.human_prompt\n                + f\"\\n{self.indent}\"\n            )\n\n    return self._user_response_final(msg, user_msg)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\n    self,\n    msg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n\n    Returns:\n        (str) User response, packaged as a ChatDocument\n\n    \"\"\"\n\n    if not self.user_can_respond(msg):\n        return None\n\n    if self.default_human_response is not None:\n        user_msg = self.default_human_response\n    else:\n        if self.callbacks.get_user_response is not None:\n            # ask user with empty prompt: no need for prompt\n            # since user has seen the conversation so far.\n            # But non-empty prompt can be useful when Agent\n            # uses a tool that requires user input, or in other scenarios.\n            user_msg = self.callbacks.get_user_response(prompt=\"\")\n        else:\n            user_msg = Prompt.ask(\n                f\"[blue]{self.indent}\"\n                + self.config.human_prompt\n                + f\"\\n{self.indent}\"\n            )\n\n    return self._user_response_final(msg, user_msg)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n\n    Returns:\n\n    \"\"\"\n    if self.llm is None:\n        return False\n\n    if message is not None and len(self.try_get_tool_messages(message)) &gt; 0:\n        # if there is a valid \"tool\" message (either JSON or via `function_call`)\n        # then LLM cannot respond to it\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.can_respond","title":"<code>can_respond(message=None)</code>","text":"<p>Whether the agent can respond to a message. Used in Task.py to skip a sub-task when we know it would not respond. Args:     message (str|ChatDocument): message or ChatDocument object to respond to.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n    \"\"\"\n    Whether the agent can respond to a message.\n    Used in Task.py to skip a sub-task when we know it would not respond.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n    \"\"\"\n    tools = self.try_get_tool_messages(message)\n    if len(tools) == 0 and self.config.respond_tools_only:\n        return False\n    if message is not None and self.has_only_unhandled_tools(message):\n        # The message has tools that are NOT enabled to be handled by this agent,\n        # which means the agent cannot respond to it.\n        return False\n    return True\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.create_llm_response","title":"<code>create_llm_response(content=None, content_any=None, tool_messages=[], oai_tool_calls=None, oai_tool_choice='auto', oai_tool_id2result=None, function_call=None, recipient='')</code>","text":"<p>Template for llm_response.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def create_llm_response(\n    self,\n    content: str | None = None,\n    content_any: Any = None,\n    tool_messages: List[ToolMessage] = [],\n    oai_tool_calls: None | List[OpenAIToolCall] = None,\n    oai_tool_choice: ToolChoiceTypes | Dict[str, Dict[str, str] | str] = \"auto\",\n    oai_tool_id2result: OrderedDict[str, str] | None = None,\n    function_call: LLMFunctionCall | None = None,\n    recipient: str = \"\",\n) -&gt; ChatDocument:\n    \"\"\"Template for llm_response.\"\"\"\n    return self.response_template(\n        Entity.LLM,\n        content=content,\n        content_any=content_any,\n        tool_messages=tool_messages,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_choice=oai_tool_choice,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=function_call,\n        recipient=recipient,\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self,\n    message: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Asynch version of `llm_response`. See there for details.\n    \"\"\"\n    if message is None or not self.llm_can_respond(message):\n        return None\n\n    if isinstance(message, ChatDocument):\n        prompt = message.content\n    else:\n        prompt = message\n\n    output_len = self.config.llm.model_max_output_tokens\n    if self.num_tokens(prompt) + output_len &gt; self.llm.completion_context_length():\n        output_len = self.llm.completion_context_length() - self.num_tokens(prompt)\n        if output_len &lt; self.config.llm.min_output_tokens:\n            raise ValueError(\n                \"\"\"\n            Token-length of Prompt + Output is longer than the\n            completion context length of the LLM!\n            \"\"\"\n            )\n        else:\n            logger.warning(\n                f\"\"\"\n            Requested output length has been shortened to {output_len}\n            so that the total length of Prompt + Output is less than\n            the completion context length of the LLM.\n            \"\"\"\n            )\n\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        response = await self.llm.agenerate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # We would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response.\n        # If we are here, it means the response has not yet been displayed.\n        cached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\n        print(cached + \"[green]\" + escape(response.message))\n    async with self.lock:\n        self.update_token_usage(\n            response,\n            prompt,\n            self.llm.get_stream(),\n            chat=False,  # i.e. it's a completion model not chat model\n            print_response_stats=self.config.show_stats and not settings.quiet,\n        )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = (\n        [] if isinstance(message, str) else message.metadata.tool_ids\n    )\n    return cdoc\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>LLM response to a prompt. Args:     message (str|ChatDocument): prompt string, or ChatDocument object</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self,\n    message: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    LLM response to a prompt.\n    Args:\n        message (str|ChatDocument): prompt string, or ChatDocument object\n\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\n    if message is None or not self.llm_can_respond(message):\n        return None\n\n    if isinstance(message, ChatDocument):\n        prompt = message.content\n    else:\n        prompt = message\n\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream():\n            # show rich spinner only if not streaming!\n            cm = status(\"LLM responding to message...\")\n            stack.enter_context(cm)\n        output_len = self.config.llm.model_max_output_tokens\n        if (\n            self.num_tokens(prompt) + output_len\n            &gt; self.llm.completion_context_length()\n        ):\n            output_len = self.llm.completion_context_length() - self.num_tokens(\n                prompt\n            )\n            if output_len &lt; self.config.llm.min_output_tokens:\n                raise ValueError(\n                    \"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n                )\n            else:\n                logger.warning(\n                    f\"\"\"\n                Requested output length has been shortened to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM.\n                \"\"\"\n                )\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        response = self.llm.generate(prompt, output_len)\n\n    if not self.llm.get_stream() or response.cached and not settings.quiet:\n        # we would have already displayed the msg \"live\" ONLY if\n        # streaming was enabled, AND we did not find a cached response\n        # If we are here, it means the response has not yet been displayed.\n        cached = \"[red](cached)[/red]\" if response.cached else \"\"\n        console.print(f\"[green]{self.indent}\", end=\"\")\n        print(cached + \"[green]\" + escape(response.message))\n    self.update_token_usage(\n        response,\n        prompt,\n        self.llm.get_stream(),\n        chat=False,  # i.e. it's a completion model not chat model\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    cdoc = ChatDocument.from_LLMResponse(response, displayed=True)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    cdoc.metadata.tool_ids = (\n        [] if isinstance(message, str) else message.metadata.tool_ids\n    )\n    return cdoc\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.has_tool_message_attempt","title":"<code>has_tool_message_attempt(msg)</code>","text":"<p>Check whether msg contains a Tool/fn-call attempt (by the LLM).</p> <p>CAUTION: This uses self.get_tool_messages(msg) which as a side-effect may update msg.tool_messages when msg is a ChatDocument, if there are any tools in msg.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_tool_message_attempt(self, msg: str | ChatDocument | None) -&gt; bool:\n    \"\"\"\n    Check whether msg contains a Tool/fn-call attempt (by the LLM).\n\n    CAUTION: This uses self.get_tool_messages(msg) which as a side-effect\n    may update msg.tool_messages when msg is a ChatDocument, if there are\n    any tools in msg.\n    \"\"\"\n    if msg is None:\n        return False\n    if isinstance(msg, ChatDocument):\n        if len(msg.tool_messages) &gt; 0:\n            return True\n        if msg.metadata.sender != Entity.LLM:\n            return False\n    try:\n        tools = self.get_tool_messages(msg)\n        return len(tools) &gt; 0\n    except (ValidationError, XMLException):\n        # there is a tool/fn-call attempt but had a validation error,\n        # so we still consider this a tool message \"attempt\"\n        return True\n    return False\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.has_only_unhandled_tools","title":"<code>has_only_unhandled_tools(msg)</code>","text":"<p>Does the msg have at least one tool, and none of the tools in the msg are handleable by this agent?</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def has_only_unhandled_tools(self, msg: str | ChatDocument) -&gt; bool:\n    \"\"\"\n    Does the msg have at least one tool, and none of the tools in the msg are\n    handleable by this agent?\n    \"\"\"\n    if msg is None:\n        return False\n    tools = self.try_get_tool_messages(msg, all_tools=True)\n    if len(tools) == 0:\n        return False\n    return all(not self._tool_recipient_match(t) for t in tools)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.get_tool_messages","title":"<code>get_tool_messages(msg, all_tools=False)</code>","text":"<p>Get ToolMessages recognized in msg, handle-able by this agent. NOTE: as a side-effect, this will update msg.tool_messages when msg is a ChatDocument and msg contains tool messages.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the message to extract tools from.</p> required <code>all_tools</code> <code>bool</code> <ul> <li>if True, return all tools,     i.e. any recognized tool in self.llm_tools_known,     whether it is handled by this agent or not;</li> <li>otherwise, return only the tools handled by this agent.</li> </ul> <code>False</code> <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_tool_messages(\n    self,\n    msg: str | ChatDocument | None,\n    all_tools: bool = False,\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Get ToolMessages recognized in msg, handle-able by this agent.\n    NOTE: as a side-effect, this will update msg.tool_messages\n    when msg is a ChatDocument and msg contains tool messages.\n\n    Args:\n        msg (str|ChatDocument): the message to extract tools from.\n        all_tools (bool):\n            - if True, return all tools,\n                i.e. any recognized tool in self.llm_tools_known,\n                whether it is handled by this agent or not;\n            - otherwise, return only the tools handled by this agent.\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n\n    if msg is None:\n        return []\n\n    if isinstance(msg, str):\n        json_tools = self.get_formatted_tool_messages(msg)\n        if all_tools:\n            return json_tools\n        else:\n            return [\n                t\n                for t in json_tools\n                if self._tool_recipient_match(t) and t.default_value(\"request\")\n            ]\n\n    if len(msg.tool_messages) &gt; 0:\n        # We've already found tool_messages,\n        # (either via OpenAI Fn-call or Langroid-native ToolMessage);\n        # or they were added by an agent_response.\n        # note these could be from a forwarded msg from another agent,\n        # so return ONLY the messages THIS agent to enabled to handle.\n        if all_tools:\n            return msg.tool_messages\n        return [t for t in msg.tool_messages if self._tool_recipient_match(t)]\n\n    if (\n        msg.all_tool_messages is not None\n        and msg.all_tool_messages_agent_id == self.id\n    ):\n        # We've already identified all_tool_messages in the msg by this same agent;\n        # so use them to return the corresponding ToolMessage objects\n        if all_tools:\n            return msg.all_tool_messages\n        msg.tool_messages = [\n            t for t in msg.all_tool_messages if self._tool_recipient_match(t)\n        ]\n        return msg.tool_messages\n\n    assert isinstance(msg, ChatDocument)\n    if (\n        SearchForTools.CONTENT.value in self.search_for_tools\n        and msg.content != \"\"\n        and msg.oai_tool_calls is None\n        and msg.function_call is None\n    ):\n\n        tools = self.get_formatted_tool_messages(\n            msg.content, from_llm=msg.metadata.sender == Entity.LLM\n        )\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        # filter for actually handle-able tools, and recipient is this agent\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n\n        if all_tools:\n            return tools\n        else:\n            return my_tools\n\n    # otherwise, we look for `tool_calls` (possibly multiple)\n    if SearchForTools.TOOLS.value in self.search_for_tools:\n        tools = self.get_oai_tool_calls_classes(msg)\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n    else:\n        tools = []\n        my_tools = []\n\n    if len(tools) == 0 and SearchForTools.FUNCTIONS.value in self.search_for_tools:\n        # otherwise, we look for a `function_call`\n        fun_call_cls = self.get_function_call_class(msg)\n        tools = [fun_call_cls] if fun_call_cls is not None else []\n        msg.all_tool_messages = tools\n        msg.all_tool_messages_agent_id = self.id\n        my_tools = [t for t in tools if self._tool_recipient_match(t)]\n        msg.tool_messages = my_tools\n    if all_tools:\n        return tools\n    else:\n        return my_tools\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.get_formatted_tool_messages","title":"<code>get_formatted_tool_messages(input_str, from_llm=True)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to tool-formatted substrings, if any. ASSUMPTION - These tools are either ALL JSON-based, or ALL XML-based (i.e. not a mix of both). Terminology: a \"formatted tool msg\" is one which the LLM generates as     part of its raw string output, rather than within a JSON object     in the API response (i.e. this method does not extract tools/fns returned     by OpenAI's tools/fns API or similar APIs).</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <code>from_llm</code> <code>bool</code> <p>whether the input was generated by the LLM. If so, we track malformed tool calls.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_formatted_tool_messages(\n    self, input_str: str, from_llm: bool = True\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Returns ToolMessage objects (tools) corresponding to\n    tool-formatted substrings, if any.\n    ASSUMPTION - These tools are either ALL JSON-based, or ALL XML-based\n    (i.e. not a mix of both).\n    Terminology: a \"formatted tool msg\" is one which the LLM generates as\n        part of its raw string output, rather than within a JSON object\n        in the API response (i.e. this method does not extract tools/fns returned\n        by OpenAI's tools/fns API or similar APIs).\n\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n        from_llm (bool): whether the input was generated by the LLM. If so,\n            we track malformed tool calls.\n\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\n    self.tool_error = False\n    substrings = XMLToolMessage.find_candidates(input_str)\n    is_json = False\n    if len(substrings) == 0:\n        substrings = extract_top_level_json(input_str)\n        is_json = len(substrings) &gt; 0\n        if not is_json:\n            return []\n\n    results = [self._get_one_tool_message(j, is_json, from_llm) for j in substrings]\n    valid_results = [r for r in results if r is not None]\n    # If any tool is correctly formed we do not set the flag\n    if len(valid_results) &gt; 0:\n        self.tool_error = False\n    return valid_results\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.get_function_call_class","title":"<code>get_function_call_class(msg)</code>","text":"<p>From ChatDocument (constructed from an LLM Response), get the <code>ToolMessage</code> corresponding to the <code>function_call</code> if it exists.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_function_call_class(self, msg: ChatDocument) -&gt; Optional[ToolMessage]:\n    \"\"\"\n    From ChatDocument (constructed from an LLM Response), get the `ToolMessage`\n    corresponding to the `function_call` if it exists.\n    \"\"\"\n    if msg.function_call is None:\n        return None\n    tool_name = msg.function_call.name\n    tool_msg = msg.function_call.arguments or {}\n    self.tool_error = False\n    if tool_name not in self.llm_tools_handled:\n        logger.warning(\n            f\"\"\"\n            The function_call '{tool_name}' is not handled\n            by the agent named '{self.config.name}'!\n            If you intended this agent to handle this function_call,\n            either the fn-call name is incorrectly generated by the LLM,\n            (in which case you may need to adjust your LLM instructions),\n            or you need to enable this agent to handle this fn-call.\n            \"\"\"\n        )\n        if (\n            tool_name not in self.all_llm_tools_known\n            and msg.metadata.sender == Entity.LLM\n        ):\n            self.tool_error = True\n        return None\n    tool_class = self.llm_tools_map[tool_name]\n    tool_msg.update(dict(request=tool_name))\n    try:\n        tool = tool_class.model_validate(tool_msg)\n    except ValidationError as ve:\n        # Store tool class as an attribute on the exception\n        ve.tool_class = tool_class  # type: ignore\n        raise ve\n    return tool\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.get_oai_tool_calls_classes","title":"<code>get_oai_tool_calls_classes(msg)</code>","text":"<p>From ChatDocument (constructed from an LLM Response), get  a list of ToolMessages corresponding to the <code>tool_calls</code>, if any.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_oai_tool_calls_classes(self, msg: ChatDocument) -&gt; List[ToolMessage]:\n    \"\"\"\n    From ChatDocument (constructed from an LLM Response), get\n     a list of ToolMessages corresponding to the `tool_calls`, if any.\n    \"\"\"\n\n    if msg.oai_tool_calls is None:\n        return []\n    tools = []\n    all_errors = True\n    for tc in msg.oai_tool_calls:\n        if tc.function is None:\n            continue\n        tool_name = tc.function.name\n        tool_msg = tc.function.arguments or {}\n        if tool_name not in self.llm_tools_handled:\n            logger.warning(\n                f\"\"\"\n                The tool_call '{tool_name}' is not handled\n                by the agent named '{self.config.name}'!\n                If you intended this agent to handle this function_call,\n                either the fn-call name is incorrectly generated by the LLM,\n                (in which case you may need to adjust your LLM instructions),\n                or you need to enable this agent to handle this fn-call.\n                \"\"\"\n            )\n            continue\n        all_errors = False\n        tool_class = self.llm_tools_map[tool_name]\n        tool_msg.update(dict(request=tool_name))\n        try:\n            tool = tool_class.model_validate(tool_msg)\n        except ValidationError as ve:\n            # Store tool class as an attribute on the exception\n            ve.tool_class = tool_class  # type: ignore\n            raise ve\n        tool.id = tc.id or \"\"\n        tools.append(tool)\n    # When no tool is valid and the message was produced\n    # by the LLM, set the recovery flag\n    self.tool_error = all_errors and msg.metadata.sender == Entity.LLM\n    return tools\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.tool_validation_error","title":"<code>tool_validation_error(ve, tool_class=None)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields. Args:     ve (ValidationError): The exception raised     tool_class (Optional[Type[ToolMessage]]): The tool class that         failed validation</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(\n    self, ve: ValidationError, tool_class: Optional[Type[ToolMessage]] = None\n) -&gt; str:\n    \"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        ve (ValidationError): The exception raised\n        tool_class (Optional[Type[ToolMessage]]): The tool class that\n            failed validation\n\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\n    # First try to get tool class from the exception itself\n    if hasattr(ve, \"tool_class\") and ve.tool_class:\n        tool_name = ve.tool_class.default_value(\"request\")  # type: ignore\n    elif tool_class is not None:\n        tool_name = tool_class.default_value(\"request\")\n    else:\n        # Fallback: try to extract from error context if available\n        tool_name = \"Unknown Tool\"\n    bad_field_errors = \"\\n\".join(\n        [f\"{e['loc']}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n    )\n    return f\"\"\"\n    There were one or more errors in your attempt to use the\n    TOOL or function_call named '{tool_name}':\n    {bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message_async","title":"<code>handle_message_async(msg)</code>  <code>async</code>","text":"<p>Asynch version of <code>handle_message</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def handle_message_async(\n    self, msg: str | ChatDocument\n) -&gt; None | str | OrderedDict[str, str] | ChatDocument:\n    \"\"\"\n    Asynch version of `handle_message`. See there for details.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n        tools = [t for t in tools if self._tool_recipient_match(t)]\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except XMLException as xe:  # from XMLToolMessage parsing\n        return str(xe)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) &gt; 1 and not self.config.allow_multiple_tools:\n        return self.to_ChatDocument(\"ERROR: Use ONE tool at a time!\")\n    if len(tools) == 0:\n        fallback_result = self.handle_message_fallback(msg)\n        if fallback_result is None:\n            return None\n        return self.to_ChatDocument(\n            fallback_result,\n            chat_doc=msg if isinstance(msg, ChatDocument) else None,\n        )\n    chat_doc = msg if isinstance(msg, ChatDocument) else None\n\n    results = self._get_multiple_orch_tool_errs(tools)\n    if not results:\n        results = [\n            await self.handle_tool_message_async(t, chat_doc=chat_doc)\n            for t in tools\n        ]\n        # if there's a solitary ChatDocument|str result, return it as is\n        if len(results) == 1 and isinstance(results[0], (str, ChatDocument)):\n            return results[0]\n\n    return self._handle_message_final(tools, results)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valid \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>None | str | OrderedDict[str, str] | ChatDocument</code> <p>The result of the handler method can be: - None if no tools successfully handled, or no tools present - str if langroid-native JSON tools were handled, and results concatenated,  OR there's a SINGLE OpenAI tool-call. (We do this so the common scenario of a single tool/fn-call  has a simple behavior). - Dict[str, str] if multiple OpenAI tool-calls were handled  (dict is an id-&gt;result map) - ChatDocument if a handler returned a ChatDocument, intended to be the  final response of the <code>agent_response</code> method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(\n    self, msg: str | ChatDocument\n) -&gt; None | str | OrderedDict[str, str] | ChatDocument:\n    \"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valid \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n\n    Returns:\n        The result of the handler method can be:\n         - None if no tools successfully handled, or no tools present\n         - str if langroid-native JSON tools were handled, and results concatenated,\n             OR there's a SINGLE OpenAI tool-call.\n            (We do this so the common scenario of a single tool/fn-call\n             has a simple behavior).\n         - Dict[str, str] if multiple OpenAI tool-calls were handled\n             (dict is an id-&gt;result map)\n         - ChatDocument if a handler returned a ChatDocument, intended to be the\n             final response of the `agent_response` method.\n    \"\"\"\n    try:\n        tools = self.get_tool_messages(msg)\n        tools = [t for t in tools if self._tool_recipient_match(t)]\n    except ValidationError as ve:\n        # correct tool name but bad fields\n        return self.tool_validation_error(ve)\n    except XMLException as xe:  # from XMLToolMessage parsing\n        return str(xe)\n    except ValueError:\n        # invalid tool name\n        # We return None since returning \"invalid tool name\" would\n        # be considered a valid result in task loop, and would be treated\n        # as a response to the tool message even though the tool was not intended\n        # for this agent.\n        return None\n    if len(tools) == 0:\n        fallback_result = self.handle_message_fallback(msg)\n        if fallback_result is None:\n            return None\n        return self.to_ChatDocument(\n            fallback_result,\n            chat_doc=msg if isinstance(msg, ChatDocument) else None,\n        )\n\n    results: List[str | ChatDocument | None] = []\n    if len(tools) &gt; 1 and not self.config.allow_multiple_tools:\n        results = [\"ERROR: Use ONE tool at a time!\"] * len(tools)\n    if not results:\n        results = self._get_multiple_orch_tool_errs(tools)\n    if not results:\n        chat_doc = msg if isinstance(msg, ChatDocument) else None\n        results = [self.handle_tool_message(t, chat_doc=chat_doc) for t in tools]\n        # if there's a solitary ChatDocument|str result, return it as is\n        if len(results) == 1 and isinstance(results[0], (str, ChatDocument)):\n            return results[0]\n\n    return self._handle_message_final(tools, results)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method for the case where the msg has no tools that can be handled by this agent. This method can be overridden by subclasses, e.g., to create a \"reminder\" message when a tool is expected but the LLM \"forgot\" to generate one.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     Any: The result of the handler method</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(self, msg: str | ChatDocument) -&gt; Any:\n    \"\"\"\n    Fallback method for the case where the msg has no tools that\n    can be handled by this agent.\n    This method can be overridden by subclasses, e.g.,\n    to create a \"reminder\" message when a tool is expected but the LLM \"forgot\"\n    to generate one.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        Any: The result of the handler method\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.to_ChatDocument","title":"<code>to_ChatDocument(msg, orig_tool_name=None, chat_doc=None, author_entity=Entity.AGENT)</code>","text":"<p>Convert result of a responder (agent_response or llm_response, or task.run()), or tool handler, or handle_message_fallback, to a ChatDocument, to enable handling by other responders/tasks in a task loop possibly involving multiple agents.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Any</code> <p>The result of a responder or tool handler or task.run()</p> required <code>orig_tool_name</code> <code>str</code> <p>The original tool name that generated the response, if any.</p> <code>None</code> <code>chat_doc</code> <code>ChatDocument</code> <p>The original ChatDocument object that <code>msg</code> is a response to.</p> <code>None</code> <code>author_entity</code> <code>Entity</code> <p>The intended author of the result ChatDocument</p> <code>AGENT</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def to_ChatDocument(\n    self,\n    msg: Any,\n    orig_tool_name: str | None = None,\n    chat_doc: Optional[ChatDocument] = None,\n    author_entity: Entity = Entity.AGENT,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Convert result of a responder (agent_response or llm_response, or task.run()),\n    or tool handler, or handle_message_fallback,\n    to a ChatDocument, to enable handling by other\n    responders/tasks in a task loop possibly involving multiple agents.\n\n    Args:\n        msg (Any): The result of a responder or tool handler or task.run()\n        orig_tool_name (str): The original tool name that generated the response,\n            if any.\n        chat_doc (ChatDocument): The original ChatDocument object that `msg`\n            is a response to.\n        author_entity (Entity): The intended author of the result ChatDocument\n    \"\"\"\n    if msg is None or isinstance(msg, ChatDocument):\n        return msg\n\n    is_agent_author = author_entity == Entity.AGENT\n\n    if isinstance(msg, str):\n        return self.response_template(author_entity, content=msg, content_any=msg)\n    elif isinstance(msg, ToolMessage):\n        # result is a ToolMessage, so...\n        result_tool_name = msg.default_value(\"request\")\n        if (\n            is_agent_author\n            and result_tool_name in self.llm_tools_handled\n            and (orig_tool_name is None or orig_tool_name != result_tool_name)\n        ):\n            # TODO: do we need to remove the tool message from the chat_doc?\n            # if (chat_doc is not None and\n            #     msg in chat_doc.tool_messages):\n            #    chat_doc.tool_messages.remove(msg)\n            # if we can handle it, do so\n            result = self.handle_tool_message(msg, chat_doc=chat_doc)\n            if result is not None and isinstance(result, ChatDocument):\n                return result\n        else:\n            # else wrap it in an agent response and return it so\n            # orchestrator can find a respondent\n            return self.response_template(author_entity, tool_messages=[msg])\n    else:\n        result = to_string(msg)\n\n    return (\n        None\n        if result is None\n        else self.response_template(author_entity, content=result, content_any=msg)\n    )\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.from_ChatDocument","title":"<code>from_ChatDocument(msg, output_type)</code>","text":"<p>Extract a desired output_type from a ChatDocument object. We use this fallback order: - if <code>msg.content_any</code> exists and matches the output_type, return it - if <code>msg.content</code> exists and output_type is str return it - if output_type is a ToolMessage, return the first tool in <code>msg.tool_messages</code> - if output_type is a list of ToolMessage,     return all tools in <code>msg.tool_messages</code> - search for a tool in <code>msg.tool_messages</code> that has a field of output_type,      and if found, return that field value - return None if all the above fail</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def from_ChatDocument(self, msg: ChatDocument, output_type: Type[T]) -&gt; Optional[T]:\n    \"\"\"\n    Extract a desired output_type from a ChatDocument object.\n    We use this fallback order:\n    - if `msg.content_any` exists and matches the output_type, return it\n    - if `msg.content` exists and output_type is str return it\n    - if output_type is a ToolMessage, return the first tool in `msg.tool_messages`\n    - if output_type is a list of ToolMessage,\n        return all tools in `msg.tool_messages`\n    - search for a tool in `msg.tool_messages` that has a field of output_type,\n         and if found, return that field value\n    - return None if all the above fail\n    \"\"\"\n    content = msg.content\n    if output_type is str and content != \"\":\n        return cast(T, content)\n    content_any = msg.content_any\n    if content_any is not None and isinstance(content_any, output_type):\n        return cast(T, content_any)\n\n    tools = self.try_get_tool_messages(msg, all_tools=True)\n\n    if get_origin(output_type) is list:\n        list_element_type = get_args(output_type)[0]\n        if issubclass(list_element_type, ToolMessage):\n            # list_element_type is a subclass of ToolMessage:\n            # We output a list of objects derived from list_element_type\n            return cast(\n                T,\n                [t for t in tools if isinstance(t, list_element_type)],\n            )\n    elif get_origin(output_type) is None and issubclass(output_type, ToolMessage):\n        # output_type is a subclass of ToolMessage:\n        # return the first tool that has this specific output_type\n        for tool in tools:\n            if isinstance(tool, output_type):\n                return cast(T, tool)\n        return None\n    elif get_origin(output_type) is None and output_type in (str, int, float, bool):\n        # attempt to get the output_type from the content,\n        # if it's a primitive type\n        primitive_value = from_string(content, output_type)  # type: ignore\n        if primitive_value is not None:\n            return cast(T, primitive_value)\n\n    # then search for output_type as a field in a tool\n    for tool in tools:\n        value = tool.get_value_of_type(output_type)\n        if value is not None:\n            return cast(T, value)\n    return None\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_tool_message_async","title":"<code>handle_tool_message_async(tool, chat_doc=None)</code>  <code>async</code>","text":"<p>Asynch version of <code>handle_tool_message</code>. See there for details.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>async def handle_tool_message_async(\n    self,\n    tool: ToolMessage,\n    chat_doc: Optional[ChatDocument] = None,\n) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Asynch version of `handle_tool_message`. See there for details.\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    if hasattr(tool, \"_handler\"):\n        handler_name = getattr(tool, \"_handler\", tool_name)\n    else:\n        handler_name = tool_name\n    handler_method = getattr(self, handler_name + \"_async\", None)\n    if handler_method is None:\n        return self.handle_tool_message(tool, chat_doc=chat_doc)\n    has_chat_doc_arg = (\n        chat_doc is not None\n        and \"chat_doc\" in inspect.signature(handler_method).parameters\n    )\n    try:\n        if has_chat_doc_arg:\n            maybe_result = await handler_method(tool, chat_doc=chat_doc)\n        else:\n            maybe_result = await handler_method(tool)\n        result = self.to_ChatDocument(maybe_result, tool_name, chat_doc)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return self._maybe_truncate_result(\n        result, tool._max_result_tokens\n    )  # type: ignore\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_tool_message","title":"<code>handle_tool_message(tool, chat_doc=None)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object. Args:     tool: ToolMessage object representing the tool request.     chat_doc: Optional ChatDocument object containing the tool request.         This is passed to the tool-handler method only if it has a <code>chat_doc</code>         argument.</p> <p>Returns:</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(\n    self,\n    tool: ToolMessage,\n    chat_doc: Optional[ChatDocument] = None,\n) -&gt; None | str | ChatDocument:\n    \"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n        chat_doc: Optional ChatDocument object containing the tool request.\n            This is passed to the tool-handler method only if it has a `chat_doc`\n            argument.\n\n    Returns:\n\n    \"\"\"\n    tool_name = tool.default_value(\"request\")\n    if hasattr(tool, \"_handler\"):\n        handler_name = getattr(tool, \"_handler\", tool_name)\n    else:\n        handler_name = tool_name\n    handler_method = getattr(self, handler_name, None)\n    if handler_method is None:\n        return None\n    has_chat_doc_arg = (\n        chat_doc is not None\n        and \"chat_doc\" in inspect.signature(handler_method).parameters\n    )\n    try:\n        if has_chat_doc_arg:\n            maybe_result = handler_method(tool, chat_doc=chat_doc)\n        else:\n            maybe_result = handler_method(tool)\n        result = self.to_ChatDocument(maybe_result, tool_name, chat_doc)\n    except Exception as e:\n        # raise the error here since we are sure it's\n        # not a pydantic validation error,\n        # which we check in `handle_message`\n        raise e\n    return self._maybe_truncate_result(\n        result, tool._max_result_tokens\n    )  # type: ignore\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.update_token_usage","title":"<code>update_token_usage(response, prompt, stream, chat=True, print_response_stats=True)</code>","text":"<p>Updates <code>response.usage</code> obj (token usage and cost fields) if needed. An update is needed only if: - stream is True (i.e. streaming was enabled), and - the response was NOT obtained from cached, and - the API did NOT provide the usage/cost fields during streaming   (As of Sep 2024, the OpenAI API started providing these; for other APIs     this may not necessarily be the case).</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>LLMResponse</code> <p>LLMResponse object</p> required <code>prompt</code> <code>str | List[LLMMessage]</code> <p>prompt or list of LLMMessage objects</p> required <code>stream</code> <code>bool</code> <p>whether to update the usage in the response object if the response is not cached.</p> required <code>chat</code> <code>bool</code> <p>whether this is a chat model or a completion model</p> <code>True</code> <code>print_response_stats</code> <code>bool</code> <p>whether to print the response stats</p> <code>True</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def update_token_usage(\n    self,\n    response: LLMResponse,\n    prompt: str | List[LLMMessage],\n    stream: bool,\n    chat: bool = True,\n    print_response_stats: bool = True,\n) -&gt; None:\n    \"\"\"\n    Updates `response.usage` obj (token usage and cost fields) if needed.\n    An update is needed only if:\n    - stream is True (i.e. streaming was enabled), and\n    - the response was NOT obtained from cached, and\n    - the API did NOT provide the usage/cost fields during streaming\n      (As of Sep 2024, the OpenAI API started providing these; for other APIs\n        this may not necessarily be the case).\n\n    Args:\n        response (LLMResponse): LLMResponse object\n        prompt (str | List[LLMMessage]): prompt or list of LLMMessage objects\n        stream (bool): whether to update the usage in the response object\n            if the response is not cached.\n        chat (bool): whether this is a chat model or a completion model\n        print_response_stats (bool): whether to print the response stats\n    \"\"\"\n    if response is None or self.llm is None:\n        return\n\n    no_usage_info = response.usage is None or response.usage.prompt_tokens == 0\n    # Note: If response was not streamed, then\n    # `response.usage` would already have been set by the API,\n    # so we only need to update in the stream case.\n    if stream and no_usage_info:\n        # usage, cost = 0 when response is from cache\n        prompt_tokens = 0\n        completion_tokens = 0\n        cost = 0.0\n        if not response.cached:\n            prompt_tokens = self.num_tokens(prompt)\n            completion_tokens = self.num_tokens(response.message)\n            if response.function_call is not None:\n                completion_tokens += self.num_tokens(str(response.function_call))\n            cost = self.compute_token_cost(prompt_tokens, 0, completion_tokens)\n        response.usage = LLMTokenUsage(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            cost=cost,\n        )\n\n    # update total counters\n    if response.usage is not None:\n        self.total_llm_token_cost += response.usage.cost\n        self.total_llm_token_usage += response.usage.total_tokens\n        self.llm.update_usage_cost(\n            chat,\n            response.usage.prompt_tokens,\n            response.usage.completion_tokens,\n            response.usage.cost,\n        )\n        chat_length = 1 if isinstance(prompt, str) else len(prompt)\n        self.token_stats_str = self._get_response_stats(\n            chat_length, self.total_llm_token_cost, response\n        )\n        if print_response_stats:\n            print(self.indent + self.token_stats_str)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and <code>RecipientTool</code> to address requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>user_confirm</code> <code>bool</code> <p>whether to gate the request with a human confirmation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\n    self,\n    agent: \"Agent\",\n    request: str,\n    no_answer: str = NO_ANSWER,\n    user_confirm: bool = True,\n) -&gt; Optional[str]:\n    \"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and\n    `RecipientTool` to address requests to other agents. It is generally best to\n    avoid using this method.\n\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer (str): expected response when agent does not know the answer\n        user_confirm (bool): whether to gate the request with a human confirmation\n\n    Returns:\n        str: response from agent\n    \"\"\"\n    agent_type = type(agent).__name__\n    if user_confirm:\n        user_response = Prompt.ask(\n            f\"\"\"[magenta]Here is the request or message:\n            {request}\n            Should I forward this to {agent_type}?\"\"\",\n            default=\"y\",\n            choices=[\"y\", \"n\"],\n        )\n        if user_response not in [\"y\", \"yes\"]:\n            return None\n    answer = agent.llm_response(request)\n    if answer != no_answer:\n        return (f\"{agent_type} says: \" + str(answer)).strip()\n    return None\n</code></pre>"},{"location":"reference/agent/batch/","title":"batch","text":"<p>langroid/agent/batch.py </p>"},{"location":"reference/agent/batch/#langroid.agent.batch.ExceptionHandling","title":"<code>ExceptionHandling</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for exception handling options.</p>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batched_tasks","title":"<code>run_batched_tasks(inputs, do_task, batch_size, stop_on_first_result, sequential, handle_exceptions, output_map, message_template, message=None)</code>","text":"<p>Common batch processing logic for both agent methods and tasks.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str | ChatDocument]</code> <p>List of inputs to process</p> required <code>do_task</code> <code>Callable[[str | ChatDocument, int], Coroutine[Any, Any, Any]]</code> <p>Task execution function</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Size of batches, if None process all at once</p> required <code>stop_on_first_result</code> <code>bool</code> <p>Whether to stop after first valid result</p> required <code>sequential</code> <code>bool</code> <p>Whether to process sequentially</p> required <code>handle_exceptions</code> <code>Union[bool, ExceptionHandling]</code> <p>How to handle exceptions: - RAISE or False: Let exceptions propagate - RETURN_NONE or True: Convert exceptions to None in results - RETURN_EXCEPTION: Include exception objects in results Boolean values are deprecated and will be removed in a future version.</p> required <code>output_map</code> <code>Callable[[Any], Any]</code> <p>Function to map results</p> required <code>message_template</code> <code>str</code> <p>Template for status message</p> required <code>message</code> <code>Optional[str]</code> <p>Optional override for status message</p> <code>None</code> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batched_tasks(\n    inputs: List[str | ChatDocument],\n    do_task: Callable[[str | ChatDocument, int], Coroutine[Any, Any, Any]],\n    batch_size: Optional[int],\n    stop_on_first_result: bool,\n    sequential: bool,\n    handle_exceptions: Union[bool, ExceptionHandling],\n    output_map: Callable[[Any], Any],\n    message_template: str,\n    message: Optional[str] = None,\n) -&gt; List[Any]:\n    \"\"\"\n    Common batch processing logic for both agent methods and tasks.\n\n    Args:\n        inputs: List of inputs to process\n        do_task: Task execution function\n        batch_size: Size of batches, if None process all at once\n        stop_on_first_result: Whether to stop after first valid result\n        sequential: Whether to process sequentially\n        handle_exceptions: How to handle exceptions:\n            - RAISE or False: Let exceptions propagate\n            - RETURN_NONE or True: Convert exceptions to None in results\n            - RETURN_EXCEPTION: Include exception objects in results\n            Boolean values are deprecated and will be removed in a future version.\n        output_map: Function to map results\n        message_template: Template for status message\n        message: Optional override for status message\n    \"\"\"\n\n    async def run_all_batched_tasks(\n        inputs: List[str | ChatDocument],\n        batch_size: int | None,\n    ) -&gt; List[Any]:\n        \"\"\"Extra wrap to run asyncio.run one single time and not once per loop\n\n        Args:\n            inputs (List[str  |  ChatDocument]): inputs to process\n            batch_size (int | None): batch size\n\n        Returns:\n            List[Any]: results\n        \"\"\"\n        results: List[Any] = []\n        if batch_size is None:\n            msg = message or message_template.format(total=len(inputs))\n            with status(msg), SuppressLoggerWarnings():\n                results = await _process_batch_async(\n                    inputs,\n                    do_task,\n                    stop_on_first_result=stop_on_first_result,\n                    sequential=sequential,\n                    handle_exceptions=handle_exceptions,\n                    output_map=output_map,\n                )\n        else:\n            batches = batched(inputs, batch_size)\n            for batch in batches:\n                start_idx = len(results)\n                complete_str = f\", {start_idx} complete\" if start_idx &gt; 0 else \"\"\n                msg = (\n                    message or message_template.format(total=len(inputs)) + complete_str\n                )\n\n                if stop_on_first_result and any(r is not None for r in results):\n                    results.extend([None] * len(batch))\n                else:\n                    with status(msg), SuppressLoggerWarnings():\n                        results.extend(\n                            await _process_batch_async(\n                                batch,\n                                do_task,\n                                start_idx=start_idx,\n                                stop_on_first_result=stop_on_first_result,\n                                sequential=sequential,\n                                handle_exceptions=handle_exceptions,\n                                output_map=output_map,\n                            )\n                        )\n        return results\n\n    return asyncio.run(run_all_batched_tasks(inputs, batch_size))\n</code></pre>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batch_task_gen","title":"<code>run_batch_task_gen(gen_task, items, input_map=lambda x: str(x), output_map=lambda x: x, stop_on_first_result=False, sequential=True, batch_size=None, turns=-1, message=None, handle_exceptions=ExceptionHandling.RAISE, max_cost=0.0, max_tokens=0)</code>","text":"<p>Generate and run copies of a task async/concurrently one per item in <code>items</code> list. For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result. Args:     gen_task (Callable[[int], Task]): generates the tasks to run     items (list[T]): list of items to process     input_map (Callable[[T], str|ChatDocument]): function to map item to         initial message to process     output_map (Callable[[ChatDocument|str], U]): function to map result         to final result. If stop_on_first_result is enabled, then         map any invalid output to None. We continue until some non-None         result is obtained.     stop_on_first_result (bool): whether to stop after the first valid         (not-None) result. In this case all other tasks are         cancelled, and their corresponding result is None in the         returned list.     sequential (bool): whether to run sequentially         (e.g. some APIs such as ooba don't support concurrent requests)     batch_size (Optional[int]): The number of tasks to run at a time,         if None, unbatched     turns (int): number of turns to run, -1 for infinite     message (Optional[str]): optionally overrides the console status messages     handle_exceptions: How to handle exceptions:         - RAISE or False: Let exceptions propagate         - RETURN_NONE or True: Convert exceptions to None in results         - RETURN_EXCEPTION: Include exception objects in results         Boolean values are deprecated and will be removed in a future version.     max_cost: float: maximum cost to run the task (default 0.0 for unlimited)     max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)</p> <p>Returns:</p> Type Description <code>list[Optional[U]]</code> <p>list[Optional[U]]: list of final results. Always list[U] if</p> <code>list[Optional[U]]</code> <p><code>stop_on_first_result</code> is disabled</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_task_gen(\n    gen_task: Callable[[int], Task],\n    items: list[T],\n    input_map: Callable[[T], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], U] = lambda x: x,  # type: ignore\n    stop_on_first_result: bool = False,\n    sequential: bool = True,\n    batch_size: Optional[int] = None,\n    turns: int = -1,\n    message: Optional[str] = None,\n    handle_exceptions: Union[bool, ExceptionHandling] = ExceptionHandling.RAISE,\n    max_cost: float = 0.0,\n    max_tokens: int = 0,\n) -&gt; list[Optional[U]]:\n    \"\"\"\n    Generate and run copies of a task async/concurrently one per item in `items` list.\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n    Args:\n        gen_task (Callable[[int], Task]): generates the tasks to run\n        items (list[T]): list of items to process\n        input_map (Callable[[T], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], U]): function to map result\n            to final result. If stop_on_first_result is enabled, then\n            map any invalid output to None. We continue until some non-None\n            result is obtained.\n        stop_on_first_result (bool): whether to stop after the first valid\n            (not-None) result. In this case all other tasks are\n            cancelled, and their corresponding result is None in the\n            returned list.\n        sequential (bool): whether to run sequentially\n            (e.g. some APIs such as ooba don't support concurrent requests)\n        batch_size (Optional[int]): The number of tasks to run at a time,\n            if None, unbatched\n        turns (int): number of turns to run, -1 for infinite\n        message (Optional[str]): optionally overrides the console status messages\n        handle_exceptions: How to handle exceptions:\n            - RAISE or False: Let exceptions propagate\n            - RETURN_NONE or True: Convert exceptions to None in results\n            - RETURN_EXCEPTION: Include exception objects in results\n            Boolean values are deprecated and will be removed in a future version.\n        max_cost: float: maximum cost to run the task (default 0.0 for unlimited)\n        max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)\n\n\n    Returns:\n        list[Optional[U]]: list of final results. Always list[U] if\n        `stop_on_first_result` is disabled\n    \"\"\"\n    inputs = [input_map(item) for item in items]\n\n    async def _do_task(\n        input: str | ChatDocument,\n        i: int,\n    ) -&gt; BaseException | Optional[ChatDocument] | tuple[int, Optional[ChatDocument]]:\n        task_i = gen_task(i)\n        if task_i.agent.llm is not None:\n            task_i.agent.llm.set_stream(False)\n        task_i.agent.config.show_stats = False\n\n        try:\n            result = await task_i.run_async(\n                input, turns=turns, max_cost=max_cost, max_tokens=max_tokens\n            )\n        except asyncio.CancelledError as e:\n            task_i.kill()\n            # exception will be handled by the caller\n            raise e\n        # ----------------------------------------\n        # Propagate any exception stored on the task that may have been\n        # swallowed inside `Task.run_async`, so that the upper-level\n        # exception-handling logic works as expected.\n        for attr in (\"_exception\", \"last_exception\", \"exception\"):\n            exc = getattr(task_i, attr, None)\n            if isinstance(exc, BaseException):\n                raise exc\n        # Fallback: treat a KILL-status result as an error\n        if (\n            isinstance(result, ChatDocument)\n            and getattr(result, \"status\", None) is not None\n            and str(getattr(result, \"status\")) == \"StatusCode.KILL\"\n        ):\n            raise RuntimeError(str(result.content))\n        return result\n\n    return run_batched_tasks(\n        inputs=inputs,\n        do_task=_do_task,\n        batch_size=batch_size,\n        stop_on_first_result=stop_on_first_result,\n        sequential=sequential,\n        handle_exceptions=handle_exceptions,\n        output_map=output_map,\n        message_template=\"[bold green]Running {total} tasks:\",\n        message=message,\n    )\n</code></pre>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batch_tasks","title":"<code>run_batch_tasks(task, items, input_map=lambda x: str(x), output_map=lambda x: x, stop_on_first_result=False, sequential=True, batch_size=None, turns=-1, max_cost=0.0, max_tokens=0)</code>","text":"<p>Run copies of <code>task</code> async/concurrently one per item in <code>items</code> list. For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result. Args:     task (Task): task to run     items (list[T]): list of items to process     input_map (Callable[[T], str|ChatDocument]): function to map item to         initial message to process     output_map (Callable[[ChatDocument|str], U]): function to map result         to final result     sequential (bool): whether to run sequentially         (e.g. some APIs such as ooba don't support concurrent requests)     batch_size (Optional[int]): The number of tasks to run at a time,         if None, unbatched     turns (int): number of turns to run, -1 for infinite     max_cost: float: maximum cost to run the task (default 0.0 for unlimited)     max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)</p> <p>Returns:</p> Type Description <code>List[Optional[U]]</code> <p>list[Optional[U]]: list of final results. Always list[U] if</p> <code>List[Optional[U]]</code> <p><code>stop_on_first_result</code> is disabled</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_tasks(\n    task: Task,\n    items: list[T],\n    input_map: Callable[[T], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], U] = lambda x: x,  # type: ignore\n    stop_on_first_result: bool = False,\n    sequential: bool = True,\n    batch_size: Optional[int] = None,\n    turns: int = -1,\n    max_cost: float = 0.0,\n    max_tokens: int = 0,\n) -&gt; List[Optional[U]]:\n    \"\"\"\n    Run copies of `task` async/concurrently one per item in `items` list.\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n    Args:\n        task (Task): task to run\n        items (list[T]): list of items to process\n        input_map (Callable[[T], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], U]): function to map result\n            to final result\n        sequential (bool): whether to run sequentially\n            (e.g. some APIs such as ooba don't support concurrent requests)\n        batch_size (Optional[int]): The number of tasks to run at a time,\n            if None, unbatched\n        turns (int): number of turns to run, -1 for infinite\n        max_cost: float: maximum cost to run the task (default 0.0 for unlimited)\n        max_tokens: int: maximum token usage (in and out) (default 0 for unlimited)\n\n    Returns:\n        list[Optional[U]]: list of final results. Always list[U] if\n        `stop_on_first_result` is disabled\n    \"\"\"\n    message = f\"[bold green]Running {len(items)} copies of {task.name}...\"\n    return run_batch_task_gen(\n        lambda i: task.clone(i),\n        items,\n        input_map,\n        output_map,\n        stop_on_first_result,\n        sequential,\n        batch_size,\n        turns,\n        message,\n        max_cost=max_cost,\n        max_tokens=max_tokens,\n    )\n</code></pre>"},{"location":"reference/agent/batch/#langroid.agent.batch.run_batch_agent_method","title":"<code>run_batch_agent_method(agent, method, items, input_map=lambda x: str(x), output_map=lambda x: x, sequential=True, stop_on_first_result=False, handle_exceptions=ExceptionHandling.RAISE, batch_size=None)</code>","text":"<p>Run the <code>method</code> on copies of <code>agent</code>, async/concurrently one per item in <code>items</code> list. ASSUMPTION: The <code>method</code> is an async method and has signature:     method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None So this would typically be used for the agent's \"responder\" methods, e.g. <code>llm_response_async</code> or <code>agent_responder_async</code>.</p> <p>For each item, apply <code>input_map</code> to get the initial message to process. For each result, apply <code>output_map</code> to get the final result.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent whose method to run</p> required <code>method</code> <code>str</code> <p>Async method to run on copies of <code>agent</code>. The method is assumed to have signature: <code>method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None</code></p> required <code>input_map</code> <code>Callable[[Any], str | ChatDocument]</code> <p>function to map item to initial message to process</p> <code>lambda x: str(x)</code> <code>output_map</code> <code>Callable[[ChatDocument | str], Any]</code> <p>function to map result to final result</p> <code>lambda x: x</code> <code>sequential</code> <code>bool</code> <p>whether to run sequentially (e.g. some APIs such as ooba don't support concurrent requests)</p> <code>True</code> <code>stop_on_first_result</code> <code>bool</code> <p>whether to stop after the first valid</p> <code>False</code> <code>handle_exceptions</code> <code>Union[bool, ExceptionHandling]</code> <p>How to handle exceptions: - RAISE or False: Let exceptions propagate - RETURN_NONE or True: Convert exceptions to None in results - RETURN_EXCEPTION: Include exception objects in results Boolean values are deprecated and will be removed in a future version.</p> <code>RAISE</code> <code>batch_size</code> <code>Optional[int]</code> <p>The number of items to process in each batch. If None, process all items at once.</p> <code>None</code> <p>Returns:     List[Any]: list of final results</p> Source code in <code>langroid/agent/batch.py</code> <pre><code>def run_batch_agent_method(\n    agent: Agent,\n    method: Callable[\n        [str | ChatDocument | None], Coroutine[Any, Any, ChatDocument | None]\n    ],\n    items: List[Any],\n    input_map: Callable[[Any], str | ChatDocument] = lambda x: str(x),\n    output_map: Callable[[ChatDocument | None], Any] = lambda x: x,\n    sequential: bool = True,\n    stop_on_first_result: bool = False,\n    handle_exceptions: Union[bool, ExceptionHandling] = ExceptionHandling.RAISE,\n    batch_size: Optional[int] = None,\n) -&gt; List[Any]:\n    \"\"\"\n    Run the `method` on copies of `agent`, async/concurrently one per\n    item in `items` list.\n    ASSUMPTION: The `method` is an async method and has signature:\n        method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None\n    So this would typically be used for the agent's \"responder\" methods,\n    e.g. `llm_response_async` or `agent_responder_async`.\n\n    For each item, apply `input_map` to get the initial message to process.\n    For each result, apply `output_map` to get the final result.\n\n    Args:\n        agent (Agent): agent whose method to run\n        method (str): Async method to run on copies of `agent`.\n            The method is assumed to have signature:\n            `method(self, input: str|ChatDocument|None) -&gt; ChatDocument|None`\n        input_map (Callable[[Any], str|ChatDocument]): function to map item to\n            initial message to process\n        output_map (Callable[[ChatDocument|str], Any]): function to map result\n            to final result\n        sequential (bool): whether to run sequentially\n            (e.g. some APIs such as ooba don't support concurrent requests)\n        stop_on_first_result (bool): whether to stop after the first valid\n        handle_exceptions: How to handle exceptions:\n            - RAISE or False: Let exceptions propagate\n            - RETURN_NONE or True: Convert exceptions to None in results\n            - RETURN_EXCEPTION: Include exception objects in results\n            Boolean values are deprecated and will be removed in a future version.\n        batch_size (Optional[int]): The number of items to process in each batch.\n            If None, process all items at once.\n    Returns:\n        List[Any]: list of final results\n    \"\"\"\n    # Check if the method is async\n    method_name = method.__name__\n    if not inspect.iscoroutinefunction(method):\n        raise ValueError(f\"The method {method_name} is not async.\")\n\n    inputs = [input_map(item) for item in items]\n    agent_cfg = copy.deepcopy(agent.config)\n    assert agent_cfg.llm is not None, \"agent must have llm config\"\n    agent_cfg.llm.stream = False\n    agent_cfg.show_stats = False\n    agent_cls = type(agent)\n    agent_name = agent_cfg.name\n\n    async def _do_task(input: str | ChatDocument, i: int) -&gt; Any:\n        agent_cfg.name = f\"{agent_cfg.name}-{i}\"\n        agent_i = agent_cls(agent_cfg)\n        method_i = getattr(agent_i, method_name, None)\n        if method_i is None:\n            raise ValueError(f\"Agent {agent_name} has no method {method_name}\")\n        result = await method_i(input)\n        return result\n\n    return run_batched_tasks(\n        inputs=inputs,\n        do_task=_do_task,\n        batch_size=batch_size,\n        stop_on_first_result=stop_on_first_result,\n        sequential=sequential,\n        handle_exceptions=handle_exceptions,\n        output_map=output_map,\n        message_template=f\"[bold green]Running {{total}} copies of {agent_name}...\",\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/","title":"chat_agent","text":"<p>langroid/agent/chat_agent.py </p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>               Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent</p> <p>Attributes:</p> Name Type Description <code>system_message</code> <code>str</code> <p>system message to include in message sequence  (typically defines role and task of agent).  Used only if <code>task</code> is not specified in the constructor.</p> <code>user_message</code> <code>Optional[str]</code> <p>user message to include in message sequence.  Used only if <code>task</code> is not specified in the constructor.</p> <code>use_tools</code> <code>bool</code> <p>whether to use our own ToolMessages mechanism</p> <code>handle_llm_no_tool</code> <code>Any</code> <p>desired agent_response when LLM generates non-tool msg.</p> <code>use_functions_api</code> <code>bool</code> <p>whether to use functions/tools native to the LLM API     (e.g. OpenAI's <code>function_call</code> or <code>tool_call</code> mechanism)</p> <code>use_tools_api</code> <code>bool</code> <p>When <code>use_functions_api</code> is True, if this is also True, the OpenAI tool-call API is used, rather than the older/deprecated function-call API. However the tool-call API has some tricky aspects, hence we set this to False by default.</p> <code>strict_recovery</code> <code>bool</code> <p>whether to enable strict schema recovery when there is a tool-generation error.</p> <code>enable_orchestration_tool_handling</code> <code>bool</code> <p>whether to enable handling of orchestration tools, e.g. ForwardTool, DoneTool, PassTool, etc.</p> <code>output_format</code> <code>Optional[type]</code> <p>When supported by the LLM (certain OpenAI LLMs and local LLMs served by providers such as vLLM), ensures that the output is a JSON matching the corresponding schema via grammar-based decoding</p> <code>handle_output_format</code> <code>bool</code> <p>When <code>output_format</code> is a <code>ToolMessage</code> T, controls whether T is \"enabled for handling\".</p> <code>use_output_format</code> <code>bool</code> <p>When <code>output_format</code> is a <code>ToolMessage</code> T, controls whether T is \"enabled for use\" (by LLM) and instructions on using T are added to the system message.</p> <code>instructions_output_format</code> <code>bool</code> <p>Controls whether we generate instructions for <code>output_format</code> in the system message.</p> <code>use_tools_on_output_format</code> <code>bool</code> <p>Controls whether to automatically switch to the Langroid-native tools mechanism when <code>output_format</code> is set. Note that LLMs may generate tool calls which do not belong to <code>output_format</code> even when strict JSON mode is enabled, so this should be enabled when such tool calls are not desired.</p> <code>output_format_include_defaults</code> <code>bool</code> <p>Whether to include fields with default arguments in the output schema</p> <code>full_citations</code> <code>bool</code> <p>Whether to show source reference citation + content for each citation, or just the main reference citation.</p> <code>search_for_tools_everywhere</code> <code>bool</code> <p>Whether to search for tools everywhere, or only in specific LLM response elements based on use_tools / use_functions_api / use_tools_api config settings.</p> <code>recognize_recipient_in_content</code> <code>bool</code> <p>Whether to parse LLM response text content for recipient routing patterns, specifically: - <code>TO[&lt;recipient&gt;]:&lt;content&gt;</code> addressing format, and - JSON <code>{\"recipient\": \"&lt;name&gt;\"}</code> at the top level of the message. When False, only structured routing via function_call/tool_call <code>recipient</code> fields is recognized. Default is True. Note: this is distinct from <code>TaskConfig.recognize_string_signals</code>, which controls Task-level signals like DONE, PASS, and SEND_TO. To fully disable all text-based routing, set both to False.</p> <code>context_overflow_strategy</code> <code>Literal['truncate', 'drop_turns']</code> <p>Strategy for handling context overflow when message history exceeds model context length. Options: - \"truncate\": Truncate content of early messages (preserves all messages   but with shortened content). This maintains the message sequence. - \"drop_turns\": Drop complete conversation turns (USER + all responses   until next USER). More aggressive but cleaner for voice agents. Default is \"truncate\" for backward compatibility.</p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent","title":"<code>ChatAgent(config=ChatAgentConfig(), task=None)</code>","text":"<p>               Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with an optional \"Task Spec\", which is a sequence of <code>LLMMessage</code>s. These are used to initialize the <code>task_messages</code> of the agent. In most applications we will use a <code>ChatAgent</code> rather than a bare <code>Agent</code>. The <code>Agent</code> class mainly exists to hold various common methods and attributes. One difference between <code>ChatAgent</code> and <code>Agent</code> is that <code>ChatAgent</code>'s <code>llm_response</code> method uses \"chat mode\" API (i.e. one that takes a message sequence rather than a single message), whereas the same method in the <code>Agent</code> class uses \"completion mode\" API (i.e. one that takes a single message).</p> <pre><code>config: settings for the agent\n</code></pre> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: ChatAgentConfig = ChatAgentConfig(),\n    task: Optional[List[LLMMessage]] = None,\n):\n    \"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n\n    \"\"\"\n    super().__init__(config)\n    self.config: ChatAgentConfig = config\n    self.config._set_fn_or_tools()\n    self.message_history: List[LLMMessage] = []\n    self.init_state()\n    # An agent's \"task\" is defined by a system msg and an optional user msg;\n    # These are \"priming\" messages that kick off the agent's conversation.\n    self.system_message: str = self.config.system_message\n    self.user_message: str | None = self.config.user_message\n\n    if task is not None:\n        # if task contains a system msg, we override the config system msg\n        if len(task) &gt; 0 and task[0].role == Role.SYSTEM:\n            self.system_message = task[0].content\n        # if task contains a user msg, we override the config user msg\n        if len(task) &gt; 1 and task[1].role == Role.USER:\n            self.user_message = task[1].content\n\n    # system-level instructions for using tools/functions:\n    # We maintain these as tools/functions are enabled/disabled,\n    # and whenever an LLM response is sought, these are used to\n    # recreate the system message (via `_create_system_and_tools_message`)\n    # each time, so it reflects the current set of enabled tools/functions.\n    # (a) these are general instructions on using certain tools/functions,\n    #   if they are specified in a ToolMessage class as a classmethod `instructions`\n    self.system_tool_instructions: str = \"\"\n    # (b) these are only for the builtin in Langroid TOOLS mechanism:\n    self.system_tool_format_instructions: str = \"\"\n\n    self.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\n    self.llm_functions_handled: Set[str] = set()\n    self.llm_functions_usable: Set[str] = set()\n    self.llm_function_force: Optional[Dict[str, str]] = None\n\n    self.output_format: Optional[type[ToolMessage | BaseModel]] = None\n\n    self.saved_requests_and_tool_setings = self._requests_and_tool_settings()\n    # This variable is not None and equals a `ToolMessage` T, if and only if:\n    # (a) T has been set as the output_format of this agent, AND\n    # (b) T has been \"enabled for use\" ONLY for enforcing this output format, AND\n    # (c) T has NOT been explicitly \"enabled for use\" by this Agent.\n    self.enabled_use_output_format: Optional[type[ToolMessage]] = None\n    # As above but deals with \"enabled for handling\" instead of \"enabled for use\".\n    self.enabled_handling_output_format: Optional[type[ToolMessage]] = None\n    if config.output_format is not None:\n        self.set_output_format(config.output_format)\n    # instructions specifically related to enforcing `output_format`\n    self.output_format_instructions = \"\"\n\n    # controls whether to disable strict schemas for this agent if\n    # strict mode causes exception\n    self.disable_strict = False\n    # Tracks whether any strict tool is enabled; used to determine whether to set\n    # `self.disable_strict` on an exception\n    self.any_strict = False\n    # Tracks the set of tools on which we force-disable strict decoding\n    self.disable_strict_tools_set: set[str] = set()\n\n    # search for tools according to the agent configuration\n    if not config.search_for_tools_everywhere:\n        if config.use_functions_api:\n            if config.use_tools_api:\n                self.search_for_tools = {SearchForTools.TOOLS.value}\n            else:\n                self.search_for_tools = {SearchForTools.FUNCTIONS.value}\n        else:\n            self.search_for_tools = {SearchForTools.CONTENT.value}\n\n    if self.config.enable_orchestration_tool_handling:\n        # Only enable HANDLING by `agent_response`, NOT LLM generation of these.\n        # This is useful where tool-handlers or agent_response generate these\n        # tools, and need to be handled.\n        # We don't want enable orch tool GENERATION by default, since that\n        # might clutter-up the LLM system message unnecessarily.\n        from langroid.agent.tools.orchestration import (\n            AgentDoneTool,\n            AgentSendTool,\n            DonePassTool,\n            DoneTool,\n            ForwardTool,\n            PassTool,\n            ResultTool,\n            SendTool,\n        )\n\n        self.enable_message(ForwardTool, use=False, handle=True)\n        self.enable_message(DoneTool, use=False, handle=True)\n        self.enable_message(AgentDoneTool, use=False, handle=True)\n        self.enable_message(PassTool, use=False, handle=True)\n        self.enable_message(DonePassTool, use=False, handle=True)\n        self.enable_message(SendTool, use=False, handle=True)\n        self.enable_message(AgentSendTool, use=False, handle=True)\n        self.enable_message(ResultTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.task_messages","title":"<code>task_messages</code>  <code>property</code>","text":"<p>The task messages are the initial messages that define the task of the agent. There will be at least a system message plus possibly a user msg. Returns:     List[LLMMessage]: the task messages</p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.all_llm_tools_known","title":"<code>all_llm_tools_known</code>  <code>property</code>","text":"<p>All known tools; we include <code>output_format</code> if it is a <code>ToolMessage</code>.</p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.init_state","title":"<code>init_state()</code>","text":"<p>Initialize the state of the agent. Just conversation state here, but subclasses can override this to initialize other state.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_state(self) -&gt; None:\n    \"\"\"\n    Initialize the state of the agent. Just conversation state here,\n    but subclasses can override this to initialize other state.\n    \"\"\"\n    super().init_state()\n    self.clear_history(0)\n    self.clear_dialog()\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.from_id","title":"<code>from_id(id)</code>  <code>staticmethod</code>","text":"<p>Get an agent from its ID Args:     agent_id (str): ID of the agent Returns:     ChatAgent: The agent with the given ID</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@staticmethod\ndef from_id(id: str) -&gt; \"ChatAgent\":\n    \"\"\"\n    Get an agent from its ID\n    Args:\n        agent_id (str): ID of the agent\n    Returns:\n        ChatAgent: The agent with the given ID\n    \"\"\"\n    return cast(ChatAgent, Agent.from_id(id))\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.clone","title":"<code>clone(i=0)</code>","text":"<p>Create i'th clone of this agent, ensuring tool use/handling is cloned. Important: We assume all member variables are in the init method here and in the Agent class. TODO: We are attempting to clone an agent after its state has been changed in possibly many ways. Below is an imperfect solution. Caution advised. Revisit later.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clone(self, i: int = 0) -&gt; \"ChatAgent\":\n    \"\"\"Create i'th clone of this agent, ensuring tool use/handling is cloned.\n    Important: We assume all member variables are in the __init__ method here\n    and in the Agent class.\n    TODO: We are attempting to clone an agent after its state has been\n    changed in possibly many ways. Below is an imperfect solution. Caution advised.\n    Revisit later.\n    \"\"\"\n    agent_cls = type(self)\n    # Use model_copy to preserve Pydantic subclass types (like MockLMConfig)\n    # instead of deepcopy which loses subclass information\n    config_copy = self.config.model_copy(deep=True)\n    config_copy.name = f\"{config_copy.name}-{i}\"\n    new_agent = agent_cls(config_copy)\n    new_agent.system_tool_instructions = self.system_tool_instructions\n    new_agent.system_tool_format_instructions = self.system_tool_format_instructions\n    new_agent.llm_tools_map = self.llm_tools_map\n    new_agent.llm_tools_known = self.llm_tools_known\n    new_agent.llm_tools_handled = self.llm_tools_handled\n    new_agent.llm_tools_usable = self.llm_tools_usable\n    new_agent.llm_functions_map = self.llm_functions_map\n    new_agent.llm_functions_handled = self.llm_functions_handled\n    new_agent.llm_functions_usable = self.llm_functions_usable\n    new_agent.llm_function_force = self.llm_function_force\n    # Ensure each clone gets its own vecdb client when supported.\n    new_agent.vecdb = None if self.vecdb is None else self.vecdb.clone()\n    self._clone_extra_state(new_agent)\n    new_agent.id = ObjectRegistry.new_id()\n    if self.config.add_to_registry:\n        ObjectRegistry.register_object(new_agent)\n    return new_agent\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.clear_history","title":"<code>clear_history(start=-2, end=-1)</code>","text":"<p>Clear the message history, deleting  messages from index <code>start</code>, up to index <code>end</code>.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> <code>end</code> <code>int</code> <p>index of last message to delete; Default = -1     (i.e. delete all messages up to the last one)</p> <code>-1</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2, end: int = -1) -&gt; None:\n    \"\"\"\n    Clear the message history, deleting  messages from index `start`,\n    up to index `end`.\n\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n        end (int): index of last message to delete; Default = -1\n                (i.e. delete all messages up to the last one)\n    \"\"\"\n    n = len(self.message_history)\n    if start &lt; 0:\n        start = max(0, n + start)\n    end_ = n if end == -1 else end + 1\n    dropped = self.message_history[start:end_]\n    # consider the dropped msgs in REVERSE order, so we are\n    # carefully updating self.oai_tool_calls\n    for msg in reversed(dropped):\n        self._drop_msg_update_tool_calls(msg)\n        # clear out the chat document from the ObjectRegistry\n        ChatDocument.delete_id(msg.chat_document_id)\n    del self.message_history[start:end_]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response. Args:     message (str): user message     response: (str): LLM response</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n    \"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\n    self.message_history.extend(\n        [\n            LLMMessage(role=Role.USER, content=message),\n            LLMMessage(role=Role.ASSISTANT, content=response),\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.tool_format_rules","title":"<code>tool_format_rules()</code>","text":"<p>Specification of tool formatting rules (typically JSON-based but can be non-JSON, e.g. XMLToolMessage), based on the currently enabled usable <code>ToolMessage</code>s</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_format_rules(self) -&gt; str:\n    \"\"\"\n    Specification of tool formatting rules\n    (typically JSON-based but can be non-JSON, e.g. XMLToolMessage),\n    based on the currently enabled usable `ToolMessage`s\n\n    Returns:\n        str: formatting rules\n    \"\"\"\n    # ONLY Usable tools (i.e. LLM-generation allowed),\n    usable_tool_classes: List[Type[ToolMessage]] = [\n        t\n        for t in list(self.llm_tools_map.values())\n        if t.default_value(\"request\") in self.llm_tools_usable\n    ]\n\n    if len(usable_tool_classes) == 0:\n        return \"\"\n    format_instructions = \"\\n\\n\".join(\n        [\n            msg_cls.format_instructions(tool=self.config.use_tools)\n            for msg_cls in usable_tool_classes\n        ]\n    )\n    # if any of the enabled classes has json_group_instructions, then use that,\n    # else fall back to ToolMessage.json_group_instructions\n    for msg_cls in usable_tool_classes:\n        if hasattr(msg_cls, \"json_group_instructions\") and callable(\n            getattr(msg_cls, \"json_group_instructions\")\n        ):\n            return msg_cls.group_format_instructions().format(\n                format_instructions=format_instructions\n            )\n    return ToolMessage.group_format_instructions().format(\n        format_instructions=format_instructions\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.tool_instructions","title":"<code>tool_instructions()</code>","text":"<p>Instructions for tools or function-calls, for enabled and usable Tools. These are inserted into system prompt regardless of whether we are using our own ToolMessage mechanism or the LLM's function-call mechanism.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>concatenation of instructions for all usable tools</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def tool_instructions(self) -&gt; str:\n    \"\"\"\n    Instructions for tools or function-calls, for enabled and usable Tools.\n    These are inserted into system prompt regardless of whether we are using\n    our own ToolMessage mechanism or the LLM's function-call mechanism.\n\n    Returns:\n        str: concatenation of instructions for all usable tools\n    \"\"\"\n    enabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n    if len(enabled_classes) == 0:\n        return \"\"\n    instructions = []\n    for msg_cls in enabled_classes:\n        if msg_cls.default_value(\"request\") in self.llm_tools_usable:\n            class_instructions = \"\"\n            if hasattr(msg_cls, \"instructions\") and inspect.ismethod(\n                msg_cls.instructions\n            ):\n                class_instructions = msg_cls.instructions()\n            if (\n                self.config.use_tools\n                and hasattr(msg_cls, \"langroid_tools_instructions\")\n                and inspect.ismethod(msg_cls.langroid_tools_instructions)\n            ):\n                class_instructions += msg_cls.langroid_tools_instructions()\n            # example will be shown in tool_format_rules() when using TOOLs,\n            # so we don't need to show it here.\n            example = \"\" if self.config.use_tools else (msg_cls.usage_examples())\n            if example != \"\":\n                example = \"EXAMPLES:\\n\" + example\n            guidance = (\n                \"\"\n                if class_instructions == \"\"\n                else (\"GUIDANCE: \" + class_instructions)\n            )\n            if guidance == \"\" and example == \"\":\n                continue\n            instructions.append(\n                textwrap.dedent(\n                    f\"\"\"\n                    TOOL: {msg_cls.default_value(\"request\")}:\n                    {guidance}\n                    {example}\n                    \"\"\".lstrip()\n                )\n            )\n    if len(instructions) == 0:\n        return \"\"\n    instructions_str = \"\\n\\n\".join(instructions)\n    return textwrap.dedent(\n        f\"\"\"\n        === GUIDELINES ON SOME TOOLS/FUNCTIONS USAGE ===\n        {instructions_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.augment_system_message","title":"<code>augment_system_message(message)</code>","text":"<p>Augment the system message with the given message. Args:     message (str): system message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def augment_system_message(self, message: str) -&gt; None:\n    \"\"\"\n    Augment the system message with the given message.\n    Args:\n        message (str): system message\n    \"\"\"\n    self.system_message += \"\\n\\n\" + message\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.last_message_with_role","title":"<code>last_message_with_role(role)</code>","text":"<p>from <code>message_history</code>, return the last message with role <code>role</code></p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_with_role(self, role: Role) -&gt; LLMMessage | None:\n    \"\"\"from `message_history`, return the last message with role `role`\"\"\"\n    n_role_msgs = len([m for m in self.message_history if m.role == role])\n    if n_role_msgs == 0:\n        return None\n    idx = self.nth_message_idx_with_role(role, n_role_msgs)\n    return self.message_history[idx]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.last_message_idx_with_role","title":"<code>last_message_idx_with_role(role)</code>","text":"<p>Index of last message in message_history, with specified role. Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def last_message_idx_with_role(self, role: Role) -&gt; int:\n    \"\"\"Index of last message in message_history, with specified role.\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n    if len(indices_with_role) == 0:\n        return -1\n    return indices_with_role[-1]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.nth_message_idx_with_role","title":"<code>nth_message_idx_with_role(role, n)</code>","text":"<p>Index of <code>n</code>th message in message_history, with specified role. (n is assumed to be 1-based, i.e. 1 is the first message with that role). Return -1 if not found. Index = 0 is the first message in the history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def nth_message_idx_with_role(self, role: Role, n: int) -&gt; int:\n    \"\"\"Index of `n`th message in message_history, with specified role.\n    (n is assumed to be 1-based, i.e. 1 is the first message with that role).\n    Return -1 if not found. Index = 0 is the first message in the history.\n    \"\"\"\n    indices_with_role = [\n        i for i, m in enumerate(self.message_history) if m.role == role\n    ]\n\n    if len(indices_with_role) &lt; n:\n        return -1\n    return indices_with_role[n - 1]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message that has role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question. Args:     message (str): new message to replace with     role (str): role of message to replace</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Update the last message that has role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): new message to replace with\n        role (str): role of message to replace\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history[i].content = message\n            break\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.delete_last_message","title":"<code>delete_last_message(role=Role.USER)</code>","text":"<p>Delete the last message that has role <code>role</code> from the message history. Args:     role (str): role of message to delete</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def delete_last_message(self, role: str = Role.USER) -&gt; None:\n    \"\"\"\n    Delete the last message that has role `role` from the message history.\n    Args:\n        role (str): role of message to delete\n    \"\"\"\n    if len(self.message_history) == 0:\n        return\n    # find last message in self.message_history with role `role`\n    for i in range(len(self.message_history) - 1, -1, -1):\n        if self.message_history[i].role == role:\n            self.message_history.pop(i)\n            break\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method for the \"no-tools\" scenario, i.e., the current <code>msg</code> (presumably emitted by the LLM) does not have any tool that the agent can handle. NOTE: The <code>msg</code> may contain tools but either (a) the agent is not enabled to handle them, or (b) there's an explicit <code>recipient</code> field in the tool that doesn't match the agent's name.</p> <p>Uses the self.config.non_tool_routing to determine the action to take.</p> <p>This method can be overridden by subclasses, e.g., to create a \"reminder\" message when a tool is expected but the LLM \"forgot\" to generate one.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:     Any: The result of the handler method</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def handle_message_fallback(self, msg: str | ChatDocument) -&gt; Any:\n    \"\"\"\n    Fallback method for the \"no-tools\" scenario, i.e., the current `msg`\n    (presumably emitted by the LLM) does not have any tool that the agent\n    can handle.\n    NOTE: The `msg` may contain tools but either (a) the agent is not\n    enabled to handle them, or (b) there's an explicit `recipient` field\n    in the tool that doesn't match the agent's name.\n\n    Uses the self.config.non_tool_routing to determine the action to take.\n\n    This method can be overridden by subclasses, e.g.,\n    to create a \"reminder\" message when a tool is expected but the LLM \"forgot\"\n    to generate one.\n\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        Any: The result of the handler method\n    \"\"\"\n    if (\n        isinstance(msg, str)\n        or msg.metadata.sender != Entity.LLM\n        or self.config.handle_llm_no_tool is None\n        or self.has_only_unhandled_tools(msg)\n    ):\n        return None\n    # we ONLY use the `handle_llm_no_tool` config option when\n    # the msg is from LLM and does not contain ANY tools at all.\n    from langroid.agent.tools.orchestration import AgentDoneTool, ForwardTool\n\n    no_tool_option = self.config.handle_llm_no_tool\n    if no_tool_option in list(NonToolAction):\n        # in case the `no_tool_option` is one of the special NonToolAction vals\n        match self.config.handle_llm_no_tool:\n            case NonToolAction.FORWARD_USER:\n                return ForwardTool(agent=\"User\")\n            case NonToolAction.DONE:\n                return AgentDoneTool(content=msg.content, tools=msg.tool_messages)\n    elif is_callable(no_tool_option):\n        return no_tool_option(msg)\n    # Otherwise just return `no_tool_option` as is:\n    # This can be any string, such as a specific nudge/reminder to the LLM,\n    # or even something like ResultTool etc.\n    return no_tool_option\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.unhandled_tools","title":"<code>unhandled_tools()</code>","text":"<p>The set of tools that are known but not handled. Useful in task flow: an agent can refuse to accept an incoming msg when it only has unhandled tools.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def unhandled_tools(self) -&gt; set[str]:\n    \"\"\"The set of tools that are known but not handled.\n    Useful in task flow: an agent can refuse to accept an incoming msg\n    when it only has unhandled tools.\n    \"\"\"\n    return self.llm_tools_known - self.llm_tools_handled\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False, include_defaults=True)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage] | List[Type[ToolMessage]]]</code> <p>The ToolMessage class OR List of such classes to enable, for USE, or HANDLING, or both. If this is a list of ToolMessage classes, then the remain args are applied to all classes. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> <code>require_recipient</code> <code>bool</code> <p>whether to require that recipient be specified when using the tool message (only applies if <code>use</code> is True).</p> <code>False</code> <code>include_defaults</code> <code>bool</code> <p>whether to include fields that have default values, in the \"properties\" section of the JSON format instructions. (Normally the OpenAI completion API ignores these fields, but the Assistant fn-calling seems to pay attn to these, and if we don't want this, we should set this to False.)</p> <code>True</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage] | List[Type[ToolMessage]]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n    include_defaults: bool = True,\n) -&gt; None:\n    \"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n\n    Args:\n        message_class: The ToolMessage class OR List of such classes to enable,\n            for USE, or HANDLING, or both.\n            If this is a list of ToolMessage classes, then the remain args are\n            applied to all classes.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n        require_recipient: whether to require that recipient be specified\n            when using the tool message (only applies if `use` is True).\n        include_defaults: whether to include fields that have default values,\n            in the \"properties\" section of the JSON format instructions.\n            (Normally the OpenAI completion API ignores these fields,\n            but the Assistant fn-calling seems to pay attn to these,\n            and if we don't want this, we should set this to False.)\n    \"\"\"\n    if message_class is not None and isinstance(message_class, list):\n        for mc in message_class:\n            self.enable_message(\n                mc,\n                use=use,\n                handle=handle,\n                force=force,\n                require_recipient=require_recipient,\n                include_defaults=include_defaults,\n            )\n        return None\n\n    # Validate that use/handle are booleans, not accidentally passed tool classes\n    if isclass(use) or isclass(handle):\n        param = \"use\" if isclass(use) else \"handle\"\n        raise TypeError(\n            textwrap.dedent(\n                f\"\"\"\n                Invalid arguments to enable_message().\n                It appears you passed multiple ToolMessage classes as separate\n                arguments instead of as a list.\n\n                Correct usage:\n                    agent.enable_message([Tool1, Tool2, Tool3])\n\n                Incorrect usage:\n                    agent.enable_message(Tool1, Tool2, Tool3)\n\n                The '{param}' parameter must be a boolean, not a class.\n                \"\"\"\n            )\n        )\n\n    if require_recipient and message_class is not None:\n        message_class = message_class.require_recipient()\n    if isinstance(message_class, XMLToolMessage):\n        # XMLToolMessage is not compatible with OpenAI's Tools/functions API,\n        # so we disable use of functions API, enable langroid-native Tools,\n        # which are prompt-based.\n        self.config.use_functions_api = False\n        self.config.use_tools = True\n    super().enable_message_handling(message_class)  # enables handling only\n    tools = self._get_tool_list(message_class)\n    if message_class is not None:\n        request = message_class.default_value(\"request\")\n        if request == \"\":\n            raise ValueError(\n                f\"\"\"\n                ToolMessage class {message_class} must have a non-empty\n                'request' field if it is to be enabled as a tool.\n                \"\"\"\n            )\n        llm_function = message_class.llm_function_schema(defaults=include_defaults)\n        self.llm_functions_map[request] = llm_function\n        if force:\n            self.llm_function_force = dict(name=request)\n        else:\n            self.llm_function_force = None\n\n    for t in tools:\n        self.llm_tools_known.add(t)\n\n        if handle:\n            self.llm_tools_handled.add(t)\n            self.llm_functions_handled.add(t)\n\n            if (\n                self.enabled_handling_output_format is not None\n                and self.enabled_handling_output_format.name() == t\n            ):\n                # `t` was designated as \"enabled for handling\" ONLY for\n                # output_format enforcement, but we are explicitly ]\n                # enabling it for handling here, so we set the variable to None.\n                self.enabled_handling_output_format = None\n        else:\n            self.llm_tools_handled.discard(t)\n            self.llm_functions_handled.discard(t)\n\n        if use:\n            tool_class = self.llm_tools_map[t]\n            allow_llm_use = tool_class._allow_llm_use\n            if isinstance(allow_llm_use, ModelPrivateAttr):\n                allow_llm_use = allow_llm_use.default\n            if allow_llm_use:\n                self.llm_tools_usable.add(t)\n                self.llm_functions_usable.add(t)\n            else:\n                logger.warning(\n                    f\"\"\"\n                    ToolMessage class {tool_class} does not allow LLM use,\n                    because `_allow_llm_use=False` either in the Tool or a\n                    parent class of this tool;\n                    so not enabling LLM use for this tool!\n                    If you intended an LLM to use this tool,\n                    set `_allow_llm_use=True` when you define the tool.\n                    \"\"\"\n                )\n            if (\n                self.enabled_use_output_format is not None\n                and self.enabled_use_output_format.default_value(\"request\") == t\n            ):\n                # `t` was designated as \"enabled for use\" ONLY for output_format\n                # enforcement, but we are explicitly enabling it for use here,\n                # so we set the variable to None.\n                self.enabled_use_output_format = None\n        else:\n            self.llm_tools_usable.discard(t)\n            self.llm_functions_usable.discard(t)\n\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.set_output_format","title":"<code>set_output_format(output_type, force_tools=None, use=None, handle=None, instructions=None, is_copy=False)</code>","text":"<p>Sets <code>output_format</code> to <code>output_type</code> and, if <code>force_tools</code> is enabled, switches to the native Langroid tools mechanism to ensure that no tool calls not of <code>output_type</code> are generated. By default, <code>force_tools</code> follows the <code>use_tools_on_output_format</code> parameter in the config.</p> <p>If <code>output_type</code> is None, restores to the state prior to setting <code>output_format</code>.</p> <p>If <code>use</code>, we enable use of <code>output_type</code> when it is a subclass of <code>ToolMesage</code>. Note that this primarily controls instruction generation: the model will always generate <code>output_type</code> regardless of whether <code>use</code> is set. Defaults to the <code>use_output_format</code> parameter in the config. Similarly, handling of <code>output_type</code> is controlled by <code>handle</code>, which defaults to the <code>handle_output_format</code> parameter in the config.</p> <p><code>instructions</code> controls whether we generate instructions specifying the output format schema. Defaults to the <code>instructions_output_format</code> parameter in the config.</p> <p><code>is_copy</code> is set when called via <code>__getitem__</code>. In that case, we must copy certain fields to ensure that we do not overwrite the main agent's setings.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def set_output_format(\n    self,\n    output_type: Optional[type],\n    force_tools: Optional[bool] = None,\n    use: Optional[bool] = None,\n    handle: Optional[bool] = None,\n    instructions: Optional[bool] = None,\n    is_copy: bool = False,\n) -&gt; None:\n    \"\"\"\n    Sets `output_format` to `output_type` and, if `force_tools` is enabled,\n    switches to the native Langroid tools mechanism to ensure that no tool\n    calls not of `output_type` are generated. By default, `force_tools`\n    follows the `use_tools_on_output_format` parameter in the config.\n\n    If `output_type` is None, restores to the state prior to setting\n    `output_format`.\n\n    If `use`, we enable use of `output_type` when it is a subclass\n    of `ToolMesage`. Note that this primarily controls instruction\n    generation: the model will always generate `output_type` regardless\n    of whether `use` is set. Defaults to the `use_output_format`\n    parameter in the config. Similarly, handling of `output_type` is\n    controlled by `handle`, which defaults to the\n    `handle_output_format` parameter in the config.\n\n    `instructions` controls whether we generate instructions specifying\n    the output format schema. Defaults to the `instructions_output_format`\n    parameter in the config.\n\n    `is_copy` is set when called via `__getitem__`. In that case, we must\n    copy certain fields to ensure that we do not overwrite the main agent's\n    setings.\n    \"\"\"\n    # Disable usage of an output format which was not specifically enabled\n    # by `enable_message`\n    if self.enabled_use_output_format is not None:\n        self.disable_message_use(self.enabled_use_output_format)\n        self.enabled_use_output_format = None\n\n    # Disable handling of an output format which did not specifically have\n    # handling enabled via `enable_message`\n    if self.enabled_handling_output_format is not None:\n        self.disable_message_handling(self.enabled_handling_output_format)\n        self.enabled_handling_output_format = None\n\n    # Reset any previous instructions\n    self.output_format_instructions = \"\"\n\n    if output_type is None:\n        self.output_format = None\n        (\n            requests_for_inference,\n            use_functions_api,\n            use_tools,\n        ) = self.saved_requests_and_tool_setings\n        self.config = self.config.model_copy()\n        self.enabled_requests_for_inference = requests_for_inference\n        self.config.use_functions_api = use_functions_api\n        self.config.use_tools = use_tools\n    else:\n        if force_tools is None:\n            force_tools = self.config.use_tools_on_output_format\n\n        if not any(\n            (isclass(output_type) and issubclass(output_type, t))\n            for t in [ToolMessage, BaseModel]\n        ):\n            output_type = get_pydantic_wrapper(output_type)\n\n        if self.output_format is None and force_tools:\n            self.saved_requests_and_tool_setings = (\n                self._requests_and_tool_settings()\n            )\n\n        self.output_format = output_type\n        if issubclass(output_type, ToolMessage):\n            name = output_type.default_value(\"request\")\n            if use is None:\n                use = self.config.use_output_format\n\n            if handle is None:\n                handle = self.config.handle_output_format\n\n            if use or handle:\n                is_usable = name in self.llm_tools_usable.union(\n                    self.llm_functions_usable\n                )\n                is_handled = name in self.llm_tools_handled.union(\n                    self.llm_functions_handled\n                )\n\n                if is_copy:\n                    if use:\n                        # We must copy `llm_tools_usable` so the base agent\n                        # is unmodified\n                        self.llm_tools_usable = self.llm_tools_usable.copy()\n                        self.llm_functions_usable = self.llm_functions_usable.copy()\n                    if handle:\n                        # If handling the tool, do the same for `llm_tools_handled`\n                        self.llm_tools_handled = self.llm_tools_handled.copy()\n                        self.llm_functions_handled = (\n                            self.llm_functions_handled.copy()\n                        )\n                # Enable `output_type`\n                self.enable_message(\n                    output_type,\n                    # Do not override existing settings\n                    use=use or is_usable,\n                    handle=handle or is_handled,\n                )\n\n                # If the `output_type` ToilMessage was not already enabled for\n                # use, this means we are ONLY enabling it for use specifically\n                # for enforcing this output format, so we set the\n                # `enabled_use_output_forma  to this output_type, to\n                # record that it should be disabled when `output_format` is changed\n                if not is_usable:\n                    self.enabled_use_output_format = output_type\n\n                # (same reasoning as for use-enabling)\n                if not is_handled:\n                    self.enabled_handling_output_format = output_type\n\n            generated_tool_instructions = name in self.llm_tools_usable.union(\n                self.llm_functions_usable\n            )\n        else:\n            generated_tool_instructions = False\n\n        if instructions is None:\n            instructions = self.config.instructions_output_format\n        if issubclass(output_type, BaseModel) and instructions:\n            if generated_tool_instructions:\n                # Already generated tool instructions as part of \"enabling for use\",\n                # so only need to generate a reminder to use this tool.\n                name = cast(ToolMessage, output_type).default_value(\"request\")\n                self.output_format_instructions = textwrap.dedent(\n                    f\"\"\"\n                    === OUTPUT FORMAT INSTRUCTIONS ===\n\n                    Please provide output using the `{name}` tool/function.\n                    \"\"\"\n                )\n            else:\n                if issubclass(output_type, ToolMessage):\n                    output_format_schema = output_type.llm_function_schema(\n                        request=True,\n                        defaults=self.config.output_format_include_defaults,\n                    ).parameters\n                else:\n                    output_format_schema = output_type.model_json_schema()\n\n                format_schema_for_strict(output_format_schema)\n\n                self.output_format_instructions = textwrap.dedent(\n                    f\"\"\"\n                    === OUTPUT FORMAT INSTRUCTIONS ===\n                    Please provide output as JSON with the following schema:\n\n                    {output_format_schema}\n                    \"\"\"\n                )\n\n        if force_tools:\n            if issubclass(output_type, ToolMessage):\n                self.enabled_requests_for_inference = {\n                    output_type.default_value(\"request\")\n                }\n            if self.config.use_functions_api:\n                self.config = self.config.model_copy()\n                self.config.use_functions_api = False\n                self.config.use_tools = True\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL. Args:     message_class: The ToolMessage class to disable; Optional.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\n    self,\n    message_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n    \"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\n    super().disable_message_handling(message_class)\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_handled.discard(t)\n        self.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools. Args:     message_class: The ToolMessage class to disable.         If None, disable all.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\n    self,\n    message_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n    \"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\n    for t in self._get_tool_list(message_class):\n        self.llm_tools_usable.discard(t)\n        self.llm_functions_usable.discard(t)\n\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool) Args:     message_class: The only ToolMessage class to allow</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n    \"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\n    request = message_class.model_fields[\"request\"].default\n    to_remove = [r for r in self.llm_tools_usable if r != request]\n    for r in to_remove:\n        self.llm_tools_usable.discard(r)\n        self.llm_functions_usable.discard(r)\n    self._update_tool_instructions()\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.get_tool_messages","title":"<code>get_tool_messages(msg, all_tools=False)</code>","text":"<p>Extracts messages and tracks whether any errors occurred. If strict mode was enabled, disables it for the tool, else triggers strict recovery.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def get_tool_messages(\n    self,\n    msg: str | ChatDocument | None,\n    all_tools: bool = False,\n) -&gt; List[ToolMessage]:\n    \"\"\"\n    Extracts messages and tracks whether any errors occurred. If strict mode\n    was enabled, disables it for the tool, else triggers strict recovery.\n    \"\"\"\n    self.tool_error = False\n    most_recent_sent_by_llm = (\n        len(self.message_history) &gt; 0\n        and self.message_history[-1].role == Role.ASSISTANT\n    )\n    was_llm = most_recent_sent_by_llm or (\n        isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM\n    )\n    try:\n        tools = super().get_tool_messages(msg, all_tools)\n    except ValidationError as ve:\n        # Check if tool class was attached to the exception\n        if hasattr(ve, \"tool_class\") and ve.tool_class:\n            tool_class = ve.tool_class  # type: ignore\n            if issubclass(tool_class, ToolMessage):\n                was_strict = (\n                    self.config.use_functions_api\n                    and self.config.use_tools_api\n                    and self._strict_mode_for_tool(tool_class)\n                )\n                # If the result of strict output for a tool using the\n                # OpenAI tools API fails to parse, we infer that the\n                # schema edits necessary for compatibility prevented\n                # adherence to the underlying `ToolMessage` schema and\n                # disable strict output for the tool\n                if was_strict:\n                    name = tool_class.default_value(\"request\")\n                    self.disable_strict_tools_set.add(name)\n                    logging.warning(\n                        f\"\"\"\n                        Validation error occured with strict tool format.\n                        Disabling strict mode for the {name} tool.\n                        \"\"\"\n                    )\n                else:\n                    # We will trigger the strict recovery mechanism to force\n                    # the LLM to correct its output, allowing us to parse\n                    if isinstance(msg, ChatDocument):\n                        self.tool_error = msg.metadata.sender == Entity.LLM\n                    else:\n                        self.tool_error = most_recent_sent_by_llm\n\n        if was_llm:\n            raise ve\n        else:\n            self.tool_error = False\n            return []\n\n    if not was_llm:\n        self.tool_error = False\n\n    return tools\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.truncate_message","title":"<code>truncate_message(idx, tokens=5, warning='...[Contents truncated!]', inplace=True)</code>","text":"<p>Truncate message at idx in msg history to <code>tokens</code> tokens.</p> <p>If inplace is True, the message is truncated in place, else it LEAVES the original message INTACT and returns a new message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def truncate_message(\n    self,\n    idx: int,\n    tokens: int = 5,\n    warning: str = \"...[Contents truncated!]\",\n    inplace: bool = True,\n) -&gt; LLMMessage:\n    \"\"\"\n    Truncate message at idx in msg history to `tokens` tokens.\n\n    If inplace is True, the message is truncated in place, else\n    it LEAVES the original message INTACT and returns a new message\n    \"\"\"\n    if inplace:\n        llm_msg = self.message_history[idx]\n    else:\n        llm_msg = copy.deepcopy(self.message_history[idx])\n    orig_content = llm_msg.content\n    new_content = (\n        self.parser.truncate_tokens(orig_content, tokens)\n        if self.parser is not None\n        else orig_content[: tokens * 4]  # approx truncation\n    )\n    llm_msg.content = new_content + \"\\n\" + warning\n    return llm_msg\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode Args:     message (str|ChatDocument): message or ChatDocument object to respond to.         If None, use the self.task_messages Returns:     LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    # If enabled and a tool error occurred, we recover by generating the tool in\n    # strict json mode\n    if (\n        self.tool_error\n        and self.output_format is None\n        and self._json_schema_available()\n        and self.config.strict_recovery\n    ):\n        self.tool_error = False\n        AnyTool = self._get_any_tool_message()\n        if AnyTool is None:\n            return None\n        self.set_output_format(\n            AnyTool,\n            force_tools=True,\n            use=True,\n            handle=True,\n            instructions=True,\n        )\n        recovery_message = self._strict_recovery_instructions(AnyTool)\n        augmented_message = message\n        if augmented_message is None:\n            augmented_message = recovery_message\n        elif isinstance(augmented_message, str):\n            augmented_message = augmented_message + recovery_message\n        else:\n            augmented_message.content = augmented_message.content + recovery_message\n\n        # only use the augmented message for this one response...\n        result = self.llm_response(augmented_message)\n        # ... restore the original user message so that the AnyTool recover\n        # instructions don't persist in the message history\n        # (this can cause the LLM to use the AnyTool directly as a tool)\n        if message is None:\n            self.delete_last_message(role=Role.USER)\n        else:\n            msg = message if isinstance(message, str) else message.content\n            self.update_last_message(msg, role=Role.USER)\n        return result\n\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    tool_choice = (\n        \"auto\"\n        if isinstance(message, str)\n        else (message.oai_tool_choice if message is not None else \"auto\")\n    )\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        try:\n            response = self.llm_response_messages(hist, output_len, tool_choice)\n        except openai.BadRequestError as e:\n            if self.any_strict:\n                self.disable_strict = True\n                self.set_output_format(None)\n                logging.warning(\n                    f\"\"\"\n                    OpenAI BadRequestError raised with strict mode enabled.\n                    Message: {e.message}\n                    Disabling strict mode and retrying.\n                    \"\"\"\n                )\n                return self.llm_response(message)\n            else:\n                raise e\n    self.message_history.extend(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    if isinstance(message, ChatDocument):\n        self._reduce_raw_tool_results(message)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of `llm_response`. See there for details.\n    \"\"\"\n    if self.llm is None:\n        return None\n\n    # If enabled and a tool error occurred, we recover by generating the tool in\n    # strict json mode\n    if (\n        self.tool_error\n        and self.output_format is None\n        and self._json_schema_available()\n        and self.config.strict_recovery\n    ):\n        self.tool_error = False\n        AnyTool = self._get_any_tool_message()\n        self.set_output_format(\n            AnyTool,\n            force_tools=True,\n            use=True,\n            handle=True,\n            instructions=True,\n        )\n        recovery_message = self._strict_recovery_instructions(AnyTool)\n        augmented_message = message\n        if augmented_message is None:\n            augmented_message = recovery_message\n        elif isinstance(augmented_message, str):\n            augmented_message = augmented_message + recovery_message\n        else:\n            augmented_message.content = augmented_message.content + recovery_message\n\n        # only use the augmented message for this one response...\n        result = self.llm_response(augmented_message)\n        # ... restore the original user message so that the AnyTool recover\n        # instructions don't persist in the message history\n        # (this can cause the LLM to use the AnyTool directly as a tool)\n        if message is None:\n            self.delete_last_message(role=Role.USER)\n        else:\n            msg = message if isinstance(message, str) else message.content\n            self.update_last_message(msg, role=Role.USER)\n        return result\n\n    hist, output_len = self._prep_llm_messages(message)\n    if len(hist) == 0:\n        return None\n    tool_choice = (\n        \"auto\"\n        if isinstance(message, str)\n        else (message.oai_tool_choice if message is not None else \"auto\")\n    )\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):\n        try:\n            response = await self.llm_response_messages_async(\n                hist, output_len, tool_choice\n            )\n        except openai.BadRequestError as e:\n            if self.any_strict:\n                self.disable_strict = True\n                self.set_output_format(None)\n                logging.warning(\n                    f\"\"\"\n                    OpenAI BadRequestError raised with strict mode enabled.\n                    Message: {e.message}\n                    Disabling strict mode and retrying.\n                    \"\"\"\n                )\n                return await self.llm_response_async(message)\n            else:\n                raise e\n    self.message_history.extend(ChatDocument.to_LLMMessage(response))\n    response.metadata.msg_idx = len(self.message_history) - 1\n    response.metadata.agent_id = self.id\n    if isinstance(message, ChatDocument):\n        self._reduce_raw_tool_results(message)\n    # Preserve trail of tool_ids for OpenAI Assistant fn-calls\n    response.metadata.tool_ids = (\n        []\n        if isinstance(message, str)\n        else message.metadata.tool_ids if message is not None else []\n    )\n\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.init_message_history","title":"<code>init_message_history()</code>","text":"<p>Initialize the message history with the system message and user message</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def init_message_history(self) -&gt; None:\n    \"\"\"\n    Initialize the message history with the system message and user message\n    \"\"\"\n    self.message_history = [self._create_system_and_tools_message()]\n    if self.user_message:\n        self.message_history.append(\n            LLMMessage(role=Role.USER, content=self.user_message)\n        )\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None, tool_choice='auto')</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion Args:     messages: seq of messages (with role, content fields) sent to LLM     output_len: max number of tokens expected in response.             If None, use the LLM's default model_max_output_tokens. Returns:     Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\n    self,\n    messages: List[LLMMessage],\n    output_len: Optional[int] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n) -&gt; ChatDocument:\n    \"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n        output_len: max number of tokens expected in response.\n                If None, use the LLM's default model_max_output_tokens.\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.model_max_output_tokens\n    streamer = noop_fn\n    if self.llm.get_stream():\n        streamer = self.callbacks.start_llm_stream()\n    self.llm.config.streamer = streamer\n    with ExitStack() as stack:  # for conditionally using rich spinner\n        if not self.llm.get_stream() and not settings.quiet:\n            # show rich spinner only if not streaming!\n            # (Why? b/c the intent of showing a spinner is to \"show progress\",\n            # and we don't need to do that when streaming, since\n            # streaming output already shows progress.)\n            cm = status(\n                \"LLM responding to messages...\",\n                log_if_quiet=False,\n            )\n            stack.enter_context(cm)\n        if self.llm.get_stream() and not settings.quiet:\n            console.print(f\"[green]{self.indent}\", end=\"\")\n        functions, fun_call, tools, force_tool, output_format = (\n            self._function_args()\n        )\n        assert self.llm is not None\n        response = self.llm.chat(\n            messages,\n            output_len,\n            tools=tools,\n            tool_choice=force_tool or tool_choice,\n            functions=functions,\n            function_call=fun_call,\n            response_format=output_format,\n        )\n    if self.llm.get_stream():\n        # Create temp ChatDocument for tool check, then clean up to avoid\n        # polluting ObjectRegistry (see PR #939 discussion)\n        temp_doc = ChatDocument.from_LLMResponse(\n            response,\n            displayed=True,\n            recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n        )\n        self._call_callback_with_reasoning(\n            \"finish_llm_stream\",\n            reasoning=response.reasoning,\n            content=response.message,\n            tools_content=response.tools_content(),\n            is_tool=self.has_tool_message_attempt(temp_doc),\n        )\n        ObjectRegistry.remove(temp_doc.id())\n    self.llm.config.streamer = noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(\n        response,\n        displayed=True,\n        recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n    )\n    self.oai_tool_calls = response.oai_tool_calls or []\n    self.oai_tool_id2call.update(\n        {t.id: t for t in self.oai_tool_calls if t.id is not None}\n    )\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(chat_doc)\n\n    return chat_doc\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_messages_async","title":"<code>llm_response_messages_async(messages, output_len=None, tool_choice='auto')</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_messages</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_messages_async(\n    self,\n    messages: List[LLMMessage],\n    output_len: Optional[int] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_messages`. See there for details.\n    \"\"\"\n    assert self.config.llm is not None and self.llm is not None\n    output_len = output_len or self.config.llm.model_max_output_tokens\n    functions, fun_call, tools, force_tool, output_format = self._function_args()\n    assert self.llm is not None\n\n    streamer_async = async_noop_fn\n    if self.llm.get_stream():\n        streamer_async = await self.callbacks.start_llm_stream_async()\n    self.llm.config.streamer_async = streamer_async\n\n    response = await self.llm.achat(\n        messages,\n        output_len,\n        tools=tools,\n        tool_choice=force_tool or tool_choice,\n        functions=functions,\n        function_call=fun_call,\n        response_format=output_format,\n    )\n    if self.llm.get_stream():\n        # Create temp ChatDocument for tool check, then clean up to avoid\n        # polluting ObjectRegistry (see PR #939 discussion)\n        temp_doc = ChatDocument.from_LLMResponse(\n            response,\n            displayed=True,\n            recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n        )\n        self._call_callback_with_reasoning(\n            \"finish_llm_stream\",\n            reasoning=response.reasoning,\n            content=response.message,\n            tools_content=response.tools_content(),\n            is_tool=self.has_tool_message_attempt(temp_doc),\n        )\n        ObjectRegistry.remove(temp_doc.id())\n    self.llm.config.streamer_async = async_noop_fn\n    if response.cached:\n        self.callbacks.cancel_llm_stream()\n    self._render_llm_response(response)\n    self.update_token_usage(\n        response,  # .usage attrib is updated!\n        messages,\n        self.llm.get_stream(),\n        chat=True,\n        print_response_stats=self.config.show_stats and not settings.quiet,\n    )\n    chat_doc = ChatDocument.from_LLMResponse(\n        response,\n        displayed=True,\n        recognize_recipient_in_content=self.config.recognize_recipient_in_content,\n    )\n    self.oai_tool_calls = response.oai_tool_calls or []\n    self.oai_tool_id2call.update(\n        {t.id: t for t in self.oai_tool_calls if t.id is not None}\n    )\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(chat_doc)\n\n    return chat_doc\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message=None)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>message to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n\n    Args:\n        message (str|ChatDocument): message to respond to.\n\n    Returns:\n        A Document object with the response.\n\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(ChatDocument, ChatAgent.llm_response(self, message))\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    # If using strict output format, parse the output JSON\n    self._load_output_format(response)\n\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_forget_async","title":"<code>llm_response_forget_async(message=None)</code>  <code>async</code>","text":"<p>Async version of <code>llm_response_forget</code>. See there for details.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>async def llm_response_forget_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; ChatDocument:\n    \"\"\"\n    Async version of `llm_response_forget`. See there for details.\n    \"\"\"\n    # explicitly call THIS class's respond method,\n    # not a derived class's (or else there would be infinite recursion!)\n    n_msgs = len(self.message_history)\n    with StreamingIfAllowed(self.llm, self.llm.get_stream()):  # type: ignore\n        response = cast(\n            ChatDocument, await ChatAgent.llm_response_async(self, message)\n        )\n    # If there is a response, then we will have two additional\n    # messages in the message history, i.e. the user message and the\n    # assistant response. We want to (carefully) remove these two messages.\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n\n    if len(self.message_history) &gt; n_msgs:\n        msg = self.message_history.pop()\n        self._drop_msg_update_tool_calls(msg)\n    return response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:     int: number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n    \"\"\"\n    Total number of tokens in the message history so far.\n\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\n            \"ChatAgent.parser is None. \"\n            \"You must set ChatAgent.parser \"\n            \"before calling chat_num_tokens().\"\n        )\n    hist = messages if messages is not None else self.message_history\n    return sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history Args:     i: if provided, return only the i-th message when i is postive,         or last k messages when i = -k. Returns:</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n    \"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\n    if i is None:\n        return \"\\n\".join([str(m) for m in self.message_history])\n    elif i &gt; 0:\n        return str(self.message_history[i])\n    else:\n        return \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/agent/chat_document/","title":"chat_document","text":"<p>langroid/agent/chat_document.py </p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.StatusCode","title":"<code>StatusCode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Codes meant to be returned by task.run(). Some are not used yet.</p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument","title":"<code>ChatDocument(**data)</code>","text":"<p>               Bases: <code>Document</code></p> <p>Represents a message in a conversation among agents. All responders of an agent have signature ChatDocument -&gt; ChatDocument (modulo None, str, etc), and so does the Task.run() method.</p> <p>Attributes:</p> Name Type Description <code>oai_tool_calls</code> <code>Optional[List[OpenAIToolCall]]</code> <p>Tool-calls from an OpenAI-compatible API</p> <code>oai_tool_id2results</code> <code>Optional[OrderedDict[str, str]]</code> <p>Results of tool-calls from OpenAI (dict is a map of tool_id -&gt; result)</p> <code>oai_tool_choice</code> <code>ToolChoiceTypes | Dict[str, Dict[str, str] | str]</code> <p>ToolChoiceTypes | Dict[str, str]: Param controlling how the LLM should choose tool-use in its response (auto, none, required, or a specific tool)</p> <code>function_call</code> <code>Optional[LLMFunctionCall]</code> <p>Function-call from an OpenAI-compatible API     (deprecated by OpenAI, in favor of tool-calls)</p> <code>tool_messages</code> <code>List[ToolMessage]</code> <p>Langroid ToolMessages extracted from - <code>content</code> field (via JSON parsing), - <code>oai_tool_calls</code>, or - <code>function_call</code></p> <code>metadata</code> <code>ChatDocMetaData</code> <p>Metadata for the message, e.g. sender, recipient.</p> <code>attachment</code> <code>None | ChatDocAttachment</code> <p>Any additional data attached.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def __init__(self, **data: Any):\n    super().__init__(**data)\n    ObjectRegistry.register_object(self)\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.delete_id","title":"<code>delete_id(id)</code>  <code>staticmethod</code>","text":"<p>Remove ChatDocument with given id from ObjectRegistry, and all its descendants.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef delete_id(id: str) -&gt; None:\n    \"\"\"Remove ChatDocument with given id from ObjectRegistry,\n    and all its descendants.\n    \"\"\"\n    chat_doc = ChatDocument.from_id(id)\n    # first delete all descendants\n    while chat_doc is not None:\n        next_chat_doc = chat_doc.child\n        ObjectRegistry.remove(chat_doc.id())\n        chat_doc = next_chat_doc\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.get_tool_names","title":"<code>get_tool_names()</code>","text":"<p>Get names of attempted tool usages (JSON or non-JSON) in the content     of the message. Returns:     List[str]: list of attempted tool names     (We say \"attempted\" since we ONLY look at the <code>request</code> component of the     tool-call representation, and we're not fully parsing it into the     corresponding tool message class)</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_tool_names(self) -&gt; List[str]:\n    \"\"\"\n    Get names of attempted tool usages (JSON or non-JSON) in the content\n        of the message.\n    Returns:\n        List[str]: list of *attempted* tool names\n        (We say \"attempted\" since we ONLY look at the `request` component of the\n        tool-call representation, and we're not fully parsing it into the\n        corresponding tool message class)\n\n    \"\"\"\n    tool_candidates = XMLToolMessage.find_candidates(self.content)\n    if len(tool_candidates) == 0:\n        tool_candidates = extract_top_level_json(self.content)\n        if len(tool_candidates) == 0:\n            return []\n        tools = [json.loads(tc).get(\"request\") for tc in tool_candidates]\n    else:\n        tool_dicts = [\n            XMLToolMessage.extract_field_values(tc) for tc in tool_candidates\n        ]\n        tools = [td.get(\"request\") for td in tool_dicts if td is not None]\n    return [str(tool) for tool in tools if tool is not None]\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger Returns:     List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n    \"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\n    tool_type = \"\"  # FUNC or TOOL\n    tool = \"\"  # tool name or function name\n\n    # Skip tool detection for system messages - they contain tool instructions,\n    # not actual tool calls\n    if self.metadata.sender != Entity.SYSTEM:\n        oai_tools = (\n            []\n            if self.oai_tool_calls is None\n            else [t for t in self.oai_tool_calls if t.function is not None]\n        )\n        if self.function_call is not None:\n            tool_type = \"FUNC\"\n            tool = self.function_call.name\n        elif len(oai_tools) &gt; 0:\n            tool_type = \"OAI_TOOL\"\n            tool = \",\".join(t.function.name for t in oai_tools)  # type: ignore\n        else:\n            try:\n                json_tools = self.get_tool_names()\n            except Exception:\n                json_tools = []\n            if json_tools != []:\n                tool_type = \"TOOL\"\n                tool = json_tools[0]\n    recipient = self.metadata.recipient\n    content = self.content\n    sender_entity = self.metadata.sender\n    sender_name = self.metadata.sender_name\n    if tool_type == \"FUNC\":\n        content += str(self.function_call)\n    return ChatDocLoggerFields(\n        sender_entity=sender_entity,\n        sender_name=sender_name,\n        recipient=recipient,\n        block=self.metadata.block,\n        tool_type=tool_type,\n        tool=tool,\n        content=content,\n    )\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.pop_tool_ids","title":"<code>pop_tool_ids()</code>","text":"<p>Pop the last tool_id from the stack of tool_ids.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def pop_tool_ids(self) -&gt; None:\n    \"\"\"\n    Pop the last tool_id from the stack of tool_ids.\n    \"\"\"\n    if len(self.metadata.tool_ids) &gt; 0:\n        self.metadata.tool_ids.pop()\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.from_LLMResponse","title":"<code>from_LLMResponse(response, displayed=False, recognize_recipient_in_content=True)</code>  <code>staticmethod</code>","text":"<p>Convert LLMResponse to ChatDocument. Args:     response (LLMResponse): LLMResponse to convert.     displayed (bool): Whether this response was displayed to the user.     recognize_recipient_in_content (bool): Whether to parse message text         for recipient routing (<code>TO[&lt;recipient&gt;]:</code> and JSON         <code>{\"recipient\": ...}</code>). Default True. Returns:     ChatDocument: ChatDocument representation of this LLMResponse.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMResponse(\n    response: LLMResponse,\n    displayed: bool = False,\n    recognize_recipient_in_content: bool = True,\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMResponse to ChatDocument.\n    Args:\n        response (LLMResponse): LLMResponse to convert.\n        displayed (bool): Whether this response was displayed to the user.\n        recognize_recipient_in_content (bool): Whether to parse message text\n            for recipient routing (``TO[&lt;recipient&gt;]:`` and JSON\n            ``{\"recipient\": ...}``). Default True.\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMResponse.\n    \"\"\"\n    recipient, message = response.get_recipient_and_message(\n        recognize_recipient_in_content\n    )\n    message = message.strip()\n    if message in [\"''\", '\"\"']:\n        message = \"\"\n    if response.function_call is not None:\n        ChatDocument._clean_fn_call(response.function_call)\n    if response.oai_tool_calls is not None:\n        # there must be at least one if it's not None\n        for oai_tc in response.oai_tool_calls:\n            ChatDocument._clean_fn_call(oai_tc.function)\n    return ChatDocument(\n        content=message,\n        reasoning=response.reasoning,\n        content_with_reasoning=response.message_with_reasoning,\n        content_any=message,\n        oai_tool_calls=response.oai_tool_calls,\n        function_call=response.function_call,\n        metadata=ChatDocMetaData(\n            source=Entity.LLM,\n            sender=Entity.LLM,\n            usage=response.usage,\n            displayed=displayed,\n            cached=response.cached,\n            recipient=recipient,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.from_LLMMessage","title":"<code>from_LLMMessage(message, sender_name='', recipient='')</code>  <code>staticmethod</code>","text":"<p>Convert LLMMessage to ChatDocument.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>LLMMessage</code> <p>LLMMessage to convert.</p> required <code>sender_name</code> <code>str</code> <p>Name of the sender. Defaults to \"\".</p> <code>''</code> <code>recipient</code> <code>str</code> <p>Name of the recipient. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>ChatDocument</code> <code>'ChatDocument'</code> <p>ChatDocument representation of this LLMMessage.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef from_LLMMessage(\n    message: LLMMessage,\n    sender_name: str = \"\",\n    recipient: str = \"\",\n) -&gt; \"ChatDocument\":\n    \"\"\"\n    Convert LLMMessage to ChatDocument.\n\n    Args:\n        message (LLMMessage): LLMMessage to convert.\n        sender_name (str): Name of the sender. Defaults to \"\".\n        recipient (str): Name of the recipient. Defaults to \"\".\n\n    Returns:\n        ChatDocument: ChatDocument representation of this LLMMessage.\n    \"\"\"\n    # Map LLMMessage Role to ChatDocument Entity\n    role_to_entity = {\n        Role.USER: Entity.USER,\n        Role.SYSTEM: Entity.SYSTEM,\n        Role.ASSISTANT: Entity.LLM,\n        Role.FUNCTION: Entity.LLM,\n        Role.TOOL: Entity.LLM,\n    }\n\n    sender_entity = role_to_entity.get(message.role, Entity.USER)\n\n    return ChatDocument(\n        content=message.content or \"\",\n        content_any=message.content,\n        files=message.files,\n        function_call=message.function_call,\n        oai_tool_calls=message.tool_calls,\n        metadata=ChatDocMetaData(\n            source=sender_entity,\n            sender=sender_entity,\n            sender_name=sender_name,\n            recipient=recipient,\n            oai_tool_id=message.tool_call_id,\n            tool_ids=[message.tool_id] if message.tool_id else [],\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message, oai_tools=None)</code>  <code>staticmethod</code>","text":"<p>Convert to list of LLMMessage, to incorporate into msg-history sent to LLM API. Usually there will be just a single LLMMessage, but when the ChatDocument contains results from multiple OpenAI tool-calls, we would have a sequence LLMMessages, one per tool-call result.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <code>oai_tools</code> <code>Optional[List[OpenAIToolCall]]</code> <p>Tool-calls currently awaiting response, from the ChatAgent's latest message.</p> <code>None</code> <p>Returns:     List[LLMMessage]: list of LLMMessages corresponding to this ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(\n    message: Union[str, \"ChatDocument\"],\n    oai_tools: Optional[List[OpenAIToolCall]] = None,\n) -&gt; List[LLMMessage]:\n    \"\"\"\n    Convert to list of LLMMessage, to incorporate into msg-history sent to LLM API.\n    Usually there will be just a single LLMMessage, but when the ChatDocument\n    contains results from multiple OpenAI tool-calls, we would have a sequence\n    LLMMessages, one per tool-call result.\n\n    Args:\n        message (str|ChatDocument): Message to convert.\n        oai_tools (Optional[List[OpenAIToolCall]]): Tool-calls currently awaiting\n            response, from the ChatAgent's latest message.\n    Returns:\n        List[LLMMessage]: list of LLMMessages corresponding to this ChatDocument.\n    \"\"\"\n\n    sender_role = Role.USER\n    if isinstance(message, str):\n        message = ChatDocument.from_str(message)\n    # Prefer content_with_reasoning when available \u2014 this preserves\n    # inline thought signatures (e.g. &lt;thinking&gt;...&lt;/thinking&gt;) in\n    # message history, which certain models (Gemini 3 Flash, Amazon\n    # Nova) need to maintain reasoning across turns.\n    # content_with_reasoning is only set when inline tags were\n    # actually extracted, so this won't interfere with models that\n    # provide reasoning via a separate API field.\n    content = (\n        message.content_with_reasoning\n        or message.content\n        or to_string(message.content_any)\n        or \"\"\n    )\n    fun_call = message.function_call\n    oai_tool_calls = message.oai_tool_calls\n    if message.metadata.sender == Entity.USER and fun_call is not None:\n        # This may happen when a (parent agent's) LLM generates a\n        # a Function-call, and it ends up being sent to the current task's\n        # LLM (possibly because the function-call is mis-named or has other\n        # issues and couldn't be handled by handler methods).\n        # But a function-call can only be generated by an entity with\n        # Role.ASSISTANT, so we instead put the content of the function-call\n        # in the content of the message.\n        content += \" \" + str(fun_call)\n        fun_call = None\n    if message.metadata.sender == Entity.USER and oai_tool_calls is not None:\n        # same reasoning as for function-call above\n        content += \" \" + \"\\n\\n\".join(str(tc) for tc in oai_tool_calls)\n        oai_tool_calls = None\n    # some LLM APIs (e.g. gemini) don't like empty msg\n    content = content or \" \"\n    sender_name = message.metadata.sender_name\n    tool_ids = message.metadata.tool_ids\n    tool_id = tool_ids[-1] if len(tool_ids) &gt; 0 else \"\"\n    chat_document_id = message.id()\n    if message.metadata.sender == Entity.SYSTEM:\n        sender_role = Role.SYSTEM\n    if (\n        message.metadata.parent is not None\n        and message.metadata.parent.function_call is not None\n    ):\n        # This is a response to a function call, so set the role to FUNCTION.\n        sender_role = Role.FUNCTION\n        sender_name = message.metadata.parent.function_call.name\n    elif oai_tools is not None and len(oai_tools) &gt; 0:\n        pending_tool_ids = [tc.id for tc in oai_tools]\n        # The ChatAgent has pending OpenAI tool-call(s),\n        # so the current ChatDocument contains\n        # results for some/all/none of them.\n\n        if len(oai_tools) == 1:\n            # Case 1:\n            # There was exactly 1 pending tool-call, and in this case\n            # the result would be a plain string in `content`\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=oai_tools[0].id,\n                    content=content,\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n            ]\n\n        elif (\n            message.metadata.oai_tool_id is not None\n            and message.metadata.oai_tool_id in pending_tool_ids\n        ):\n            # Case 2:\n            # ChatDocument.content has result of a single tool-call\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=message.metadata.oai_tool_id,\n                    content=content,\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n            ]\n        elif message.oai_tool_id2result is not None:\n            # Case 2:\n            # There were &gt; 1 tool-calls awaiting response,\n            assert (\n                len(message.oai_tool_id2result) &gt; 1\n            ), \"oai_tool_id2result must have more than 1 item.\"\n            return [\n                LLMMessage(\n                    role=Role.TOOL,\n                    tool_call_id=tool_id,\n                    content=result or \" \",\n                    files=message.files,\n                    chat_document_id=chat_document_id,\n                )\n                for tool_id, result in message.oai_tool_id2result.items()\n            ]\n    elif message.metadata.sender == Entity.LLM:\n        sender_role = Role.ASSISTANT\n\n    return [\n        LLMMessage(\n            role=sender_role,\n            tool_id=tool_id,  # for OpenAI Assistant\n            content=content,\n            files=message.files,\n            function_call=fun_call,\n            tool_calls=oai_tool_calls,\n            name=sender_name,\n            chat_document_id=chat_document_id,\n        )\n    ]\n</code></pre>"},{"location":"reference/agent/done_sequence_parser/","title":"done_sequence_parser","text":"<p>langroid/agent/done_sequence_parser.py </p> <p>Parser for done sequence DSL (Domain Specific Language).</p> <p>Converts string patterns into DoneSequence objects for convenient task completion configuration.</p> <p>Examples:</p> <p>\"T, A\" -&gt; Tool followed by Agent response \"T[calculator], A\" -&gt; Specific tool 'calculator' followed by Agent response \"L, T, A, L\" -&gt; LLM, Tool, Agent, LLM sequence \"C[quit|exit]\" -&gt; Content matching regex pattern</p>"},{"location":"reference/agent/done_sequence_parser/#langroid.agent.done_sequence_parser.parse_done_sequence","title":"<code>parse_done_sequence(sequence, tools_map=None)</code>","text":"<p>Parse a string pattern or return existing DoneSequence unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>Union[str, DoneSequence]</code> <p>Either a DoneSequence object or a string pattern to parse</p> required <code>tools_map</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dict mapping tool names to tool classes (e.g., agent.llm_tools_map)</p> <code>None</code> <p>Returns:</p> Type Description <code>DoneSequence</code> <p>DoneSequence object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the string pattern is invalid</p> Source code in <code>langroid/agent/done_sequence_parser.py</code> <pre><code>def parse_done_sequence(\n    sequence: Union[str, DoneSequence], tools_map: Optional[Dict[str, Any]] = None\n) -&gt; DoneSequence:\n    \"\"\"Parse a string pattern or return existing DoneSequence unchanged.\n\n    Args:\n        sequence: Either a DoneSequence object or a string pattern to parse\n        tools_map: Optional dict mapping tool names to tool classes\n            (e.g., agent.llm_tools_map)\n\n    Returns:\n        DoneSequence object\n\n    Raises:\n        ValueError: If the string pattern is invalid\n    \"\"\"\n    if isinstance(sequence, DoneSequence):\n        return sequence\n\n    if not isinstance(sequence, str):\n        raise ValueError(f\"Expected string or DoneSequence, got {type(sequence)}\")\n\n    events = _parse_string_pattern(sequence, tools_map)\n    return DoneSequence(events=events)\n</code></pre>"},{"location":"reference/agent/done_sequence_parser/#langroid.agent.done_sequence_parser.parse_done_sequences","title":"<code>parse_done_sequences(sequences, tools_map=None)</code>","text":"<p>Parse a list of mixed string patterns and DoneSequence objects.</p> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>List[Union[str, DoneSequence]]</code> <p>List containing strings and/or DoneSequence objects</p> required <code>tools_map</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dict mapping tool names to tool classes (e.g., agent.llm_tools_map)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[DoneSequence]</code> <p>List of DoneSequence objects</p> Source code in <code>langroid/agent/done_sequence_parser.py</code> <pre><code>def parse_done_sequences(\n    sequences: List[Union[str, DoneSequence]],\n    tools_map: Optional[Dict[str, Any]] = None,\n) -&gt; List[DoneSequence]:\n    \"\"\"Parse a list of mixed string patterns and DoneSequence objects.\n\n    Args:\n        sequences: List containing strings and/or DoneSequence objects\n        tools_map: Optional dict mapping tool names to tool classes\n            (e.g., agent.llm_tools_map)\n\n    Returns:\n        List of DoneSequence objects\n    \"\"\"\n    return [parse_done_sequence(seq, tools_map) for seq in sequences]\n</code></pre>"},{"location":"reference/agent/openai_assistant/","title":"openai_assistant","text":"<p>langroid/agent/openai_assistant.py </p>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant","title":"<code>OpenAIAssistant(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>A ChatAgent powered by OpenAI Assistant API: mainly, in <code>llm_response</code> method, we avoid maintaining conversation state, and instead let the Assistant API do it for us. Also handles persistent storage of Assistant and Threads: stores their ids (for given user, org) in a cache, and reuses them based on config.use_cached_assistant and config.use_cached_thread.</p> <p>This class can be used as a drop-in replacement for ChatAgent.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def __init__(self, config: OpenAIAssistantConfig):\n    super().__init__(config)\n    self.config: OpenAIAssistantConfig = config\n    self.llm: OpenAIGPT = OpenAIGPT(self.config.llm)\n    assert (\n        self.llm.cache is not None\n    ), \"OpenAIAssistant requires a cache to store Assistant and Thread ids\"\n\n    if not isinstance(self.llm.client, openai.OpenAI):\n        raise ValueError(\"Client must be OpenAI\")\n    # handles for various entities and methods\n    self.client: openai.OpenAI = self.llm.client\n    self.runs = self.client.beta.threads.runs\n    self.threads = self.client.beta.threads\n    self.thread_messages = self.client.beta.threads.messages\n    self.assistants = self.client.beta.assistants\n    # which tool_ids are awaiting output submissions\n    self.pending_tool_ids: List[str] = []\n    self.cached_tool_ids: List[str] = []\n\n    self.thread: Thread | None = None\n    self.assistant: Assistant | None = None\n    self.run: Run | None = None\n\n    self._maybe_create_assistant(self.config.assistant_id)\n    self._maybe_create_thread(self.config.thread_id)\n    self._cache_store()\n\n    self.add_assistant_files(self.config.files)\n    self.add_assistant_tools(self.config.tools)\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.add_assistant_files","title":"<code>add_assistant_files(files)</code>","text":"<p>Add file_ids to assistant</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def add_assistant_files(self, files: List[str]) -&gt; None:\n    \"\"\"Add file_ids to assistant\"\"\"\n    if self.assistant is None:\n        raise ValueError(\"Assistant is None\")\n    self.files = [\n        self.client.files.create(file=open(f, \"rb\"), purpose=\"assistants\")\n        for f in files\n    ]\n    self.config.files = list(set(self.config.files + files))\n    self.assistant = self.assistants.update(\n        self.assistant.id,\n        tool_resources=ToolResources(\n            code_interpreter=ToolResourcesCodeInterpreter(\n                file_ids=[f.id for f in self.files],\n            ),\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.add_assistant_tools","title":"<code>add_assistant_tools(tools)</code>","text":"<p>Add tools to assistant</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def add_assistant_tools(self, tools: List[AssistantTool]) -&gt; None:\n    \"\"\"Add tools to assistant\"\"\"\n    if self.assistant is None:\n        raise ValueError(\"Assistant is None\")\n    all_tool_dicts = [t.dct() for t in self.config.tools]\n    for t in tools:\n        if t.dct() not in all_tool_dicts:\n            self.config.tools.append(t)\n    self.assistant = self.assistants.update(\n        self.assistant.id,\n        tools=[tool.dct() for tool in self.config.tools],  # type: ignore\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False, require_recipient=False, include_defaults=True)</code>","text":"<p>Override ChatAgent's method: extract the function-related args. See that method for details. But specifically about the <code>include_defaults</code> arg: Normally the OpenAI completion API ignores these fields, but the Assistant fn-calling seems to pay attn to these, and if we don't want this, we should set this to False.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def enable_message(\n    self,\n    message_class: Optional[Type[ToolMessage] | List[Type[ToolMessage]]],\n    use: bool = True,\n    handle: bool = True,\n    force: bool = False,\n    require_recipient: bool = False,\n    include_defaults: bool = True,\n) -&gt; None:\n    \"\"\"Override ChatAgent's method: extract the function-related args.\n    See that method for details. But specifically about the `include_defaults` arg:\n    Normally the OpenAI completion API ignores these fields, but the Assistant\n    fn-calling seems to pay attn to these, and if we don't want this,\n    we should set this to False.\n    \"\"\"\n    if message_class is not None and isinstance(message_class, list):\n        for msg_class in message_class:\n            self.enable_message(\n                msg_class,\n                use=use,\n                handle=handle,\n                force=force,\n                require_recipient=require_recipient,\n                include_defaults=include_defaults,\n            )\n        return\n    super().enable_message(\n        message_class,\n        use=use,\n        handle=handle,\n        force=force,\n        require_recipient=require_recipient,\n        include_defaults=include_defaults,\n    )\n    if message_class is None or not use:\n        # no specific msg class, or\n        # we are not enabling USAGE/GENERATION of this tool/fn,\n        # then there's no need to attach the fn to the assistant\n        # (HANDLING the fn will still work via self.agent_response)\n        return\n    if self.config.use_tools:\n        sys_msg = self._create_system_and_tools_message()\n        self.set_system_message(sys_msg.content)\n    if not self.config.use_functions_api:\n        return\n    functions, _, _, _, _ = self._function_args()\n    if functions is None:\n        return\n    # add the functions to the assistant:\n    if self.assistant is None:\n        raise ValueError(\"Assistant is None\")\n    tools = self.assistant.tools\n    tools.extend(\n        [\n            {\n                \"type\": \"function\",  # type: ignore\n                \"function\": f.model_dump(),\n            }\n            for f in functions\n        ]\n    )\n    self.assistant = self.assistants.update(\n        self.assistant.id,\n        tools=tools,  # type: ignore\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.thread_msg_to_llm_msg","title":"<code>thread_msg_to_llm_msg(msg)</code>  <code>staticmethod</code>","text":"<p>Convert a Message to an LLMMessage</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>@staticmethod\ndef thread_msg_to_llm_msg(msg: Message) -&gt; LLMMessage:\n    \"\"\"\n    Convert a Message to an LLMMessage\n    \"\"\"\n    return LLMMessage(\n        content=msg.content[0].text.value,  # type: ignore\n        role=Role(msg.role),\n    )\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.set_system_message","title":"<code>set_system_message(msg)</code>","text":"<p>Override ChatAgent's method. The Task may use this method to set the system message of the chat assistant.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def set_system_message(self, msg: str) -&gt; None:\n    \"\"\"\n    Override ChatAgent's method.\n    The Task may use this method to set the system message\n    of the chat assistant.\n    \"\"\"\n    super().set_system_message(msg)\n    if self.assistant is None:\n        raise ValueError(\"Assistant is None\")\n    self.assistant = self.assistants.update(self.assistant.id, instructions=msg)\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.process_citations","title":"<code>process_citations(thread_msg)</code>","text":"<p>Process citations in the thread message. Modifies the thread message in-place.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def process_citations(self, thread_msg: Message) -&gt; None:\n    \"\"\"\n    Process citations in the thread message.\n    Modifies the thread message in-place.\n    \"\"\"\n    # could there be multiple content items?\n    # TODO content could be MessageContentImageFile; handle that later\n    annotated_content = thread_msg.content[0].text  # type: ignore\n    annotations = annotated_content.annotations\n    citations = []\n    # Iterate over the annotations and add footnotes\n    for index, annotation in enumerate(annotations):\n        # Replace the text with a footnote\n        annotated_content.value = annotated_content.value.replace(\n            annotation.text, f\" [{index}]\"\n        )\n        # Gather citations based on annotation attributes\n        if file_citation := getattr(annotation, \"file_citation\", None):\n            try:\n                cited_file = self.client.files.retrieve(file_citation.file_id)\n            except Exception:\n                logger.warning(\n                    f\"\"\"\n                    Could not retrieve cited file with id {file_citation.file_id}, \n                    ignoring. \n                    \"\"\"\n                )\n                continue\n            citations.append(\n                f\"[{index}] '{file_citation.quote}',-- from {cited_file.filename}\"\n            )\n        elif file_path := getattr(annotation, \"file_path\", None):\n            cited_file = self.client.files.retrieve(file_path.file_id)\n            citations.append(\n                f\"[{index}] Click &lt;here&gt; to download {cited_file.filename}\"\n            )\n        # Note: File download functionality not implemented above for brevity\n    sep = \"\\n\" if len(citations) &gt; 0 else \"\"\n    annotated_content.value += sep + \"\\n\".join(citations)\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Override ChatAgent's method: this is the main LLM response method. In the ChatAgent, this updates <code>self.message_history</code> and then calls <code>self.llm_response_messages</code>, but since we are relying on the Assistant API to maintain conversation state, this method is simpler: Simply start a run on the message-thread, and wait for it to complete.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[str | ChatDocument]</code> <p>message to respond to (if absent, the LLM response will be based on the instructions in the system_message). Defaults to None.</p> <code>None</code> <p>Returns:     Optional[ChatDocument]: LLM response</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>def llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Override ChatAgent's method: this is the main LLM response method.\n    In the ChatAgent, this updates `self.message_history` and then calls\n    `self.llm_response_messages`, but since we are relying on the Assistant API\n    to maintain conversation state, this method is simpler: Simply start a run\n    on the message-thread, and wait for it to complete.\n\n    Args:\n        message (Optional[str | ChatDocument], optional): message to respond to\n            (if absent, the LLM response will be based on the\n            instructions in the system_message). Defaults to None.\n    Returns:\n        Optional[ChatDocument]: LLM response\n    \"\"\"\n    response = self._llm_response_preprocess(message)\n    cached = True\n    if response is None:\n        cached = False\n        response = self._run_result()\n    return self._llm_response_postprocess(response, cached=cached, message=message)\n</code></pre>"},{"location":"reference/agent/openai_assistant/#langroid.agent.openai_assistant.OpenAIAssistant.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Async version of llm_response.</p> Source code in <code>langroid/agent/openai_assistant.py</code> <pre><code>async def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async version of llm_response.\n    \"\"\"\n    response = self._llm_response_preprocess(message)\n    cached = True\n    if response is None:\n        cached = False\n        response = await self._run_result_async()\n    return self._llm_response_postprocess(response, cached=cached, message=message)\n</code></pre>"},{"location":"reference/agent/task/","title":"task","text":"<p>langroid/agent/task.py </p>"},{"location":"reference/agent/task/#langroid.agent.task.EventType","title":"<code>EventType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of events that can occur in a task</p>"},{"location":"reference/agent/task/#langroid.agent.task.AgentEvent","title":"<code>AgentEvent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single event in a task sequence</p>"},{"location":"reference/agent/task/#langroid.agent.task.DoneSequence","title":"<code>DoneSequence</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A sequence of events that triggers task completion</p>"},{"location":"reference/agent/task/#langroid.agent.task.TaskConfig","title":"<code>TaskConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a Task. This is a container for any params that we didn't include in the task <code>__init__</code> method. We may eventually move all the task init params to this class, analogous to how we have config classes for <code>Agent</code>, <code>ChatAgent</code>, <code>LanguageModel</code>, etc.</p> <p>Attributes:</p> Name Type Description <code>inf_loop_cycle_len</code> <code>int</code> <p>max exact-loop cycle length: 0 =&gt; no inf loop test</p> <code>inf_loop_dominance_factor</code> <code>float</code> <p>dominance factor for exact-loop detection</p> <code>inf_loop_wait_factor</code> <code>int</code> <p>wait this * cycle_len msgs before loop-check</p> <code>restart_as_subtask</code> <code>bool</code> <p>whether to restart every run of this task when run as a subtask.</p> <code>addressing_prefix</code> <code>str</code> <p>\"@\"-like prefix an agent can use to address other agents, or entities of the agent. E.g., if this is \"@\", the addressing string would be \"@Alice\", or \"@user\", \"@llm\", \"@agent\", etc. If this is an empty string, then addressing is disabled. Default is empty string \"\". CAUTION: this is a deprecated practice, since normal prompts can accidentally contain such addressing prefixes, and will break your runs. This could happen especially when your prompt/context contains code, but of course could occur in normal text as well. Instead, use the <code>RecipientTool</code> to have agents address other agents or entities. If you do choose to use <code>addressing_prefix</code>, the recommended setting is to use <code>langroid.utils.constants.AT</code>, which currently is \"|@|\". Note that this setting does NOT affect the use of <code>constants.SEND_TO</code> -- this is always enabled since this is a critical way for responders to indicate that the message should be sent to a specific entity/agent. (Search for \"SEND_TO\" in the examples/ dir to see how this is used.)</p> <code>allow_subtask_multi_oai_tools</code> <code>bool</code> <p>whether to allow multiple OpenAI tool-calls to be sent to a sub-task.</p> <code>recognize_string_signals</code> <code>bool</code> <p>whether to recognize string-based signaling like DONE, SEND_TO, PASS, etc. Default is True, but note that we don't need to use string-based signaling, and it is recommended to use the new Orchestration tools instead (see agent/tools/orchestration.py), e.g. DoneTool, SendTool, etc. Note: this is distinct from <code>ChatAgentConfig.recognize_recipient_in_content</code>, which controls whether LLM response text is parsed for <code>TO[&lt;recipient&gt;]:</code> and JSON <code>{\"recipient\": ...}</code> patterns at the Agent level. To fully disable all text-based routing, set both to False.</p> <code>done_if_tool</code> <code>bool</code> <p>whether to consider the task done if the pending message contains a Tool attempt by the LLM (including tools not handled by the agent). Default is False.</p> <code>done_sequences</code> <code>List[DoneSequence]</code> <p>List of event sequences that trigger task completion. Task is done if ANY sequence matches the recent event history. Each sequence is checked against the message parent chain. Tool classes can be referenced in sequences like \"T[MyToolClass]\".</p>"},{"location":"reference/agent/task/#langroid.agent.task.Task","title":"<code>Task(agent=None, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=True, default_human_response=None, interactive=True, only_user_quits_root=True, erase_substeps=False, allow_null_result=False, max_stalled_steps=5, default_return_type=None, done_if_no_response=[], done_if_response=[], config=TaskConfig(), **kwargs)</code>","text":"<p>A <code>Task</code> wraps an <code>Agent</code> object, and sets up the <code>Agent</code>'s goals and instructions. A <code>Task</code> maintains two key variables:</p> <ul> <li><code>self.pending_message</code>, which is the message awaiting a response, and</li> <li><code>self.pending_sender</code>, which is the entity that sent the pending message.</li> </ul> <p>The possible responders to <code>self.pending_message</code> are the <code>Agent</code>'s own \"native\" responders (<code>agent_response</code>, <code>llm_response</code>, and <code>user_response</code>), and the <code>run()</code> methods of any sub-tasks. All responders have the same type-signature (somewhat simplified): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> Responders may or may not specify an intended recipient of their generated response.</p> <p>The main top-level method in the <code>Task</code> class is <code>run()</code>, which repeatedly calls <code>step()</code> until <code>done()</code> returns true. The <code>step()</code> represents a \"turn\" in the conversation: this method sequentially (in round-robin fashion) calls the responders until it finds one that generates a valid response to the <code>pending_message</code> (as determined by the <code>valid()</code> method). Once a valid response is found, <code>step()</code> updates the <code>pending_message</code> and <code>pending_sender</code> variables, and on the next iteration, <code>step()</code> re-starts its search for a valid response from the beginning of the list of responders (the exception being that the human user always gets a chance to respond after each non-human valid response). This process repeats until <code>done()</code> returns true, at which point <code>run()</code> returns the value of <code>result()</code>, which is the final result of the task.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent associated with the task</p> <code>None</code> <code>name</code> <code>str</code> <p>name of the task</p> <code>''</code> <code>llm_delegate</code> <code>bool</code> <p>Whether to delegate \"control\" to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve, and decides when a task is done. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity). See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by \"controller\" (i.e. LLM if <code>llm_delegate</code> is true, otherwise USER) and subsequent response by non-controller [When a tool is involved, this will not give intended results. See <code>done_if_response</code>, <code>done_if_no_response</code> below]. termination]. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\". See also: <code>done_if_response</code>, <code>done_if_no_response</code> for more granular control of task termination.</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's system_message</p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's user_message</p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true (default), resets the agent's message history at every run when it is the top-level task. Ignored when the task is a subtask of another task. Restart behavior of a subtask's <code>run()</code> can be controlled via the <code>TaskConfig.restart_as_subtask</code> setting.</p> <code>True</code> <code>default_human_response</code> <code>str | None</code> <p>default response from user; useful for testing, to avoid interactive input from user. [Instead of this, setting <code>interactive</code> usually suffices]</p> <code>None</code> <code>default_return_type</code> <code>Optional[type]</code> <p>if not None, extracts a value of this type from the result of self.run()</p> <code>None</code> <code>interactive</code> <code>bool</code> <p>if true, wait for human input after each non-human response (prevents infinite loop of non-human responses). Default is true. If false, then <code>default_human_response</code> is set to \"\" Note: When interactive = False, the one exception is when the user is explicitly addressed, via \"@user\" or using RecipientTool, in which case the system will wait for a user response. In other words, use <code>interactive=False</code> when you want a \"largely non-interactive\" run, with the exception of explicit user addressing.</p> <code>True</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, when interactive=True, only user can quit the root task (Ignored when interactive=False).</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> <code>allow_null_result</code> <code>bool</code> <p>If true, create dummy NO_ANSWER response when no valid response is found in a step. Optional, default is False. Note: In non-interactive mode, when this is set to True, you can have a situation where an LLM generates (non-tool) text, and no other responders have valid responses, and a \"Null result\" is inserted as a dummy response from the User entity, so the LLM will now respond to this Null result, and this will continue until the LLM emits a DONE signal (if instructed to do so), otherwise langroid detects a potential infinite loop after a certain number of such steps (= <code>TaskConfig.inf_loop_wait_factor</code>) and will raise an InfiniteLoopException.</p> <code>False</code> <code>max_stalled_steps</code> <code>int</code> <p>task considered done after this many consecutive steps with no progress. Default is 3.</p> <code>5</code> <code>done_if_no_response</code> <code>List[Responder]</code> <p>consider task done if NULL response from any of these responders. Default is empty list.</p> <code>[]</code> <code>done_if_response</code> <code>List[Responder]</code> <p>consider task done if NON-NULL response from any of these responders. Default is empty list.</p> <code>[]</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\n    self,\n    agent: Optional[Agent] = None,\n    name: str = \"\",\n    llm_delegate: bool = False,\n    single_round: bool = False,\n    system_message: str = \"\",\n    user_message: str | None = \"\",\n    restart: bool = True,\n    default_human_response: Optional[str] = None,\n    interactive: bool = True,\n    only_user_quits_root: bool = True,\n    erase_substeps: bool = False,\n    allow_null_result: bool = False,\n    max_stalled_steps: int = 5,\n    default_return_type: Optional[type] = None,\n    done_if_no_response: List[Responder] = [],\n    done_if_response: List[Responder] = [],\n    config: TaskConfig = TaskConfig(),\n    **kwargs: Any,  # catch-all for any legacy params, for backwards compatibility\n):\n    \"\"\"\n    A task to be performed by an agent.\n\n    Args:\n        agent (Agent): agent associated with the task\n        name (str): name of the task\n        llm_delegate (bool):\n            Whether to delegate \"control\" to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve, and decides when a task is done.\n            The \"controlling entity\" is either the LLM or the USER.\n            (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        single_round (bool):\n            If true, task runs until one message by \"controller\"\n            (i.e. LLM if `llm_delegate` is true, otherwise USER)\n            and subsequent response by non-controller [When a tool is involved,\n            this will not give intended results. See `done_if_response`,\n            `done_if_no_response` below].\n            termination]. If false, runs for the specified number of turns in\n            `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n            See also: `done_if_response`, `done_if_no_response` for more granular\n            control of task termination.\n        system_message (str): if not empty, overrides agent's system_message\n        user_message (str): if not empty, overrides agent's user_message\n        restart (bool): if true (default), resets the agent's message history\n            *at every run* when it is the top-level task. Ignored when\n            the task is a subtask of another task. Restart behavior of a subtask's\n            `run()` can be controlled via the `TaskConfig.restart_as_subtask`\n            setting.\n        default_human_response (str|None): default response from user; useful for\n            testing, to avoid interactive input from user.\n            [Instead of this, setting `interactive` usually suffices]\n        default_return_type: if not None, extracts a value of this type from the\n            result of self.run()\n        interactive (bool): if true, wait for human input after each non-human\n            response (prevents infinite loop of non-human responses).\n            Default is true. If false, then `default_human_response` is set to \"\"\n            Note: When interactive = False, the one exception is when the user\n            is explicitly addressed, via \"@user\" or using RecipientTool, in which\n            case the system will wait for a user response. In other words, use\n            `interactive=False` when you want a \"largely non-interactive\"\n            run, with the exception of explicit user addressing.\n        only_user_quits_root (bool): if true, when interactive=True, only user can\n            quit the root task (Ignored when interactive=False).\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n        allow_null_result (bool):\n            If true, create dummy NO_ANSWER response when no valid response is found\n            in a step.\n            Optional, default is False.\n            *Note:* In non-interactive mode, when this is set to True,\n            you can have a situation where an LLM generates (non-tool) text,\n            and no other responders have valid responses, and a \"Null result\"\n            is inserted as a dummy response from the User entity, so the LLM\n            will now respond to this Null result, and this will continue\n            until the LLM emits a DONE signal (if instructed to do so),\n            otherwise langroid detects a potential infinite loop after\n            a certain number of such steps (= `TaskConfig.inf_loop_wait_factor`)\n            and will raise an InfiniteLoopException.\n        max_stalled_steps (int): task considered done after this many consecutive\n            steps with no progress. Default is 3.\n        done_if_no_response (List[Responder]): consider task done if NULL\n            response from any of these responders. Default is empty list.\n        done_if_response (List[Responder]): consider task done if NON-NULL\n            response from any of these responders. Default is empty list.\n    \"\"\"\n    if agent is None:\n        agent = ChatAgent()\n    self.callbacks = SimpleNamespace(\n        show_subtask_response=noop_fn,\n        set_parent_agent=noop_fn,\n    )\n    self.config = config\n    # Store parsed done sequences (will be initialized after agent assignment)\n    self._parsed_done_sequences: Optional[List[DoneSequence]] = None\n    # how to behave as a sub-task; can be overridden by `add_sub_task()`\n    self.config_sub_task = copy.deepcopy(config)\n    # counts of distinct pending messages in history,\n    # to help detect (exact) infinite loops\n    self.message_counter: Counter[str] = Counter()\n    self._init_message_counter()\n\n    self.history: Deque[str] = deque(\n        maxlen=self.config.inf_loop_cycle_len * self.config.inf_loop_wait_factor\n    )\n    # copy the agent's config, so that we don't modify the original agent's config,\n    # which may be shared by other agents.\n    try:\n        config_copy = copy.deepcopy(agent.config)\n        agent.config = config_copy\n    except Exception:\n        logger.warning(\n            \"\"\"\n            Failed to deep-copy Agent config during task creation, \n            proceeding with original config. Be aware that changes to \n            the config may affect other agents using the same config.\n            \"\"\"\n        )\n    self.restart = restart\n    agent = cast(ChatAgent, agent)\n    self.agent: ChatAgent = agent\n    if isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\n        self.agent.init_state()\n        # possibly change the system and user messages\n        if system_message:\n            # we always have at least 1 task_message\n            self.agent.set_system_message(system_message)\n        if user_message:\n            self.agent.set_user_message(user_message)\n\n    # Initialize parsed done sequences now that self.agent is available\n    if self.config.done_sequences:\n        from .done_sequence_parser import parse_done_sequences\n\n        # Pass agent's llm_tools_map directly\n        tools_map = (\n            self.agent.llm_tools_map\n            if hasattr(self.agent, \"llm_tools_map\")\n            else None\n        )\n        self._parsed_done_sequences = parse_done_sequences(\n            self.config.done_sequences, tools_map\n        )\n\n    self.max_cost: float = 0\n    self.max_tokens: int = 0\n    self.session_id: str = \"\"\n    self.logger: None | RichFileLogger = None\n    self.tsv_logger: None | logging.Logger = None\n    self.html_logger: Optional[HTMLLogger] = None\n    self.color_log: bool = False if settings.notebook else True\n\n    self.n_stalled_steps = 0  # how many consecutive steps with no progress?\n    # how many 2-step-apart alternations of no_answer step-result have we had,\n    # i.e. x1, N/A, x2, N/A, x3, N/A ...\n    self.n_no_answer_alternations = 0\n    self._no_answer_step: int = -5\n    self._step_idx = -1  # current step index\n    self.max_stalled_steps = max_stalled_steps\n    self.done_if_response = [r.value for r in done_if_response]\n    self.done_if_no_response = [r.value for r in done_if_no_response]\n    self.is_done = False  # is task done (based on response)?\n    self.is_pass_thru = False  # is current response a pass-thru?\n    if name:\n        # task name overrides name in agent config\n        agent.config.name = name\n    self.name = name or agent.config.name\n    self.value: str = self.name\n\n    self.default_human_response = default_human_response\n    if default_human_response is not None:\n        # only override agent's default_human_response if it is explicitly set\n        self.agent.default_human_response = default_human_response\n    self.interactive = interactive\n    self.agent.interactive = interactive\n    self.only_user_quits_root = only_user_quits_root\n    self.message_history_idx = -1\n    self.default_return_type = default_return_type\n\n    # set to True if we want to collapse multi-turn conversation with sub-tasks into\n    # just the first outgoing message and last incoming message.\n    # Note this also completely erases sub-task agents' message_history.\n    self.erase_substeps = erase_substeps\n    self.allow_null_result = allow_null_result\n\n    agent_entity_responders = agent.entity_responders()\n    agent_entity_responders_async = agent.entity_responders_async()\n    self.responders: List[Responder] = [e for e, _ in agent_entity_responders]\n    self.responders_async: List[Responder] = [\n        e for e, _ in agent_entity_responders_async\n    ]\n    self.non_human_responders: List[Responder] = [\n        r for r in self.responders if r != Entity.USER\n    ]\n    self.non_human_responders_async: List[Responder] = [\n        r for r in self.responders_async if r != Entity.USER\n    ]\n\n    self.human_tried = False  # did human get a chance to respond in last step?\n    self._entity_responder_map: Dict[\n        Entity, Callable[..., Optional[ChatDocument]]\n    ] = dict(agent_entity_responders)\n\n    self._entity_responder_async_map: Dict[\n        Entity, Callable[..., Coroutine[Any, Any, Optional[ChatDocument]]]\n    ] = dict(agent_entity_responders_async)\n\n    self.name_sub_task_map: Dict[str, Task] = {}\n    # latest message in a conversation among entities and agents.\n    self.pending_message: Optional[ChatDocument] = None\n    self.pending_sender: Responder = Entity.USER\n    self.single_round = single_round\n    self.turns = -1  # no limit\n    self.llm_delegate = llm_delegate\n    # Track last responder for done sequence checking\n    self._last_responder: Optional[Responder] = None\n    # Track response sequence for message chain\n    self.response_sequence: List[ChatDocument] = []\n    if llm_delegate:\n        if self.single_round:\n            # 0: User instructs (delegating to LLM);\n            # 1: LLM (as the Controller) asks;\n            # 2: user replies.\n            self.turns = 2\n    else:\n        if self.single_round:\n            # 0: User (as Controller) asks,\n            # 1: LLM replies.\n            self.turns = 1\n    # other sub_tasks this task can delegate to\n    self.sub_tasks: List[Task] = []\n    self.caller: Task | None = None  # which task called this task's `run` method\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.clone","title":"<code>clone(i)</code>","text":"<p>Returns a copy of this task, with a new agent.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def clone(self, i: int) -&gt; \"Task\":\n    \"\"\"\n    Returns a copy of this task, with a new agent.\n    \"\"\"\n    assert isinstance(self.agent, ChatAgent), \"Task clone only works for ChatAgent\"\n    agent: ChatAgent = self.agent.clone(i)\n    return Task(\n        agent,\n        name=self.name + f\"-{i}\",\n        llm_delegate=self.llm_delegate,\n        single_round=self.single_round,\n        system_message=self.agent.system_message,\n        user_message=self.agent.user_message,\n        restart=self.restart,\n        default_human_response=self.default_human_response,\n        interactive=self.interactive,\n        erase_substeps=self.erase_substeps,\n        allow_null_result=self.allow_null_result,\n        max_stalled_steps=self.max_stalled_steps,\n        done_if_no_response=[Entity(s) for s in self.done_if_no_response],\n        done_if_response=[Entity(s) for s in self.done_if_response],\n        default_return_type=self.default_return_type,\n        config=self.config,\n    )\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.kill_session","title":"<code>kill_session(session_id='')</code>  <code>classmethod</code>","text":"<p>Kill the session with the given session_id.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>@classmethod\ndef kill_session(cls, session_id: str = \"\") -&gt; None:\n    \"\"\"\n    Kill the session with the given session_id.\n    \"\"\"\n    session_id_kill_key = f\"{session_id}:kill\"\n    cls.cache().store(session_id_kill_key, \"1\")\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.kill","title":"<code>kill()</code>","text":"<p>Kill the task run associated with the current session.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def kill(self) -&gt; None:\n    \"\"\"\n    Kill the task run associated with the current session.\n    \"\"\"\n    self._cache_session_store(\"kill\", \"1\")\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response (unless a sub-task is explicitly addressed).</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]</code> <p>A task, or list of tasks, or a tuple of task and task config, or a list of tuples of task and task config. These tasks are added as sub-tasks of the current task. The task configs (if any) dictate how the tasks are run when invoked as sub-tasks of other tasks. This allows users to specify behavior applicable only in the context of a particular task-subtask combination.</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(\n    self,\n    task: (\n        Task | List[Task] | Tuple[Task, TaskConfig] | List[Tuple[Task, TaskConfig]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response (unless a sub-task is explicitly addressed).\n\n    Args:\n        task: A task, or list of tasks, or a tuple of task and task config,\n            or a list of tuples of task and task config.\n            These tasks are added as sub-tasks of the current task.\n            The task configs (if any) dictate how the tasks are run when\n            invoked as sub-tasks of other tasks. This allows users to specify\n            behavior applicable only in the context of a particular task-subtask\n            combination.\n    \"\"\"\n    if isinstance(task, list):\n        for t in task:\n            self.add_sub_task(t)\n        return\n\n    if isinstance(task, tuple):\n        task, config = task\n    else:\n        config = TaskConfig()\n    task.config_sub_task = config\n    self.sub_tasks.append(task)\n    self.name_sub_task_map[task.name] = task\n    self.responders.append(cast(Responder, task))\n    self.responders_async.append(cast(Responder, task))\n    self.non_human_responders.append(cast(Responder, task))\n    self.non_human_responders_async.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>. Args:     msg (str|ChatDocument): optional message to start the conversation.</p> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\n    self.pending_sender = Entity.USER\n    if isinstance(msg, str):\n        self.pending_message = ChatDocument(\n            content=msg,\n            metadata=ChatDocMetaData(\n                sender=Entity.USER,\n            ),\n        )\n    elif msg is None and len(self.agent.message_history) &gt; 1:\n        # if agent has a history beyond system msg, set the\n        # pending message to the ChatDocument linked from\n        # last message in the history\n        last_agent_msg = self.agent.message_history[-1]\n        self.pending_message = ChatDocument.from_id(last_agent_msg.chat_document_id)\n        if self.pending_message is not None:\n            self.pending_sender = self.pending_message.metadata.sender\n    else:\n        if isinstance(msg, ChatDocument):\n            # carefully deep-copy: fresh metadata.id, register\n            # as new obj in registry\n            original_parent_id = msg.metadata.parent_id\n            self.pending_message = ChatDocument.deepcopy(msg)\n            # Preserve the parent pointer from the original message\n            self.pending_message.metadata.parent_id = original_parent_id\n        if self.pending_message is not None and self.caller is not None:\n            # msg may have come from `caller`, so we pretend this is from\n            # the CURRENT task's USER entity\n            self.pending_message.metadata.sender = Entity.USER\n            # update parent, child, agent pointers\n            if msg is not None:\n                msg.metadata.child_id = self.pending_message.metadata.id\n                # Only override parent_id if it wasn't already set in the\n                # original message. This preserves parent chains from TaskTool\n                if not msg.metadata.parent_id:\n                    self.pending_message.metadata.parent_id = msg.metadata.id\n        if self.pending_message is not None:\n            self.pending_message.metadata.agent_id = self.agent.id\n\n    self._show_pending_message_if_debug()\n    self.init_loggers()\n    # Log system message if it exists\n    if (\n        hasattr(self.agent, \"_create_system_and_tools_message\")\n        and hasattr(self.agent, \"system_message\")\n        and self.agent.system_message\n    ):\n        system_msg = self.agent._create_system_and_tools_message()\n        system_message_chat_doc = ChatDocument.from_LLMMessage(\n            system_msg,\n            sender_name=self.name or \"system\",\n        )\n        # log the system message\n        self.log_message(Entity.SYSTEM, system_message_chat_doc, mark=True)\n    self.log_message(Entity.USER, self.pending_message, mark=True)\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.init_loggers","title":"<code>init_loggers()</code>","text":"<p>Initialise per-task Rich and TSV loggers.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init_loggers(self) -&gt; None:\n    \"\"\"Initialise per-task Rich and TSV loggers.\"\"\"\n    from langroid.utils.logging import RichFileLogger\n\n    if not self.config.enable_loggers:\n        return\n\n    if self.caller is not None and self.caller.logger is not None:\n        self.logger = self.caller.logger\n    elif self.logger is None:\n        self.logger = RichFileLogger(\n            str(Path(self.config.logs_dir) / f\"{self.name}.log\"),\n            append=True,\n            color=self.color_log,\n        )\n\n    if self.caller is not None and self.caller.tsv_logger is not None:\n        self.tsv_logger = self.caller.tsv_logger\n    elif self.tsv_logger is None:\n        # unique logger name ensures a distinct `logging.Logger` object\n        self.tsv_logger = setup_file_logger(\n            f\"tsv_logger.{self.name}.{id(self)}\",\n            str(Path(self.config.logs_dir) / f\"{self.name}.tsv\"),\n        )\n        header = ChatDocLoggerFields().tsv_header()\n        self.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\n\n    # HTML logger\n    if self.config.enable_html_logging:\n        if (\n            self.caller is not None\n            and hasattr(self.caller, \"html_logger\")\n            and self.caller.html_logger is not None\n        ):\n            self.html_logger = self.caller.html_logger\n        elif not hasattr(self, \"html_logger\") or self.html_logger is None:\n            from langroid.utils.html_logger import HTMLLogger\n\n            model_info = \"\"\n            if (\n                hasattr(self, \"agent\")\n                and hasattr(self.agent, \"config\")\n                and hasattr(self.agent.config, \"llm\")\n            ):\n                model_info = getattr(self.agent.config.llm, \"chat_model\", \"\")\n            self.html_logger = HTMLLogger(\n                filename=self.name,\n                log_dir=self.config.logs_dir,\n                model_info=model_info,\n                append=False,\n            )\n            # Log clickable file:// link to the HTML log\n            html_log_path = self.html_logger.file_path.resolve()\n            logger.warning(f\"\ud83d\udcca HTML Log: file://{html_log_path}\")\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.reset_all_sub_tasks","title":"<code>reset_all_sub_tasks()</code>","text":"<p>Recursively reset message history &amp; state of own agent and those of all sub-tasks.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def reset_all_sub_tasks(self) -&gt; None:\n    \"\"\"\n    Recursively reset message history &amp; state of own agent and\n    those of all sub-tasks.\n    \"\"\"\n    self.agent.init_state()\n    for t in self.sub_tasks:\n        t.reset_all_sub_tasks()\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.run","title":"<code>run(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='', allow_restart=True, return_type=None)</code>","text":"<pre><code>run(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True\n) -&gt; Optional[ChatDocument]\n</code></pre><pre><code>run(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Type[T]\n) -&gt; Optional[T]\n</code></pre> <p>Synchronous version of <code>run_async()</code>. See <code>run_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\n    self,\n    msg: Any = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Optional[Type[T]] = None,\n) -&gt; Optional[ChatDocument | T]:\n    \"\"\"Synchronous version of `run_async()`.\n    See `run_async()` for details.\"\"\"\n    if allow_restart and (\n        (self.restart and caller is None)\n        or (self.config_sub_task.restart_as_subtask and caller is not None)\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    msg_input = self.agent.to_ChatDocument(msg, author_entity=Entity.USER)\n\n    if (\n        isinstance(msg_input, ChatDocument)\n        and msg_input.metadata.recipient != \"\"\n        and msg_input.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n\n    self._pre_run_loop(\n        msg=msg_input,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        self.step()\n        # Track pending message in response sequence\n        if self.pending_message is not None:\n            if (\n                not self.response_sequence\n                or self.pending_message.id() != self.response_sequence[-1].id()\n            ):\n                self.response_sequence.append(self.pending_message)\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    if final_result is None:\n        return None\n\n    if return_type is None:\n        return_type = self.default_return_type\n\n    # If possible, take a final strict decoding step\n    # when the output does not match `return_type`\n    if return_type is not None and return_type != ChatDocument:\n        parsed_result = self.agent.from_ChatDocument(final_result, return_type)\n\n        if (\n            parsed_result is None\n            and isinstance(self.agent, ChatAgent)\n            and self.agent._json_schema_available()\n        ):\n            strict_agent = self.agent[return_type]\n            output_args = strict_agent._function_args()[-1]\n            if output_args is not None:\n                schema = output_args.function.parameters\n                strict_result = strict_agent.llm_response(\n                    f\"\"\"\n                    A response adhering to the following JSON schema was expected:\n                    {schema}\n\n                    Please resubmit with the correct schema. \n                    \"\"\"\n                )\n\n                if strict_result is not None:\n                    return cast(\n                        Optional[T],\n                        strict_agent.from_ChatDocument(strict_result, return_type),\n                    )\n\n        return parsed_result\n\n    return final_result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.run_async","title":"<code>run_async(msg=None, turns=-1, caller=None, max_cost=0, max_tokens=0, session_id='', allow_restart=True, return_type=None)</code>  <code>async</code>","text":"<pre><code>run_async(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True\n) -&gt; Optional[ChatDocument]\n</code></pre><pre><code>run_async(\n    msg: Any = None,\n    *,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Type[T]\n) -&gt; Optional[T]\n</code></pre> <p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached. Runs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>Any</code> <p>initial user-role message to process; if None, the LLM will respond to its initial <code>self.task_messages</code> which set up and kick off the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User). Note that <code>msg</code>, if passed, is treated as message with role <code>user</code>; a \"system\" role message should not be passed here.</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <code>caller</code> <code>Task | None</code> <p>the calling task, if any</p> <code>None</code> <code>max_cost</code> <code>float</code> <p>max cost allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>max_tokens</code> <code>int</code> <p>max tokens allowed for the task (default 0 -&gt; no limit)</p> <code>0</code> <code>session_id</code> <code>str</code> <p>session id for the task</p> <code>''</code> <code>allow_restart</code> <code>bool</code> <p>whether to allow restarting the task</p> <code>True</code> <code>return_type</code> <code>Optional[Type[T]]</code> <p>desired final result type</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument | T]</code> <p>Optional[ChatDocument]: valid result of the task.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def run_async(\n    self,\n    msg: Any = None,\n    turns: int = -1,\n    caller: None | Task = None,\n    max_cost: float = 0,\n    max_tokens: int = 0,\n    session_id: str = \"\",\n    allow_restart: bool = True,\n    return_type: Optional[Type[T]] = None,\n) -&gt; Optional[ChatDocument | T]:\n    \"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Runs asynchronously.\n\n    Args:\n        msg (Any): initial *user-role* message to process; if None,\n            the LLM will respond to its initial `self.task_messages`\n            which set up and kick off the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User). Note that `msg`, if passed, is treated as\n            message with role `user`; a \"system\" role message should not be\n            passed here.\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n        caller (Task|None): the calling task, if any\n        max_cost (float): max cost allowed for the task (default 0 -&gt; no limit)\n        max_tokens (int): max tokens allowed for the task (default 0 -&gt; no limit)\n        session_id (str): session id for the task\n        allow_restart (bool): whether to allow restarting the task\n        return_type (Optional[Type[T]]): desired final result type\n\n    Returns:\n        Optional[ChatDocument]: valid result of the task.\n    \"\"\"\n\n    # Even if the initial \"sender\" is not literally the USER (since the task could\n    # have come from another LLM), as far as this agent is concerned, the initial\n    # message can be considered to be from the USER\n    # (from the POV of this agent's LLM).\n\n    if allow_restart and (\n        (self.restart and caller is None)\n        or (self.config_sub_task.restart_as_subtask and caller is not None)\n    ):\n        # We are either at top level, with restart = True, OR\n        # we are a sub-task with restart_as_subtask = True,\n        # so reset own agent and recursively for all sub-tasks\n        self.reset_all_sub_tasks()\n\n    self.n_stalled_steps = 0\n    self._no_answer_step = -5  # last step where the best explicit response was N/A\n    # how many N/A alternations have we had so far? (for Inf loop detection)\n    self.n_no_answer_alternations = 0\n    self.max_cost = max_cost\n    self.max_tokens = max_tokens\n    self.session_id = session_id\n    self._set_alive()\n    self._init_message_counter()\n    self.history.clear()\n\n    msg_input = self.agent.to_ChatDocument(msg, author_entity=Entity.USER)\n\n    if (\n        isinstance(msg_input, ChatDocument)\n        and msg_input.metadata.recipient != \"\"\n        and msg_input.metadata.recipient != self.name\n    ):\n        # this task is not the intended recipient so return None\n        return None\n\n    self._pre_run_loop(\n        msg=msg_input,\n        caller=caller,\n        is_async=False,\n    )\n    # self.turns overrides if it is &gt; 0 and turns not set (i.e. = -1)\n    turns = self.turns if turns &lt; 0 else turns\n    i = 0\n    while True:\n        self._step_idx = i  # used in step() below\n        await self.step_async()\n        await asyncio.sleep(0.01)  # temp yield to avoid blocking\n        # Track pending message in response sequence\n        if self.pending_message is not None:\n            if (\n                not self.response_sequence\n                or self.pending_message.id() != self.response_sequence[-1].id()\n            ):\n                self.response_sequence.append(self.pending_message)\n\n        done, status = self.done()\n        if done:\n            if self._level == 0 and not settings.quiet:\n                print(\"[magenta]Bye, hope this was useful!\")\n            break\n        i += 1\n        max_turns = (\n            min(turns, settings.max_turns)\n            if turns &gt; 0 and settings.max_turns &gt; 0\n            else max(turns, settings.max_turns)\n        )\n        if max_turns &gt; 0 and i &gt;= max_turns:\n            # Important to distinguish between:\n            # (a) intentional run for a\n            #     fixed number of turns, where we expect the pending message\n            #     at that stage to be the desired result, and\n            # (b) hitting max_turns limit, which is not intentional, and is an\n            #     exception, resulting in a None task result\n            status = (\n                StatusCode.MAX_TURNS\n                if i == settings.max_turns\n                else StatusCode.FIXED_TURNS\n            )\n            break\n        if (\n            self.config.inf_loop_cycle_len &gt; 0\n            and i % self.config.inf_loop_cycle_len == 0\n            and self._maybe_infinite_loop()\n            or self.n_no_answer_alternations &gt; self.config.inf_loop_wait_factor\n        ):\n            raise InfiniteLoopException(\n                \"\"\"Possible infinite loop detected!\n                You can adjust infinite loop detection (or turn it off)\n                by changing the params in the TaskConfig passed to the Task \n                constructor; see here:\n                https://langroid.github.io/langroid/reference/agent/task/#langroid.agent.task.TaskConfig\n                \"\"\"\n            )\n\n    final_result = self.result(status)\n    self._post_run_loop()\n    if final_result is None:\n        return None\n\n    if return_type is None:\n        return_type = self.default_return_type\n\n    # If possible, take a final strict decoding step\n    # when the output does not match `return_type`\n    if return_type is not None and return_type != ChatDocument:\n        parsed_result = self.agent.from_ChatDocument(final_result, return_type)\n\n        if (\n            parsed_result is None\n            and isinstance(self.agent, ChatAgent)\n            and self.agent._json_schema_available()\n        ):\n            strict_agent = self.agent[return_type]\n            output_args = strict_agent._function_args()[-1]\n            if output_args is not None:\n                schema = output_args.function.parameters\n                strict_result = await strict_agent.llm_response_async(\n                    f\"\"\"\n                    A response adhering to the following JSON schema was expected:\n                    {schema}\n\n                    Please resubmit with the correct schema. \n                    \"\"\"\n                )\n\n                if strict_result is not None:\n                    return cast(\n                        Optional[T],\n                        strict_agent.from_ChatDocument(strict_result, return_type),\n                    )\n\n        return parsed_result\n\n    return final_result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>Synchronous version of <code>step_async()</code>. See <code>step_async()</code> for details. TODO: Except for the self.response() calls, this fn should be identical to <code>step_async()</code>. Consider refactoring to avoid duplication.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    Synchronous version of `step_async()`. See `step_async()` for details.\n    TODO: Except for the self.response() calls, this fn should be identical to\n    `step_async()`. Consider refactoring to avoid duplication.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # (When `interactive=False`, human is only allowed to respond only if\n        #  if explicitly addressed)\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    n_non_responders = 0\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            n_non_responders += 1\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            if n_non_responders == len(responders):\n                # don't stay in this \"non-response\" loop forever\n                break\n            continue\n        self.human_tried = r == Entity.USER\n        result = self.response(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:  # did not find a valid response\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.step_async","title":"<code>step_async(turns=-1)</code>  <code>async</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def step_async(self, turns: int = -1) -&gt; ChatDocument | None:\n    \"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\n    self.is_done = False\n    parent = self.pending_message\n    recipient = (\n        \"\"\n        if self.pending_message is None\n        else self.pending_message.metadata.recipient\n    )\n    if not self._valid_recipient(recipient):\n        logger.warning(f\"Invalid recipient: {recipient}\")\n        error_doc = ChatDocument(\n            content=f\"Invalid recipient: {recipient}\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                sender_name=Entity.AGENT,\n            ),\n        )\n        self._process_valid_responder_result(Entity.AGENT, parent, error_doc)\n        return error_doc\n\n    responders: List[Responder] = self.non_human_responders_async.copy()\n\n    if (\n        Entity.USER in self.responders\n        and not self.human_tried\n        and not self.agent.has_tool_message_attempt(self.pending_message)\n    ):\n        # Give human first chance if they haven't been tried in last step,\n        # and the msg is not a tool-call attempt;\n        # This ensures human gets a chance to respond,\n        #   other than to a LLM tool-call.\n        # When there's a tool msg attempt we want the\n        #  Agent to be the next responder; this only makes a difference in an\n        #  interactive setting: LLM generates tool, then we don't want user to\n        #  have to respond, and instead let the agent_response handle the tool.\n        responders.insert(0, Entity.USER)\n\n    found_response = False\n    # (responder, result) from a responder who explicitly said NO_ANSWER\n    no_answer_response: None | Tuple[Responder, ChatDocument] = None\n    for r in responders:\n        self.is_pass_thru = False\n        if not self._can_respond(r):\n            # create dummy msg for logging\n            log_doc = ChatDocument(\n                content=\"[CANNOT RESPOND]\",\n                metadata=ChatDocMetaData(\n                    sender=r if isinstance(r, Entity) else Entity.USER,\n                    sender_name=str(r),\n                    recipient=recipient,\n                ),\n            )\n            # no need to register this dummy msg in ObjectRegistry\n            ChatDocument.delete_id(log_doc.id())\n            self.log_message(r, log_doc)\n            continue\n        self.human_tried = r == Entity.USER\n        result = await self.response_async(r, turns)\n        if result and NO_ANSWER in result.content:\n            no_answer_response = (r, result)\n        self.is_done = self._is_done_response(result, r)\n        self.is_pass_thru = PASS in result.content if result else False\n        if self.valid(result, r):\n            found_response = True\n            assert result is not None\n            self._process_valid_responder_result(r, parent, result)\n            break\n        else:\n            self.log_message(r, result)\n        if self.is_done:\n            # skip trying other responders in this step\n            break\n    if not found_response:\n        if no_answer_response:\n            # even though there was no valid response from anyone in this step,\n            # if there was at least one who EXPLICITLY said NO_ANSWER, then\n            # we process that as a valid response.\n            r, result = no_answer_response\n            self._process_valid_responder_result(r, parent, result)\n        else:\n            self._process_invalid_step_result(parent)\n    self._show_pending_message_if_debug()\n    return self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Sync version of <code>response_async()</code>. See <code>response_async()</code> for details.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Sync version of `response_async()`. See `response_async()` for details.\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        # e.callbacks.set_parent_agent(self.agent)\n        pending_tools = self.agent.try_get_tool_messages(self.pending_message)\n        # TODO disable this\n        if (\n            len(pending_tools) &gt; 1\n            and len(self.agent.oai_tool_calls) &gt; 1\n            and not self.config.allow_subtask_multi_oai_tools\n        ):\n            result = self._forbid_multi_oai_tools(e)\n        else:\n            result = e.run(\n                self.pending_message,\n                turns=actual_turns,\n                caller=self,\n                max_cost=self.max_cost,\n                max_tokens=self.max_tokens,\n            )\n            # update result.tool_messages if any\n            if isinstance(result, ChatDocument):\n                self.agent.try_get_tool_messages(result)\n            if result is not None:\n                content, id2result, oai_tool_id = self.agent.process_tool_results(\n                    result.content,\n                    result.oai_tool_id2result,\n                    (\n                        self.pending_message.oai_tool_calls\n                        if isinstance(self.pending_message, ChatDocument)\n                        else None\n                    ),\n                )\n                result.content = content\n                result.oai_tool_id2result = id2result\n                result.metadata.oai_tool_id = oai_tool_id\n\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\"\n            if result is None\n            else \"\\n\\n\".join(str(m) for m in ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_map[cast(Entity, e)]\n        result = response_fn(self.pending_message)\n        # update result.tool_messages if any.\n        # Do this only if sender is LLM, since this could be\n        # a tool-call result from the Agent responder, which may\n        # contain strings that look like tools, and we don't want to\n        # trigger strict tool recovery due to that.\n        if (\n            isinstance(result, ChatDocument)\n            and result.metadata.sender == Entity.LLM\n        ):\n            self.agent.try_get_tool_messages(result)\n\n    result_chat_doc = self.agent.to_ChatDocument(\n        result,\n        chat_doc=self.pending_message,\n        author_entity=e if isinstance(e, Entity) else Entity.USER,\n    )\n    return self._process_result_routing(result_chat_doc, e)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.response_async","title":"<code>response_async(e, turns=-1)</code>  <code>async</code>","text":"<p>Get response to <code>self.pending_message</code> from a responder. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None. Args:     e (Responder): responder to get response from.     turns (int): number of turns to run the task for.         Default is -1, which means run until task is done.</p> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>async def response_async(\n    self,\n    e: Responder,\n    turns: int = -1,\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Get response to `self.pending_message` from a responder.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Responder): responder to get response from.\n        turns (int): number of turns to run the task for.\n            Default is -1, which means run until task is done.\n\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\n    if isinstance(e, Task):\n        actual_turns = e.turns if e.turns &gt; 0 else turns\n        e.agent.callbacks.set_parent_agent(self.agent)\n        pending_tools = self.agent.try_get_tool_messages(self.pending_message)\n        # TODO disable this\n        if (\n            len(pending_tools) &gt; 1\n            and len(self.agent.oai_tool_calls) &gt; 1\n            and not self.config.allow_subtask_multi_oai_tools\n        ):\n            result = self._forbid_multi_oai_tools(e)\n        else:\n            # e.callbacks.set_parent_agent(self.agent)\n            result = await e.run_async(\n                self.pending_message,\n                turns=actual_turns,\n                caller=self,\n                max_cost=self.max_cost,\n                max_tokens=self.max_tokens,\n            )\n            # update result.tool_messages if any\n            if isinstance(result, ChatDocument):\n                self.agent.try_get_tool_messages(result)\n            if result is not None:\n                content, id2result, oai_tool_id = self.agent.process_tool_results(\n                    result.content,\n                    result.oai_tool_id2result,\n                    (\n                        self.pending_message.oai_tool_calls\n                        if isinstance(self.pending_message, ChatDocument)\n                        else None\n                    ),\n                )\n                result.content = content\n                result.oai_tool_id2result = id2result\n                result.metadata.oai_tool_id = oai_tool_id\n\n        result_str = (  # only used by callback to display content and possible tool\n            \"NONE\"\n            if result is None\n            else \"\\n\\n\".join(str(m) for m in ChatDocument.to_LLMMessage(result))\n        )\n        maybe_tool = len(extract_top_level_json(result_str)) &gt; 0\n        self.callbacks.show_subtask_response(\n            task=e,\n            content=result_str,\n            is_tool=maybe_tool,\n        )\n    else:\n        response_fn = self._entity_responder_async_map[cast(Entity, e)]\n        result = await response_fn(self.pending_message)\n        # update result.tool_messages if any\n        if (\n            isinstance(result, ChatDocument)\n            and result.metadata.sender == Entity.LLM\n        ):\n            self.agent.try_get_tool_messages(result)\n\n    result_chat_doc = self.agent.to_ChatDocument(\n        result,\n        chat_doc=self.pending_message,\n        author_entity=e if isinstance(e, Entity) else Entity.USER,\n    )\n    return self._process_result_routing(result_chat_doc, e)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.result","title":"<code>result(status=None)</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this.</p> <p>Note the result of a task is returned as if it is from the User entity.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>StatusCode</code> <p>status of the task when it ended</p> <code>None</code> <p>Returns:     ChatDocument: result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self, status: StatusCode | None = None) -&gt; ChatDocument | None:\n    \"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n\n    Note the result of a task is returned as if it is from the User entity.\n\n    Args:\n        status (StatusCode): status of the task when it ended\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\n    if status in [StatusCode.STALLED, StatusCode.MAX_TURNS, StatusCode.INF_LOOP]:\n        # In these case we don't know (and don't want to try to guess)\n        # what the task result should be, so we return None\n        return None\n\n    result_msg = self.pending_message\n\n    content = result_msg.content if result_msg else \"\"\n    content_any = result_msg.content_any if result_msg else None\n    if DONE in content and self.config.recognize_string_signals:\n        # assuming it is of the form \"DONE: &lt;content&gt;\"\n        content = content.replace(DONE, \"\").strip()\n    oai_tool_calls = result_msg.oai_tool_calls if result_msg else None\n    oai_tool_id2result = result_msg.oai_tool_id2result if result_msg else None\n    fun_call = result_msg.function_call if result_msg else None\n    tool_messages = result_msg.tool_messages if result_msg else []\n    # if there is a DoneTool or AgentDoneTool among these,\n    # we extract content and tools from here, and ignore all others\n    for t in tool_messages:\n        if isinstance(t, FinalResultTool):\n            content = \"\"\n            content_any = None\n            tool_messages = [t]  # pass it on to parent so it also quits\n            break\n        elif isinstance(t, (AgentDoneTool, DoneTool)):\n            # there shouldn't be multiple tools like this; just take the first\n            content = to_string(t.content)\n            content_any = t.content\n            fun_call = None\n            oai_tool_calls = None\n            if isinstance(t, AgentDoneTool):\n                # AgentDoneTool may have tools, unlike DoneTool\n                tool_messages = t.tools\n            break\n    # drop the \"Done\" tools since they should not be part of the task result,\n    # or else they would cause the parent task to get unintentionally done!\n    tool_messages = [\n        t for t in tool_messages if not isinstance(t, (DoneTool, AgentDoneTool))\n    ]\n    block = result_msg.metadata.block if result_msg else None\n    recipient = result_msg.metadata.recipient if result_msg else \"\"\n    tool_ids = result_msg.metadata.tool_ids if result_msg else []\n\n    # regardless of which entity actually produced the result,\n    # when we return the result, we set entity to USER\n    # since to the \"parent\" task, this result is equivalent to a response from USER\n    result_doc = ChatDocument(\n        content=content,\n        content_any=content_any,\n        oai_tool_calls=oai_tool_calls,\n        oai_tool_id2result=oai_tool_id2result,\n        function_call=fun_call,\n        tool_messages=tool_messages,\n        metadata=ChatDocMetaData(\n            source=Entity.USER,\n            sender=Entity.USER,\n            block=block,\n            status=status or (result_msg.metadata.status if result_msg else None),\n            sender_name=self.name,\n            recipient=recipient,\n            tool_ids=tool_ids,\n            parent_id=result_msg.id() if result_msg else \"\",\n            agent_id=str(self.agent.id),\n        ),\n    )\n    if self.pending_message is not None:\n        self.pending_message.metadata.child_id = result_doc.id()\n\n    return result_doc\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.done","title":"<code>done(result=None, r=None)</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this. Args:     result (ChatDocument|None): result from a responder     r (Responder|None): responder that produced the result         Not used here, but could be used by derived classes. Returns:     bool: True if task is done, False otherwise     StatusCode: status code indicating why task is done</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(\n    self, result: ChatDocument | None = None, r: Responder | None = None\n) -&gt; Tuple[bool, StatusCode]:\n    \"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Args:\n        result (ChatDocument|None): result from a responder\n        r (Responder|None): responder that produced the result\n            Not used here, but could be used by derived classes.\n    Returns:\n        bool: True if task is done, False otherwise\n        StatusCode: status code indicating why task is done\n    \"\"\"\n    if self._is_kill():\n        return (True, StatusCode.KILL)\n    result = result or self.pending_message\n\n    # Check if task should be done if message contains a tool\n    if self.config.done_if_tool and result is not None:\n        if isinstance(result, ChatDocument) and self.agent.try_get_tool_messages(\n            result, all_tools=True\n        ):\n            return (True, StatusCode.DONE)\n\n    # Check done sequences\n    if self._parsed_done_sequences and result is not None:\n        # Get the message chain from the current result\n        msg_chain = self._get_message_chain(result)\n\n        # Use last responder if r not provided\n        responder = r if r is not None else self._last_responder\n\n        # Check each sequence\n        for sequence in self._parsed_done_sequences:\n            if self._matches_sequence_with_current(\n                msg_chain, sequence, result, responder\n            ):\n                seq_name = sequence.name or \"unnamed\"\n                logger.info(f\"Task {self.name} done: matched sequence '{seq_name}'\")\n                return (True, StatusCode.DONE)\n\n    allow_done_string = self.config.recognize_string_signals\n    # An entity decided task is done, either via DoneTool,\n    # or by explicitly saying DONE\n    done_result = result is not None and (\n        (\n            DONE in (result.content if isinstance(result, str) else result.content)\n            and allow_done_string\n        )\n        or any(\n            isinstance(t, (DoneTool, AgentDoneTool, FinalResultTool))\n            for t in result.tool_messages\n        )\n    )\n\n    user_quit = (\n        result is not None\n        and (result.content in USER_QUIT_STRINGS or done_result)\n        and result.metadata.sender == Entity.USER\n    )\n\n    if self.n_stalled_steps &gt;= self.max_stalled_steps:\n        # we are stuck, so bail to avoid infinite loop\n        logger.warning(\n            f\"Task {self.name} stuck for {self.max_stalled_steps} steps; exiting.\"\n        )\n        return (True, StatusCode.STALLED)\n\n    if self.max_cost &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[1] &gt; self.max_cost:\n                logger.warning(\n                    f\"Task {self.name} cost exceeded {self.max_cost}; exiting.\"\n                )\n                return (True, StatusCode.MAX_COST)\n        except Exception:\n            pass\n\n    if self.max_tokens &gt; 0 and self.agent.llm is not None:\n        try:\n            if self.agent.llm.tot_tokens_cost()[0] &gt; self.max_tokens:\n                logger.warning(\n                    f\"Task {self.name} uses &gt; {self.max_tokens} tokens; exiting.\"\n                )\n                return (True, StatusCode.MAX_TOKENS)\n        except Exception:\n            pass\n\n    if self._level == 0 and self._user_can_respond() and self.only_user_quits_root:\n        # for top-level task, only user can quit out\n        return (user_quit, StatusCode.USER_QUIT if user_quit else StatusCode.OK)\n\n    if self.is_done:\n        return (True, StatusCode.DONE)\n\n    final = (\n        # no valid response from any entity/agent in current turn\n        result is None\n        or done_result\n        or (  # current task is addressing message to caller task\n            self.caller is not None\n            and self.caller.name != \"\"\n            and result.metadata.recipient == self.caller.name\n        )\n        or user_quit\n    )\n    return (final, StatusCode.OK)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.valid","title":"<code>valid(result, r)</code>","text":"<p>Is the result from a Responder (i.e. an entity or sub-task) such that we can stop searching for responses in this step?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(\n    self,\n    result: Optional[ChatDocument],\n    r: Responder,\n) -&gt; bool:\n    \"\"\"\n    Is the result from a Responder (i.e. an entity or sub-task)\n    such that we can stop searching for responses in this step?\n    \"\"\"\n    # TODO caution we should ensure that no handler method (tool) returns simply\n    # an empty string (e.g when showing contents of an empty file), since that\n    # would be considered an invalid response, and other responders will wrongly\n    # be given a chance to respond.\n\n    # if task would be considered done given responder r's `result`,\n    # then consider the result valid.\n    if result is not None and self.done(result, r)[0]:\n        return True\n    return (\n        result is not None\n        and not self._is_empty_message(result)\n        # some weaker LLMs, including even GPT-4o, may say \"DO-NOT-KNOW.\"\n        # (with a punctuation at the end), so need to strip out punctuation\n        and re.sub(r\"[,.!?:]\", \"\", result.content.strip()) != NO_ANSWER\n    )\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\n    self,\n    resp: Responder,\n    msg: ChatDocument | None = None,\n    mark: bool = False,\n) -&gt; None:\n    \"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\n    from langroid.agent.chat_document import ChatDocLoggerFields\n\n    default_values = ChatDocLoggerFields().model_dump().values()\n    msg_str_tsv = \"\\t\".join(str(v) for v in default_values)\n    if msg is not None:\n        msg_str_tsv = msg.tsv_str()\n\n    mark_str = \"*\" if mark else \" \"\n    task_name = self.name if self.name != \"\" else \"root\"\n    resp_color = \"white\" if mark else \"red\"\n    resp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\n\n    if msg is None:\n        msg_str = f\"{mark_str}({task_name}) {resp_str}\"\n    else:\n        color = {\n            Entity.LLM: \"green\",\n            Entity.USER: \"blue\",\n            Entity.AGENT: \"red\",\n            Entity.SYSTEM: \"magenta\",\n        }[msg.metadata.sender]\n        f = msg.log_fields()\n        tool_type = f.tool_type.rjust(6)\n        tool_name = f.tool.rjust(10)\n        tool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\n        sender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\n        sender_name = f.sender_name.rjust(10)\n        recipient = \"=&gt;\" + str(f.recipient).rjust(10)\n        block = \"X \" + str(f.block or \"\").rjust(10)\n        content = f\"[{color}]{f.content}[/{color}]\"\n        msg_str = (\n            f\"{mark_str}({task_name}) \"\n            f\"{resp_str} {sender}({sender_name}) \"\n            f\"({recipient}) ({block}) {tool_str} {content}\"\n        )\n\n    if self.logger is not None:\n        self.logger.log(msg_str)\n    if self.tsv_logger is not None:\n        resp_str = str(resp)\n        self.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n\n    # HTML logger\n    if self.html_logger is not None:\n        if msg is None:\n            # Create a minimal fields object for None messages\n            from langroid.agent.chat_document import ChatDocLoggerFields\n\n            fields_dict = {\n                \"responder\": str(resp),\n                \"mark\": \"*\" if mark else \"\",\n                \"task_name\": self.name or \"root\",\n                \"content\": \"\",\n                \"sender_entity\": str(resp),\n                \"sender_name\": \"\",\n                \"recipient\": \"\",\n                \"block\": None,\n                \"tool_type\": \"\",\n                \"tool\": \"\",\n            }\n        else:\n            # Get fields from the message\n            fields = msg.log_fields()\n            fields_dict = fields.model_dump()\n            fields_dict.update(\n                {\n                    \"responder\": str(resp),\n                    \"mark\": \"*\" if mark else \"\",\n                    \"task_name\": self.name or \"root\",\n                }\n            )\n\n        # Create a ChatDocLoggerFields-like object for the HTML logger\n        # Create a simple BaseModel subclass dynamically\n        from pydantic import BaseModel\n\n        class LogFields(BaseModel):\n            model_config = ConfigDict(extra=\"allow\")  # Allow extra fields\n\n        log_obj = LogFields(**fields_dict)\n        self.html_logger.log(log_obj)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.set_color_log","title":"<code>set_color_log(enable=True)</code>","text":"<p>Flag to enable/disable color logging using rich.console. In some contexts, such as Colab notebooks, we may want to disable color logging using rich.console, since those logs show up in the cell output rather than in the log file. Turning off this feature will still create logs, but without the color formatting from rich.console Args:     enable (bool): value of <code>self.color_log</code> to set to,         which will enable/diable rich logging</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def set_color_log(self, enable: bool = True) -&gt; None:\n    \"\"\"\n    Flag to enable/disable color logging using rich.console.\n    In some contexts, such as Colab notebooks, we may want to disable color logging\n    using rich.console, since those logs show up in the cell output rather than\n    in the log file. Turning off this feature will still create logs, but without\n    the color formatting from rich.console\n    Args:\n        enable (bool): value of `self.color_log` to set to,\n            which will enable/diable rich logging\n    \"\"\"\n    self.color_log = enable\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.close_loggers","title":"<code>close_loggers()</code>","text":"<p>Close all loggers to ensure clean shutdown.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def close_loggers(self) -&gt; None:\n    \"\"\"Close all loggers to ensure clean shutdown.\"\"\"\n    if hasattr(self, \"logger\") and self.logger is not None:\n        self.logger.close()\n    if hasattr(self, \"html_logger\") and self.html_logger is not None:\n        self.html_logger.close()\n</code></pre>"},{"location":"reference/agent/tool_message/","title":"tool_message","text":"<p>langroid/agent/tool_message.py </p> <p>Structured messages to an agent, typically from an LLM, to be handled by an agent. The messages could represent, for example: - information or data given to the agent - request for information or data from the agent - request to run a method of the agent</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>               Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently these \"tools\" are handled by methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.instructions","title":"<code>instructions()</code>  <code>classmethod</code>","text":"<p>Instructions on tool usage.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    \"\"\"\n    Instructions on tool usage.\n    \"\"\"\n    return \"\"\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.langroid_tools_instructions","title":"<code>langroid_tools_instructions()</code>  <code>classmethod</code>","text":"<p>Instructions on tool usage when <code>use_tools == True</code>, i.e. when using langroid built-in tools (as opposed to OpenAI-like function calls/tools).</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef langroid_tools_instructions(cls) -&gt; str:\n    \"\"\"\n    Instructions on tool usage when `use_tools == True`, i.e.\n    when using langroid built-in tools\n    (as opposed to OpenAI-like function calls/tools).\n    \"\"\"\n    return \"\"\"\n    IMPORTANT: When using this or any other tool/function, you MUST include a \n    `request` field and set it equal to the FUNCTION/TOOL NAME you intend to use.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Examples to use in few-shot demos with formatting instructions. Each example can be either: - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or - a tuple (description, ToolMessage instance), where the description is     a natural language \"thought\" that leads to the tool usage,     e.g. (\"I want to find the square of 5\",  SquareTool(num=5))     In some scenarios, including such a description can significantly     enhance reliability of tool use. Returns:</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[\"ToolMessage\" | Tuple[str, \"ToolMessage\"]]:\n    \"\"\"\n    Examples to use in few-shot demos with formatting instructions.\n    Each example can be either:\n    - just a ToolMessage instance, e.g. MyTool(param1=1, param2=\"hello\"), or\n    - a tuple (description, ToolMessage instance), where the description is\n        a natural language \"thought\" that leads to the tool usage,\n        e.g. (\"I want to find the square of 5\",  SquareTool(num=5))\n        In some scenarios, including such a description can significantly\n        enhance reliability of tool use.\n    Returns:\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.usage_examples","title":"<code>usage_examples(random=False)</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing examples of how to use the tool-message.</p> <p>Parameters:</p> Name Type Description Default <code>random</code> <code>bool</code> <p>whether to pick a random example from the list of examples. Set to <code>true</code> when using this to illustrate a dialog between LLM and user. (if false, use ALL examples)</p> <code>False</code> <p>Returns:     str: examples of how to use the tool/function-call</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_examples(cls, random: bool = False) -&gt; str:\n    \"\"\"\n    Instruction to the LLM showing examples of how to use the tool-message.\n\n    Args:\n        random (bool): whether to pick a random example from the list of examples.\n            Set to `true` when using this to illustrate a dialog between LLM and\n            user.\n            (if false, use ALL examples)\n    Returns:\n        str: examples of how to use the tool/function-call\n    \"\"\"\n    # pick a random example of the fields\n    if len(cls.examples()) == 0:\n        return \"\"\n    if random:\n        examples = [choice(cls.examples())]\n    else:\n        examples = cls.examples()\n    formatted_examples = [\n        (\n            f\"EXAMPLE {i}: (THOUGHT: {ex[0]}) =&gt; \\n{ex[1].format_example()}\"\n            if isinstance(ex, tuple)\n            else f\"EXAMPLE {i}:\\n {ex.format_example()}\"\n        )\n        for i, ex in enumerate(examples, 1)\n    ]\n    return \"\\n\\n\".join(formatted_examples)\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.get_value_of_type","title":"<code>get_value_of_type(target_type)</code>","text":"<p>Try to find a value of a desired type in the fields of the ToolMessage.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>def get_value_of_type(self, target_type: Type[Any]) -&gt; Any:\n    \"\"\"Try to find a value of a desired type in the fields of the ToolMessage.\"\"\"\n    ignore_fields = self._get_excluded_fields().union({\"request\"})\n    for field_name in set(self.model_dump().keys()) - ignore_fields:\n        value = getattr(self, field_name)\n        if is_instance_of(value, target_type):\n            return value\n    return None\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class Args:     f (str): field name</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>default value of the field, or None if not set or if the field does not exist.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n    \"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n\n    Returns:\n        Any: default value of the field, or None if not set or if the\n            field does not exist.\n    \"\"\"\n    schema = cls.model_json_schema()\n    properties = schema[\"properties\"]\n    return properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.format_instructions","title":"<code>format_instructions(tool=False)</code>  <code>classmethod</code>","text":"<p>Default Instructions to the LLM showing how to use the tool/function-call. Works for GPT4 but override this for weaker LLMs if needed.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>bool</code> <p>instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM) (or else it would be for OpenAI Function calls). Ignored in the default implementation, but can be used in subclasses.</p> <code>False</code> <p>Returns:     str: instructions on how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef format_instructions(cls, tool: bool = False) -&gt; str:\n    \"\"\"\n    Default Instructions to the LLM showing how to use the tool/function-call.\n    Works for GPT4 but override this for weaker LLMs if needed.\n\n    Args:\n        tool: instructions for Langroid-native tool use? (e.g. for non-OpenAI LLM)\n            (or else it would be for OpenAI Function calls).\n            Ignored in the default implementation, but can be used in subclasses.\n    Returns:\n        str: instructions on how to use the message\n    \"\"\"\n    # TODO: when we attempt to use a \"simpler schema\"\n    # (i.e. all nested fields explicit without definitions),\n    # we seem to get worse results, so we turn it off for now\n    param_dict = (\n        # cls.simple_schema() if tool else\n        cls.llm_function_schema(request=True).parameters\n    )\n    examples_str = \"\"\n    if cls.examples():\n        examples_str = \"EXAMPLES:\\n\" + cls.usage_examples()\n    return textwrap.dedent(\n        f\"\"\"\n        TOOL: {cls.default_value(\"request\")}\n        PURPOSE: {cls.default_value(\"purpose\")} \n        JSON FORMAT: {\n            json.dumps(param_dict, indent=4)\n        }\n        {examples_str}\n        \"\"\".lstrip()\n    )\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.group_format_instructions","title":"<code>group_format_instructions()</code>  <code>staticmethod</code>","text":"<p>Template for instructions for a group of tools. Works with GPT4 but override this for weaker LLMs if needed.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@staticmethod\ndef group_format_instructions() -&gt; str:\n    \"\"\"Template for instructions for a group of tools.\n    Works with GPT4 but override this for weaker LLMs if needed.\n    \"\"\"\n    return textwrap.dedent(\n        \"\"\"\n        === ALL AVAILABLE TOOLS and THEIR FORMAT INSTRUCTIONS ===\n        You have access to the following TOOLS to accomplish your task:\n\n        {format_instructions}\n\n        When one of the above TOOLs is applicable, you must express your \n        request as \"TOOL:\" followed by the request in the above format.\n        \"\"\"\n    )\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.llm_function_schema","title":"<code>llm_function_schema(request=False, defaults=True)</code>  <code>classmethod</code>","text":"<p>Clean up the schema of the Pydantic class (which can recursively contain other Pydantic classes), to create a version compatible with OpenAI Function-call API.</p> <p>Adapted from this excellent library: https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>bool</code> <p>whether to include the \"request\" field in the schema. (we set this to True when using Langroid-native TOOLs as opposed to OpenAI Function calls)</p> <code>False</code> <code>defaults</code> <code>bool</code> <p>whether to include fields with default values in the schema,     in the \"properties\" section.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>LLMFunctionSpec</code> <code>LLMFunctionSpec</code> <p>the schema as an LLMFunctionSpec</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(\n    cls,\n    request: bool = False,\n    defaults: bool = True,\n) -&gt; LLMFunctionSpec:\n    \"\"\"\n    Clean up the schema of the Pydantic class (which can recursively contain\n    other Pydantic classes), to create a version compatible with OpenAI\n    Function-call API.\n\n    Adapted from this excellent library:\n    https://github.com/jxnl/instructor/blob/main/instructor/function_calls.py\n\n    Args:\n        request: whether to include the \"request\" field in the schema.\n            (we set this to True when using Langroid-native TOOLs as opposed to\n            OpenAI Function calls)\n        defaults: whether to include fields with default values in the schema,\n                in the \"properties\" section.\n\n    Returns:\n        LLMFunctionSpec: the schema as an LLMFunctionSpec\n\n    \"\"\"\n    schema = copy.deepcopy(cls.model_json_schema())\n    docstring = parse(cls.__doc__ or \"\")\n    parameters = {\n        k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n    }\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (\n            description := param.description\n        ):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    excludes = cls._get_excluded_fields().copy()\n    if not request:\n        excludes = excludes.union({\"request\"})\n    # exclude 'excludes' from parameters[\"properties\"]:\n    parameters[\"properties\"] = {\n        field: details\n        for field, details in parameters[\"properties\"].items()\n        if field not in excludes and (defaults or details.get(\"default\") is None)\n    }\n    parameters[\"required\"] = sorted(\n        k\n        for k, v in parameters[\"properties\"].items()\n        if (\"default\" not in v and k not in excludes)\n    )\n    if request:\n        parameters[\"required\"].append(\"request\")\n\n        # If request is present it must match the default value\n        # Similar to defining request as a literal type\n        parameters[\"request\"] = {\n            \"enum\": [cls.default_value(\"request\")],\n            \"type\": \"string\",\n        }\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            schema[\"description\"] = (\n                f\"Correctly extracted `{cls.__name__}` with all \"\n                f\"the required parameters with correct types\"\n            )\n\n    # Handle nested ToolMessage fields\n    if \"definitions\" in parameters:\n        for v in parameters[\"definitions\"].values():\n            if \"exclude\" in v:\n                v.pop(\"exclude\")\n\n                remove_if_exists(\"purpose\", v[\"properties\"])\n                remove_if_exists(\"id\", v[\"properties\"])\n                if (\n                    \"request\" in v[\"properties\"]\n                    and \"default\" in v[\"properties\"][\"request\"]\n                ):\n                    if \"required\" not in v:\n                        v[\"required\"] = []\n                    v[\"required\"].append(\"request\")\n                    v[\"properties\"][\"request\"] = {\n                        \"type\": \"string\",\n                        \"enum\": [v[\"properties\"][\"request\"][\"default\"]],\n                    }\n\n    parameters.pop(\"exclude\")\n    _recursive_purge_dict_key(parameters, \"title\")\n    _recursive_purge_dict_key(parameters, \"additionalProperties\")\n    return LLMFunctionSpec(\n        name=cls.default_value(\"request\"),\n        description=cls.default_value(\"purpose\")\n        or f\"Tool for {cls.default_value('request')}\",\n        parameters=parameters,\n    )\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.simple_schema","title":"<code>simple_schema()</code>  <code>classmethod</code>","text":"<p>Return a simplified schema for the message, with only the request and required fields. Returns:     Dict[str, Any]: simplified schema</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef simple_schema(cls) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return a simplified schema for the message, with only the request and\n    required fields.\n    Returns:\n        Dict[str, Any]: simplified schema\n    \"\"\"\n    schema = generate_simple_schema(\n        cls,\n        exclude=list(cls._get_excluded_fields()),\n    )\n    return schema\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.remove_if_exists","title":"<code>remove_if_exists(k, d)</code>","text":"<p>Removes key <code>k</code> from <code>d</code> if present.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>def remove_if_exists(k: K, d: dict[K, Any]) -&gt; None:\n    \"\"\"Removes key `k` from `d` if present.\"\"\"\n    if k in d:\n        d.pop(k)\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.format_schema_for_strict","title":"<code>format_schema_for_strict(schema)</code>","text":"<p>Recursively set additionalProperties to False and replace oneOf and allOf with anyOf, required for OpenAI structured outputs. Additionally, remove all defaults and set all fields to required. This may not be equivalent to the original schema.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>def format_schema_for_strict(schema: Any) -&gt; None:\n    \"\"\"\n    Recursively set additionalProperties to False and replace\n    oneOf and allOf with anyOf, required for OpenAI structured outputs.\n    Additionally, remove all defaults and set all fields to required.\n    This may not be equivalent to the original schema.\n    \"\"\"\n    if isinstance(schema, dict):\n        # Handle $ref nodes - they can't have any other properties\n        if \"$ref\" in schema:\n            # Keep only the $ref, remove all other properties like description\n            ref_value = schema[\"$ref\"]\n            schema.clear()\n            schema[\"$ref\"] = ref_value\n            return\n\n        if \"type\" in schema and schema[\"type\"] == \"object\":\n            schema[\"additionalProperties\"] = False\n\n            if \"properties\" in schema:\n                properties = schema[\"properties\"]\n                all_properties = list(properties.keys())\n                for k, v in properties.items():\n                    if \"default\" in v:\n                        if k == \"request\":\n                            v[\"enum\"] = [v[\"default\"]]\n\n                        v.pop(\"default\")\n                schema[\"required\"] = all_properties\n            else:\n                schema[\"properties\"] = {}\n                schema[\"required\"] = []\n\n        anyOf = (\n            schema.get(\"oneOf\", []) + schema.get(\"allOf\", []) + schema.get(\"anyOf\", [])\n        )\n        if \"allOf\" in schema or \"oneOf\" in schema or \"anyOf\" in schema:\n            schema[\"anyOf\"] = anyOf\n\n        remove_if_exists(\"allOf\", schema)\n        remove_if_exists(\"oneOf\", schema)\n\n        for v in schema.values():\n            format_schema_for_strict(v)\n    elif isinstance(schema, list):\n        for v in schema:\n            format_schema_for_strict(v)\n</code></pre>"},{"location":"reference/agent/xml_tool_message/","title":"xml_tool_message","text":"<p>langroid/agent/xml_tool_message.py </p>"},{"location":"reference/agent/xml_tool_message/#langroid.agent.xml_tool_message.XMLToolMessage","title":"<code>XMLToolMessage</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Abstract class for tools formatted using XML instead of JSON.</p> <p>When a subclass defines a field with the attribute <code>verbatim=True</code>, instructions are sent to the LLM to ensure the field's content is:     - preserved as is, including whitespace, indents, quotes, newlines, etc         with no escaping, and     - enclosed in a CDATA section in the XML output. This is useful for LLMs sending code as part of a tool; results can be far superior compared to sending code in JSON-formatted tools, where code needs to confirm to JSON's strict rules and escaping requirements. (see test_xml_tool_message.py for an example).</p>"},{"location":"reference/agent/xml_tool_message/#langroid.agent.xml_tool_message.XMLToolMessage.extract_field_values","title":"<code>extract_field_values(formatted_string)</code>  <code>classmethod</code>","text":"<p>Extracts field values from an XML-formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>formatted_string</code> <code>str</code> <p>The XML-formatted string to parse.</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]]: A dictionary containing the extracted field values, where keys are the XML element names and values are their corresponding contents.</p> <code>Optional[Dict[str, Any]]</code> <p>Returns None if parsing fails or the root element is not a dictionary.</p> <p>Raises:</p> Type Description <code>XMLSyntaxError</code> <p>If the input string is not valid XML.</p> Source code in <code>langroid/agent/xml_tool_message.py</code> <pre><code>@classmethod\ndef extract_field_values(cls, formatted_string: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extracts field values from an XML-formatted string.\n\n    Args:\n        formatted_string (str): The XML-formatted string to parse.\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary containing the extracted field\n            values, where keys are the XML element names and values are their\n            corresponding contents.\n        Returns None if parsing fails or the root element is not a dictionary.\n\n    Raises:\n        etree.XMLSyntaxError: If the input string is not valid XML.\n    \"\"\"\n    # SECURITY: Initialize XMLParser with flags to prevent\n    # XML External Entity (XXE), billion laughs, and external DTD attacks by\n    # disabling entity resolution, DTD loading, and network access;\n    # `strip_cdata=False` is needed to preserve\n    # content within CDATA sections (e.g., for code).\n    parser = etree.XMLParser(\n        strip_cdata=False,\n        resolve_entities=False,\n        load_dtd=False,\n        no_network=True,\n    )\n    root = etree.fromstring(formatted_string.encode(\"utf-8\"), parser=parser)\n\n    def parse_element(element: etree._Element) -&gt; Any:\n        # Skip elements starting with underscore\n        if element.tag.startswith(\"_\"):\n            return {}\n\n        field_info = cls.model_fields.get(element.tag)\n        is_verbatim = (\n            field_info\n            and hasattr(field_info, \"json_schema_extra\")\n            and field_info.json_schema_extra is not None\n            and isinstance(field_info.json_schema_extra, dict)\n            and field_info.json_schema_extra.get(\"verbatim\", False)\n        )\n\n        if is_verbatim:\n            # For code elements, preserve the content as is, including whitespace\n            content = element.text if element.text else \"\"\n            # Strip leading and trailing triple backticks if present,\n            # accounting for whitespace\n            return (\n                content.strip().removeprefix(\"```\").removesuffix(\"```\").strip()\n                if content.strip().startswith(\"```\")\n                and content.strip().endswith(\"```\")\n                else content\n            )\n        elif len(element) == 0:\n            # For non-code leaf elements, strip whitespace\n            return element.text.strip() if element.text else \"\"\n        else:\n            # For branch elements, handle potential lists or nested structures\n            children = [parse_element(child) for child in element]\n            if all(child.tag == element[0].tag for child in element):\n                # If all children have the same tag, treat as a list\n                return children\n            else:\n                # Otherwise, treat as a dictionary\n                result = {child.tag: parse_element(child) for child in element}\n                # Check if this corresponds to a nested Pydantic model\n                if (\n                    field_info\n                    and isinstance(field_info.annotation, type)\n                    and issubclass(field_info.annotation, BaseModel)\n                ):\n                    return field_info.annotation(**result)\n                return result\n\n    result = parse_element(root)\n    if not isinstance(result, dict):\n        return None\n    # Filter out empty dictionaries from skipped underscore fields\n    return {k: v for k, v in result.items() if v != {}}\n</code></pre>"},{"location":"reference/agent/xml_tool_message/#langroid.agent.xml_tool_message.XMLToolMessage.parse","title":"<code>parse(formatted_string)</code>  <code>classmethod</code>","text":"<p>Parses the XML-formatted string and returns an instance of the class.</p> <p>Parameters:</p> Name Type Description Default <code>formatted_string</code> <code>str</code> <p>The XML-formatted string to parse.</p> required <p>Returns:</p> Type Description <code>Optional[XMLToolMessage]</code> <p>Optional[\"XMLToolMessage\"]: An instance of the class if parsing succeeds, None otherwise.</p> Source code in <code>langroid/agent/xml_tool_message.py</code> <pre><code>@classmethod\ndef parse(cls, formatted_string: str) -&gt; Optional[\"XMLToolMessage\"]:\n    \"\"\"\n    Parses the XML-formatted string and returns an instance of the class.\n\n    Args:\n        formatted_string (str): The XML-formatted string to parse.\n\n    Returns:\n        Optional[\"XMLToolMessage\"]: An instance of the class if parsing succeeds,\n            None otherwise.\n    \"\"\"\n    try:\n        parsed_data = cls.extract_field_values(formatted_string)\n        if parsed_data is None:\n            return None\n\n        # Use Pydantic's parse_obj to create and validate the instance\n        return cls.model_validate(parsed_data)\n    except Exception as e:\n        from langroid.exceptions import XMLException\n\n        raise XMLException(f\"Error parsing XML: {str(e)}\")\n</code></pre>"},{"location":"reference/agent/xml_tool_message/#langroid.agent.xml_tool_message.XMLToolMessage.format_example","title":"<code>format_example()</code>","text":"<p>Format the current instance as an XML example.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the current instance in XML format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result from etree.tostring is not a string.</p> Source code in <code>langroid/agent/xml_tool_message.py</code> <pre><code>def format_example(self) -&gt; str:\n    \"\"\"\n    Format the current instance as an XML example.\n\n    Returns:\n        str: A string representation of the current instance in XML format.\n\n    Raises:\n        ValueError: If the result from etree.tostring is not a string.\n    \"\"\"\n\n    def create_element(\n        parent: etree._Element, name: str, value: Any, path: str = \"\"\n    ) -&gt; None:\n        if value is None:\n            return\n\n        elem = etree.SubElement(parent, name)\n        current_path = f\"{path}.{name}\" if path else name\n\n        if isinstance(value, list):\n            for item in value:\n                create_element(elem, \"item\", item, current_path)\n        elif isinstance(value, dict):\n            for k, v in value.items():\n                create_element(elem, k, v, current_path)\n        elif isinstance(value, BaseModel):\n            # Handle nested Pydantic models\n            for field_name, field_value in value.model_dump().items():\n                create_element(elem, field_name, field_value, current_path)\n        else:\n            if current_path in self.__class__.find_verbatim_fields():\n                elem.text = etree.CDATA(str(value))\n            else:\n                elem.text = str(value)\n\n    root = etree.Element(self._get_root_element())\n    exclude_fields: set[str] = self._get_excluded_fields()\n    for name, value in self.model_dump().items():\n        if name not in exclude_fields:\n            create_element(root, name, value)\n\n    result = etree.tostring(root, encoding=\"unicode\", pretty_print=True)\n    if not isinstance(result, str):\n        raise ValueError(\"Unexpected non-string result from etree.tostring\")\n    return result\n</code></pre>"},{"location":"reference/agent/xml_tool_message/#langroid.agent.xml_tool_message.XMLToolMessage.find_candidates","title":"<code>find_candidates(text)</code>  <code>classmethod</code>","text":"<p>Finds XML-like tool message candidates in text, with relaxed opening tag rules.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to search for XML structures.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of XML strings. For fragments missing the root opening tag but having</p> <code>List[str]</code> <p>valid XML structure and root closing tag, prepends the root opening tag.</p> Example <p>With root_tag=\"tool\", given: \"Hello data \" Returns: [\"data\"]</p> Source code in <code>langroid/agent/xml_tool_message.py</code> <pre><code>@classmethod\ndef find_candidates(cls, text: str) -&gt; List[str]:\n    \"\"\"\n    Finds XML-like tool message candidates in text, with relaxed opening tag rules.\n\n    Args:\n        text: Input text to search for XML structures.\n\n    Returns:\n        List of XML strings. For fragments missing the root opening tag but having\n        valid XML structure and root closing tag, prepends the root opening tag.\n\n    Example:\n        With root_tag=\"tool\", given:\n        \"Hello &lt;field1&gt;data&lt;/field1&gt; &lt;/tool&gt;\"\n        Returns: [\"&lt;tool&gt;&lt;field1&gt;data&lt;/field1&gt;&lt;/tool&gt;\"]\n    \"\"\"\n\n    root_tag = cls._get_root_element()\n    opening_tag = f\"&lt;{root_tag}&gt;\"\n    closing_tag = f\"&lt;/{root_tag}&gt;\"\n\n    candidates = []\n    pos = 0\n    while True:\n        # Look for either proper opening tag or closing tag\n        start_normal = text.find(opening_tag, pos)\n        end = text.find(closing_tag, pos)\n\n        if start_normal == -1 and end == -1:\n            break\n\n        if start_normal != -1:\n            # Handle normal case (has opening tag)\n            end = text.find(closing_tag, start_normal)\n            if end != -1:\n                candidates.append(text[start_normal : end + len(closing_tag)])\n                pos = max(end + len(closing_tag), start_normal + 1)\n                continue\n            elif start_normal == text.rfind(opening_tag):\n                # last fragment - ok to miss closing tag\n                candidates.append(text[start_normal:] + closing_tag)\n                return candidates\n            else:\n                pos = start_normal + 1\n                continue\n\n        if end != -1:\n            # Look backwards for first XML tag\n            text_before = text[pos:end]\n            first_tag_match = re.search(r\"&lt;\\w+&gt;\", text_before)\n            if first_tag_match:\n                start = pos + first_tag_match.start()\n                candidates.append(\n                    opening_tag + text[start : end + len(closing_tag)]\n                )\n            pos = end + len(closing_tag)\n\n    return candidates\n</code></pre>"},{"location":"reference/agent/callbacks/","title":"callbacks","text":"<p>langroid/agent/callbacks/init.py </p>"},{"location":"reference/agent/callbacks/chainlit/","title":"chainlit","text":"<p>langroid/agent/callbacks/chainlit.py </p> <p>Callbacks for Chainlit integration.</p>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks","title":"<code>ChainlitAgentCallbacks(agent, config=ChainlitCallbackConfig())</code>","text":"<p>Inject Chainlit callbacks into a Langroid Agent</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def __init__(\n    self,\n    agent: \"Agent\",\n    config: ChainlitCallbackConfig = ChainlitCallbackConfig(),\n):\n    \"\"\"Add callbacks to the agent, and save the initial message,\n    so we can alter the display of the first user message.\n    \"\"\"\n    agent.callbacks.start_llm_stream = self.start_llm_stream\n    agent.callbacks.start_llm_stream_async = self.start_llm_stream_async\n    agent.callbacks.cancel_llm_stream = self.cancel_llm_stream\n    agent.callbacks.finish_llm_stream = self.finish_llm_stream\n    agent.callbacks.show_llm_response = self.show_llm_response\n    agent.callbacks.show_agent_response = self.show_agent_response\n    agent.callbacks.get_user_response = self.get_user_response\n    agent.callbacks.get_user_response_async = self.get_user_response_async\n    agent.callbacks.get_last_step = self.get_last_step\n    agent.callbacks.set_parent_agent = self.set_parent_agent\n    agent.callbacks.show_error_message = self.show_error_message\n    agent.callbacks.show_start_response = self.show_start_response\n    self.config = config\n    self.agent: \"Agent\" = agent\n    if self.agent.llm is not None:\n        # We don't want to suppress LLM output in async + streaming,\n        # since we often use chainlit async callbacks to display LLM output\n        self.agent.llm.config.async_stream_quiet = False\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.start_llm_stream","title":"<code>start_llm_stream()</code>","text":"<p>Returns a streaming fn that can be passed to the LLM class</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def start_llm_stream(self) -&gt; Callable[[str, StreamEventType], None]:\n    \"\"\"Returns a streaming fn that can be passed to the LLM class\"\"\"\n    self.stream = cl.Message(\n        content=\"\",\n        id=self.curr_step.id if self.curr_step is not None else None,\n        author=self._entity_name(\"llm\"),\n        type=\"assistant_message\",\n        parent_id=self._get_parent_id(),\n    )\n    self.last_step = self.stream\n    self.curr_step = None\n    logger.info(\n        f\"\"\"\n        Starting LLM stream for {self.agent.config.name}\n        id = {self.stream.id}\n        under parent {self._get_parent_id()}\n    \"\"\"\n    )\n\n    def stream_token(t: str, e: StreamEventType) -&gt; None:\n        if self.stream is None:\n            raise ValueError(\"Stream not initialized\")\n        run_sync(self.stream.stream_token(t))\n\n    return stream_token\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.start_llm_stream_async","title":"<code>start_llm_stream_async()</code>  <code>async</code>","text":"<p>Returns a streaming fn that can be passed to the LLM class</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>async def start_llm_stream_async(self) -&gt; Callable[[str, StreamEventType], None]:\n    \"\"\"Returns a streaming fn that can be passed to the LLM class\"\"\"\n    self.stream = cl.Message(\n        content=\"\",\n        id=self.curr_step.id if self.curr_step is not None else None,\n        author=self._entity_name(\"llm\"),\n        type=\"assistant_message\",\n        parent_id=self._get_parent_id(),\n    )\n    self.last_step = self.stream\n    self.curr_step = None\n    logger.info(\n        f\"\"\"\n        Starting LLM stream for {self.agent.config.name}\n        id = {self.stream.id}\n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n\n    async def stream_token(t: str, e: StreamEventType) -&gt; None:\n        if self.stream is None:\n            raise ValueError(\"Stream not initialized\")\n        await self.stream.stream_token(t)\n\n    return stream_token\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.cancel_llm_stream","title":"<code>cancel_llm_stream()</code>","text":"<p>Called when cached response found.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def cancel_llm_stream(self) -&gt; None:\n    \"\"\"Called when cached response found.\"\"\"\n    self.last_step = None\n    if self.stream is not None:\n        run_sync(self.stream.remove())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.finish_llm_stream","title":"<code>finish_llm_stream(content, tools_content='', is_tool=False, reasoning='')</code>","text":"<p>Update the stream, and display entire response in the right language.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The main LLM response content</p> required <code>tools_content</code> <code>str</code> <p>Tool-related content if any</p> <code>''</code> <code>is_tool</code> <code>bool</code> <p>Whether this is a tool response</p> <code>False</code> <code>reasoning</code> <code>str</code> <p>Chain-of-thought reasoning from the LLM (if available)</p> <code>''</code> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def finish_llm_stream(\n    self,\n    content: str,\n    tools_content: str = \"\",\n    is_tool: bool = False,\n    reasoning: str = \"\",\n) -&gt; None:\n    \"\"\"Update the stream, and display entire response in the right language.\n\n    Args:\n        content: The main LLM response content\n        tools_content: Tool-related content if any\n        is_tool: Whether this is a tool response\n        reasoning: Chain-of-thought reasoning from the LLM (if available)\n    \"\"\"\n    if self.agent.llm is None or self.stream is None:\n        raise ValueError(\"LLM or stream not initialized\")\n    if not content and not tools_content:\n        run_sync(self.stream.remove())  # type: ignore\n    else:\n        run_sync(self.stream.update())  # type: ignore\n    stream_id = self.stream.id if tools_content or content else None\n    step = cl.Message(\n        content=textwrap.dedent(tools_content or content) or NO_ANSWER,\n        id=stream_id,\n        author=self._entity_name(\"llm\", tool=is_tool),\n        type=\"assistant_message\",\n        parent_id=self._get_parent_id(),\n        language=\"json\" if is_tool else None,\n    )\n    logger.info(\n        f\"\"\"\n        Finish STREAM LLM response for {self.agent.config.name}\n        id = {step.id}\n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.update())  # type: ignore\n\n    # Display reasoning content if available (e.g., from thinking models)\n    if reasoning:\n        reasoning_step = cl.Message(\n            content=textwrap.dedent(reasoning),\n            author=self._entity_name(\"llm\") + \" \ud83d\udcad Reasoning\",\n            type=\"assistant_message\",\n            parent_id=step.id,\n        )\n        run_sync(reasoning_step.send())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_llm_response","title":"<code>show_llm_response(content, tools_content='', is_tool=False, cached=False, language=None, reasoning='')</code>","text":"<p>Show non-streaming LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The main LLM response content</p> required <code>tools_content</code> <code>str</code> <p>Tool-related content if any</p> <code>''</code> <code>is_tool</code> <code>bool</code> <p>Whether this is a tool response</p> <code>False</code> <code>cached</code> <code>bool</code> <p>Whether this response was from cache</p> <code>False</code> <code>language</code> <code>str | None</code> <p>Language for syntax highlighting</p> <code>None</code> <code>reasoning</code> <code>str</code> <p>Chain-of-thought reasoning from the LLM (if available)</p> <code>''</code> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_llm_response(\n    self,\n    content: str,\n    tools_content: str = \"\",\n    is_tool: bool = False,\n    cached: bool = False,\n    language: str | None = None,\n    reasoning: str = \"\",\n) -&gt; None:\n    \"\"\"Show non-streaming LLM response.\n\n    Args:\n        content: The main LLM response content\n        tools_content: Tool-related content if any\n        is_tool: Whether this is a tool response\n        cached: Whether this response was from cache\n        language: Language for syntax highlighting\n        reasoning: Chain-of-thought reasoning from the LLM (if available)\n    \"\"\"\n    step = cl.Message(\n        content=textwrap.dedent(tools_content or content) or NO_ANSWER,\n        id=self.curr_step.id if self.curr_step is not None else None,\n        author=self._entity_name(\"llm\", tool=is_tool, cached=cached),\n        type=\"assistant_message\",\n        language=language or (\"json\" if is_tool else None),\n        parent_id=self._get_parent_id(),\n    )\n    self.last_step = step\n    self.curr_step = None\n    logger.info(\n        f\"\"\"\n        Showing NON-STREAM LLM response for {self.agent.config.name}\n        id = {step.id}\n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.send())  # type: ignore\n\n    # Display reasoning content if available (e.g., from thinking models)\n    if reasoning:\n        reasoning_step = cl.Message(\n            content=textwrap.dedent(reasoning),\n            author=self._entity_name(\"llm\", cached=cached) + \" \ud83d\udcad Reasoning\",\n            type=\"assistant_message\",\n            parent_id=step.id,\n        )\n        run_sync(reasoning_step.send())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_error_message","title":"<code>show_error_message(error)</code>","text":"<p>Show error message.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_error_message(self, error: str) -&gt; None:\n    \"\"\"Show error message.\"\"\"\n    step = cl.Message(\n        content=error,\n        author=self.agent.config.name + f\"({ERROR})\",\n        type=\"run\",\n        language=\"text\",\n        parent_id=self._get_parent_id(),\n    )\n    self.last_step = step\n    run_sync(step.send())\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_agent_response","title":"<code>show_agent_response(content, language='text', is_tool=False)</code>","text":"<p>Show message from agent (typically tool handler).</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_agent_response(\n    self,\n    content: str,\n    language=\"text\",\n    is_tool: bool = False,\n) -&gt; None:\n    \"\"\"Show message from agent (typically tool handler).\"\"\"\n    if language == \"text\":\n        content = wrap_text_preserving_structure(content, width=90)\n    step = cl.Message(\n        content=content,\n        id=self.curr_step.id if self.curr_step is not None else None,\n        author=self._entity_name(\"agent\"),\n        type=\"tool\",\n        language=language,\n        parent_id=self._get_parent_id(),\n    )\n    self.last_step = step\n    self.curr_step = None\n    logger.info(\n        f\"\"\"\n        Showing AGENT response for {self.agent.config.name}\n        id = {step.id}\n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.send())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.show_start_response","title":"<code>show_start_response(entity)</code>","text":"<p>When there's a potentially long-running process, start a step, so that the UI displays a spinner while the process is running.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_start_response(self, entity: str) -&gt; None:\n    \"\"\"When there's a potentially long-running process, start a step,\n    so that the UI displays a spinner while the process is running.\"\"\"\n    if self.curr_step is not None:\n        run_sync(self.curr_step.remove())  # type: ignore\n    step = cl.Message(\n        content=\"\",\n        author=self._entity_name(entity),\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n        language=\"text\",\n    )\n    self.last_step = step\n    self.curr_step = step\n    logger.info(\n        f\"\"\"\n        Showing START response for {self.agent.config.name} ({entity})\n        id = {step.id}\n        under parent {self._get_parent_id()}\n        \"\"\"\n    )\n    run_sync(step.send())  # type: ignore\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.get_user_response","title":"<code>get_user_response(prompt)</code>","text":"<p>Ask for user response, wait for it, and return it</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def get_user_response(self, prompt: str) -&gt; str:\n    \"\"\"Ask for user response, wait for it, and return it\"\"\"\n\n    return run_sync(self.ask_user(prompt=prompt, suppress_values=[\"c\"]))\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.get_user_response_async","title":"<code>get_user_response_async(prompt)</code>  <code>async</code>","text":"<p>Ask for user response, wait for it, and return it</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>async def get_user_response_async(self, prompt: str) -&gt; str:\n    \"\"\"Ask for user response, wait for it, and return it\"\"\"\n\n    return await self.ask_user(prompt=prompt, suppress_values=[\"c\"])\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitAgentCallbacks.ask_user","title":"<code>ask_user(prompt, timeout=USER_TIMEOUT, suppress_values=['c'])</code>  <code>async</code>","text":"<p>Ask user for input.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to display to user</p> required <code>timeout</code> <code>int</code> <p>Timeout in seconds</p> <code>USER_TIMEOUT</code> <code>suppress_values</code> <code>List[str]</code> <p>List of values to suppress from display (e.g. \"c\" for continue)</p> <code>['c']</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>User response</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>async def ask_user(\n    self,\n    prompt: str,\n    timeout: int = USER_TIMEOUT,\n    suppress_values: List[str] = [\"c\"],\n) -&gt; str:\n    \"\"\"\n    Ask user for input.\n\n    Args:\n        prompt (str): Prompt to display to user\n        timeout (int): Timeout in seconds\n        suppress_values (List[str]): List of values to suppress from display\n            (e.g. \"c\" for continue)\n\n    Returns:\n        str: User response\n    \"\"\"\n    ask_msg = cl.AskUserMessage(\n        content=prompt,\n        author=f\"{self.agent.config.name}(Awaiting user input...)\",\n        type=\"assistant_message\",\n        timeout=timeout,\n    )\n    res = await ask_msg.send()\n    if prompt == \"\":\n        # if there was no actual prompt, clear the row from the UI for clarity.\n        await ask_msg.remove()\n\n    if res is None:\n        run_sync(\n            cl.Message(\n                content=f\"Timed out after {USER_TIMEOUT} seconds. Exiting.\"\n            ).send()\n        )\n        return \"x\"\n\n    # Finally, reproduce the user response at right nesting level\n    if res[\"output\"] in suppress_values:\n        return \"\"\n\n    return res[\"output\"]\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitTaskCallbacks","title":"<code>ChainlitTaskCallbacks(task, config=ChainlitCallbackConfig())</code>","text":"<p>               Bases: <code>ChainlitAgentCallbacks</code></p> <p>Recursively inject ChainlitAgentCallbacks into a Langroid Task's agent and agents of sub-tasks.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    config: ChainlitCallbackConfig = ChainlitCallbackConfig(),\n):\n    \"\"\"Inject callbacks recursively, ensuring msg is passed to the\n    top-level agent\"\"\"\n\n    super().__init__(task.agent, config)\n    self._inject_callbacks(task)\n    self.task = task\n    if config.show_subtask_response:\n        self.task.callbacks.show_subtask_response = self.show_subtask_response\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.ChainlitTaskCallbacks.show_subtask_response","title":"<code>show_subtask_response(task, content, is_tool=False)</code>","text":"<p>Show sub-task response as a step, nested at the right level.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def show_subtask_response(\n    self, task: \"Task\", content: str, is_tool: bool = False\n) -&gt; None:\n    \"\"\"Show sub-task response as a step, nested at the right level.\"\"\"\n\n    # The step should nest under the calling agent's last step\n    step = cl.Message(\n        content=content or NO_ANSWER,\n        author=(\n            self.task.agent.config.name + f\"( \u23ce From {task.agent.config.name})\"\n        ),\n        type=\"run\",\n        parent_id=self._get_parent_id(),\n        language=\"json\" if is_tool else None,\n    )\n    self.last_step = step\n    run_sync(step.send())\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.setup_llm","title":"<code>setup_llm()</code>  <code>async</code>","text":"<p>From the session <code>llm_settings</code>, create new LLMConfig and LLM objects, save them in session state.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>@no_type_check\nasync def setup_llm() -&gt; None:\n    \"\"\"From the session `llm_settings`, create new LLMConfig and LLM objects,\n    save them in session state.\"\"\"\n    llm_settings = cl.user_session.get(\"llm_settings\", {})\n    model = llm_settings.get(\"chat_model\")\n    context_length = llm_settings.get(\"context_length\", 16_000)\n    temperature = llm_settings.get(\"temperature\", 0.2)\n    timeout = llm_settings.get(\"timeout\", 90)\n    logger.info(f\"Using model: {model}\")\n    llm_config = lm.OpenAIGPTConfig(\n        chat_model=model or lm.OpenAIChatModel.GPT4o,\n        # or, other possibilities for example:\n        # \"litellm/ollama_chat/mistral\"\n        # \"litellm/ollama_chat/mistral:7b-instruct-v0.2-q8_0\"\n        # \"litellm/ollama/llama2\"\n        # \"local/localhost:8000/v1\"\n        # \"local/localhost:8000\"\n        chat_context_length=context_length,  # adjust based on model\n        temperature=temperature,\n        timeout=timeout,\n    )\n    llm = lm.OpenAIGPT(llm_config)\n    cl.user_session.set(\"llm_config\", llm_config)\n    cl.user_session.set(\"llm\", llm)\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.update_llm","title":"<code>update_llm(new_settings)</code>  <code>async</code>","text":"<p>Update LLMConfig and LLM from settings, and save in session state.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>@no_type_check\nasync def update_llm(new_settings: Dict[str, Any]) -&gt; None:\n    \"\"\"Update LLMConfig and LLM from settings, and save in session state.\"\"\"\n    cl.user_session.set(\"llm_settings\", new_settings)\n    await inform_llm_settings()\n    await setup_llm()\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.get_text_files","title":"<code>get_text_files(message, extensions=['.txt', '.pdf', '.doc', '.docx'])</code>  <code>async</code>","text":"<p>Get dict (file_name -&gt; file_path) from files uploaded in chat msg</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>async def get_text_files(\n    message: cl.Message,\n    extensions: List[str] = [\".txt\", \".pdf\", \".doc\", \".docx\"],\n) -&gt; Dict[str, str]:\n    \"\"\"Get dict (file_name -&gt; file_path) from files uploaded in chat msg\"\"\"\n\n    files = [file for file in message.elements if file.path.endswith(tuple(extensions))]\n    return {file.name: file.path for file in files}\n</code></pre>"},{"location":"reference/agent/callbacks/chainlit/#langroid.agent.callbacks.chainlit.wrap_text_preserving_structure","title":"<code>wrap_text_preserving_structure(text, width=90)</code>","text":"<p>Wrap text preserving paragraph breaks. Typically used to format an agent_response output, which may have long lines with no newlines or paragraph breaks.</p> Source code in <code>langroid/agent/callbacks/chainlit.py</code> <pre><code>def wrap_text_preserving_structure(text: str, width: int = 90) -&gt; str:\n    \"\"\"Wrap text preserving paragraph breaks. Typically used to\n    format an agent_response output, which may have long lines\n    with no newlines or paragraph breaks.\"\"\"\n\n    paragraphs = text.split(\"\\n\\n\")  # Split the text into paragraphs\n    wrapped_text = []\n\n    for para in paragraphs:\n        if para.strip():  # If the paragraph is not just whitespace\n            # Wrap this paragraph and add it to the result\n            wrapped_paragraph = textwrap.fill(para, width=width)\n            wrapped_text.append(wrapped_paragraph)\n        else:\n            # Preserve paragraph breaks\n            wrapped_text.append(\"\")\n\n    return \"\\n\\n\".join(wrapped_text)\n</code></pre>"},{"location":"reference/agent/special/","title":"special","text":"<p>langroid/agent/special/init.py </p>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent","title":"<code>RelevanceExtractorAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Agent for extracting segments from text, that are relevant to a given query.</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def __init__(self, config: RelevanceExtractorAgentConfig):\n    super().__init__(config)\n    self.config: RelevanceExtractorAgentConfig = config\n    self.enable_message(SegmentExtractTool)\n    self.numbered_passage: Optional[str] = None\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    \"\"\"\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    &lt;Instructions&gt;\n    Given the PASSAGE below with NUMBERED segments, and the QUERY,\n    extract ONLY the segment-numbers that are RELEVANT to the QUERY,\n    and present them using the `extract_segments` tool/function,\n    i.e. your response MUST be a JSON-formatted string starting with\n    `{{\"request\": \"extract_segments\", ...}}`\n    &lt;/Instructions&gt;\n\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    response = super().llm_response(prompt)\n    return response\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM The LLM is expected to generate a structured msg according to the SegmentExtractTool schema, i.e. it should contain a <code>segment_list</code> field whose value is a list of segment numbers or ranges, like \"10,12,14-17\".</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    The LLM is expected to generate a structured msg according to the\n    SegmentExtractTool schema, i.e. it should contain a `segment_list` field\n    whose value is a list of segment numbers or ranges, like \"10,12,14-17\".\n    \"\"\"\n\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    response = await super().llm_response_async(prompt)\n    return response\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RelevanceExtractorAgent.extract_segments","title":"<code>extract_segments(msg)</code>","text":"<p>Method to handle a segmentExtractTool message from LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def extract_segments(self, msg: SegmentExtractTool) -&gt; str:\n    \"\"\"Method to handle a segmentExtractTool message from LLM\"\"\"\n    spec = msg.segment_list\n    if len(self.message_history) == 0:\n        return DONE + \" \" + NO_ANSWER\n    if spec is None or spec.strip() in [\"\", NO_ANSWER]:\n        return DONE + \" \" + NO_ANSWER\n    assert self.numbered_passage is not None, \"No numbered passage\"\n    # assume this has numbered segments\n    try:\n        extracts = extract_numbered_segments(self.numbered_passage, spec)\n    except Exception:\n        return DONE + \" \" + NO_ANSWER\n    # this response ends the task by saying DONE\n    return DONE + \" \" + extracts\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent","title":"<code>DocChatAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: DocChatAgentConfig,\n):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    self.original_docs: List[Document] = []\n    self.original_docs_length = 0\n    self.from_dataframe = False\n    self.df_description = \"\"\n    self.chunked_docs: List[Document] = []\n    self.chunked_docs_clean: List[Document] = []\n    self.response: None | Document = None\n    if (\n        self.config.cross_encoder_reranking_model != \"\"\n        and self.config.use_reciprocal_rank_fusion\n    ):\n        logger.warning(\n            \"\"\"\n            Ignoring `cross_encoder_reranking_model` since you have set  \n            `use_reciprocal_rank_fusion` to True.\n            To use cross-encoder reranking, set\n            `use_reciprocal_rank_fusion` to False.\n            \"\"\"\n        )\n\n    if (\n        self.config.cross_encoder_reranking_model == \"\"\n        and not self.config.use_reciprocal_rank_fusion\n        and (self.config.use_fuzzy_match or self.config.use_bm25_search)\n        and (\n            self.config.n_relevant_chunks\n            &lt; self.config.n_similar_chunks\n            * (self.config.use_bm25_search + self.config.use_fuzzy_match)\n        )\n    ):\n        logger.warning(\n            \"\"\"\n            DocChatAgent has been configured to have no cross encoder reranking,\n            AND `use_reciprocal_rank_fusion` is set to False,\n            AND `use_fuzzy_match` or `use_bm25_search` is True,\n            AND `n_relevant_chunks` is less than `n_similar_chunks` * (\n                `use_bm25_search` + `use_fuzzy_match`\n            ), \n            BUT there is no way to rerank the chunks retrieved by multiple methods,\n            so we will set `use_reciprocal_rank_fusion` to True.\n            \"\"\"\n        )\n        self.config.use_reciprocal_rank_fusion = True\n\n    # Handle backward compatibility for deprecated n_similar_docs\n    if self.config.parsing.n_similar_docs is not None:\n        logger.warning(\n            \"\"\"\n            The parameter `parsing.n_similar_docs` is deprecated and will be\n            removed in a future version. Please use `n_similar_chunks` and\n            `n_relevant_chunks` instead, which provide more fine-grained\n            control over retrieval.\n            - n_similar_chunks: number of chunks to retrieve by each method\n            - n_relevant_chunks: final number of chunks to return after reranking\n            \"\"\"\n        )\n        # Use the deprecated value for both parameters\n        self.config.n_similar_chunks = self.config.parsing.n_similar_docs\n        self.config.n_relevant_chunks = self.config.parsing.n_similar_docs\n\n    self.ingest()\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.clear","title":"<code>clear()</code>","text":"<p>Clear the document collection and the specific collection in vecdb</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear the document collection and the specific collection in vecdb\"\"\"\n    self.original_docs = []\n    self.original_docs_length = 0\n    self.chunked_docs = []\n    self.chunked_docs_clean = []\n    if self.vecdb is None:\n        logger.warning(\"Attempting to clear VecDB, but VecDB not set.\")\n        return\n    collection_name = self.vecdb.config.collection_name\n    if collection_name is None:\n        return\n    try:\n        # Note we may have used a vecdb with a config.collection_name\n        # different from the agent's config.vecdb.collection_name!!\n        self.vecdb.delete_collection(collection_name)\n        # Close the old vecdb before creating a new one\n        old_vecdb = self.vecdb\n        if old_vecdb and hasattr(old_vecdb, \"close\"):\n            old_vecdb.close()\n        self.vecdb = VectorStore.create(self.vecdb.config)\n    except Exception as e:\n        logger.warning(\n            f\"\"\"\n            Error while deleting collection {collection_name}:\n            {e}\n            \"\"\"\n        )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.ingest","title":"<code>ingest()</code>","text":"<p>Chunk + embed + store docs specified by self.config.doc_paths</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest(self) -&gt; None:\n    \"\"\"\n    Chunk + embed + store docs specified by self.config.doc_paths\n    \"\"\"\n    if len(self.config.doc_paths) == 0:\n        # we must be using a previously defined collection\n        # But let's get all the chunked docs so we can\n        # do keyword and other non-vector searches\n        if self.vecdb is None:\n            logger.warning(\"VecDB not set: cannot ingest docs.\")\n        else:\n            self.setup_documents(filter=self.config.filter)\n        return\n    self.ingest_doc_paths(self.config.doc_paths)  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.ingest_doc_paths","title":"<code>ingest_doc_paths(paths, metadata=[], doc_type=None)</code>","text":"<p>Split, ingest docs from specified paths, do not add these to config.doc_paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>str | bytes | List[str | bytes]</code> <p>document paths, urls or byte-content of docs. The bytes option is intended to support cases where a document has already been read in as bytes (e.g. from an API or a database), and we want to avoid having to write it to a temporary file just to read it back in.</p> required <code>metadata</code> <code>List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]</code> <p>List of metadata dicts, one for each path. If a single dict is passed in, it is used for all paths.</p> <code>[]</code> <code>doc_type</code> <code>str | DocumentType | None</code> <p>DocumentType to use for parsing, if known. MUST apply to all docs if specified. This is especially useful when the <code>paths</code> are of bytes type, to help with document type detection.</p> <code>None</code> <p>Returns:     List of Document objects</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_doc_paths(\n    self,\n    paths: str | bytes | List[str | bytes],\n    metadata: (\n        List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]\n    ) = [],\n    doc_type: str | DocumentType | None = None,\n) -&gt; List[Document]:\n    \"\"\"Split, ingest docs from specified paths,\n    do not add these to config.doc_paths.\n\n    Args:\n        paths: document paths, urls or byte-content of docs.\n            The bytes option is intended to support cases where a document\n            has already been read in as bytes (e.g. from an API or a database),\n            and we want to avoid having to write it to a temporary file\n            just to read it back in.\n        metadata: List of metadata dicts, one for each path.\n            If a single dict is passed in, it is used for all paths.\n        doc_type: DocumentType to use for parsing, if known.\n            MUST apply to all docs if specified.\n            This is especially useful when the `paths` are of bytes type,\n            to help with document type detection.\n    Returns:\n        List of Document objects\n    \"\"\"\n    if isinstance(paths, str) or isinstance(paths, bytes):\n        paths = [paths]\n    all_paths = paths\n    paths_meta: Dict[int, Any] = {}\n    urls_meta: Dict[int, Any] = {}\n    idxs = range(len(all_paths))\n    url_idxs, path_idxs, bytes_idxs = get_urls_paths_bytes_indices(all_paths)\n    urls = [all_paths[i] for i in url_idxs]\n    paths = [all_paths[i] for i in path_idxs]\n    bytes_list = [all_paths[i] for i in bytes_idxs]\n    path_idxs.extend(bytes_idxs)\n    paths.extend(bytes_list)\n    if (isinstance(metadata, list) and len(metadata) &gt; 0) or not isinstance(\n        metadata, list\n    ):\n        if isinstance(metadata, list):\n            idx2meta = {\n                p: (\n                    m\n                    if isinstance(m, dict)\n                    else (isinstance(m, DocMetaData) and m.model_dump())\n                )  # appease mypy\n                for p, m in zip(idxs, metadata)\n            }\n        elif isinstance(metadata, dict):\n            idx2meta = {p: metadata for p in idxs}\n        else:\n            idx2meta = {p: metadata.model_dump() for p in idxs}\n        urls_meta = {u: idx2meta[u] for u in url_idxs}\n        paths_meta = {p: idx2meta[p] for p in path_idxs}\n    docs: List[Document] = []\n    parser: Parser = Parser(self.config.parsing)\n    if len(urls) &gt; 0:\n        for ui in url_idxs:\n            meta = urls_meta.get(ui, {})\n            loader = URLLoader(\n                urls=[all_paths[ui]],\n                parsing_config=self.config.parsing,\n                crawler_config=self.config.crawler_config,\n            )  # type: ignore\n            url_docs = loader.load()\n            # update metadata of each doc with meta\n            for d in url_docs:\n                orig_source = d.metadata.source\n                d.metadata = d.metadata.model_copy(update=meta)\n                d.metadata.source = _append_metadata_source(\n                    orig_source, meta.get(\"source\", \"\")\n                )\n            docs.extend(url_docs)\n    if len(paths) &gt; 0:  # paths OR bytes are handled similarly\n        for pi in path_idxs:\n            meta = paths_meta.get(pi, {})\n            p = all_paths[pi]\n            path_docs = RepoLoader.get_documents(\n                p,\n                parser=parser,\n                doc_type=doc_type,\n            )\n            # update metadata of each doc with meta\n            for d in path_docs:\n                orig_source = d.metadata.source\n                d.metadata = d.metadata.model_copy(update=meta)\n                d.metadata.source = _append_metadata_source(\n                    orig_source, meta.get(\"source\", \"\")\n                )\n            docs.extend(path_docs)\n    n_docs = len(docs)\n    n_splits = self.ingest_docs(docs, split=self.config.split)\n    if n_docs == 0:\n        return []\n    n_urls = len(urls)\n    n_paths = len(paths)\n    print(\n        f\"\"\"\n    [green]I have processed the following {n_urls} URLs\n    and {n_paths} docs into {n_splits} parts:\n    \"\"\".strip()\n    )\n    path_reps = [p if isinstance(p, str) else \"bytes\" for p in paths]\n    print(\"\\n\".join([u for u in urls if isinstance(u, str)]))  # appease mypy\n    print(\"\\n\".join(path_reps))\n    return docs\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.ingest_docs","title":"<code>ingest_docs(docs, split=True, metadata=[])</code>","text":"<p>Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects</p> required <code>split</code> <code>bool</code> <p>Whether to split docs into chunks. Default is True. If False, docs are treated as \"chunks\" and are not split.</p> <code>True</code> <code>metadata</code> <code>List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]</code> <p>List of metadata dicts, one for each doc, to augment whatever metadata is already in the doc. [ASSUME no conflicting keys between the two metadata dicts.] If a single dict is passed in, it is used for all docs.</p> <code>[]</code> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_docs(\n    self,\n    docs: List[Document],\n    split: bool = True,\n    metadata: (\n        List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]\n    ) = [],\n) -&gt; int:\n    \"\"\"\n    Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db\n\n    Args:\n        docs: List of Document objects\n        split: Whether to split docs into chunks. Default is True.\n            If False, docs are treated as \"chunks\" and are not split.\n        metadata: List of metadata dicts, one for each doc, to augment\n            whatever metadata is already in the doc.\n            [ASSUME no conflicting keys between the two metadata dicts.]\n            If a single dict is passed in, it is used for all docs.\n    \"\"\"\n    if isinstance(metadata, list) and len(metadata) &gt; 0:\n        for d, m in zip(docs, metadata):\n            orig_source = d.metadata.source\n            m_dict = m if isinstance(m, dict) else m.model_dump()  # type: ignore\n            d.metadata = d.metadata.model_copy(update=m_dict)  # type: ignore\n            d.metadata.source = _append_metadata_source(\n                orig_source, m_dict.get(\"source\", \"\")\n            )\n    elif isinstance(metadata, dict):\n        for d in docs:\n            orig_source = d.metadata.source\n            d.metadata = d.metadata.model_copy(update=metadata)\n            d.metadata.source = _append_metadata_source(\n                orig_source, metadata.get(\"source\", \"\")\n            )\n    elif isinstance(metadata, DocMetaData):\n        for d in docs:\n            orig_source = d.metadata.source\n            d.metadata = d.metadata.model_copy(update=metadata.model_dump())\n            d.metadata.source = _append_metadata_source(\n                orig_source, metadata.source\n            )\n\n    self.original_docs.extend(docs)\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    for d in docs:\n        if d.metadata.id in [None, \"\"]:\n            d.metadata.id = ObjectRegistry.new_id()\n    if split:\n        docs = self.parser.split(docs)\n    else:\n        if self.config.n_neighbor_chunks &gt; 0:\n            self.parser.add_window_ids(docs)\n        # we're not splitting, so we mark each doc as a chunk\n        for d in docs:\n            d.metadata.is_chunk = True\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    if self.config.chunk_enrichment_config is not None:\n        docs = self.enrich_chunks(docs)\n\n    # If any additional fields need to be added to content,\n    # add them as key=value pairs for all docs, before batching.\n    # This helps retrieval for table-like data.\n    # Note we need to do this at stage so that the embeddings\n    # are computed on the full content with these additional fields.\n    if len(self.config.add_fields_to_content) &gt; 0:\n        fields = [\n            f for f in extract_fields(docs[0], self.config.add_fields_to_content)\n        ]\n        if len(fields) &gt; 0:\n            for d in docs:\n                key_vals = extract_fields(d, fields)\n                d.content = (\n                    \",\".join(f\"{k}={v}\" for k, v in key_vals.items())\n                    + \",content=\"\n                    + d.content\n                )\n    docs = docs[: self.config.parsing.max_chunks]\n    # vecdb should take care of adding docs in batches;\n    # batching can be controlled via vecdb.config.batch_size\n    if not docs:\n        logging.warning(\n            \"No documents to ingest after processing. Skipping VecDB addition.\"\n        )\n        return 0  # Return 0 since no documents were added\n    self.vecdb.add_documents(docs)\n    self.original_docs_length = self.doc_length(docs)\n    self.setup_documents(docs, filter=self.config.filter)\n    return len(docs)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.retrieval_tool","title":"<code>retrieval_tool(msg)</code>","text":"<p>Handle the RetrievalTool message</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def retrieval_tool(self, msg: RetrievalTool) -&gt; str:\n    \"\"\"Handle the RetrievalTool message\"\"\"\n    self.config.retrieve_only = True\n    self.config.n_relevant_chunks = msg.num_results\n    content_doc = self.answer_from_docs(msg.query)\n    return content_doc.content\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.document_compatible_dataframe","title":"<code>document_compatible_dataframe(df, content='content', metadata=[])</code>  <code>staticmethod</code>","text":"<p>Convert dataframe so it is compatible with Document class: - has \"content\" column - has an \"id\" column to be used as Document.metadata.id</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to convert</p> required <code>content</code> <code>str</code> <p>name of content column</p> <code>'content'</code> <code>metadata</code> <code>List[str]</code> <p>list of metadata column names</p> <code>[]</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, List[str]]</code> <p>Tuple[pd.DataFrame, List[str]]: dataframe, metadata - dataframe: dataframe with \"content\" column and \"id\" column - metadata: list of metadata column names, including \"id\"</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef document_compatible_dataframe(\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; Tuple[pd.DataFrame, List[str]]:\n    \"\"\"\n    Convert dataframe so it is compatible with Document class:\n    - has \"content\" column\n    - has an \"id\" column to be used as Document.metadata.id\n\n    Args:\n        df: dataframe to convert\n        content: name of content column\n        metadata: list of metadata column names\n\n    Returns:\n        Tuple[pd.DataFrame, List[str]]: dataframe, metadata\n            - dataframe: dataframe with \"content\" column and \"id\" column\n            - metadata: list of metadata column names, including \"id\"\n    \"\"\"\n    if content not in df.columns:\n        raise ValueError(\n            f\"\"\"\n            Content column {content} not in dataframe,\n            so we cannot ingest into the DocChatAgent.\n            Please specify the `content` parameter as a suitable\n            text-based column in the dataframe.\n            \"\"\"\n        )\n    if content != \"content\":\n        # rename content column to \"content\", leave existing column intact\n        df = df.rename(columns={content: \"content\"}, inplace=False)\n\n    actual_metadata = metadata.copy()\n    if \"id\" not in df.columns:\n        docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n        ids = [str(d.id()) for d in docs]\n        df[\"id\"] = ids\n\n    if \"id\" not in actual_metadata:\n        actual_metadata += [\"id\"]\n\n    return df, actual_metadata\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.ingest_dataframe","title":"<code>ingest_dataframe(df, content='content', metadata=[])</code>","text":"<p>Ingest a dataframe into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; int:\n    \"\"\"\n    Ingest a dataframe into vecdb.\n    \"\"\"\n    self.from_dataframe = True\n    self.df_description = describe_dataframe(\n        df, filter_fields=self.config.filter_fields, n_vals=5\n    )\n    df, metadata = DocChatAgent.document_compatible_dataframe(df, content, metadata)\n    docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n    # When ingesting a dataframe we will no longer do any chunking,\n    # so we mark each doc as a chunk.\n    # TODO - revisit this since we may still want to chunk large text columns\n    for d in docs:\n        d.metadata.is_chunk = True\n    return self.ingest_docs(docs)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.setup_documents","title":"<code>setup_documents(docs=[], filter=None)</code>","text":"<p>Setup <code>self.chunked_docs</code> and <code>self.chunked_docs_clean</code> based on possible filter. These will be used in various non-vector-based search functions, e.g. self.get_similar_chunks_bm25(), self.get_fuzzy_matches(), etc.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects. This is empty when we are calling this method after initial doc ingestion.</p> <code>[]</code> <code>filter</code> <code>str | None</code> <p>Filter condition for various lexical/semantic search fns.</p> <code>None</code> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def setup_documents(\n    self,\n    docs: List[Document] = [],\n    filter: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Setup `self.chunked_docs` and `self.chunked_docs_clean`\n    based on possible filter.\n    These will be used in various non-vector-based search functions,\n    e.g. self.get_similar_chunks_bm25(), self.get_fuzzy_matches(), etc.\n\n    Args:\n        docs: List of Document objects. This is empty when we are calling this\n            method after initial doc ingestion.\n        filter: Filter condition for various lexical/semantic search fns.\n    \"\"\"\n    if filter is None and len(docs) &gt; 0:\n        # no filter, so just use the docs passed in\n        self.chunked_docs.extend(docs)\n    else:\n        if self.vecdb is None:\n            raise ValueError(\"VecDB not set\")\n        self.chunked_docs = self.vecdb.get_all_documents(where=filter or \"\")\n\n    self.chunked_docs_clean = [\n        Document(content=preprocess_text(d.content), metadata=d.metadata)\n        for d in self.chunked_docs\n    ]\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_field_values","title":"<code>get_field_values(fields)</code>","text":"<p>Get string-listing of possible values of each field, e.g. {     \"genre\": \"crime, drama, mystery, ... (10 more)\",     \"certificate\": \"R, PG-13, PG, R\", } The field names may have \"metadata.\" prefix, e.g. \"metadata.genre\".</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_field_values(self, fields: list[str]) -&gt; Dict[str, str]:\n    \"\"\"Get string-listing of possible values of each field,\n    e.g.\n    {\n        \"genre\": \"crime, drama, mystery, ... (10 more)\",\n        \"certificate\": \"R, PG-13, PG, R\",\n    }\n    The field names may have \"metadata.\" prefix, e.g. \"metadata.genre\".\n    \"\"\"\n    field_values: Dict[str, Set[str]] = {}\n    # make empty set for each field\n    for f in fields:\n        field_values[f] = set()\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # get all documents and accumulate possible values of each field until 10\n    docs = self.vecdb.get_all_documents()  # only works for vecdbs that support this\n    for d in docs:\n        # extract fields from d\n        doc_field_vals = extract_fields(d, fields)\n        # the `field` returned by extract_fields may contain only the last\n        # part of the field name, e.g. \"genre\" instead of \"metadata.genre\",\n        # so we use the orig_field name to fill in the values\n        for (field, val), orig_field in zip(doc_field_vals.items(), fields):\n            field_values[orig_field].add(val)\n    # For each field make a string showing list of possible values,\n    # truncate to 20 values, and if there are more, indicate how many\n    # more there are, e.g. Genre: crime, drama, mystery, ... (20 more)\n    field_values_list = {}\n    for f in fields:\n        vals = list(field_values[f])\n        n = len(vals)\n        remaining = n - 20\n        vals = vals[:20]\n        if n &gt; 20:\n            vals.append(f\"(...{remaining} more)\")\n        # make a string of the values, ensure they are strings\n        field_values_list[f] = \", \".join(str(v) for v in vals)\n    return field_values_list\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.doc_length","title":"<code>doc_length(docs)</code>","text":"<p>Calc token-length of a list of docs Args:     docs: list of Document objects Returns:     int: number of tokens</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def doc_length(self, docs: List[Document]) -&gt; int:\n    \"\"\"\n    Calc token-length of a list of docs\n    Args:\n        docs: list of Document objects\n    Returns:\n        int: number of tokens\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    return self.parser.num_tokens(self.doc_string(docs))\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.user_docs_ingest_dialog","title":"<code>user_docs_ingest_dialog()</code>","text":"<p>Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def user_docs_ingest_dialog(self) -&gt; None:\n    \"\"\"\n    Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    n_deletes = self.vecdb.clear_empty_collections()\n    collections = self.vecdb.list_collections()\n    collection_name = \"NEW\"\n    is_new_collection = False\n    replace_collection = False\n    if len(collections) &gt; 0:\n        n = len(collections)\n        delete_str = (\n            f\"(deleted {n_deletes} empty collections)\" if n_deletes &gt; 0 else \"\"\n        )\n        print(f\"Found {n} collections: {delete_str}\")\n        for i, option in enumerate(collections, start=1):\n            print(f\"{i}. {option}\")\n        while True:\n            choice = Prompt.ask(\n                f\"Enter 1-{n} to select a collection, \"\n                \"or hit ENTER to create a NEW collection, \"\n                \"or -1 to DELETE ALL COLLECTIONS\",\n                default=\"0\",\n            )\n            try:\n                if -1 &lt;= int(choice) &lt;= n:\n                    break\n            except Exception:\n                pass\n\n        if choice == \"-1\":\n            confirm = Prompt.ask(\n                \"Are you sure you want to delete all collections?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            if confirm == \"y\":\n                self.vecdb.clear_all_collections(really=True)\n                collection_name = \"NEW\"\n\n        if int(choice) &gt; 0:\n            collection_name = collections[int(choice) - 1]\n            print(f\"Using collection {collection_name}\")\n            choice = Prompt.ask(\n                \"Would you like to replace this collection?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            replace_collection = choice == \"y\"\n\n    if collection_name == \"NEW\":\n        is_new_collection = True\n        collection_name = Prompt.ask(\n            \"What would you like to name the NEW collection?\",\n            default=\"doc-chat\",\n        )\n\n    self.vecdb.set_collection(collection_name, replace=replace_collection)\n\n    default_urls_str = (\n        \" (or leave empty for default URLs)\" if is_new_collection else \"\"\n    )\n    print(f\"[blue]Enter some URLs or file/dir paths below {default_urls_str}\")\n    inputs = get_list_from_user()\n    if len(inputs) == 0:\n        if is_new_collection:\n            inputs = self.config.default_paths\n    self.config.doc_paths = inputs  # type: ignore\n    self.ingest()\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.doc_string","title":"<code>doc_string(docs)</code>  <code>staticmethod</code>","text":"<p>Generate a string representation of a list of docs. Args:     docs: list of Document objects Returns:     str: string representation</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef doc_string(docs: List[Document]) -&gt; str:\n    \"\"\"\n    Generate a string representation of a list of docs.\n    Args:\n        docs: list of Document objects\n    Returns:\n        str: string representation\n    \"\"\"\n    contents = [d.content for d in docs]\n    sources = [d.metadata.source for d in docs]\n    sources = [f\"SOURCE: {s}\" if s is not None else \"\" for s in sources]\n    return \"\\n\".join(\n        [\n            f\"\"\"\n            -----[EXTRACT #{i+1}]----------\n            {content}\n            {source}\n            -----END OF EXTRACT------------\n\n            \"\"\"\n            for i, (content, source) in enumerate(zip(contents, sources))\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible Args:     question: question to answer     passages: list of <code>Document</code> objects each containing a possibly relevant         snippet, and metadata Returns:     a <code>Document</code> object containing the answer,     and metadata containing source citations</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_summary_answer(\n    self, question: str, passages: List[Document]\n) -&gt; ChatDocument:\n    \"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n\n    \"\"\"\n\n    passages_str = self.doc_string(passages)\n    # Substitute Q and P into the templatized prompt\n\n    final_prompt = self.config.summarize_prompt.format(\n        question=question, extracts=passages_str\n    )\n    show_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n\n    # Generate the final verbatim extract based on the final prompt.\n    # Note this will send entire message history, plus this final_prompt\n    # to the LLM, and self.message_history will be updated to include\n    # 2 new LLMMessage objects:\n    # one for `final_prompt`, and one for the LLM response\n\n    if self.config.conversation_mode:\n        if self.config.retain_context:\n            answer_doc = super().llm_response(final_prompt)\n        else:\n            # respond with temporary context\n            answer_doc = super()._llm_response_temp_context(question, final_prompt)\n    else:\n        answer_doc = super().llm_response_forget(final_prompt)\n\n    assert answer_doc is not None, \"LLM response should not be None here\"\n    final_answer = answer_doc.content.strip()\n    show_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\n\n    # extract references like [^2], [^3], etc. from the final answer\n    citations = extract_markdown_references(final_answer)\n    # format the cited references as a string suitable for markdown footnote\n    full_citations_str, citations_str = format_cited_references(citations, passages)\n\n    return ChatDocument(\n        content=final_answer,  # does not contain citations\n        metadata=ChatDocMetaData(\n            source=citations_str,  # only the reference headers\n            source_content=full_citations_str,  # reference + content\n            sender=Entity.LLM,\n            has_citation=len(citations) &gt; 0,\n            cached=getattr(answer_doc.metadata, \"cached\", False),\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.enrich_chunks","title":"<code>enrich_chunks(docs)</code>","text":"<p>Enrich chunks using Agent configured with self.config.chunk_enrichment_config.</p> <p>We assume that the system message of the agent is set in such a way that when we run <pre><code>prompt = self.config.chunk_enrichment_config.enrichment_prompt_fn(text)\nresult = await agent.llm_response_forget_async(prompt)\n</code></pre></p> <p>then <code>result.content</code> will contain the augmentation to the text.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of document chunks to enrich</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: Documents (chunks) enriched with additional text, separated by a delimiter.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def enrich_chunks(self, docs: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Enrich chunks using Agent configured with self.config.chunk_enrichment_config.\n\n    We assume that the system message of the agent is set in such a way\n    that when we run\n    ```\n    prompt = self.config.chunk_enrichment_config.enrichment_prompt_fn(text)\n    result = await agent.llm_response_forget_async(prompt)\n    ```\n\n    then `result.content` will contain the augmentation to the text.\n\n    Args:\n        docs: List of document chunks to enrich\n\n    Returns:\n        List[Document]: Documents (chunks) enriched with additional text,\n            separated by a delimiter.\n    \"\"\"\n    if self.config.chunk_enrichment_config is None:\n        return docs\n    enrichment_config = self.config.chunk_enrichment_config\n    agent = ChatAgent(enrichment_config)\n    if agent.llm is None:\n        raise ValueError(\"LLM not set\")\n\n    with status(\"[cyan]Augmenting chunks...\"):\n        # Process chunks in parallel using run_batch_agent_method\n        questions_batch = run_batch_agent_method(\n            agent=agent,\n            method=agent.llm_response_forget_async,\n            items=docs,\n            input_map=lambda doc: (\n                enrichment_config.enrichment_prompt_fn(doc.content)\n            ),\n            output_map=lambda response: response.content if response else \"\",\n            sequential=False,\n            batch_size=enrichment_config.batch_size,\n        )\n\n        # Combine original content with generated questions\n        augmented_docs = []\n        for doc, enrichment in zip(docs, questions_batch):\n            if not enrichment:\n                augmented_docs.append(doc)\n                continue\n\n            # Combine original content with questions in a structured way\n            combined_content = (\n                f\"{doc.content}{enrichment_config.delimiter}{enrichment}\"\n            )\n\n            new_doc = doc.model_copy(\n                update={\n                    \"content\": combined_content,\n                    \"metadata\": doc.metadata.model_copy(\n                        update={\"has_enrichment\": True}\n                    ),\n                }\n            )\n            augmented_docs.append(new_doc)\n\n        return augmented_docs\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.rerank_with_diversity","title":"<code>rerank_with_diversity(passages)</code>","text":"<p>Rerank a list of items in such a way that each successive item is least similar (on average) to the earlier items.</p> <p>Args: query (str): The query for which the passages are relevant. passages (List[Document]): A list of Documents to be reranked.</p> <p>Returns: List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_with_diversity(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank a list of items in such a way that each successive item is least similar\n    (on average) to the earlier items.\n\n    Args:\n    query (str): The query for which the passages are relevant.\n    passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n    List[Documents]: A reranked list of Documents.\n    \"\"\"\n\n    if self.vecdb is None:\n        logger.warning(\"No vecdb; cannot use rerank_with_diversity\")\n        return passages\n    emb_model = self.vecdb.embedding_model\n    emb_fn = emb_model.embedding_fn()\n    embs = emb_fn([p.content for p in passages])\n    embs_arr = [np.array(e) for e in embs]\n    indices = list(range(len(passages)))\n\n    # Helper function to compute average similarity to\n    # items in the current result list.\n    def avg_similarity_to_result(i: int, result: List[int]) -&gt; float:\n        return sum(  # type: ignore\n            (embs_arr[i] @ embs_arr[j])\n            / (np.linalg.norm(embs_arr[i]) * np.linalg.norm(embs_arr[j]))\n            for j in result\n        ) / len(result)\n\n    # copy passages to items\n    result = [indices.pop(0)]  # Start with the first item.\n\n    while indices:\n        # Find the item that has the least average similarity\n        # to items in the result list.\n        least_similar_item = min(\n            indices, key=lambda i: avg_similarity_to_result(i, result)\n        )\n        result.append(least_similar_item)\n        indices.remove(least_similar_item)\n\n    # return passages in order of result list\n    return [passages[i] for i in result]\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.rerank_to_periphery","title":"<code>rerank_to_periphery(passages)</code>","text":"<p>Rerank to avoid Lost In the Middle (LIM) problem, where LLMs pay more attention to items at the ends of a list, rather than the middle. So we re-rank to make the best passages appear at the periphery of the list. https://arxiv.org/abs/2307.03172</p> <p>Example reranking: 1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2</p> <p>Parameters:</p> Name Type Description Default <code>passages</code> <code>List[Document]</code> <p>A list of Documents to be reranked.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_to_periphery(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank to avoid Lost In the Middle (LIM) problem,\n    where LLMs pay more attention to items at the ends of a list,\n    rather than the middle. So we re-rank to make the best passages\n    appear at the periphery of the list.\n    https://arxiv.org/abs/2307.03172\n\n    Example reranking:\n    1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2\n\n    Args:\n        passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n        List[Documents]: A reranked list of Documents.\n\n    \"\"\"\n    # Splitting items into odds and evens based on index, not value\n    odds = passages[::2]\n    evens = passages[1::2][::-1]\n\n    # Merging them back together\n    return odds + evens\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.add_context_window","title":"<code>add_context_window(docs_scores)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. We use these stored window_ids to retrieve the desired number (self.config.n_neighbor_chunks) of neighbors on either side of the current chunk.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def add_context_window(\n    self,\n    docs_scores: List[Tuple[Document, float]],\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk. We use these stored\n    window_ids to retrieve the desired number\n    (self.config.n_neighbor_chunks) of neighbors\n    on either side of the current chunk.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None or self.config.n_neighbor_chunks == 0:\n        return docs_scores\n    if len(docs_scores) == 0:\n        return []\n    if set(docs_scores[0][0].model_fields) != {\"content\", \"metadata\"}:\n        # Do not add context window when there are other fields besides just\n        # content and metadata, since we do not know how to set those other fields\n        # for newly created docs with combined content.\n        return docs_scores\n    return self.vecdb.add_context_window(docs_scores, self.config.n_neighbor_chunks)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_semantic_search_results","title":"<code>get_semantic_search_results(query, k=10)</code>","text":"<p>Get semantic search results from vecdb. Args:     query (str): query to search for     k (int): number of results to return Returns:     List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_semantic_search_results(\n    self,\n    query: str,\n    k: int = 10,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Get semantic search results from vecdb.\n    Args:\n        query (str): query to search for\n        k (int): number of results to return\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # Note: for dynamic filtering based on a query, users can\n    # use the `temp_update` context-manager to pass in a `filter` to self.config,\n    # e.g.:\n    # with temp_update(self.config, {\"filter\": \"metadata.source=='source1'\"}):\n    #     docs_scores = self.get_semantic_search_results(query, k=k)\n    # This avoids having pass the `filter` argument to every function call\n    # upstream of this one.\n    # The `temp_update` context manager is defined in\n    # `langroid/utils/pydantic_utils.py`\n    return self.vecdb.similar_texts_with_scores(\n        query,\n        k=k,\n        where=self.config.filter,\n    )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_relevant_chunks","title":"<code>get_relevant_chunks(query, query_proxies=[])</code>","text":"<p>The retrieval stage in RAG: get doc-chunks that are most \"relevant\" to the query (and possibly any proxy queries), from the document-store, which currently is the vector store, but in theory could be any document store, or even web-search. This stage does NOT involve an LLM, and the retrieved chunks could either be pre-chunked text (from the initial pre-processing stage where chunks were stored in the vector store), or they could be dynamically retrieved based on a window around a lexical match.</p> <p>These are the steps (some optional based on config): - semantic search based on vector-embedding distance, from vecdb - lexical search using bm25-ranking (keyword similarity) - fuzzy matching (keyword similarity) - re-ranking of doc-chunks by relevance to query, using cross-encoder,    and pick top k</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>original query (assumed to be in stand-alone form)</p> required <code>query_proxies</code> <code>List[str]</code> <p>possible rephrases, or hypothetical answer to query     (e.g. for HyDE-type retrieval)</p> <code>[]</code> <p>Returns:</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_relevant_chunks(\n    self, query: str, query_proxies: List[str] = []\n) -&gt; List[Document]:\n    \"\"\"\n    The retrieval stage in RAG: get doc-chunks that are most \"relevant\"\n    to the query (and possibly any proxy queries), from the document-store,\n    which currently is the vector store,\n    but in theory could be any document store, or even web-search.\n    This stage does NOT involve an LLM, and the retrieved chunks\n    could either be pre-chunked text (from the initial pre-processing stage\n    where chunks were stored in the vector store), or they could be\n    dynamically retrieved based on a window around a lexical match.\n\n    These are the steps (some optional based on config):\n    - semantic search based on vector-embedding distance, from vecdb\n    - lexical search using bm25-ranking (keyword similarity)\n    - fuzzy matching (keyword similarity)\n    - re-ranking of doc-chunks by relevance to query, using cross-encoder,\n       and pick top k\n\n    Args:\n        query: original query (assumed to be in stand-alone form)\n        query_proxies: possible rephrases, or hypothetical answer to query\n                (e.g. for HyDE-type retrieval)\n\n    Returns:\n\n    \"\"\"\n\n    if (\n        self.vecdb is None\n        or self.vecdb.config.collection_name\n        not in self.vecdb.list_collections(empty=False)\n    ):\n        return []\n\n    # if we are using cross-encoder reranking or reciprocal rank fusion (RRF),\n    # we can retrieve more docs during retrieval, and leave it to the cross-encoder\n    # or RRF reranking to whittle down to self.config.n_similar_chunks\n    retrieval_multiple = (\n        1\n        if (\n            self.config.cross_encoder_reranking_model == \"\"\n            and not self.config.use_reciprocal_rank_fusion\n        )\n        else 3\n    )\n\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n\n    with status(\"[cyan]Searching VecDB for relevant doc passages...\"):\n        docs_and_scores: List[Tuple[Document, float]] = []\n        for q in [query] + query_proxies:\n            docs_and_scores += self.get_semantic_search_results(\n                q,\n                k=self.config.n_similar_chunks * retrieval_multiple,\n            )\n            # sort by score descending\n            docs_and_scores = sorted(\n                docs_and_scores, key=lambda x: x[1], reverse=True\n            )\n\n    # keep only docs with unique d.id()\n    id2_rank_semantic = {d.id(): i for i, (d, _) in enumerate(docs_and_scores)}\n    id2doc = {d.id(): d for d, _ in docs_and_scores}\n    # make sure we get unique docs\n    passages = [id2doc[id] for id in id2_rank_semantic.keys()]\n\n    id2_rank_bm25 = {}\n    if self.config.use_bm25_search:\n        # TODO: Add score threshold in config\n        docs_scores = self.get_similar_chunks_bm25(query, retrieval_multiple)\n        id2doc.update({d.id(): d for d, _ in docs_scores})\n        if self.config.use_reciprocal_rank_fusion:\n            # if we're not re-ranking with a cross-encoder, and have RRF enabled,\n            # instead of accumulating the bm25 results into passages,\n            # we collect these ranks for Reciprocal Rank Fusion down below.\n            docs_scores = sorted(docs_scores, key=lambda x: x[1], reverse=True)\n            id2_rank_bm25 = {d.id(): i for i, (d, _) in enumerate(docs_scores)}\n        else:\n            passages += [d for (d, _) in docs_scores]\n            # eliminate duplicate ids\n            passages = [id2doc[id] for id in id2doc.keys()]\n\n    id2_rank_fuzzy = {}\n    if self.config.use_fuzzy_match:\n        # TODO: Add score threshold in config\n        fuzzy_match_doc_scores = self.get_fuzzy_matches(query, retrieval_multiple)\n        if self.config.use_reciprocal_rank_fusion:\n            # if we're not re-ranking with a cross-encoder,\n            # instead of accumulating the fuzzy match results into passages,\n            # we collect these ranks for Reciprocal Rank Fusion down below.\n            fuzzy_match_doc_scores = sorted(\n                fuzzy_match_doc_scores, key=lambda x: x[1], reverse=True\n            )\n            id2_rank_fuzzy = {\n                d.id(): i for i, (d, _) in enumerate(fuzzy_match_doc_scores)\n            }\n            id2doc.update({d.id(): d for d, _ in fuzzy_match_doc_scores})\n        else:\n            passages += [d for (d, _) in fuzzy_match_doc_scores]\n            # eliminate duplicate ids\n            passages = [id2doc[id] for id in id2doc.keys()]\n\n    if self.config.use_reciprocal_rank_fusion and (\n        self.config.use_bm25_search or self.config.use_fuzzy_match\n    ):\n        # Since we're not using cross-enocder re-ranking,\n        # we need to re-order the retrieved chunks from potentially three\n        # different retrieval methods (semantic, bm25, fuzzy), where the\n        # similarity scores are on different scales.\n        # We order the retrieved chunks using Reciprocal Rank Fusion (RRF) score.\n        # Combine the ranks from each id2doc_rank_* dict into a single dict,\n        # where the reciprocal rank score is the sum of\n        # 1/(rank + self.config.reciprocal_rank_fusion_constant).\n        # See https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking\n        #\n        # Note: diversity/periphery-reranking below may modify the final ranking.\n        id2_reciprocal_score = {}\n        for id_ in (\n            set(id2_rank_semantic.keys())\n            | set(id2_rank_bm25.keys())\n            | set(id2_rank_fuzzy.keys())\n        ):\n            # Use max_rank instead of infinity to avoid bias against\n            # single-method docs\n            max_rank = self.config.n_similar_chunks * retrieval_multiple\n            rank_semantic = id2_rank_semantic.get(id_, max_rank + 1)\n            rank_bm25 = id2_rank_bm25.get(id_, max_rank + 1)\n            rank_fuzzy = id2_rank_fuzzy.get(id_, max_rank + 1)\n            c = self.config.reciprocal_rank_fusion_constant\n            reciprocal_fusion_score = (\n                1 / (rank_semantic + c) + 1 / (rank_bm25 + c) + 1 / (rank_fuzzy + c)\n            )\n            id2_reciprocal_score[id_] = reciprocal_fusion_score\n\n        # sort the docs by the reciprocal score, in descending order\n        id2_reciprocal_score = OrderedDict(\n            sorted(\n                id2_reciprocal_score.items(),\n                key=lambda x: x[1],\n                reverse=True,\n            )\n        )\n        # each method retrieved up to retrieval_multiple * n_similar_chunks,\n        # so we need to take the top n_similar_chunks from the combined list\n        passages = [\n            id2doc[id]\n            for id, _ in list(id2_reciprocal_score.items())[\n                : self.config.n_similar_chunks\n            ]\n        ]\n        # passages must have distinct ids\n        assert len(passages) == len(set([d.id() for d in passages])), (\n            f\"Duplicate passages in retrieved docs: {len(passages)} != \"\n            f\"{len(set([d.id() for d in passages]))}\"\n        )\n\n    if len(passages) == 0:\n        logger.debug(\"No passages retrieved for query '%s'\", query)\n        return []\n\n    if self.config.rerank_after_adding_context:\n        passages_scores = [(p, 0.0) for p in passages]\n        passages_scores = self.add_context_window(passages_scores)\n        passages = [p for p, _ in passages_scores]\n    # now passages can potentially have a lot of doc chunks,\n    # so we re-rank them using a cross-encoder scoring model\n    # (provided that `reciprocal_rank_fusion` is not enabled),\n    # and pick top k where k = config..n_similar_chunks\n    # https://www.sbert.net/examples/applications/retrieve_rerank\n    if (\n        self.config.cross_encoder_reranking_model != \"\"\n        and not self.config.use_reciprocal_rank_fusion\n    ):\n        passages = self.rerank_with_cross_encoder(query, passages)\n\n    if self.config.rerank_diversity:\n        # reorder to increase diversity among top docs\n        passages = self.rerank_with_diversity(passages)\n\n    if self.config.rerank_periphery:\n        # reorder so most important docs are at periphery\n        # (see Lost In the Middle issue).\n        passages = self.rerank_to_periphery(passages)\n\n    if not self.config.rerank_after_adding_context:\n        passages_scores = [(p, 0.0) for p in passages]\n        passages_scores = self.add_context_window(passages_scores)\n        passages = [p for p, _ in passages_scores]\n\n    return passages[: self.config.n_relevant_chunks]\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_relevant_extracts","title":"<code>get_relevant_extracts(query)</code>","text":"<p>Get list of (verbatim) extracts from doc-chunks relevant to answering a query.</p> <p>These are the stages (some optional based on config): - use LLM to convert query to stand-alone query - optionally use LLM to rephrase query to use below - optionally use LLM to generate hypothetical answer (HyDE) to use below. - get_relevant_chunks(): get doc-chunks relevant to query and proxies - use LLM to get relevant extracts from doc-chunks</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to search for</p> required <p>Returns:</p> Name Type Description <code>query</code> <code>str</code> <p>stand-alone version of input query</p> <code>List[Document]</code> <p>List[Document]: list of relevant extracts</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@no_type_check\ndef get_relevant_extracts(self, query: str) -&gt; Tuple[str, List[Document]]:\n    \"\"\"\n    Get list of (verbatim) extracts from doc-chunks relevant to answering a query.\n\n    These are the stages (some optional based on config):\n    - use LLM to convert query to stand-alone query\n    - optionally use LLM to rephrase query to use below\n    - optionally use LLM to generate hypothetical answer (HyDE) to use below.\n    - get_relevant_chunks(): get doc-chunks relevant to query and proxies\n    - use LLM to get relevant extracts from doc-chunks\n\n    Args:\n        query (str): query to search for\n\n    Returns:\n        query (str): stand-alone version of input query\n        List[Document]: list of relevant extracts\n\n    \"\"\"\n    collection_name = (\n        None if self.vecdb is None else self.vecdb.config.collection_name\n    )\n    has_vecdb_collection = (\n        collection_name is not None\n        and collection_name in self.vecdb.list_collections(empty=False)\n        if self.vecdb is not None\n        else False\n    )\n\n    if not has_vecdb_collection and len(self.chunked_docs) == 0:\n        return query, []\n\n    if len(self.dialog) &gt; 0 and not self.config.assistant_mode:\n        # Regardless of whether we are in conversation mode or not,\n        # for relevant doc/chunk extraction, we must convert the query\n        # to a standalone query to get more relevant results.\n        with status(\"[cyan]Converting to stand-alone query...[/cyan]\"):\n            with StreamingIfAllowed(self.llm, False):\n                query = self.llm.followup_to_standalone(self.dialog, query)\n        print(f\"[orange2]New query: {query}\")\n\n    proxies = []\n    if self.config.hypothetical_answer:\n        answer = self.llm_hypothetical_answer(query)\n        proxies = [answer]\n\n    if self.config.n_query_rephrases &gt; 0:\n        rephrases = self.llm_rephrase_query(query)\n        proxies += rephrases\n    if has_vecdb_collection:\n        passages = self.get_relevant_chunks(query, proxies)  # no LLM involved\n    else:\n        passages = self.chunked_docs\n\n    if len(passages) == 0:\n        return query, []\n\n    if self.config.relevance_extractor_config is None:\n        extracts = passages\n    else:\n        with status(\"[cyan]LLM Extracting verbatim passages...\"):\n            with StreamingIfAllowed(self.llm, False):\n                # these are async calls, one per passage; turn off streaming\n                extracts = self.get_verbatim_extracts(query, passages)\n                extracts = [e for e in extracts if e.content != NO_ANSWER]\n\n    return query, extracts\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.remove_chunk_enrichments","title":"<code>remove_chunk_enrichments(passages)</code>","text":"<p>Remove any enrichments (like hypothetical questions, or keywords) from documents. Only cleans if enrichment was enabled in config.</p> <p>Parameters:</p> Name Type Description Default <code>passages</code> <code>List[Document]</code> <p>List of documents to clean</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of documents with only original content</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def remove_chunk_enrichments(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"Remove any enrichments (like hypothetical questions, or keywords)\n    from documents.\n    Only cleans if enrichment was enabled in config.\n\n    Args:\n        passages: List of documents to clean\n\n    Returns:\n        List of documents with only original content\n    \"\"\"\n    if self.config.chunk_enrichment_config is None:\n        return passages\n    delimiter = self.config.chunk_enrichment_config.delimiter\n    return [\n        (\n            doc.model_copy(update={\"content\": doc.content.split(delimiter)[0]})\n            if doc.content and getattr(doc.metadata, \"has_enrichment\", False)\n            else doc\n        )\n        for doc in passages\n    ]\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.get_verbatim_extracts","title":"<code>get_verbatim_extracts(query, passages)</code>","text":"<p>Run RelevanceExtractorAgent in async/concurrent mode on passages, to extract portions relevant to answering query, from each passage. Args:     query (str): query to answer     passages (List[Documents]): list of passages to extract from</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of Documents containing extracts and metadata.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_verbatim_extracts(\n    self,\n    query: str,\n    passages: List[Document],\n) -&gt; List[Document]:\n    \"\"\"\n    Run RelevanceExtractorAgent in async/concurrent mode on passages,\n    to extract portions relevant to answering query, from each passage.\n    Args:\n        query (str): query to answer\n        passages (List[Documents]): list of passages to extract from\n\n    Returns:\n        List[Document]: list of Documents containing extracts and metadata.\n    \"\"\"\n    passages = self.remove_chunk_enrichments(passages)\n\n    agent_cfg = self.config.relevance_extractor_config\n    if agent_cfg is None:\n        # no relevance extraction: simply return passages\n        return passages\n    if agent_cfg.llm is None:\n        # Use main DocChatAgent's LLM if not provided explicitly:\n        # this reduces setup burden on the user\n        agent_cfg.llm = self.config.llm\n    agent_cfg.query = query\n    agent_cfg.segment_length = self.config.extraction_granularity\n    agent_cfg.llm.stream = False  # disable streaming for concurrent calls\n\n    agent = RelevanceExtractorAgent(agent_cfg)\n    task = Task(\n        agent,\n        name=\"Relevance-Extractor\",\n        interactive=False,\n    )\n\n    extracts: list[str] = run_batch_tasks(\n        task,\n        passages,\n        input_map=lambda msg: msg.content,\n        output_map=lambda ans: ans.content if ans is not None else NO_ANSWER,\n    )  # type: ignore\n\n    # Caution: Retain ALL other fields in the Documents (which could be\n    # other than just `content` and `metadata`), while simply replacing\n    # `content` with the extracted portions\n    passage_extracts = []\n    for p, e in zip(passages, extracts):\n        if e == NO_ANSWER or len(e) == 0:\n            continue\n        p_copy = p.model_copy()\n        p_copy.content = e\n        passage_extracts.append(p_copy)\n\n    return passage_extracts\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.answer_from_docs","title":"<code>answer_from_docs(query)</code>","text":"<p>Answer query based on relevant docs from the VecDB</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to answer</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>ChatDocument</code> <p>answer</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def answer_from_docs(self, query: str) -&gt; ChatDocument:\n    \"\"\"\n    Answer query based on relevant docs from the VecDB\n\n    Args:\n        query (str): query to answer\n\n    Returns:\n        Document: answer\n    \"\"\"\n    response = ChatDocument(\n        content=NO_ANSWER,\n        metadata=ChatDocMetaData(\n            source=\"None\",\n            sender=Entity.LLM,\n        ),\n    )\n    # query may be updated to a stand-alone version\n    query, extracts = self.get_relevant_extracts(query)\n    if len(extracts) == 0:\n        return response\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if self.config.retrieve_only:\n        # only return extracts, skip LLM-based summary answer\n        meta = dict(\n            sender=Entity.LLM,\n        )\n        # copy metadata from first doc, unclear what to do here.\n        meta.update(extracts[0].metadata.model_dump())\n        return ChatDocument(\n            content=\"\\n\\n\".join([e.content for e in extracts]),\n            metadata=ChatDocMetaData(**meta),  # type: ignore\n        )\n    response = self.get_summary_answer(query, extracts)\n\n    self.update_dialog(query, response.content)\n    self.response = response  # save last response\n    return response\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.summarize_docs","title":"<code>summarize_docs(instruction='Give a concise summary of the following text:')</code>","text":"<p>Summarize all docs</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def summarize_docs(\n    self,\n    instruction: str = \"Give a concise summary of the following text:\",\n) -&gt; None | ChatDocument:\n    \"\"\"Summarize all docs\"\"\"\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if len(self.original_docs) == 0:\n        logger.warning(\n            \"\"\"\n            No docs to summarize! Perhaps you are re-using a previously\n            defined collection?\n            In that case, we don't have access to the original docs.\n            To create a summary, use a new collection, and specify a list of docs.\n            \"\"\"\n        )\n        return None\n    full_text = \"\\n\\n\".join([d.content for d in self.original_docs])\n    if self.parser is None:\n        raise ValueError(\"No parser defined\")\n    tot_tokens = self.parser.num_tokens(full_text)\n    MAX_INPUT_TOKENS = (\n        self.llm.completion_context_length()\n        - self.config.llm.model_max_output_tokens\n        - 100\n    )\n    if tot_tokens &gt; MAX_INPUT_TOKENS:\n        # truncate\n        full_text = self.parser.tokenizer.decode(\n            self.parser.tokenizer.encode(full_text)[:MAX_INPUT_TOKENS]\n        )\n        logger.warning(\n            f\"Summarizing after truncating text to {MAX_INPUT_TOKENS} tokens\"\n        )\n    prompt = f\"\"\"\n    {instruction}\n\n    FULL TEXT:\n    {full_text}\n    \"\"\".strip()\n    with StreamingIfAllowed(self.llm):\n        summary = ChatAgent.llm_response(self, prompt)\n        return summary\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.DocChatAgent.justify_response","title":"<code>justify_response()</code>","text":"<p>Show evidence for last response</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def justify_response(self) -&gt; ChatDocument | None:\n    \"\"\"Show evidence for last response\"\"\"\n    if self.response is None:\n        print(\"[magenta]No response yet\")\n        return None\n    source = self.response.metadata.source\n    if len(source) &gt; 0:\n        print(\"[magenta]\" + source)\n    else:\n        print(\"[magenta]No source found\")\n    return None\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.RetrieverAgent","title":"<code>RetrieverAgent(config)</code>","text":"<p>               Bases: <code>DocChatAgent</code></p> <p>Agent for just retrieving chunks/docs/extracts matching a query</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def __init__(self, config: DocChatAgentConfig):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    logger.warning(\n        \"\"\"\n    `RetrieverAgent` is deprecated. Use `DocChatAgent` instead, with\n    `DocChatAgentConfig.retrieve_only=True`, and if you want to retrieve\n    FULL relevant doc-contents rather than just extracts, then set\n    `DocChatAgentConfig.extraction_granularity=-1`\n    \"\"\"\n    )\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.LanceDocChatAgent","title":"<code>LanceDocChatAgent(cfg)</code>","text":"<p>               Bases: <code>DocChatAgent</code></p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def __init__(self, cfg: DocChatAgentConfig):\n    super().__init__(cfg)\n    self.config: DocChatAgentConfig = cfg\n    self.enable_message(QueryPlanTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.LanceDocChatAgent.query_plan","title":"<code>query_plan(msg)</code>","text":"<p>Handle the LLM's use of the FilterTool. Temporarily set the config filter and either return the final answer in case there's a dataframe_calc, or return the rephrased query so the LLM can handle it.</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def query_plan(self, msg: QueryPlanTool) -&gt; AgentDoneTool | str:\n    \"\"\"\n    Handle the LLM's use of the FilterTool.\n    Temporarily set the config filter and either return the final answer\n    in case there's a dataframe_calc, or return the rephrased query\n    so the LLM can handle it.\n    \"\"\"\n    # create document-subset based on this filter\n    plan = msg.plan\n    try:\n        self.setup_documents(filter=plan.filter or None)\n    except Exception as e:\n        logger.error(f\"Error setting up documents: {e}\")\n        # say DONE with err msg so it goes back to LanceFilterAgent\n        return AgentDoneTool(\n            content=f\"\"\"\n            Possible Filter Error:\\n {e}\n\n            Note that only the following fields are allowed in the filter\n            of a query plan: \n            {\", \".join(self.config.filter_fields)}\n            \"\"\"\n        )\n\n    # update the filter so it is used in the DocChatAgent\n    self.config.filter = plan.filter or None\n    if plan.dataframe_calc:\n        # we just get relevant docs then do the calculation\n        # TODO if calc causes err, it is captured in result,\n        # and LLM can correct the calc based on the err,\n        # and this will cause retrieval all over again,\n        # which may be wasteful if only the calc part is wrong.\n        # The calc step can later be done with a separate Agent/Tool.\n        if plan.query is None or plan.query.strip() == \"\":\n            if plan.filter is None or plan.filter.strip() == \"\":\n                return AgentDoneTool(\n                    content=\"\"\"\n                    Cannot execute Query Plan since filter as well as \n                    rephrased query are empty.                    \n                    \"\"\"\n                )\n            else:\n                # no query to match, so just get all docs matching filter\n                docs = self.vecdb.get_all_documents(plan.filter)\n        else:\n            _, docs = self.get_relevant_extracts(plan.query)\n        if len(docs) == 0:\n            return AgentDoneTool(content=NO_ANSWER)\n        answer = self.vecdb.compute_from_docs(docs, plan.dataframe_calc)\n    else:\n        # pass on the query so LLM can handle it\n        response = self.llm_response(plan.query)\n        answer = NO_ANSWER if response is None else response.content\n    return AgentDoneTool(tools=[AnswerTool(answer=answer)])\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.LanceDocChatAgent.ingest_dataframe","title":"<code>ingest_dataframe(df, content='content', metadata=[])</code>","text":"<p>Ingest from a dataframe. Assume we are doing this once, not incrementally</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def ingest_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; int:\n    \"\"\"Ingest from a dataframe. Assume we are doing this once, not incrementally\"\"\"\n\n    self.from_dataframe = True\n    if df.shape[0] == 0:\n        raise ValueError(\n            \"\"\"\n            LanceDocChatAgent.ingest_dataframe() received an empty dataframe.\n            \"\"\"\n        )\n    n = df.shape[0]\n\n    # If any additional fields need to be added to content,\n    # add them as key=value pairs, into the `content` field for all rows.\n    # This helps retrieval for table-like data.\n    # Note we need to do this at stage so that the embeddings\n    # are computed on the full content with these additional fields.\n    fields = [f for f in self.config.add_fields_to_content if f in df.columns]\n    if len(fields) &gt; 0:\n        df[content] = df.apply(\n            lambda row: (\",\".join(f\"{f}={row[f]}\" for f in fields))\n            + \", content=\"\n            + row[content],\n            axis=1,\n        )\n\n    df, metadata = DocChatAgent.document_compatible_dataframe(df, content, metadata)\n    self.df_description = describe_dataframe(\n        df,\n        filter_fields=self.config.filter_fields,\n        n_vals=10,\n    )\n    self.vecdb.add_dataframe(df, content=\"content\", metadata=metadata)\n\n    tbl = self.vecdb.client.open_table(self.vecdb.config.collection_name)\n    # We assume \"content\" is available as top-level field\n    if \"content\" in tbl.schema.names:\n        tbl.create_fts_index(\"content\", replace=True)\n    # We still need to do the below so that\n    # other types of searches in DocChatAgent\n    # can work, as they require Document objects\n    docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n    self.setup_documents(docs)\n    # mark each doc as already-chunked so we don't try to split them further\n    # TODO later we may want to split large text-columns\n    for d in docs:\n        d.metadata.is_chunk = True\n    return n  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.LanceDocChatAgent.get_similar_chunks_bm25","title":"<code>get_similar_chunks_bm25(query, multiple)</code>","text":"<p>Override the DocChatAgent.get_similar_chunks_bm25() to use LanceDB FTS (Full Text Search).</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def get_similar_chunks_bm25(\n    self, query: str, multiple: int\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Override the DocChatAgent.get_similar_chunks_bm25()\n    to use LanceDB FTS (Full Text Search).\n    \"\"\"\n    # Clean up query: replace all newlines with spaces in query,\n    # force special search keywords to lower case, remove quotes,\n    # so it's not interpreted as search syntax\n    query_clean = (\n        query.replace(\"\\n\", \" \")\n        .replace(\"AND\", \"and\")\n        .replace(\"OR\", \"or\")\n        .replace(\"NOT\", \"not\")\n        .replace(\"'\", \"\")\n        .replace('\"', \"\")\n        .replace(\":\", \"--\")\n    )\n\n    tbl = self.vecdb.client.open_table(self.vecdb.config.collection_name)\n    result = (\n        tbl.search(query_clean)\n        .where(self.config.filter or None)\n        .limit(self.config.n_similar_chunks * multiple)\n    )\n    docs = self.vecdb._lance_result_to_docs(result)\n    scores = [r[\"score\"] for r in result.to_list()]\n    return list(zip(docs, scores))\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.TableChatAgent","title":"<code>TableChatAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def __init__(self, config: TableChatAgentConfig):\n    if isinstance(config.data, pd.DataFrame):\n        df = config.data\n    else:\n        df = read_tabular_data(config.data, config.separator)\n\n    df.columns = df.columns.str.strip().str.replace(\" +\", \"_\", regex=True)\n\n    self.df = df\n    summary = dataframe_summary(df)\n    config.system_message = config.system_message.format(summary=summary)\n\n    super().__init__(config)\n    self.config: TableChatAgentConfig = config\n\n    logger.info(\n        f\"\"\"TableChatAgent initialized with dataframe of shape {self.df.shape}\n        and columns: \n        {self.df.columns}\n        \"\"\"\n    )\n    # enable the agent to use and handle the PandasEvalTool\n    self.enable_message(PandasEvalTool)\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.TableChatAgent.pandas_eval","title":"<code>pandas_eval(msg)</code>","text":"<p>Handle a PandasEvalTool message by evaluating the <code>expression</code> field     and returning the result. Args:     msg (PandasEvalTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of running the code along with any print output.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def pandas_eval(self, msg: PandasEvalTool) -&gt; str:\n    \"\"\"\n    Handle a PandasEvalTool message by evaluating the `expression` field\n        and returning the result.\n    Args:\n        msg (PandasEvalTool): The tool-message to handle.\n\n    Returns:\n        str: The result of running the code along with any print output.\n    \"\"\"\n    self.sent_expression = True\n    exprn = msg.expression\n    vars = {\"df\": self.df}\n    # Create a string-based I/O stream\n    code_out = io.StringIO()\n\n    # Temporarily redirect standard output to our string-based I/O stream\n    sys.stdout = code_out\n\n    # Evaluate the last line and get the result;\n    # SECURITY MITIGATION: Eval input is sanitized by default to prevent most\n    # common code injection attack vectors.\n    try:\n        if not self.config.full_eval:\n            exprn = sanitize_command(exprn)\n        code = compile(exprn, \"&lt;calc&gt;\", \"eval\")\n        eval_result = eval(code, vars, {})\n    except Exception as e:\n        eval_result = f\"ERROR: {type(e)}: {e}\"\n\n    if eval_result is None:\n        eval_result = \"\"\n\n    # Always restore the original standard output\n    sys.stdout = sys.__stdout__\n\n    # If df has been modified in-place, save the changes back to self.df\n    self.df = vars[\"df\"]\n\n    # Get the resulting string from the I/O stream\n    print_result = code_out.getvalue() or \"\"\n    sep = \"\\n\" if print_result else \"\"\n    # Combine the print and eval results\n    result = f\"{print_result}{sep}{eval_result}\"\n    if result == \"\":\n        result = \"No result\"\n    # Return the result\n    return result\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.TableChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Handle various LLM deviations</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"Handle various LLM deviations\"\"\"\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == lr.Entity.LLM:\n        if msg.content.strip() == DONE and self.sent_expression:\n            # LLM sent an expression (i.e. used the `pandas_eval` tool)\n            # but upon receiving the results, simply said DONE without\n            # narrating the result as instructed.\n            return \"\"\"\n                You forgot to PRESENT the answer to the user's query\n                based on the results from `pandas_eval` tool.\n            \"\"\"\n        if self.sent_expression:\n            # LLM forgot to say DONE\n            self.sent_expression = False\n            return DONE + \" \" + PASS\n        else:\n            # LLM forgot to use the `pandas_eval` tool\n            return \"\"\"\n                You forgot to use the `pandas_eval` tool/function \n                to find the answer.\n                Try again using the `pandas_eval` tool/function.\n                \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/#langroid.agent.special.PandasEvalTool","title":"<code>PandasEvalTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool/function to evaluate a pandas expression involving a dataframe <code>df</code></p>"},{"location":"reference/agent/special/#langroid.agent.special.dataframe_summary","title":"<code>dataframe_summary(df)</code>","text":"<p>Generate a structured summary for a pandas DataFrame containing numerical and categorical values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to summarize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A nicely structured and formatted summary string.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>@no_type_check\ndef dataframe_summary(df: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Generate a structured summary for a pandas DataFrame containing numerical\n    and categorical values.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to summarize.\n\n    Returns:\n        str: A nicely structured and formatted summary string.\n    \"\"\"\n\n    # Column names display\n    col_names_str = (\n        \"COLUMN NAMES:\\n\" + \" \".join([f\"'{col}'\" for col in df.columns]) + \"\\n\\n\"\n    )\n\n    # Numerical data summary\n    num_summary = df.describe().map(lambda x: \"{:.2f}\".format(x))\n    num_str = \"Numerical Column Summary:\\n\" + num_summary.to_string() + \"\\n\\n\"\n\n    # Categorical data summary\n    cat_columns = df.select_dtypes(include=[np.object_]).columns\n    cat_summary_list = []\n\n    for col in cat_columns:\n        unique_values = df[col].unique()\n        if len(unique_values) &lt; 10:\n            cat_summary_list.append(f\"'{col}': {', '.join(map(str, unique_values))}\")\n        else:\n            cat_summary_list.append(f\"'{col}': {df[col].nunique()} unique values\")\n\n    cat_str = \"Categorical Column Summary:\\n\" + \"\\n\".join(cat_summary_list) + \"\\n\\n\"\n\n    # Missing values summary\n    nan_summary = df.isnull().sum().rename(\"missing_values\").to_frame()\n    nan_str = \"Missing Values Column Summary:\\n\" + nan_summary.to_string() + \"\\n\"\n\n    # Combine the summaries into one structured string\n    summary_str = col_names_str + num_str + cat_str + nan_str\n\n    return summary_str\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/","title":"doc_chat_agent","text":"<p>langroid/agent/special/doc_chat_agent.py </p> <p>Agent that supports asking queries about a set of documents, using retrieval-augmented generation (RAG).</p> <p>Functionality includes: - summarizing a document, with a custom instruction; see <code>summarize_docs</code> - asking a question about a document; see <code>answer_from_docs</code></p> <p>Note: to use the sentence-transformer embeddings, you must install langroid with the [hf-embeddings] extra, e.g.:</p> <p>pip install \"langroid[hf-embeddings]\"</p>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent","title":"<code>DocChatAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def __init__(\n    self,\n    config: DocChatAgentConfig,\n):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    self.original_docs: List[Document] = []\n    self.original_docs_length = 0\n    self.from_dataframe = False\n    self.df_description = \"\"\n    self.chunked_docs: List[Document] = []\n    self.chunked_docs_clean: List[Document] = []\n    self.response: None | Document = None\n    if (\n        self.config.cross_encoder_reranking_model != \"\"\n        and self.config.use_reciprocal_rank_fusion\n    ):\n        logger.warning(\n            \"\"\"\n            Ignoring `cross_encoder_reranking_model` since you have set  \n            `use_reciprocal_rank_fusion` to True.\n            To use cross-encoder reranking, set\n            `use_reciprocal_rank_fusion` to False.\n            \"\"\"\n        )\n\n    if (\n        self.config.cross_encoder_reranking_model == \"\"\n        and not self.config.use_reciprocal_rank_fusion\n        and (self.config.use_fuzzy_match or self.config.use_bm25_search)\n        and (\n            self.config.n_relevant_chunks\n            &lt; self.config.n_similar_chunks\n            * (self.config.use_bm25_search + self.config.use_fuzzy_match)\n        )\n    ):\n        logger.warning(\n            \"\"\"\n            DocChatAgent has been configured to have no cross encoder reranking,\n            AND `use_reciprocal_rank_fusion` is set to False,\n            AND `use_fuzzy_match` or `use_bm25_search` is True,\n            AND `n_relevant_chunks` is less than `n_similar_chunks` * (\n                `use_bm25_search` + `use_fuzzy_match`\n            ), \n            BUT there is no way to rerank the chunks retrieved by multiple methods,\n            so we will set `use_reciprocal_rank_fusion` to True.\n            \"\"\"\n        )\n        self.config.use_reciprocal_rank_fusion = True\n\n    # Handle backward compatibility for deprecated n_similar_docs\n    if self.config.parsing.n_similar_docs is not None:\n        logger.warning(\n            \"\"\"\n            The parameter `parsing.n_similar_docs` is deprecated and will be\n            removed in a future version. Please use `n_similar_chunks` and\n            `n_relevant_chunks` instead, which provide more fine-grained\n            control over retrieval.\n            - n_similar_chunks: number of chunks to retrieve by each method\n            - n_relevant_chunks: final number of chunks to return after reranking\n            \"\"\"\n        )\n        # Use the deprecated value for both parameters\n        self.config.n_similar_chunks = self.config.parsing.n_similar_docs\n        self.config.n_relevant_chunks = self.config.parsing.n_similar_docs\n\n    self.ingest()\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.clear","title":"<code>clear()</code>","text":"<p>Clear the document collection and the specific collection in vecdb</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear the document collection and the specific collection in vecdb\"\"\"\n    self.original_docs = []\n    self.original_docs_length = 0\n    self.chunked_docs = []\n    self.chunked_docs_clean = []\n    if self.vecdb is None:\n        logger.warning(\"Attempting to clear VecDB, but VecDB not set.\")\n        return\n    collection_name = self.vecdb.config.collection_name\n    if collection_name is None:\n        return\n    try:\n        # Note we may have used a vecdb with a config.collection_name\n        # different from the agent's config.vecdb.collection_name!!\n        self.vecdb.delete_collection(collection_name)\n        # Close the old vecdb before creating a new one\n        old_vecdb = self.vecdb\n        if old_vecdb and hasattr(old_vecdb, \"close\"):\n            old_vecdb.close()\n        self.vecdb = VectorStore.create(self.vecdb.config)\n    except Exception as e:\n        logger.warning(\n            f\"\"\"\n            Error while deleting collection {collection_name}:\n            {e}\n            \"\"\"\n        )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest","title":"<code>ingest()</code>","text":"<p>Chunk + embed + store docs specified by self.config.doc_paths</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest(self) -&gt; None:\n    \"\"\"\n    Chunk + embed + store docs specified by self.config.doc_paths\n    \"\"\"\n    if len(self.config.doc_paths) == 0:\n        # we must be using a previously defined collection\n        # But let's get all the chunked docs so we can\n        # do keyword and other non-vector searches\n        if self.vecdb is None:\n            logger.warning(\"VecDB not set: cannot ingest docs.\")\n        else:\n            self.setup_documents(filter=self.config.filter)\n        return\n    self.ingest_doc_paths(self.config.doc_paths)  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_doc_paths","title":"<code>ingest_doc_paths(paths, metadata=[], doc_type=None)</code>","text":"<p>Split, ingest docs from specified paths, do not add these to config.doc_paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>str | bytes | List[str | bytes]</code> <p>document paths, urls or byte-content of docs. The bytes option is intended to support cases where a document has already been read in as bytes (e.g. from an API or a database), and we want to avoid having to write it to a temporary file just to read it back in.</p> required <code>metadata</code> <code>List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]</code> <p>List of metadata dicts, one for each path. If a single dict is passed in, it is used for all paths.</p> <code>[]</code> <code>doc_type</code> <code>str | DocumentType | None</code> <p>DocumentType to use for parsing, if known. MUST apply to all docs if specified. This is especially useful when the <code>paths</code> are of bytes type, to help with document type detection.</p> <code>None</code> <p>Returns:     List of Document objects</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_doc_paths(\n    self,\n    paths: str | bytes | List[str | bytes],\n    metadata: (\n        List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]\n    ) = [],\n    doc_type: str | DocumentType | None = None,\n) -&gt; List[Document]:\n    \"\"\"Split, ingest docs from specified paths,\n    do not add these to config.doc_paths.\n\n    Args:\n        paths: document paths, urls or byte-content of docs.\n            The bytes option is intended to support cases where a document\n            has already been read in as bytes (e.g. from an API or a database),\n            and we want to avoid having to write it to a temporary file\n            just to read it back in.\n        metadata: List of metadata dicts, one for each path.\n            If a single dict is passed in, it is used for all paths.\n        doc_type: DocumentType to use for parsing, if known.\n            MUST apply to all docs if specified.\n            This is especially useful when the `paths` are of bytes type,\n            to help with document type detection.\n    Returns:\n        List of Document objects\n    \"\"\"\n    if isinstance(paths, str) or isinstance(paths, bytes):\n        paths = [paths]\n    all_paths = paths\n    paths_meta: Dict[int, Any] = {}\n    urls_meta: Dict[int, Any] = {}\n    idxs = range(len(all_paths))\n    url_idxs, path_idxs, bytes_idxs = get_urls_paths_bytes_indices(all_paths)\n    urls = [all_paths[i] for i in url_idxs]\n    paths = [all_paths[i] for i in path_idxs]\n    bytes_list = [all_paths[i] for i in bytes_idxs]\n    path_idxs.extend(bytes_idxs)\n    paths.extend(bytes_list)\n    if (isinstance(metadata, list) and len(metadata) &gt; 0) or not isinstance(\n        metadata, list\n    ):\n        if isinstance(metadata, list):\n            idx2meta = {\n                p: (\n                    m\n                    if isinstance(m, dict)\n                    else (isinstance(m, DocMetaData) and m.model_dump())\n                )  # appease mypy\n                for p, m in zip(idxs, metadata)\n            }\n        elif isinstance(metadata, dict):\n            idx2meta = {p: metadata for p in idxs}\n        else:\n            idx2meta = {p: metadata.model_dump() for p in idxs}\n        urls_meta = {u: idx2meta[u] for u in url_idxs}\n        paths_meta = {p: idx2meta[p] for p in path_idxs}\n    docs: List[Document] = []\n    parser: Parser = Parser(self.config.parsing)\n    if len(urls) &gt; 0:\n        for ui in url_idxs:\n            meta = urls_meta.get(ui, {})\n            loader = URLLoader(\n                urls=[all_paths[ui]],\n                parsing_config=self.config.parsing,\n                crawler_config=self.config.crawler_config,\n            )  # type: ignore\n            url_docs = loader.load()\n            # update metadata of each doc with meta\n            for d in url_docs:\n                orig_source = d.metadata.source\n                d.metadata = d.metadata.model_copy(update=meta)\n                d.metadata.source = _append_metadata_source(\n                    orig_source, meta.get(\"source\", \"\")\n                )\n            docs.extend(url_docs)\n    if len(paths) &gt; 0:  # paths OR bytes are handled similarly\n        for pi in path_idxs:\n            meta = paths_meta.get(pi, {})\n            p = all_paths[pi]\n            path_docs = RepoLoader.get_documents(\n                p,\n                parser=parser,\n                doc_type=doc_type,\n            )\n            # update metadata of each doc with meta\n            for d in path_docs:\n                orig_source = d.metadata.source\n                d.metadata = d.metadata.model_copy(update=meta)\n                d.metadata.source = _append_metadata_source(\n                    orig_source, meta.get(\"source\", \"\")\n                )\n            docs.extend(path_docs)\n    n_docs = len(docs)\n    n_splits = self.ingest_docs(docs, split=self.config.split)\n    if n_docs == 0:\n        return []\n    n_urls = len(urls)\n    n_paths = len(paths)\n    print(\n        f\"\"\"\n    [green]I have processed the following {n_urls} URLs\n    and {n_paths} docs into {n_splits} parts:\n    \"\"\".strip()\n    )\n    path_reps = [p if isinstance(p, str) else \"bytes\" for p in paths]\n    print(\"\\n\".join([u for u in urls if isinstance(u, str)]))  # appease mypy\n    print(\"\\n\".join(path_reps))\n    return docs\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_docs","title":"<code>ingest_docs(docs, split=True, metadata=[])</code>","text":"<p>Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects</p> required <code>split</code> <code>bool</code> <p>Whether to split docs into chunks. Default is True. If False, docs are treated as \"chunks\" and are not split.</p> <code>True</code> <code>metadata</code> <code>List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]</code> <p>List of metadata dicts, one for each doc, to augment whatever metadata is already in the doc. [ASSUME no conflicting keys between the two metadata dicts.] If a single dict is passed in, it is used for all docs.</p> <code>[]</code> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_docs(\n    self,\n    docs: List[Document],\n    split: bool = True,\n    metadata: (\n        List[Dict[str, Any]] | Dict[str, Any] | DocMetaData | List[DocMetaData]\n    ) = [],\n) -&gt; int:\n    \"\"\"\n    Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db\n\n    Args:\n        docs: List of Document objects\n        split: Whether to split docs into chunks. Default is True.\n            If False, docs are treated as \"chunks\" and are not split.\n        metadata: List of metadata dicts, one for each doc, to augment\n            whatever metadata is already in the doc.\n            [ASSUME no conflicting keys between the two metadata dicts.]\n            If a single dict is passed in, it is used for all docs.\n    \"\"\"\n    if isinstance(metadata, list) and len(metadata) &gt; 0:\n        for d, m in zip(docs, metadata):\n            orig_source = d.metadata.source\n            m_dict = m if isinstance(m, dict) else m.model_dump()  # type: ignore\n            d.metadata = d.metadata.model_copy(update=m_dict)  # type: ignore\n            d.metadata.source = _append_metadata_source(\n                orig_source, m_dict.get(\"source\", \"\")\n            )\n    elif isinstance(metadata, dict):\n        for d in docs:\n            orig_source = d.metadata.source\n            d.metadata = d.metadata.model_copy(update=metadata)\n            d.metadata.source = _append_metadata_source(\n                orig_source, metadata.get(\"source\", \"\")\n            )\n    elif isinstance(metadata, DocMetaData):\n        for d in docs:\n            orig_source = d.metadata.source\n            d.metadata = d.metadata.model_copy(update=metadata.model_dump())\n            d.metadata.source = _append_metadata_source(\n                orig_source, metadata.source\n            )\n\n    self.original_docs.extend(docs)\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    for d in docs:\n        if d.metadata.id in [None, \"\"]:\n            d.metadata.id = ObjectRegistry.new_id()\n    if split:\n        docs = self.parser.split(docs)\n    else:\n        if self.config.n_neighbor_chunks &gt; 0:\n            self.parser.add_window_ids(docs)\n        # we're not splitting, so we mark each doc as a chunk\n        for d in docs:\n            d.metadata.is_chunk = True\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    if self.config.chunk_enrichment_config is not None:\n        docs = self.enrich_chunks(docs)\n\n    # If any additional fields need to be added to content,\n    # add them as key=value pairs for all docs, before batching.\n    # This helps retrieval for table-like data.\n    # Note we need to do this at stage so that the embeddings\n    # are computed on the full content with these additional fields.\n    if len(self.config.add_fields_to_content) &gt; 0:\n        fields = [\n            f for f in extract_fields(docs[0], self.config.add_fields_to_content)\n        ]\n        if len(fields) &gt; 0:\n            for d in docs:\n                key_vals = extract_fields(d, fields)\n                d.content = (\n                    \",\".join(f\"{k}={v}\" for k, v in key_vals.items())\n                    + \",content=\"\n                    + d.content\n                )\n    docs = docs[: self.config.parsing.max_chunks]\n    # vecdb should take care of adding docs in batches;\n    # batching can be controlled via vecdb.config.batch_size\n    if not docs:\n        logging.warning(\n            \"No documents to ingest after processing. Skipping VecDB addition.\"\n        )\n        return 0  # Return 0 since no documents were added\n    self.vecdb.add_documents(docs)\n    self.original_docs_length = self.doc_length(docs)\n    self.setup_documents(docs, filter=self.config.filter)\n    return len(docs)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.retrieval_tool","title":"<code>retrieval_tool(msg)</code>","text":"<p>Handle the RetrievalTool message</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def retrieval_tool(self, msg: RetrievalTool) -&gt; str:\n    \"\"\"Handle the RetrievalTool message\"\"\"\n    self.config.retrieve_only = True\n    self.config.n_relevant_chunks = msg.num_results\n    content_doc = self.answer_from_docs(msg.query)\n    return content_doc.content\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.document_compatible_dataframe","title":"<code>document_compatible_dataframe(df, content='content', metadata=[])</code>  <code>staticmethod</code>","text":"<p>Convert dataframe so it is compatible with Document class: - has \"content\" column - has an \"id\" column to be used as Document.metadata.id</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to convert</p> required <code>content</code> <code>str</code> <p>name of content column</p> <code>'content'</code> <code>metadata</code> <code>List[str]</code> <p>list of metadata column names</p> <code>[]</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, List[str]]</code> <p>Tuple[pd.DataFrame, List[str]]: dataframe, metadata - dataframe: dataframe with \"content\" column and \"id\" column - metadata: list of metadata column names, including \"id\"</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef document_compatible_dataframe(\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; Tuple[pd.DataFrame, List[str]]:\n    \"\"\"\n    Convert dataframe so it is compatible with Document class:\n    - has \"content\" column\n    - has an \"id\" column to be used as Document.metadata.id\n\n    Args:\n        df: dataframe to convert\n        content: name of content column\n        metadata: list of metadata column names\n\n    Returns:\n        Tuple[pd.DataFrame, List[str]]: dataframe, metadata\n            - dataframe: dataframe with \"content\" column and \"id\" column\n            - metadata: list of metadata column names, including \"id\"\n    \"\"\"\n    if content not in df.columns:\n        raise ValueError(\n            f\"\"\"\n            Content column {content} not in dataframe,\n            so we cannot ingest into the DocChatAgent.\n            Please specify the `content` parameter as a suitable\n            text-based column in the dataframe.\n            \"\"\"\n        )\n    if content != \"content\":\n        # rename content column to \"content\", leave existing column intact\n        df = df.rename(columns={content: \"content\"}, inplace=False)\n\n    actual_metadata = metadata.copy()\n    if \"id\" not in df.columns:\n        docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n        ids = [str(d.id()) for d in docs]\n        df[\"id\"] = ids\n\n    if \"id\" not in actual_metadata:\n        actual_metadata += [\"id\"]\n\n    return df, actual_metadata\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_dataframe","title":"<code>ingest_dataframe(df, content='content', metadata=[])</code>","text":"<p>Ingest a dataframe into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; int:\n    \"\"\"\n    Ingest a dataframe into vecdb.\n    \"\"\"\n    self.from_dataframe = True\n    self.df_description = describe_dataframe(\n        df, filter_fields=self.config.filter_fields, n_vals=5\n    )\n    df, metadata = DocChatAgent.document_compatible_dataframe(df, content, metadata)\n    docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n    # When ingesting a dataframe we will no longer do any chunking,\n    # so we mark each doc as a chunk.\n    # TODO - revisit this since we may still want to chunk large text columns\n    for d in docs:\n        d.metadata.is_chunk = True\n    return self.ingest_docs(docs)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.setup_documents","title":"<code>setup_documents(docs=[], filter=None)</code>","text":"<p>Setup <code>self.chunked_docs</code> and <code>self.chunked_docs_clean</code> based on possible filter. These will be used in various non-vector-based search functions, e.g. self.get_similar_chunks_bm25(), self.get_fuzzy_matches(), etc.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Document objects. This is empty when we are calling this method after initial doc ingestion.</p> <code>[]</code> <code>filter</code> <code>str | None</code> <p>Filter condition for various lexical/semantic search fns.</p> <code>None</code> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def setup_documents(\n    self,\n    docs: List[Document] = [],\n    filter: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Setup `self.chunked_docs` and `self.chunked_docs_clean`\n    based on possible filter.\n    These will be used in various non-vector-based search functions,\n    e.g. self.get_similar_chunks_bm25(), self.get_fuzzy_matches(), etc.\n\n    Args:\n        docs: List of Document objects. This is empty when we are calling this\n            method after initial doc ingestion.\n        filter: Filter condition for various lexical/semantic search fns.\n    \"\"\"\n    if filter is None and len(docs) &gt; 0:\n        # no filter, so just use the docs passed in\n        self.chunked_docs.extend(docs)\n    else:\n        if self.vecdb is None:\n            raise ValueError(\"VecDB not set\")\n        self.chunked_docs = self.vecdb.get_all_documents(where=filter or \"\")\n\n    self.chunked_docs_clean = [\n        Document(content=preprocess_text(d.content), metadata=d.metadata)\n        for d in self.chunked_docs\n    ]\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_field_values","title":"<code>get_field_values(fields)</code>","text":"<p>Get string-listing of possible values of each field, e.g. {     \"genre\": \"crime, drama, mystery, ... (10 more)\",     \"certificate\": \"R, PG-13, PG, R\", } The field names may have \"metadata.\" prefix, e.g. \"metadata.genre\".</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_field_values(self, fields: list[str]) -&gt; Dict[str, str]:\n    \"\"\"Get string-listing of possible values of each field,\n    e.g.\n    {\n        \"genre\": \"crime, drama, mystery, ... (10 more)\",\n        \"certificate\": \"R, PG-13, PG, R\",\n    }\n    The field names may have \"metadata.\" prefix, e.g. \"metadata.genre\".\n    \"\"\"\n    field_values: Dict[str, Set[str]] = {}\n    # make empty set for each field\n    for f in fields:\n        field_values[f] = set()\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # get all documents and accumulate possible values of each field until 10\n    docs = self.vecdb.get_all_documents()  # only works for vecdbs that support this\n    for d in docs:\n        # extract fields from d\n        doc_field_vals = extract_fields(d, fields)\n        # the `field` returned by extract_fields may contain only the last\n        # part of the field name, e.g. \"genre\" instead of \"metadata.genre\",\n        # so we use the orig_field name to fill in the values\n        for (field, val), orig_field in zip(doc_field_vals.items(), fields):\n            field_values[orig_field].add(val)\n    # For each field make a string showing list of possible values,\n    # truncate to 20 values, and if there are more, indicate how many\n    # more there are, e.g. Genre: crime, drama, mystery, ... (20 more)\n    field_values_list = {}\n    for f in fields:\n        vals = list(field_values[f])\n        n = len(vals)\n        remaining = n - 20\n        vals = vals[:20]\n        if n &gt; 20:\n            vals.append(f\"(...{remaining} more)\")\n        # make a string of the values, ensure they are strings\n        field_values_list[f] = \", \".join(str(v) for v in vals)\n    return field_values_list\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_length","title":"<code>doc_length(docs)</code>","text":"<p>Calc token-length of a list of docs Args:     docs: list of Document objects Returns:     int: number of tokens</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def doc_length(self, docs: List[Document]) -&gt; int:\n    \"\"\"\n    Calc token-length of a list of docs\n    Args:\n        docs: list of Document objects\n    Returns:\n        int: number of tokens\n    \"\"\"\n    if self.parser is None:\n        raise ValueError(\"Parser not set\")\n    return self.parser.num_tokens(self.doc_string(docs))\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.user_docs_ingest_dialog","title":"<code>user_docs_ingest_dialog()</code>","text":"<p>Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def user_docs_ingest_dialog(self) -&gt; None:\n    \"\"\"\n    Ask user to select doc-collection, enter filenames/urls, and ingest into vecdb.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    n_deletes = self.vecdb.clear_empty_collections()\n    collections = self.vecdb.list_collections()\n    collection_name = \"NEW\"\n    is_new_collection = False\n    replace_collection = False\n    if len(collections) &gt; 0:\n        n = len(collections)\n        delete_str = (\n            f\"(deleted {n_deletes} empty collections)\" if n_deletes &gt; 0 else \"\"\n        )\n        print(f\"Found {n} collections: {delete_str}\")\n        for i, option in enumerate(collections, start=1):\n            print(f\"{i}. {option}\")\n        while True:\n            choice = Prompt.ask(\n                f\"Enter 1-{n} to select a collection, \"\n                \"or hit ENTER to create a NEW collection, \"\n                \"or -1 to DELETE ALL COLLECTIONS\",\n                default=\"0\",\n            )\n            try:\n                if -1 &lt;= int(choice) &lt;= n:\n                    break\n            except Exception:\n                pass\n\n        if choice == \"-1\":\n            confirm = Prompt.ask(\n                \"Are you sure you want to delete all collections?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            if confirm == \"y\":\n                self.vecdb.clear_all_collections(really=True)\n                collection_name = \"NEW\"\n\n        if int(choice) &gt; 0:\n            collection_name = collections[int(choice) - 1]\n            print(f\"Using collection {collection_name}\")\n            choice = Prompt.ask(\n                \"Would you like to replace this collection?\",\n                choices=[\"y\", \"n\"],\n                default=\"n\",\n            )\n            replace_collection = choice == \"y\"\n\n    if collection_name == \"NEW\":\n        is_new_collection = True\n        collection_name = Prompt.ask(\n            \"What would you like to name the NEW collection?\",\n            default=\"doc-chat\",\n        )\n\n    self.vecdb.set_collection(collection_name, replace=replace_collection)\n\n    default_urls_str = (\n        \" (or leave empty for default URLs)\" if is_new_collection else \"\"\n    )\n    print(f\"[blue]Enter some URLs or file/dir paths below {default_urls_str}\")\n    inputs = get_list_from_user()\n    if len(inputs) == 0:\n        if is_new_collection:\n            inputs = self.config.default_paths\n    self.config.doc_paths = inputs  # type: ignore\n    self.ingest()\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_string","title":"<code>doc_string(docs)</code>  <code>staticmethod</code>","text":"<p>Generate a string representation of a list of docs. Args:     docs: list of Document objects Returns:     str: string representation</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef doc_string(docs: List[Document]) -&gt; str:\n    \"\"\"\n    Generate a string representation of a list of docs.\n    Args:\n        docs: list of Document objects\n    Returns:\n        str: string representation\n    \"\"\"\n    contents = [d.content for d in docs]\n    sources = [d.metadata.source for d in docs]\n    sources = [f\"SOURCE: {s}\" if s is not None else \"\" for s in sources]\n    return \"\\n\".join(\n        [\n            f\"\"\"\n            -----[EXTRACT #{i+1}]----------\n            {content}\n            {source}\n            -----END OF EXTRACT------------\n\n            \"\"\"\n            for i, (content, source) in enumerate(zip(contents, sources))\n        ]\n    )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible Args:     question: question to answer     passages: list of <code>Document</code> objects each containing a possibly relevant         snippet, and metadata Returns:     a <code>Document</code> object containing the answer,     and metadata containing source citations</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_summary_answer(\n    self, question: str, passages: List[Document]\n) -&gt; ChatDocument:\n    \"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n\n    \"\"\"\n\n    passages_str = self.doc_string(passages)\n    # Substitute Q and P into the templatized prompt\n\n    final_prompt = self.config.summarize_prompt.format(\n        question=question, extracts=passages_str\n    )\n    show_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n\n    # Generate the final verbatim extract based on the final prompt.\n    # Note this will send entire message history, plus this final_prompt\n    # to the LLM, and self.message_history will be updated to include\n    # 2 new LLMMessage objects:\n    # one for `final_prompt`, and one for the LLM response\n\n    if self.config.conversation_mode:\n        if self.config.retain_context:\n            answer_doc = super().llm_response(final_prompt)\n        else:\n            # respond with temporary context\n            answer_doc = super()._llm_response_temp_context(question, final_prompt)\n    else:\n        answer_doc = super().llm_response_forget(final_prompt)\n\n    assert answer_doc is not None, \"LLM response should not be None here\"\n    final_answer = answer_doc.content.strip()\n    show_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\n\n    # extract references like [^2], [^3], etc. from the final answer\n    citations = extract_markdown_references(final_answer)\n    # format the cited references as a string suitable for markdown footnote\n    full_citations_str, citations_str = format_cited_references(citations, passages)\n\n    return ChatDocument(\n        content=final_answer,  # does not contain citations\n        metadata=ChatDocMetaData(\n            source=citations_str,  # only the reference headers\n            source_content=full_citations_str,  # reference + content\n            sender=Entity.LLM,\n            has_citation=len(citations) &gt; 0,\n            cached=getattr(answer_doc.metadata, \"cached\", False),\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.enrich_chunks","title":"<code>enrich_chunks(docs)</code>","text":"<p>Enrich chunks using Agent configured with self.config.chunk_enrichment_config.</p> <p>We assume that the system message of the agent is set in such a way that when we run <pre><code>prompt = self.config.chunk_enrichment_config.enrichment_prompt_fn(text)\nresult = await agent.llm_response_forget_async(prompt)\n</code></pre></p> <p>then <code>result.content</code> will contain the augmentation to the text.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of document chunks to enrich</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: Documents (chunks) enriched with additional text, separated by a delimiter.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def enrich_chunks(self, docs: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Enrich chunks using Agent configured with self.config.chunk_enrichment_config.\n\n    We assume that the system message of the agent is set in such a way\n    that when we run\n    ```\n    prompt = self.config.chunk_enrichment_config.enrichment_prompt_fn(text)\n    result = await agent.llm_response_forget_async(prompt)\n    ```\n\n    then `result.content` will contain the augmentation to the text.\n\n    Args:\n        docs: List of document chunks to enrich\n\n    Returns:\n        List[Document]: Documents (chunks) enriched with additional text,\n            separated by a delimiter.\n    \"\"\"\n    if self.config.chunk_enrichment_config is None:\n        return docs\n    enrichment_config = self.config.chunk_enrichment_config\n    agent = ChatAgent(enrichment_config)\n    if agent.llm is None:\n        raise ValueError(\"LLM not set\")\n\n    with status(\"[cyan]Augmenting chunks...\"):\n        # Process chunks in parallel using run_batch_agent_method\n        questions_batch = run_batch_agent_method(\n            agent=agent,\n            method=agent.llm_response_forget_async,\n            items=docs,\n            input_map=lambda doc: (\n                enrichment_config.enrichment_prompt_fn(doc.content)\n            ),\n            output_map=lambda response: response.content if response else \"\",\n            sequential=False,\n            batch_size=enrichment_config.batch_size,\n        )\n\n        # Combine original content with generated questions\n        augmented_docs = []\n        for doc, enrichment in zip(docs, questions_batch):\n            if not enrichment:\n                augmented_docs.append(doc)\n                continue\n\n            # Combine original content with questions in a structured way\n            combined_content = (\n                f\"{doc.content}{enrichment_config.delimiter}{enrichment}\"\n            )\n\n            new_doc = doc.model_copy(\n                update={\n                    \"content\": combined_content,\n                    \"metadata\": doc.metadata.model_copy(\n                        update={\"has_enrichment\": True}\n                    ),\n                }\n            )\n            augmented_docs.append(new_doc)\n\n        return augmented_docs\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.rerank_with_diversity","title":"<code>rerank_with_diversity(passages)</code>","text":"<p>Rerank a list of items in such a way that each successive item is least similar (on average) to the earlier items.</p> <p>Args: query (str): The query for which the passages are relevant. passages (List[Document]): A list of Documents to be reranked.</p> <p>Returns: List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_with_diversity(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank a list of items in such a way that each successive item is least similar\n    (on average) to the earlier items.\n\n    Args:\n    query (str): The query for which the passages are relevant.\n    passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n    List[Documents]: A reranked list of Documents.\n    \"\"\"\n\n    if self.vecdb is None:\n        logger.warning(\"No vecdb; cannot use rerank_with_diversity\")\n        return passages\n    emb_model = self.vecdb.embedding_model\n    emb_fn = emb_model.embedding_fn()\n    embs = emb_fn([p.content for p in passages])\n    embs_arr = [np.array(e) for e in embs]\n    indices = list(range(len(passages)))\n\n    # Helper function to compute average similarity to\n    # items in the current result list.\n    def avg_similarity_to_result(i: int, result: List[int]) -&gt; float:\n        return sum(  # type: ignore\n            (embs_arr[i] @ embs_arr[j])\n            / (np.linalg.norm(embs_arr[i]) * np.linalg.norm(embs_arr[j]))\n            for j in result\n        ) / len(result)\n\n    # copy passages to items\n    result = [indices.pop(0)]  # Start with the first item.\n\n    while indices:\n        # Find the item that has the least average similarity\n        # to items in the result list.\n        least_similar_item = min(\n            indices, key=lambda i: avg_similarity_to_result(i, result)\n        )\n        result.append(least_similar_item)\n        indices.remove(least_similar_item)\n\n    # return passages in order of result list\n    return [passages[i] for i in result]\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.rerank_to_periphery","title":"<code>rerank_to_periphery(passages)</code>","text":"<p>Rerank to avoid Lost In the Middle (LIM) problem, where LLMs pay more attention to items at the ends of a list, rather than the middle. So we re-rank to make the best passages appear at the periphery of the list. https://arxiv.org/abs/2307.03172</p> <p>Example reranking: 1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2</p> <p>Parameters:</p> Name Type Description Default <code>passages</code> <code>List[Document]</code> <p>A list of Documents to be reranked.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Documents]: A reranked list of Documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def rerank_to_periphery(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Rerank to avoid Lost In the Middle (LIM) problem,\n    where LLMs pay more attention to items at the ends of a list,\n    rather than the middle. So we re-rank to make the best passages\n    appear at the periphery of the list.\n    https://arxiv.org/abs/2307.03172\n\n    Example reranking:\n    1 2 3 4 5 6 7 8 9 ==&gt; 1 3 5 7 9 8 6 4 2\n\n    Args:\n        passages (List[Document]): A list of Documents to be reranked.\n\n    Returns:\n        List[Documents]: A reranked list of Documents.\n\n    \"\"\"\n    # Splitting items into odds and evens based on index, not value\n    odds = passages[::2]\n    evens = passages[1::2][::-1]\n\n    # Merging them back together\n    return odds + evens\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.add_context_window","title":"<code>add_context_window(docs_scores)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. We use these stored window_ids to retrieve the desired number (self.config.n_neighbor_chunks) of neighbors on either side of the current chunk.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def add_context_window(\n    self,\n    docs_scores: List[Tuple[Document, float]],\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk. We use these stored\n    window_ids to retrieve the desired number\n    (self.config.n_neighbor_chunks) of neighbors\n    on either side of the current chunk.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None or self.config.n_neighbor_chunks == 0:\n        return docs_scores\n    if len(docs_scores) == 0:\n        return []\n    if set(docs_scores[0][0].model_fields) != {\"content\", \"metadata\"}:\n        # Do not add context window when there are other fields besides just\n        # content and metadata, since we do not know how to set those other fields\n        # for newly created docs with combined content.\n        return docs_scores\n    return self.vecdb.add_context_window(docs_scores, self.config.n_neighbor_chunks)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_semantic_search_results","title":"<code>get_semantic_search_results(query, k=10)</code>","text":"<p>Get semantic search results from vecdb. Args:     query (str): query to search for     k (int): number of results to return Returns:     List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_semantic_search_results(\n    self,\n    query: str,\n    k: int = 10,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Get semantic search results from vecdb.\n    Args:\n        query (str): query to search for\n        k (int): number of results to return\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n    # Note: for dynamic filtering based on a query, users can\n    # use the `temp_update` context-manager to pass in a `filter` to self.config,\n    # e.g.:\n    # with temp_update(self.config, {\"filter\": \"metadata.source=='source1'\"}):\n    #     docs_scores = self.get_semantic_search_results(query, k=k)\n    # This avoids having pass the `filter` argument to every function call\n    # upstream of this one.\n    # The `temp_update` context manager is defined in\n    # `langroid/utils/pydantic_utils.py`\n    return self.vecdb.similar_texts_with_scores(\n        query,\n        k=k,\n        where=self.config.filter,\n    )\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_relevant_chunks","title":"<code>get_relevant_chunks(query, query_proxies=[])</code>","text":"<p>The retrieval stage in RAG: get doc-chunks that are most \"relevant\" to the query (and possibly any proxy queries), from the document-store, which currently is the vector store, but in theory could be any document store, or even web-search. This stage does NOT involve an LLM, and the retrieved chunks could either be pre-chunked text (from the initial pre-processing stage where chunks were stored in the vector store), or they could be dynamically retrieved based on a window around a lexical match.</p> <p>These are the steps (some optional based on config): - semantic search based on vector-embedding distance, from vecdb - lexical search using bm25-ranking (keyword similarity) - fuzzy matching (keyword similarity) - re-ranking of doc-chunks by relevance to query, using cross-encoder,    and pick top k</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>original query (assumed to be in stand-alone form)</p> required <code>query_proxies</code> <code>List[str]</code> <p>possible rephrases, or hypothetical answer to query     (e.g. for HyDE-type retrieval)</p> <code>[]</code> <p>Returns:</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_relevant_chunks(\n    self, query: str, query_proxies: List[str] = []\n) -&gt; List[Document]:\n    \"\"\"\n    The retrieval stage in RAG: get doc-chunks that are most \"relevant\"\n    to the query (and possibly any proxy queries), from the document-store,\n    which currently is the vector store,\n    but in theory could be any document store, or even web-search.\n    This stage does NOT involve an LLM, and the retrieved chunks\n    could either be pre-chunked text (from the initial pre-processing stage\n    where chunks were stored in the vector store), or they could be\n    dynamically retrieved based on a window around a lexical match.\n\n    These are the steps (some optional based on config):\n    - semantic search based on vector-embedding distance, from vecdb\n    - lexical search using bm25-ranking (keyword similarity)\n    - fuzzy matching (keyword similarity)\n    - re-ranking of doc-chunks by relevance to query, using cross-encoder,\n       and pick top k\n\n    Args:\n        query: original query (assumed to be in stand-alone form)\n        query_proxies: possible rephrases, or hypothetical answer to query\n                (e.g. for HyDE-type retrieval)\n\n    Returns:\n\n    \"\"\"\n\n    if (\n        self.vecdb is None\n        or self.vecdb.config.collection_name\n        not in self.vecdb.list_collections(empty=False)\n    ):\n        return []\n\n    # if we are using cross-encoder reranking or reciprocal rank fusion (RRF),\n    # we can retrieve more docs during retrieval, and leave it to the cross-encoder\n    # or RRF reranking to whittle down to self.config.n_similar_chunks\n    retrieval_multiple = (\n        1\n        if (\n            self.config.cross_encoder_reranking_model == \"\"\n            and not self.config.use_reciprocal_rank_fusion\n        )\n        else 3\n    )\n\n    if self.vecdb is None:\n        raise ValueError(\"VecDB not set\")\n\n    with status(\"[cyan]Searching VecDB for relevant doc passages...\"):\n        docs_and_scores: List[Tuple[Document, float]] = []\n        for q in [query] + query_proxies:\n            docs_and_scores += self.get_semantic_search_results(\n                q,\n                k=self.config.n_similar_chunks * retrieval_multiple,\n            )\n            # sort by score descending\n            docs_and_scores = sorted(\n                docs_and_scores, key=lambda x: x[1], reverse=True\n            )\n\n    # keep only docs with unique d.id()\n    id2_rank_semantic = {d.id(): i for i, (d, _) in enumerate(docs_and_scores)}\n    id2doc = {d.id(): d for d, _ in docs_and_scores}\n    # make sure we get unique docs\n    passages = [id2doc[id] for id in id2_rank_semantic.keys()]\n\n    id2_rank_bm25 = {}\n    if self.config.use_bm25_search:\n        # TODO: Add score threshold in config\n        docs_scores = self.get_similar_chunks_bm25(query, retrieval_multiple)\n        id2doc.update({d.id(): d for d, _ in docs_scores})\n        if self.config.use_reciprocal_rank_fusion:\n            # if we're not re-ranking with a cross-encoder, and have RRF enabled,\n            # instead of accumulating the bm25 results into passages,\n            # we collect these ranks for Reciprocal Rank Fusion down below.\n            docs_scores = sorted(docs_scores, key=lambda x: x[1], reverse=True)\n            id2_rank_bm25 = {d.id(): i for i, (d, _) in enumerate(docs_scores)}\n        else:\n            passages += [d for (d, _) in docs_scores]\n            # eliminate duplicate ids\n            passages = [id2doc[id] for id in id2doc.keys()]\n\n    id2_rank_fuzzy = {}\n    if self.config.use_fuzzy_match:\n        # TODO: Add score threshold in config\n        fuzzy_match_doc_scores = self.get_fuzzy_matches(query, retrieval_multiple)\n        if self.config.use_reciprocal_rank_fusion:\n            # if we're not re-ranking with a cross-encoder,\n            # instead of accumulating the fuzzy match results into passages,\n            # we collect these ranks for Reciprocal Rank Fusion down below.\n            fuzzy_match_doc_scores = sorted(\n                fuzzy_match_doc_scores, key=lambda x: x[1], reverse=True\n            )\n            id2_rank_fuzzy = {\n                d.id(): i for i, (d, _) in enumerate(fuzzy_match_doc_scores)\n            }\n            id2doc.update({d.id(): d for d, _ in fuzzy_match_doc_scores})\n        else:\n            passages += [d for (d, _) in fuzzy_match_doc_scores]\n            # eliminate duplicate ids\n            passages = [id2doc[id] for id in id2doc.keys()]\n\n    if self.config.use_reciprocal_rank_fusion and (\n        self.config.use_bm25_search or self.config.use_fuzzy_match\n    ):\n        # Since we're not using cross-enocder re-ranking,\n        # we need to re-order the retrieved chunks from potentially three\n        # different retrieval methods (semantic, bm25, fuzzy), where the\n        # similarity scores are on different scales.\n        # We order the retrieved chunks using Reciprocal Rank Fusion (RRF) score.\n        # Combine the ranks from each id2doc_rank_* dict into a single dict,\n        # where the reciprocal rank score is the sum of\n        # 1/(rank + self.config.reciprocal_rank_fusion_constant).\n        # See https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking\n        #\n        # Note: diversity/periphery-reranking below may modify the final ranking.\n        id2_reciprocal_score = {}\n        for id_ in (\n            set(id2_rank_semantic.keys())\n            | set(id2_rank_bm25.keys())\n            | set(id2_rank_fuzzy.keys())\n        ):\n            # Use max_rank instead of infinity to avoid bias against\n            # single-method docs\n            max_rank = self.config.n_similar_chunks * retrieval_multiple\n            rank_semantic = id2_rank_semantic.get(id_, max_rank + 1)\n            rank_bm25 = id2_rank_bm25.get(id_, max_rank + 1)\n            rank_fuzzy = id2_rank_fuzzy.get(id_, max_rank + 1)\n            c = self.config.reciprocal_rank_fusion_constant\n            reciprocal_fusion_score = (\n                1 / (rank_semantic + c) + 1 / (rank_bm25 + c) + 1 / (rank_fuzzy + c)\n            )\n            id2_reciprocal_score[id_] = reciprocal_fusion_score\n\n        # sort the docs by the reciprocal score, in descending order\n        id2_reciprocal_score = OrderedDict(\n            sorted(\n                id2_reciprocal_score.items(),\n                key=lambda x: x[1],\n                reverse=True,\n            )\n        )\n        # each method retrieved up to retrieval_multiple * n_similar_chunks,\n        # so we need to take the top n_similar_chunks from the combined list\n        passages = [\n            id2doc[id]\n            for id, _ in list(id2_reciprocal_score.items())[\n                : self.config.n_similar_chunks\n            ]\n        ]\n        # passages must have distinct ids\n        assert len(passages) == len(set([d.id() for d in passages])), (\n            f\"Duplicate passages in retrieved docs: {len(passages)} != \"\n            f\"{len(set([d.id() for d in passages]))}\"\n        )\n\n    if len(passages) == 0:\n        logger.debug(\"No passages retrieved for query '%s'\", query)\n        return []\n\n    if self.config.rerank_after_adding_context:\n        passages_scores = [(p, 0.0) for p in passages]\n        passages_scores = self.add_context_window(passages_scores)\n        passages = [p for p, _ in passages_scores]\n    # now passages can potentially have a lot of doc chunks,\n    # so we re-rank them using a cross-encoder scoring model\n    # (provided that `reciprocal_rank_fusion` is not enabled),\n    # and pick top k where k = config..n_similar_chunks\n    # https://www.sbert.net/examples/applications/retrieve_rerank\n    if (\n        self.config.cross_encoder_reranking_model != \"\"\n        and not self.config.use_reciprocal_rank_fusion\n    ):\n        passages = self.rerank_with_cross_encoder(query, passages)\n\n    if self.config.rerank_diversity:\n        # reorder to increase diversity among top docs\n        passages = self.rerank_with_diversity(passages)\n\n    if self.config.rerank_periphery:\n        # reorder so most important docs are at periphery\n        # (see Lost In the Middle issue).\n        passages = self.rerank_to_periphery(passages)\n\n    if not self.config.rerank_after_adding_context:\n        passages_scores = [(p, 0.0) for p in passages]\n        passages_scores = self.add_context_window(passages_scores)\n        passages = [p for p, _ in passages_scores]\n\n    return passages[: self.config.n_relevant_chunks]\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_relevant_extracts","title":"<code>get_relevant_extracts(query)</code>","text":"<p>Get list of (verbatim) extracts from doc-chunks relevant to answering a query.</p> <p>These are the stages (some optional based on config): - use LLM to convert query to stand-alone query - optionally use LLM to rephrase query to use below - optionally use LLM to generate hypothetical answer (HyDE) to use below. - get_relevant_chunks(): get doc-chunks relevant to query and proxies - use LLM to get relevant extracts from doc-chunks</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to search for</p> required <p>Returns:</p> Name Type Description <code>query</code> <code>str</code> <p>stand-alone version of input query</p> <code>List[Document]</code> <p>List[Document]: list of relevant extracts</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@no_type_check\ndef get_relevant_extracts(self, query: str) -&gt; Tuple[str, List[Document]]:\n    \"\"\"\n    Get list of (verbatim) extracts from doc-chunks relevant to answering a query.\n\n    These are the stages (some optional based on config):\n    - use LLM to convert query to stand-alone query\n    - optionally use LLM to rephrase query to use below\n    - optionally use LLM to generate hypothetical answer (HyDE) to use below.\n    - get_relevant_chunks(): get doc-chunks relevant to query and proxies\n    - use LLM to get relevant extracts from doc-chunks\n\n    Args:\n        query (str): query to search for\n\n    Returns:\n        query (str): stand-alone version of input query\n        List[Document]: list of relevant extracts\n\n    \"\"\"\n    collection_name = (\n        None if self.vecdb is None else self.vecdb.config.collection_name\n    )\n    has_vecdb_collection = (\n        collection_name is not None\n        and collection_name in self.vecdb.list_collections(empty=False)\n        if self.vecdb is not None\n        else False\n    )\n\n    if not has_vecdb_collection and len(self.chunked_docs) == 0:\n        return query, []\n\n    if len(self.dialog) &gt; 0 and not self.config.assistant_mode:\n        # Regardless of whether we are in conversation mode or not,\n        # for relevant doc/chunk extraction, we must convert the query\n        # to a standalone query to get more relevant results.\n        with status(\"[cyan]Converting to stand-alone query...[/cyan]\"):\n            with StreamingIfAllowed(self.llm, False):\n                query = self.llm.followup_to_standalone(self.dialog, query)\n        print(f\"[orange2]New query: {query}\")\n\n    proxies = []\n    if self.config.hypothetical_answer:\n        answer = self.llm_hypothetical_answer(query)\n        proxies = [answer]\n\n    if self.config.n_query_rephrases &gt; 0:\n        rephrases = self.llm_rephrase_query(query)\n        proxies += rephrases\n    if has_vecdb_collection:\n        passages = self.get_relevant_chunks(query, proxies)  # no LLM involved\n    else:\n        passages = self.chunked_docs\n\n    if len(passages) == 0:\n        return query, []\n\n    if self.config.relevance_extractor_config is None:\n        extracts = passages\n    else:\n        with status(\"[cyan]LLM Extracting verbatim passages...\"):\n            with StreamingIfAllowed(self.llm, False):\n                # these are async calls, one per passage; turn off streaming\n                extracts = self.get_verbatim_extracts(query, passages)\n                extracts = [e for e in extracts if e.content != NO_ANSWER]\n\n    return query, extracts\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.remove_chunk_enrichments","title":"<code>remove_chunk_enrichments(passages)</code>","text":"<p>Remove any enrichments (like hypothetical questions, or keywords) from documents. Only cleans if enrichment was enabled in config.</p> <p>Parameters:</p> Name Type Description Default <code>passages</code> <code>List[Document]</code> <p>List of documents to clean</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of documents with only original content</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def remove_chunk_enrichments(self, passages: List[Document]) -&gt; List[Document]:\n    \"\"\"Remove any enrichments (like hypothetical questions, or keywords)\n    from documents.\n    Only cleans if enrichment was enabled in config.\n\n    Args:\n        passages: List of documents to clean\n\n    Returns:\n        List of documents with only original content\n    \"\"\"\n    if self.config.chunk_enrichment_config is None:\n        return passages\n    delimiter = self.config.chunk_enrichment_config.delimiter\n    return [\n        (\n            doc.model_copy(update={\"content\": doc.content.split(delimiter)[0]})\n            if doc.content and getattr(doc.metadata, \"has_enrichment\", False)\n            else doc\n        )\n        for doc in passages\n    ]\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_verbatim_extracts","title":"<code>get_verbatim_extracts(query, passages)</code>","text":"<p>Run RelevanceExtractorAgent in async/concurrent mode on passages, to extract portions relevant to answering query, from each passage. Args:     query (str): query to answer     passages (List[Documents]): list of passages to extract from</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of Documents containing extracts and metadata.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_verbatim_extracts(\n    self,\n    query: str,\n    passages: List[Document],\n) -&gt; List[Document]:\n    \"\"\"\n    Run RelevanceExtractorAgent in async/concurrent mode on passages,\n    to extract portions relevant to answering query, from each passage.\n    Args:\n        query (str): query to answer\n        passages (List[Documents]): list of passages to extract from\n\n    Returns:\n        List[Document]: list of Documents containing extracts and metadata.\n    \"\"\"\n    passages = self.remove_chunk_enrichments(passages)\n\n    agent_cfg = self.config.relevance_extractor_config\n    if agent_cfg is None:\n        # no relevance extraction: simply return passages\n        return passages\n    if agent_cfg.llm is None:\n        # Use main DocChatAgent's LLM if not provided explicitly:\n        # this reduces setup burden on the user\n        agent_cfg.llm = self.config.llm\n    agent_cfg.query = query\n    agent_cfg.segment_length = self.config.extraction_granularity\n    agent_cfg.llm.stream = False  # disable streaming for concurrent calls\n\n    agent = RelevanceExtractorAgent(agent_cfg)\n    task = Task(\n        agent,\n        name=\"Relevance-Extractor\",\n        interactive=False,\n    )\n\n    extracts: list[str] = run_batch_tasks(\n        task,\n        passages,\n        input_map=lambda msg: msg.content,\n        output_map=lambda ans: ans.content if ans is not None else NO_ANSWER,\n    )  # type: ignore\n\n    # Caution: Retain ALL other fields in the Documents (which could be\n    # other than just `content` and `metadata`), while simply replacing\n    # `content` with the extracted portions\n    passage_extracts = []\n    for p, e in zip(passages, extracts):\n        if e == NO_ANSWER or len(e) == 0:\n            continue\n        p_copy = p.model_copy()\n        p_copy.content = e\n        passage_extracts.append(p_copy)\n\n    return passage_extracts\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.answer_from_docs","title":"<code>answer_from_docs(query)</code>","text":"<p>Answer query based on relevant docs from the VecDB</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query to answer</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>ChatDocument</code> <p>answer</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def answer_from_docs(self, query: str) -&gt; ChatDocument:\n    \"\"\"\n    Answer query based on relevant docs from the VecDB\n\n    Args:\n        query (str): query to answer\n\n    Returns:\n        Document: answer\n    \"\"\"\n    response = ChatDocument(\n        content=NO_ANSWER,\n        metadata=ChatDocMetaData(\n            source=\"None\",\n            sender=Entity.LLM,\n        ),\n    )\n    # query may be updated to a stand-alone version\n    query, extracts = self.get_relevant_extracts(query)\n    if len(extracts) == 0:\n        return response\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if self.config.retrieve_only:\n        # only return extracts, skip LLM-based summary answer\n        meta = dict(\n            sender=Entity.LLM,\n        )\n        # copy metadata from first doc, unclear what to do here.\n        meta.update(extracts[0].metadata.model_dump())\n        return ChatDocument(\n            content=\"\\n\\n\".join([e.content for e in extracts]),\n            metadata=ChatDocMetaData(**meta),  # type: ignore\n        )\n    response = self.get_summary_answer(query, extracts)\n\n    self.update_dialog(query, response.content)\n    self.response = response  # save last response\n    return response\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.summarize_docs","title":"<code>summarize_docs(instruction='Give a concise summary of the following text:')</code>","text":"<p>Summarize all docs</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def summarize_docs(\n    self,\n    instruction: str = \"Give a concise summary of the following text:\",\n) -&gt; None | ChatDocument:\n    \"\"\"Summarize all docs\"\"\"\n    if self.llm is None:\n        raise ValueError(\"LLM not set\")\n    if len(self.original_docs) == 0:\n        logger.warning(\n            \"\"\"\n            No docs to summarize! Perhaps you are re-using a previously\n            defined collection?\n            In that case, we don't have access to the original docs.\n            To create a summary, use a new collection, and specify a list of docs.\n            \"\"\"\n        )\n        return None\n    full_text = \"\\n\\n\".join([d.content for d in self.original_docs])\n    if self.parser is None:\n        raise ValueError(\"No parser defined\")\n    tot_tokens = self.parser.num_tokens(full_text)\n    MAX_INPUT_TOKENS = (\n        self.llm.completion_context_length()\n        - self.config.llm.model_max_output_tokens\n        - 100\n    )\n    if tot_tokens &gt; MAX_INPUT_TOKENS:\n        # truncate\n        full_text = self.parser.tokenizer.decode(\n            self.parser.tokenizer.encode(full_text)[:MAX_INPUT_TOKENS]\n        )\n        logger.warning(\n            f\"Summarizing after truncating text to {MAX_INPUT_TOKENS} tokens\"\n        )\n    prompt = f\"\"\"\n    {instruction}\n\n    FULL TEXT:\n    {full_text}\n    \"\"\".strip()\n    with StreamingIfAllowed(self.llm):\n        summary = ChatAgent.llm_response(self, prompt)\n        return summary\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.justify_response","title":"<code>justify_response()</code>","text":"<p>Show evidence for last response</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def justify_response(self) -&gt; ChatDocument | None:\n    \"\"\"Show evidence for last response\"\"\"\n    if self.response is None:\n        print(\"[magenta]No response yet\")\n        return None\n    source = self.response.metadata.source\n    if len(source) &gt; 0:\n        print(\"[magenta]\" + source)\n    else:\n        print(\"[magenta]No source found\")\n    return None\n</code></pre>"},{"location":"reference/agent/special/lance_doc_chat_agent/","title":"lance_doc_chat_agent","text":"<p>langroid/agent/special/lance_doc_chat_agent.py </p> <p>LanceDocChatAgent is a subclass of DocChatAgent that uses LanceDB as a vector store: - Uses the DocChatAgentConfig.filter variable     (a sql string) in the <code>where</code> clause to do filtered vector search. - Overrides the get_similar_chunks_bm25() to use LanceDB FTS (Full Text Search).</p> For usage see <ul> <li><code>tests/main/test_lance_doc_chat_agent.py</code>.</li> <li>example script <code>examples/docqa/lance_rag.py</code>.</li> </ul>"},{"location":"reference/agent/special/lance_doc_chat_agent/#langroid.agent.special.lance_doc_chat_agent.LanceDocChatAgent","title":"<code>LanceDocChatAgent(cfg)</code>","text":"<p>               Bases: <code>DocChatAgent</code></p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def __init__(self, cfg: DocChatAgentConfig):\n    super().__init__(cfg)\n    self.config: DocChatAgentConfig = cfg\n    self.enable_message(QueryPlanTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/special/lance_doc_chat_agent/#langroid.agent.special.lance_doc_chat_agent.LanceDocChatAgent.query_plan","title":"<code>query_plan(msg)</code>","text":"<p>Handle the LLM's use of the FilterTool. Temporarily set the config filter and either return the final answer in case there's a dataframe_calc, or return the rephrased query so the LLM can handle it.</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def query_plan(self, msg: QueryPlanTool) -&gt; AgentDoneTool | str:\n    \"\"\"\n    Handle the LLM's use of the FilterTool.\n    Temporarily set the config filter and either return the final answer\n    in case there's a dataframe_calc, or return the rephrased query\n    so the LLM can handle it.\n    \"\"\"\n    # create document-subset based on this filter\n    plan = msg.plan\n    try:\n        self.setup_documents(filter=plan.filter or None)\n    except Exception as e:\n        logger.error(f\"Error setting up documents: {e}\")\n        # say DONE with err msg so it goes back to LanceFilterAgent\n        return AgentDoneTool(\n            content=f\"\"\"\n            Possible Filter Error:\\n {e}\n\n            Note that only the following fields are allowed in the filter\n            of a query plan: \n            {\", \".join(self.config.filter_fields)}\n            \"\"\"\n        )\n\n    # update the filter so it is used in the DocChatAgent\n    self.config.filter = plan.filter or None\n    if plan.dataframe_calc:\n        # we just get relevant docs then do the calculation\n        # TODO if calc causes err, it is captured in result,\n        # and LLM can correct the calc based on the err,\n        # and this will cause retrieval all over again,\n        # which may be wasteful if only the calc part is wrong.\n        # The calc step can later be done with a separate Agent/Tool.\n        if plan.query is None or plan.query.strip() == \"\":\n            if plan.filter is None or plan.filter.strip() == \"\":\n                return AgentDoneTool(\n                    content=\"\"\"\n                    Cannot execute Query Plan since filter as well as \n                    rephrased query are empty.                    \n                    \"\"\"\n                )\n            else:\n                # no query to match, so just get all docs matching filter\n                docs = self.vecdb.get_all_documents(plan.filter)\n        else:\n            _, docs = self.get_relevant_extracts(plan.query)\n        if len(docs) == 0:\n            return AgentDoneTool(content=NO_ANSWER)\n        answer = self.vecdb.compute_from_docs(docs, plan.dataframe_calc)\n    else:\n        # pass on the query so LLM can handle it\n        response = self.llm_response(plan.query)\n        answer = NO_ANSWER if response is None else response.content\n    return AgentDoneTool(tools=[AnswerTool(answer=answer)])\n</code></pre>"},{"location":"reference/agent/special/lance_doc_chat_agent/#langroid.agent.special.lance_doc_chat_agent.LanceDocChatAgent.ingest_dataframe","title":"<code>ingest_dataframe(df, content='content', metadata=[])</code>","text":"<p>Ingest from a dataframe. Assume we are doing this once, not incrementally</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def ingest_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; int:\n    \"\"\"Ingest from a dataframe. Assume we are doing this once, not incrementally\"\"\"\n\n    self.from_dataframe = True\n    if df.shape[0] == 0:\n        raise ValueError(\n            \"\"\"\n            LanceDocChatAgent.ingest_dataframe() received an empty dataframe.\n            \"\"\"\n        )\n    n = df.shape[0]\n\n    # If any additional fields need to be added to content,\n    # add them as key=value pairs, into the `content` field for all rows.\n    # This helps retrieval for table-like data.\n    # Note we need to do this at stage so that the embeddings\n    # are computed on the full content with these additional fields.\n    fields = [f for f in self.config.add_fields_to_content if f in df.columns]\n    if len(fields) &gt; 0:\n        df[content] = df.apply(\n            lambda row: (\",\".join(f\"{f}={row[f]}\" for f in fields))\n            + \", content=\"\n            + row[content],\n            axis=1,\n        )\n\n    df, metadata = DocChatAgent.document_compatible_dataframe(df, content, metadata)\n    self.df_description = describe_dataframe(\n        df,\n        filter_fields=self.config.filter_fields,\n        n_vals=10,\n    )\n    self.vecdb.add_dataframe(df, content=\"content\", metadata=metadata)\n\n    tbl = self.vecdb.client.open_table(self.vecdb.config.collection_name)\n    # We assume \"content\" is available as top-level field\n    if \"content\" in tbl.schema.names:\n        tbl.create_fts_index(\"content\", replace=True)\n    # We still need to do the below so that\n    # other types of searches in DocChatAgent\n    # can work, as they require Document objects\n    docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n    self.setup_documents(docs)\n    # mark each doc as already-chunked so we don't try to split them further\n    # TODO later we may want to split large text-columns\n    for d in docs:\n        d.metadata.is_chunk = True\n    return n  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/lance_doc_chat_agent/#langroid.agent.special.lance_doc_chat_agent.LanceDocChatAgent.get_similar_chunks_bm25","title":"<code>get_similar_chunks_bm25(query, multiple)</code>","text":"<p>Override the DocChatAgent.get_similar_chunks_bm25() to use LanceDB FTS (Full Text Search).</p> Source code in <code>langroid/agent/special/lance_doc_chat_agent.py</code> <pre><code>def get_similar_chunks_bm25(\n    self, query: str, multiple: int\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Override the DocChatAgent.get_similar_chunks_bm25()\n    to use LanceDB FTS (Full Text Search).\n    \"\"\"\n    # Clean up query: replace all newlines with spaces in query,\n    # force special search keywords to lower case, remove quotes,\n    # so it's not interpreted as search syntax\n    query_clean = (\n        query.replace(\"\\n\", \" \")\n        .replace(\"AND\", \"and\")\n        .replace(\"OR\", \"or\")\n        .replace(\"NOT\", \"not\")\n        .replace(\"'\", \"\")\n        .replace('\"', \"\")\n        .replace(\":\", \"--\")\n    )\n\n    tbl = self.vecdb.client.open_table(self.vecdb.config.collection_name)\n    result = (\n        tbl.search(query_clean)\n        .where(self.config.filter or None)\n        .limit(self.config.n_similar_chunks * multiple)\n    )\n    docs = self.vecdb._lance_result_to_docs(result)\n    scores = [r[\"score\"] for r in result.to_list()]\n    return list(zip(docs, scores))\n</code></pre>"},{"location":"reference/agent/special/lance_tools/","title":"lance_tools","text":"<p>langroid/agent/special/lance_tools.py </p>"},{"location":"reference/agent/special/lance_tools/#langroid.agent.special.lance_tools.AnswerTool","title":"<code>AnswerTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Wrapper for answer from LanceDocChatAgent</p>"},{"location":"reference/agent/special/relevance_extractor_agent/","title":"relevance_extractor_agent","text":"<p>langroid/agent/special/relevance_extractor_agent.py </p> <p>Agent to retrieve relevant segments from a body of text, that are relevant to a query.</p>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent","title":"<code>RelevanceExtractorAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Agent for extracting segments from text, that are relevant to a given query.</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def __init__(self, config: RelevanceExtractorAgentConfig):\n    super().__init__(config)\n    self.config: RelevanceExtractorAgentConfig = config\n    self.enable_message(SegmentExtractTool)\n    self.numbered_passage: Optional[str] = None\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\ndef llm_response(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    \"\"\"\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    &lt;Instructions&gt;\n    Given the PASSAGE below with NUMBERED segments, and the QUERY,\n    extract ONLY the segment-numbers that are RELEVANT to the QUERY,\n    and present them using the `extract_segments` tool/function,\n    i.e. your response MUST be a JSON-formatted string starting with\n    `{{\"request\": \"extract_segments\", ...}}`\n    &lt;/Instructions&gt;\n\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    response = super().llm_response(prompt)\n    return response\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.llm_response_async","title":"<code>llm_response_async(message=None)</code>  <code>async</code>","text":"<p>Compose a prompt asking to extract relevant segments from a passage. Steps: - number the segments in the passage - compose prompt - send to LLM The LLM is expected to generate a structured msg according to the SegmentExtractTool schema, i.e. it should contain a <code>segment_list</code> field whose value is a list of segment numbers or ranges, like \"10,12,14-17\".</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>@no_type_check\nasync def llm_response_async(\n    self, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Compose a prompt asking to extract relevant segments from a passage.\n    Steps:\n    - number the segments in the passage\n    - compose prompt\n    - send to LLM\n    The LLM is expected to generate a structured msg according to the\n    SegmentExtractTool schema, i.e. it should contain a `segment_list` field\n    whose value is a list of segment numbers or ranges, like \"10,12,14-17\".\n    \"\"\"\n\n    assert self.config.query is not None, \"No query specified\"\n    assert message is not None, \"No message specified\"\n    message_str = message.content if isinstance(message, ChatDocument) else message\n    # number the segments in the passage\n    self.numbered_passage = number_segments(message_str, self.config.segment_length)\n    # compose prompt\n    prompt = f\"\"\"\n    PASSAGE:\n    {self.numbered_passage}\n\n    QUERY: {self.config.query}\n    \"\"\"\n    # send to LLM\n    response = await super().llm_response_async(prompt)\n    return response\n</code></pre>"},{"location":"reference/agent/special/relevance_extractor_agent/#langroid.agent.special.relevance_extractor_agent.RelevanceExtractorAgent.extract_segments","title":"<code>extract_segments(msg)</code>","text":"<p>Method to handle a segmentExtractTool message from LLM</p> Source code in <code>langroid/agent/special/relevance_extractor_agent.py</code> <pre><code>def extract_segments(self, msg: SegmentExtractTool) -&gt; str:\n    \"\"\"Method to handle a segmentExtractTool message from LLM\"\"\"\n    spec = msg.segment_list\n    if len(self.message_history) == 0:\n        return DONE + \" \" + NO_ANSWER\n    if spec is None or spec.strip() in [\"\", NO_ANSWER]:\n        return DONE + \" \" + NO_ANSWER\n    assert self.numbered_passage is not None, \"No numbered passage\"\n    # assume this has numbered segments\n    try:\n        extracts = extract_numbered_segments(self.numbered_passage, spec)\n    except Exception:\n        return DONE + \" \" + NO_ANSWER\n    # this response ends the task by saying DONE\n    return DONE + \" \" + extracts\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/","title":"retriever_agent","text":"<p>langroid/agent/special/retriever_agent.py </p> <p>DEPRECATED: use DocChatAgent instead, with DocChatAgentConfig.retrieve_only=True, and if you want to retrieve FULL relevant doc-contents rather than just extracts, then set DocChatAgentConfig.extraction_granularity=-1</p> <p>This is an agent to retrieve relevant extracts from a vector store, where the LLM is used to filter for \"true\" relevance after retrieval from the vector store. This is essentially the same as DocChatAgent, except that instead of generating final summary answer based on relevant extracts, it just returns those extracts. See test_retriever_agent.py for example usage.</p>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent","title":"<code>RetrieverAgent(config)</code>","text":"<p>               Bases: <code>DocChatAgent</code></p> <p>Agent for just retrieving chunks/docs/extracts matching a query</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def __init__(self, config: DocChatAgentConfig):\n    super().__init__(config)\n    self.config: DocChatAgentConfig = config\n    logger.warning(\n        \"\"\"\n    `RetrieverAgent` is deprecated. Use `DocChatAgent` instead, with\n    `DocChatAgentConfig.retrieve_only=True`, and if you want to retrieve\n    FULL relevant doc-contents rather than just extracts, then set\n    `DocChatAgentConfig.extraction_granularity=-1`\n    \"\"\"\n    )\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/","title":"table_chat_agent","text":"<p>langroid/agent/special/table_chat_agent.py </p> <p>Agent that supports asking queries about a tabular dataset, internally represented as a Pandas dataframe. The <code>TableChatAgent</code> is configured with a dataset, which can be a Pandas df, file or URL. The delimiter/separator is auto-detected. In response to a user query, the Agent's LLM generates a Pandas expression (involving a dataframe <code>df</code>) to answer the query. The expression is passed via the <code>pandas_eval</code> tool/function-call, which is handled by the Agent's <code>pandas_eval</code> method. This method evaluates the expression and returns the result as a string.</p> <p>WARNING: This Agent should be used only with trusted input, as it can execute system commands. </p> <p>The <code>full_eval</code> flag is false by default, which means that the input is sanitized against most common code injection attack vectors. <code>full_eval</code> may be set to True to  disable sanitization at all. Both cases should be used with caution.</p>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.PandasEvalTool","title":"<code>PandasEvalTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool/function to evaluate a pandas expression involving a dataframe <code>df</code></p>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.TableChatAgent","title":"<code>TableChatAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def __init__(self, config: TableChatAgentConfig):\n    if isinstance(config.data, pd.DataFrame):\n        df = config.data\n    else:\n        df = read_tabular_data(config.data, config.separator)\n\n    df.columns = df.columns.str.strip().str.replace(\" +\", \"_\", regex=True)\n\n    self.df = df\n    summary = dataframe_summary(df)\n    config.system_message = config.system_message.format(summary=summary)\n\n    super().__init__(config)\n    self.config: TableChatAgentConfig = config\n\n    logger.info(\n        f\"\"\"TableChatAgent initialized with dataframe of shape {self.df.shape}\n        and columns: \n        {self.df.columns}\n        \"\"\"\n    )\n    # enable the agent to use and handle the PandasEvalTool\n    self.enable_message(PandasEvalTool)\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.TableChatAgent.pandas_eval","title":"<code>pandas_eval(msg)</code>","text":"<p>Handle a PandasEvalTool message by evaluating the <code>expression</code> field     and returning the result. Args:     msg (PandasEvalTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of running the code along with any print output.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def pandas_eval(self, msg: PandasEvalTool) -&gt; str:\n    \"\"\"\n    Handle a PandasEvalTool message by evaluating the `expression` field\n        and returning the result.\n    Args:\n        msg (PandasEvalTool): The tool-message to handle.\n\n    Returns:\n        str: The result of running the code along with any print output.\n    \"\"\"\n    self.sent_expression = True\n    exprn = msg.expression\n    vars = {\"df\": self.df}\n    # Create a string-based I/O stream\n    code_out = io.StringIO()\n\n    # Temporarily redirect standard output to our string-based I/O stream\n    sys.stdout = code_out\n\n    # Evaluate the last line and get the result;\n    # SECURITY MITIGATION: Eval input is sanitized by default to prevent most\n    # common code injection attack vectors.\n    try:\n        if not self.config.full_eval:\n            exprn = sanitize_command(exprn)\n        code = compile(exprn, \"&lt;calc&gt;\", \"eval\")\n        eval_result = eval(code, vars, {})\n    except Exception as e:\n        eval_result = f\"ERROR: {type(e)}: {e}\"\n\n    if eval_result is None:\n        eval_result = \"\"\n\n    # Always restore the original standard output\n    sys.stdout = sys.__stdout__\n\n    # If df has been modified in-place, save the changes back to self.df\n    self.df = vars[\"df\"]\n\n    # Get the resulting string from the I/O stream\n    print_result = code_out.getvalue() or \"\"\n    sep = \"\\n\" if print_result else \"\"\n    # Combine the print and eval results\n    result = f\"{print_result}{sep}{eval_result}\"\n    if result == \"\":\n        result = \"No result\"\n    # Return the result\n    return result\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.TableChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Handle various LLM deviations</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"Handle various LLM deviations\"\"\"\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == lr.Entity.LLM:\n        if msg.content.strip() == DONE and self.sent_expression:\n            # LLM sent an expression (i.e. used the `pandas_eval` tool)\n            # but upon receiving the results, simply said DONE without\n            # narrating the result as instructed.\n            return \"\"\"\n                You forgot to PRESENT the answer to the user's query\n                based on the results from `pandas_eval` tool.\n            \"\"\"\n        if self.sent_expression:\n            # LLM forgot to say DONE\n            self.sent_expression = False\n            return DONE + \" \" + PASS\n        else:\n            # LLM forgot to use the `pandas_eval` tool\n            return \"\"\"\n                You forgot to use the `pandas_eval` tool/function \n                to find the answer.\n                Try again using the `pandas_eval` tool/function.\n                \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/table_chat_agent/#langroid.agent.special.table_chat_agent.dataframe_summary","title":"<code>dataframe_summary(df)</code>","text":"<p>Generate a structured summary for a pandas DataFrame containing numerical and categorical values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to summarize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A nicely structured and formatted summary string.</p> Source code in <code>langroid/agent/special/table_chat_agent.py</code> <pre><code>@no_type_check\ndef dataframe_summary(df: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Generate a structured summary for a pandas DataFrame containing numerical\n    and categorical values.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to summarize.\n\n    Returns:\n        str: A nicely structured and formatted summary string.\n    \"\"\"\n\n    # Column names display\n    col_names_str = (\n        \"COLUMN NAMES:\\n\" + \" \".join([f\"'{col}'\" for col in df.columns]) + \"\\n\\n\"\n    )\n\n    # Numerical data summary\n    num_summary = df.describe().map(lambda x: \"{:.2f}\".format(x))\n    num_str = \"Numerical Column Summary:\\n\" + num_summary.to_string() + \"\\n\\n\"\n\n    # Categorical data summary\n    cat_columns = df.select_dtypes(include=[np.object_]).columns\n    cat_summary_list = []\n\n    for col in cat_columns:\n        unique_values = df[col].unique()\n        if len(unique_values) &lt; 10:\n            cat_summary_list.append(f\"'{col}': {', '.join(map(str, unique_values))}\")\n        else:\n            cat_summary_list.append(f\"'{col}': {df[col].nunique()} unique values\")\n\n    cat_str = \"Categorical Column Summary:\\n\" + \"\\n\".join(cat_summary_list) + \"\\n\\n\"\n\n    # Missing values summary\n    nan_summary = df.isnull().sum().rename(\"missing_values\").to_frame()\n    nan_str = \"Missing Values Column Summary:\\n\" + nan_summary.to_string() + \"\\n\"\n\n    # Combine the summaries into one structured string\n    summary_str = col_names_str + num_str + cat_str + nan_str\n\n    return summary_str\n</code></pre>"},{"location":"reference/agent/special/arangodb/","title":"arangodb","text":"<p>langroid/agent/special/arangodb/init.py </p>"},{"location":"reference/agent/special/arangodb/arangodb_agent/","title":"arangodb_agent","text":"<p>langroid/agent/special/arangodb/arangodb_agent.py </p>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent","title":"<code>ArangoChatAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def __init__(self, config: ArangoChatAgentConfig):\n    super().__init__(config)\n    self.config: ArangoChatAgentConfig = config\n    self.init_state()\n    self._validate_config()\n    self._import_arango()\n    self._initialize_db()\n    self._init_tools_sys_message()\n</code></pre>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent.with_retry","title":"<code>with_retry(func, max_retries=3, delay=1.0)</code>","text":"<p>Execute a function with retries on connection error</p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def with_retry(\n    self, func: Callable[[], T], max_retries: int = 3, delay: float = 1.0\n) -&gt; T:\n    \"\"\"Execute a function with retries on connection error\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except ArangoError:\n            if attempt == max_retries - 1:\n                raise\n            logger.warning(\n                f\"Connection failed (attempt {attempt + 1}/{max_retries}). \"\n                f\"Retrying in {delay} seconds...\"\n            )\n            time.sleep(delay)\n            # Reconnect if needed\n            self._initialize_db()\n    return func()  # Final attempt after loop if not raised\n</code></pre>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent.read_query","title":"<code>read_query(query, bind_vars=None)</code>","text":"<p>Execute a read query with connection retry.</p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def read_query(\n    self, query: str, bind_vars: Optional[Dict[Any, Any]] = None\n) -&gt; QueryResult:\n    \"\"\"Execute a read query with connection retry.\"\"\"\n    if not self.db:\n        return QueryResult(\n            success=False, data=\"No database connection is established.\"\n        )\n\n    def execute_read() -&gt; QueryResult:\n        try:\n            cursor = self.db.aql.execute(query, bind_vars=bind_vars)\n            records = [doc for doc in cursor]  # type: ignore\n            records = records[: self.config.max_num_results]\n            logger.warning(f\"Records retrieved: {records}\")\n            return QueryResult(success=True, data=records if records else [])\n        except Exception as e:\n            if isinstance(e, ServerConnectionError):\n                raise\n            logger.error(f\"Failed to execute query: {query}\\n{e}\")\n            error_message = self.retry_query(e, query)\n            return QueryResult(success=False, data=error_message)\n\n    try:\n        return self.with_retry(execute_read)  # type: ignore\n    except Exception as e:\n        return QueryResult(\n            success=False, data=f\"Failed after max retries: {str(e)}\"\n        )\n</code></pre>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent.write_query","title":"<code>write_query(query, bind_vars=None)</code>","text":"<p>Execute a write query with connection retry.</p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def write_query(\n    self, query: str, bind_vars: Optional[Dict[Any, Any]] = None\n) -&gt; QueryResult:\n    \"\"\"Execute a write query with connection retry.\"\"\"\n    if not self.db:\n        return QueryResult(\n            success=False, data=\"No database connection is established.\"\n        )\n\n    def execute_write() -&gt; QueryResult:\n        try:\n            self.db.aql.execute(query, bind_vars=bind_vars)\n            return QueryResult(success=True)\n        except Exception as e:\n            if isinstance(e, ServerConnectionError):\n                raise\n            logger.error(f\"Failed to execute query: {query}\\n{e}\")\n            error_message = self.retry_query(e, query)\n            return QueryResult(success=False, data=error_message)\n\n    try:\n        return self.with_retry(execute_write)  # type: ignore\n    except Exception as e:\n        return QueryResult(\n            success=False, data=f\"Failed after max retries: {str(e)}\"\n        )\n</code></pre>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent.aql_retrieval_tool","title":"<code>aql_retrieval_tool(msg)</code>","text":"<p>Handle AQL query for data retrieval</p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def aql_retrieval_tool(self, msg: AQLRetrievalTool) -&gt; str:\n    \"\"\"Handle AQL query for data retrieval\"\"\"\n    if not self.tried_schema:\n        return f\"\"\"\n        You need to use `{arango_schema_tool_name}` first to get the \n        database schema before using `{aql_retrieval_tool_name}`. This ensures\n        you know the correct collection names and edge definitions.\n        \"\"\"\n    elif not self.config.database_created:\n        return \"\"\"\n        You need to create the database first using `{aql_creation_tool_name}`.\n        \"\"\"\n    self.num_tries += 1\n    query = msg.aql_query\n    if query == self.current_retrieval_aql_query:\n        return \"\"\"\n        You have already tried this query, so you will get the same results again!\n        If you need to retry, please MODIFY the query to get different results.\n        \"\"\"\n    self.current_retrieval_aql_query = query\n    logger.info(f\"Executing AQL query: {query}\")\n    response = self.read_query(query)\n\n    if isinstance(response.data, list) and len(response.data) == 0:\n        return \"\"\"\n        No results found. Check if your collection names are correct - \n        they are case-sensitive. Use exact names from the schema.\n        Try modifying your query based on the RETRY-SUGGESTIONS \n        in your instructions.\n        \"\"\"\n    return str(response.data)\n</code></pre>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent.aql_creation_tool","title":"<code>aql_creation_tool(msg)</code>","text":"<p>Handle AQL query for creating data</p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def aql_creation_tool(self, msg: AQLCreationTool) -&gt; str:\n    \"\"\"Handle AQL query for creating data\"\"\"\n    self.num_tries += 1\n    query = msg.aql_query\n    logger.info(f\"Executing AQL query: {query}\")\n    response = self.write_query(query)\n\n    if response.success:\n        self.config.database_created = True\n        return \"AQL query executed successfully\"\n    return str(response.data)\n</code></pre>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent.arango_schema_tool","title":"<code>arango_schema_tool(msg)</code>","text":"<p>Get database schema. If collections=None, include all collections. If properties=False, show only connection info, else show all properties and example-docs.</p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def arango_schema_tool(\n    self,\n    msg: ArangoSchemaTool | None,\n) -&gt; Dict[str, List[Dict[str, Any]]] | str:\n    \"\"\"Get database schema. If collections=None, include all collections.\n    If properties=False, show only connection info,\n    else show all properties and example-docs.\n    \"\"\"\n\n    if (\n        msg is not None\n        and msg.collections == self.current_schema_params.collections\n        and msg.properties == self.current_schema_params.properties\n    ):\n        return \"\"\"\n        You have already tried this schema TOOL, so you will get the same results \n        again! Please MODIFY the tool params `collections` or `properties` to get\n        different results.\n        \"\"\"\n\n    if msg is not None:\n        collections = msg.collections\n        properties = msg.properties\n    else:\n        collections = None\n        properties = True\n    self.tried_schema = True\n    if (\n        self.config.kg_schema is not None\n        and len(self.config.kg_schema) &gt; 0\n        and msg is None\n    ):\n        # we are trying to pre-populate full schema before the agent runs,\n        # so get it if it's already available\n        # (Note of course that this \"full schema\" may actually be incomplete)\n        return self.config.kg_schema\n\n    # increment tries only if the LLM is asking for the schema,\n    # in which case msg will not be None\n    self.num_tries += msg is not None\n\n    try:\n        # Get graph schemas (keeping full graph info)\n        graph_schema = [\n            {\"graph_name\": g[\"name\"], \"edge_definitions\": g[\"edge_definitions\"]}\n            for g in self.db.graphs()  # type: ignore\n        ]\n\n        # Get collection schemas\n        collection_schema = []\n        for collection in self.db.collections():  # type: ignore\n            if collection[\"name\"].startswith(\"_\"):\n                continue\n\n            col_name = collection[\"name\"]\n            if collections and col_name not in collections:\n                continue\n\n            col_type = collection[\"type\"]\n            col_size = self.db.collection(col_name).count()\n\n            if col_size == 0:\n                continue\n\n            if properties:\n                # Full property collection with sampling\n                lim = self.config.schema_sample_pct * col_size  # type: ignore\n                limit_amount = ceil(lim / 100.0) or 1\n                sample_query = f\"\"\"\n                    FOR doc in {col_name}\n                    LIMIT {limit_amount}\n                    RETURN doc\n                \"\"\"\n\n                properties_list = []\n                example_doc = None\n\n                def simplify_doc(doc: Any) -&gt; Any:\n                    if isinstance(doc, list) and len(doc) &gt; 0:\n                        return [simplify_doc(doc[0])]\n                    if isinstance(doc, dict):\n                        return {k: simplify_doc(v) for k, v in doc.items()}\n                    return doc\n\n                for doc in self.db.aql.execute(sample_query):  # type: ignore\n                    if example_doc is None:\n                        example_doc = simplify_doc(doc)\n                    for key, value in doc.items():\n                        prop = {\"name\": key, \"type\": type(value).__name__}\n                        if prop not in properties_list:\n                            properties_list.append(prop)\n\n                collection_schema.append(\n                    {\n                        \"collection_name\": col_name,\n                        \"collection_type\": col_type,\n                        f\"{col_type}_properties\": properties_list,\n                        f\"example_{col_type}\": example_doc,\n                    }\n                )\n            else:\n                # Basic info + from/to for edges only\n                collection_info = {\n                    \"collection_name\": col_name,\n                    \"collection_type\": col_type,\n                }\n                if col_type == \"edge\":\n                    # Get a sample edge to extract from/to fields\n                    sample_edge = next(\n                        self.db.aql.execute(  # type: ignore\n                            f\"FOR e IN {col_name} LIMIT 1 RETURN e\"\n                        ),\n                        None,\n                    )\n                    if sample_edge:\n                        collection_info[\"from_collection\"] = sample_edge[\n                            \"_from\"\n                        ].split(\"/\")[0]\n                        collection_info[\"to_collection\"] = sample_edge[\"_to\"].split(\n                            \"/\"\n                        )[0]\n\n                collection_schema.append(collection_info)\n\n        schema = {\n            \"Graph Schema\": graph_schema,\n            \"Collection Schema\": collection_schema,\n        }\n        schema_str = json.dumps(schema, indent=2)\n        logger.warning(f\"Schema retrieved:\\n{schema_str}\")\n        with open(\"logs/arango-schema.json\", \"w\") as f:\n            f.write(schema_str)\n        if (n_fields := count_fields(schema)) &gt; self.config.max_schema_fields:\n            logger.warning(\n                f\"\"\"\n                Schema has {n_fields} fields, which exceeds the maximum of\n                {self.config.max_schema_fields}. Showing a trimmed version\n                that only includes edge info and no other properties.\n                \"\"\"\n            )\n            schema = trim_schema(schema)\n            n_fields = count_fields(schema)\n            logger.warning(f\"Schema trimmed down to {n_fields} fields.\")\n            schema_str = (\n                json.dumps(schema)\n                + \"\\n\"\n                + f\"\"\"\n\n                CAUTION: The requested schema was too large, so \n                the schema has been trimmed down to show only all collection names,\n                their types, \n                and edge relationships (from/to collections) without any properties.\n                To find out more about the schema, you can EITHER:\n                - Use the `{arango_schema_tool_name}` tool again with the \n                  `properties` arg set to True, and `collections` arg set to\n                    specific collections you want to know more about, OR\n                - Use the `{aql_retrieval_tool_name}` tool to learn more about\n                  the schema by querying the database.\n\n                \"\"\"\n            )\n            if msg is None:\n                self.config.kg_schema = schema_str\n            return schema_str\n        self.config.kg_schema = schema\n        return schema\n\n    except Exception as e:\n        logger.error(f\"Schema retrieval failed: {str(e)}\")\n        return f\"Failed to retrieve schema: {str(e)}\"\n</code></pre>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>When LLM sends a no-tool msg, assume user is the intended recipient, and if in interactive mode, forward the msg to the user.</p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ForwardTool | None:\n    \"\"\"When LLM sends a no-tool msg, assume user is the intended recipient,\n    and if in interactive mode, forward the msg to the user.\n    \"\"\"\n    done_tool_name = DoneTool.default_value(\"request\")\n    forward_tool_name = ForwardTool.default_value(\"request\")\n    aql_retrieval_tool_instructions = AQLRetrievalTool.instructions()\n    # TODO the aql_retrieval_tool_instructions may be empty/minimal\n    # when using self.config.use_functions_api = True.\n    tools_instruction = f\"\"\"\n      For example you may want to use the TOOL\n      `{aql_retrieval_tool_name}`  according to these instructions:\n       {aql_retrieval_tool_instructions}\n    \"\"\"\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM:\n        if self.interactive:\n            return ForwardTool(agent=\"User\")\n        else:\n            if self.config.chat_mode:\n                return f\"\"\"\n                Since you did not explicitly address the User, it is not clear\n                whether:\n                - you intend this to be the final response to the \n                  user's query/request, in which case you must use the \n                  `{forward_tool_name}` to indicate this.\n                - OR, you FORGOT to use an Appropriate TOOL,\n                  in which case you should use the available tools to\n                  make progress on the user's query/request.\n                  {tools_instruction}\n                \"\"\"\n            return f\"\"\"\n            The intent of your response is not clear:\n            - if you intended this to be the FINAL answer to the user's query,\n                then use the `{done_tool_name}` to indicate so,\n                with the `content` set to the answer or result.\n            - otherwise, use one of the available tools to make progress \n                to arrive at the final answer.\n                {tools_instruction}\n            \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/arangodb/arangodb_agent/#langroid.agent.special.arangodb.arangodb_agent.ArangoChatAgent.retry_query","title":"<code>retry_query(e, query)</code>","text":"<p>Generate error message for failed AQL query</p> Source code in <code>langroid/agent/special/arangodb/arangodb_agent.py</code> <pre><code>def retry_query(self, e: Exception, query: str) -&gt; str:\n    \"\"\"Generate error message for failed AQL query\"\"\"\n    logger.error(f\"AQL Query failed: {query}\\nException: {e}\")\n\n    error_message = f\"\"\"\\\n    {ARANGO_ERROR_MSG}: '{query}'\n    {str(e)}\n    Please try again with a corrected query.\n    \"\"\"\n\n    return error_message\n</code></pre>"},{"location":"reference/agent/special/arangodb/system_messages/","title":"system_messages","text":"<p>langroid/agent/special/arangodb/system_messages.py </p>"},{"location":"reference/agent/special/arangodb/tools/","title":"tools","text":"<p>langroid/agent/special/arangodb/tools.py </p>"},{"location":"reference/agent/special/arangodb/tools/#langroid.agent.special.arangodb.tools.AQLRetrievalTool","title":"<code>AQLRetrievalTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/special/arangodb/tools/#langroid.agent.special.arangodb.tools.AQLRetrievalTool.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Few-shot examples to include in tool instructions.</p> Source code in <code>langroid/agent/special/arangodb/tools.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[ToolMessage | Tuple[str, ToolMessage]]:\n    \"\"\"Few-shot examples to include in tool instructions.\"\"\"\n    return [\n        (\n            \"I want to see who Bob's Father is\",\n            cls(\n                aql_query=\"\"\"\n                FOR v, e, p IN 1..1 OUTBOUND 'users/Bob' GRAPH 'family_tree'\n                FILTER p.edges[0].type == 'father'\n                RETURN v\n                \"\"\"\n            ),\n        ),\n        (\n            \"I want to know the properties of the Actor node\",\n            cls(\n                aql_query=\"\"\"\n                FOR doc IN Actor\n                LIMIT 1\n                RETURN ATTRIBUTES(doc)                    \n                \"\"\"\n            ),\n        ),\n    ]\n</code></pre>"},{"location":"reference/agent/special/arangodb/tools/#langroid.agent.special.arangodb.tools.AQLCreationTool","title":"<code>AQLCreationTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/special/arangodb/tools/#langroid.agent.special.arangodb.tools.AQLCreationTool.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Few-shot examples to include in tool instructions.</p> Source code in <code>langroid/agent/special/arangodb/tools.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[ToolMessage | Tuple[str, ToolMessage]]:\n    \"\"\"Few-shot examples to include in tool instructions.\"\"\"\n    return [\n        (\n            \"Create a new document in the collection 'users'\",\n            cls(\n                aql_query=\"\"\"\n                INSERT {\n                  \"name\": \"Alice\",\n                  \"age\": 30\n                } INTO users\n                \"\"\"\n            ),\n        ),\n    ]\n</code></pre>"},{"location":"reference/agent/special/arangodb/utils/","title":"utils","text":"<p>langroid/agent/special/arangodb/utils.py </p>"},{"location":"reference/agent/special/arangodb/utils/#langroid.agent.special.arangodb.utils.trim_schema","title":"<code>trim_schema(schema)</code>","text":"<p>Keep only edge connection info, remove properties and examples</p> Source code in <code>langroid/agent/special/arangodb/utils.py</code> <pre><code>def trim_schema(\n    schema: Dict[str, List[Dict[str, Any]]]\n) -&gt; Dict[str, List[Dict[str, Any]]]:\n    \"\"\"Keep only edge connection info, remove properties and examples\"\"\"\n    trimmed: Dict[str, List[Dict[str, Any]]] = {\n        \"Graph Schema\": schema[\"Graph Schema\"],\n        \"Collection Schema\": [],\n    }\n    for coll in schema[\"Collection Schema\"]:\n        col_info: Dict[str, Any] = {\n            \"collection_name\": coll[\"collection_name\"],\n            \"collection_type\": coll[\"collection_type\"],\n        }\n        if coll[\"collection_type\"] == \"edge\":\n            # preserve from/to info if present\n            if f\"example_{coll['collection_type']}\" in coll:\n                example = coll[f\"example_{coll['collection_type']}\"]\n                if example and \"_from\" in example:\n                    col_info[\"from_collection\"] = example[\"_from\"].split(\"/\")[0]\n                    col_info[\"to_collection\"] = example[\"_to\"].split(\"/\")[0]\n        trimmed[\"Collection Schema\"].append(col_info)\n    return trimmed\n</code></pre>"},{"location":"reference/agent/special/lance_rag/","title":"lance_rag","text":"<p>langroid/agent/special/lance_rag/init.py </p>"},{"location":"reference/agent/special/lance_rag/critic_agent/","title":"critic_agent","text":"<p>langroid/agent/special/lance_rag/critic_agent.py </p> <p>QueryPlanCritic is a ChatAgent that is created with a specific document schema.</p> <p>Its role is to provide feedback on a Query Plan, which consists of: - filter condition if needed (or empty string if no filter is needed) - query - a possibly rephrased query that can be used to match the <code>content</code> field - dataframe_calc - a Pandas-dataframe calculation/aggregation string, possibly empty - original_query - the original query for reference - result - the answer received from an assistant that used this QUERY PLAN.</p> <p>This agent has access to two tools: - QueryPlanTool: The handler method for this tool re-writes the query plan   in plain text (non-JSON) so the LLM can provide its feedback using the   QueryPlanFeedbackTool. - QueryPlanFeedbackTool: LLM uses this tool to provide feedback on the Query Plan</p>"},{"location":"reference/agent/special/lance_rag/critic_agent/#langroid.agent.special.lance_rag.critic_agent.QueryPlanCritic","title":"<code>QueryPlanCritic(cfg)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Critic for LanceQueryPlanAgent, provides feedback on query plan + answer.</p> Source code in <code>langroid/agent/special/lance_rag/critic_agent.py</code> <pre><code>def __init__(self, cfg: LanceQueryPlanAgentConfig):\n    super().__init__(cfg)\n    self.config = cfg\n    self.enable_message(QueryPlanAnswerTool, use=False, handle=True)\n    self.enable_message(QueryPlanFeedbackTool, use=True, handle=True)\n    self.enable_message(AgentDoneTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/special/lance_rag/critic_agent/#langroid.agent.special.lance_rag.critic_agent.QueryPlanCritic.query_plan_answer","title":"<code>query_plan_answer(msg)</code>","text":"<p>Present query plan + answer in plain text (not JSON) so LLM can give feedback</p> Source code in <code>langroid/agent/special/lance_rag/critic_agent.py</code> <pre><code>def query_plan_answer(self, msg: QueryPlanAnswerTool) -&gt; str:\n    \"\"\"Present query plan + answer in plain text (not JSON)\n    so LLM can give feedback\"\"\"\n    self.expecting_feedback_tool = True\n    return plain_text_query_plan(msg)\n</code></pre>"},{"location":"reference/agent/special/lance_rag/critic_agent/#langroid.agent.special.lance_rag.critic_agent.QueryPlanCritic.query_plan_feedback","title":"<code>query_plan_feedback(msg)</code>","text":"<p>Format Valid so return to Query Planner</p> Source code in <code>langroid/agent/special/lance_rag/critic_agent.py</code> <pre><code>def query_plan_feedback(self, msg: QueryPlanFeedbackTool) -&gt; AgentDoneTool:\n    \"\"\"Format Valid so return to Query Planner\"\"\"\n    self.expecting_feedback_tool = False\n    # indicate this task is Done, and return the tool as result\n    return AgentDoneTool(tools=[msg])\n</code></pre>"},{"location":"reference/agent/special/lance_rag/critic_agent/#langroid.agent.special.lance_rag.critic_agent.QueryPlanCritic.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Remind the LLM to use QueryPlanFeedbackTool since it forgot</p> Source code in <code>langroid/agent/special/lance_rag/critic_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"Remind the LLM to use QueryPlanFeedbackTool since it forgot\"\"\"\n    if self.expecting_feedback_tool:\n        return \"\"\"\n        You forgot to use the `query_plan_feedback` tool/function.\n        Re-try your response using the `query_plan_feedback` tool/function,\n        remember to provide feedback in the `feedback` field,\n        and if any fix is suggested, provide it in the `suggested_fix` field.\n        \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/lance_rag/lance_rag_task/","title":"lance_rag_task","text":"<p>langroid/agent/special/lance_rag/lance_rag_task.py </p> <p>The LanceRAGTaskCreator.new() method creates a 3-Agent system that uses this agent. It takes a LanceDocChatAgent instance as argument, and adds two more agents: - LanceQueryPlanAgent, which is given the LanceDB schema in LanceDocChatAgent, and based on this schema, for a given user query, creates a Query Plan using the QueryPlanTool, which contains a filter, a rephrased query, and a dataframe_calc. - QueryPlanCritic, which is given the LanceDB schema in LanceDocChatAgent,  and gives feedback on the Query Plan and Result using the QueryPlanFeedbackTool.</p> <p>The LanceRAGTaskCreator.new() method sets up the given LanceDocChatAgent and QueryPlanCritic as sub-tasks of the LanceQueryPlanAgent's task.</p> <p>Langroid's built-in task orchestration ensures that: - the LanceQueryPlanAgent reformulates the plan based     on the QueryPlanCritics's feedback, - LLM deviations are corrected via tools and overrides of ChatAgent methods.</p>"},{"location":"reference/agent/special/lance_rag/lance_rag_task/#langroid.agent.special.lance_rag.lance_rag_task.LanceRAGTaskCreator","title":"<code>LanceRAGTaskCreator</code>","text":""},{"location":"reference/agent/special/lance_rag/lance_rag_task/#langroid.agent.special.lance_rag.lance_rag_task.LanceRAGTaskCreator.new","title":"<code>new(agent, interactive=True)</code>  <code>staticmethod</code>","text":"<p>Add a LanceFilterAgent to the LanceDocChatAgent, set up the corresponding Tasks, connect them, and return the top-level query_plan_task.</p> Source code in <code>langroid/agent/special/lance_rag/lance_rag_task.py</code> <pre><code>@staticmethod\ndef new(\n    agent: LanceDocChatAgent,\n    interactive: bool = True,\n) -&gt; Task:\n    \"\"\"\n    Add a LanceFilterAgent to the LanceDocChatAgent,\n    set up the corresponding Tasks, connect them,\n    and return the top-level query_plan_task.\n    \"\"\"\n    doc_agent_name = \"LanceRAG\"\n    critic_name = \"QueryPlanCritic\"\n    query_plan_agent_config = LanceQueryPlanAgentConfig(\n        critic_name=critic_name,\n        doc_agent_name=doc_agent_name,\n        doc_schema=agent._get_clean_vecdb_schema(),\n        llm=agent.config.llm,\n    )\n    query_plan_agent_config.set_system_message()\n\n    critic_config = QueryPlanCriticConfig(\n        doc_schema=agent._get_clean_vecdb_schema(),\n        llm=agent.config.llm,\n    )\n    critic_config.set_system_message()\n\n    query_planner = LanceQueryPlanAgent(query_plan_agent_config)\n    query_plan_task = Task(\n        query_planner,\n        interactive=interactive,\n    )\n    critic_agent = QueryPlanCritic(critic_config)\n    critic_task = Task(\n        critic_agent,\n        interactive=False,\n    )\n    rag_task = Task(\n        agent,\n        name=\"LanceRAG\",\n        interactive=False,\n        done_if_response=[Entity.LLM],  # done when non-null response from LLM\n        done_if_no_response=[Entity.LLM],  # done when null response from LLM\n    )\n    query_plan_task.add_sub_task([critic_task, rag_task])\n    return query_plan_task\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/","title":"query_planner_agent","text":"<p>langroid/agent/special/lance_rag/query_planner_agent.py </p> <p>LanceQueryPlanAgent is a ChatAgent created with a specific document schema. Given a QUERY, the LLM constructs a Query Plan consisting of: - filter condition if needed (or empty string if no filter is needed) - query - a possibly rephrased query that can be used to match the <code>content</code> field - dataframe_calc - a Pandas-dataframe calculation/aggregation string, possibly empty - original_query - the original query for reference</p> <p>This agent has access to two tools: - QueryPlanTool, which is used to generate the Query Plan, and the handler of     this tool simply passes it on to the RAG agent named in config.doc_agent_name. - QueryPlanFeedbackTool, which is used to handle feedback on the Query Plan and   Result from the RAG agent. The QueryPlanFeedbackTool is used by   the QueryPlanCritic, who inserts feedback into the <code>feedback</code> field</p>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent","title":"<code>LanceQueryPlanAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def __init__(self, config: LanceQueryPlanAgentConfig):\n    super().__init__(config)\n    self.config: LanceQueryPlanAgentConfig = config\n    # This agent should generate the QueryPlanTool\n    # as well as handle it for validation\n    self.enable_message(QueryPlanTool, use=True, handle=True)\n    self.enable_message(QueryPlanFeedbackTool, use=False, handle=True)\n    self.enable_message(AnswerTool, use=False, handle=True)\n    # neither use nor handle! Added to \"known\" tools so that the Planner agent\n    # can avoid processing it\n    self.enable_message(QueryPlanAnswerTool, use=False, handle=False)\n    # LLM will not use this, so set use=False (Agent generates it)\n    self.enable_message(AgentDoneTool, use=False, handle=True)\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent.query_plan","title":"<code>query_plan(msg)</code>","text":"<p>Valid, tool msg, forward chat_doc to RAG Agent. Note this chat_doc will already have the QueryPlanTool in its tool_messages list. We just update the recipient to the doc_agent_name.</p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def query_plan(self, msg: QueryPlanTool) -&gt; ForwardTool | str:\n    \"\"\"Valid, tool msg, forward chat_doc to RAG Agent.\n    Note this chat_doc will already have the\n    QueryPlanTool in its tool_messages list.\n    We just update the recipient to the doc_agent_name.\n    \"\"\"\n    # save, to be used to assemble QueryPlanResultTool\n    if len(msg.plan.dataframe_calc.split(\"\\n\")) &gt; 1:\n        return \"DATAFRAME CALCULATION must be a SINGLE LINE; Retry the `query_plan`\"\n    self.curr_query_plan = msg.plan\n    self.expecting_query_plan = False\n\n    # To forward the QueryPlanTool to doc_agent, we could either:\n\n    # (a) insert `recipient` in the QueryPlanTool:\n    # QPWithRecipient = QueryPlanTool.require_recipient()\n    # qp = QPWithRecipient(**msg.model_dump(), recipient=self.config.doc_agent_name)\n    # return qp\n    #\n    # OR\n    #\n    # (b) create an agent response with recipient and tool_messages.\n    # response = self.create_agent_response(\n    #     recipient=self.config.doc_agent_name, tool_messages=[msg]\n    # )\n    # return response\n\n    # OR\n    # (c) use the ForwardTool:\n    return ForwardTool(agent=self.config.doc_agent_name)\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent.query_plan_feedback","title":"<code>query_plan_feedback(msg)</code>","text":"<p>Process Critic feedback on QueryPlan + Answer from RAG Agent</p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def query_plan_feedback(self, msg: QueryPlanFeedbackTool) -&gt; str | AgentDoneTool:\n    \"\"\"Process Critic feedback on QueryPlan + Answer from RAG Agent\"\"\"\n    # We should have saved answer in self.result by this time,\n    # since this Agent seeks feedback only after receiving RAG answer.\n    if (\n        msg.suggested_fix == \"\"\n        and NO_ANSWER not in self.result\n        and self.result != \"\"\n    ):\n        # This means the result is good AND Query Plan is fine,\n        # as judged by Critic\n        # (Note sometimes critic may have empty suggested_fix even when\n        # the result is NO_ANSWER)\n        self.n_retries = 0  # good answer, so reset this\n        return AgentDoneTool(content=self.result)\n    self.n_retries += 1\n    if self.n_retries &gt;= self.config.max_retries:\n        # bail out to avoid infinite loop\n        self.n_retries = 0\n        return AgentDoneTool(content=NO_ANSWER)\n\n    # there is a suggested_fix, OR the result is empty or NO_ANSWER\n    if self.result == \"\" or NO_ANSWER in self.result:\n        # if result is empty or NO_ANSWER, we should retry the query plan\n        feedback = \"\"\"\n        There was no answer, which might mean there is a problem in your query.\n        \"\"\"\n        suggested = \"Retry the `query_plan` to try to get a non-null answer\"\n    else:\n        feedback = msg.feedback\n        suggested = msg.suggested_fix\n\n    self.expecting_query_plan = True\n\n    return f\"\"\"\n    here is FEEDBACK about your QUERY PLAN, and a SUGGESTED FIX.\n    Modify the QUERY PLAN if needed:\n    ANSWER: {self.result}\n    FEEDBACK: {feedback}\n    SUGGESTED FIX: {suggested}\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent.answer_tool","title":"<code>answer_tool(msg)</code>","text":"<p>Handle AnswerTool received from LanceRagAgent: Construct a QueryPlanAnswerTool with the answer</p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def answer_tool(self, msg: AnswerTool) -&gt; QueryPlanAnswerTool:\n    \"\"\"Handle AnswerTool received from LanceRagAgent:\n    Construct a QueryPlanAnswerTool with the answer\"\"\"\n    self.result = msg.answer  # save answer to interpret feedback later\n    assert self.curr_query_plan is not None\n    query_plan_answer_tool = QueryPlanAnswerTool(\n        plan=self.curr_query_plan,\n        answer=msg.answer,\n    )\n    self.curr_query_plan = None  # reset\n    return query_plan_answer_tool\n</code></pre>"},{"location":"reference/agent/special/lance_rag/query_planner_agent/#langroid.agent.special.lance_rag.query_planner_agent.LanceQueryPlanAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Remind to use QueryPlanTool if we are expecting it.</p> Source code in <code>langroid/agent/special/lance_rag/query_planner_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Remind to use QueryPlanTool if we are expecting it.\n    \"\"\"\n    if self.expecting_query_plan and self.n_query_plan_reminders &lt; 5:\n        self.n_query_plan_reminders += 1\n        return \"\"\"\n        You FORGOT to use the `query_plan` tool/function, \n        OR you had a WRONG JSON SYNTAX when trying to use it.\n        Re-try your response using the `query_plan` tool/function CORRECTLY.\n        \"\"\"\n    self.n_query_plan_reminders = 0  # reset\n    return None\n</code></pre>"},{"location":"reference/agent/special/neo4j/","title":"neo4j","text":"<p>langroid/agent/special/neo4j/init.py </p>"},{"location":"reference/agent/special/neo4j/csv_kg_chat/","title":"csv_kg_chat","text":"<p>langroid/agent/special/neo4j/csv_kg_chat.py </p>"},{"location":"reference/agent/special/neo4j/csv_kg_chat/#langroid.agent.special.neo4j.csv_kg_chat.CSVGraphAgent","title":"<code>CSVGraphAgent(config)</code>","text":"<p>               Bases: <code>Neo4jChatAgent</code></p> Source code in <code>langroid/agent/special/neo4j/csv_kg_chat.py</code> <pre><code>def __init__(self, config: CSVGraphAgentConfig):\n    formatted_build_instr = \"\"\n    if isinstance(config.data, pd.DataFrame):\n        df = config.data\n        self.df = df\n    else:\n        if config.data:\n            df = read_tabular_data(config.data, config.separator)\n            df_cleaned = _preprocess_dataframe_for_neo4j(df)\n\n            df_cleaned.columns = df_cleaned.columns.str.strip().str.replace(\n                \" +\", \"_\", regex=True\n            )\n\n            self.df = df_cleaned\n\n            formatted_build_instr = BUILD_KG_INSTRUCTIONS.format(\n                header=self.df.columns, sample_rows=self.df.head(3)\n            )\n\n    config.system_message = config.system_message + formatted_build_instr\n    super().__init__(config)\n\n    self.config: Neo4jChatAgentConfig = config\n\n    self.enable_message(PandasToKGTool)\n</code></pre>"},{"location":"reference/agent/special/neo4j/csv_kg_chat/#langroid.agent.special.neo4j.csv_kg_chat.CSVGraphAgent.pandas_to_kg","title":"<code>pandas_to_kg(msg)</code>","text":"<p>Creates nodes and relationships in the graph database based on the data in a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>PandasToKGTool</code> <p>An instance of the PandasToKGTool class containing the necessary information for generating nodes.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string indicating the success or failure of the operation.</p> Source code in <code>langroid/agent/special/neo4j/csv_kg_chat.py</code> <pre><code>def pandas_to_kg(self, msg: PandasToKGTool) -&gt; str:\n    \"\"\"\n    Creates nodes and relationships in the graph database based on the data in\n    a CSV file.\n\n    Args:\n        msg (PandasToKGTool): An instance of the PandasToKGTool class containing\n            the necessary information for generating nodes.\n\n    Returns:\n        str: A string indicating the success or failure of the operation.\n    \"\"\"\n    with status(\"[cyan]Generating graph database...\"):\n        if self.df is not None and hasattr(self.df, \"iterrows\"):\n            for counter, (index, row) in enumerate(self.df.iterrows()):\n                row_dict = row.to_dict()\n                response = self.write_query(\n                    msg.cypherQuery,\n                    parameters={header: row_dict[header] for header in msg.args},\n                )\n                # there is a possibility the generated cypher query is not correct\n                # so we need to check the response before continuing to the\n                # iteration\n                if counter == 0 and not response.success:\n                    return str(response.data)\n        return \"Graph database successfully generated\"\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/","title":"neo4j_chat_agent","text":"<p>langroid/agent/special/neo4j/neo4j_chat_agent.py </p>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent","title":"<code>Neo4jChatAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If database information is not provided in the config.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def __init__(self, config: Neo4jChatAgentConfig):\n    \"\"\"Initialize the Neo4jChatAgent.\n\n    Raises:\n        ValueError: If database information is not provided in the config.\n    \"\"\"\n    self.config: Neo4jChatAgentConfig = config\n    self._validate_config()\n    self._import_neo4j()\n    self._initialize_db()\n    self._init_tools_sys_message()\n    self.init_state()\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>When LLM sends a no-tool msg, assume user is the intended recipient, and if in interactive mode, forward the msg to the user.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def handle_message_fallback(\n    self, msg: str | ChatDocument\n) -&gt; str | ForwardTool | None:\n    \"\"\"\n    When LLM sends a no-tool msg, assume user is the intended recipient,\n    and if in interactive mode, forward the msg to the user.\n    \"\"\"\n\n    done_tool_name = DoneTool.default_value(\"request\")\n    forward_tool_name = ForwardTool.default_value(\"request\")\n    if isinstance(msg, ChatDocument) and msg.metadata.sender == Entity.LLM:\n        if self.interactive:\n            return ForwardTool(agent=\"User\")\n        else:\n            if self.config.chat_mode:\n                return f\"\"\"\n                Since you did not explicitly address the User, it is not clear\n                whether:\n                - you intend this to be the final response to the \n                  user's query/request, in which case you must use the \n                  `{forward_tool_name}` to indicate this.\n                - OR, you FORGOT to use an Appropriate TOOL,\n                  in which case you should use the available tools to\n                  make progress on the user's query/request.\n                \"\"\"\n            return f\"\"\"\n            The intent of your response is not clear:\n            - if you intended this to be the final answer to the user's query,\n                then use the `{done_tool_name}` to indicate so,\n                with the `content` set to the answer or result.\n            - otherwise, use one of the available tools to make progress \n                to arrive at the final answer.\n            \"\"\"\n    return None\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.close","title":"<code>close()</code>","text":"<p>close the connection</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"close the connection\"\"\"\n    if self.driver:\n        self.driver.close()\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.retry_query","title":"<code>retry_query(e, query)</code>","text":"<p>Generate an error message for a failed Cypher query and return it.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Exception</code> <p>The exception raised during the Cypher query execution.</p> required <code>query</code> <code>str</code> <p>The Cypher query that failed.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def retry_query(self, e: Exception, query: str) -&gt; str:\n    \"\"\"\n    Generate an error message for a failed Cypher query and return it.\n\n    Args:\n        e (Exception): The exception raised during the Cypher query execution.\n        query (str): The Cypher query that failed.\n\n    Returns:\n        str: The error message.\n    \"\"\"\n    logger.error(f\"Cypher Query failed: {query}\\nException: {e}\")\n\n    # Construct the error message\n    error_message_template = f\"\"\"\\\n    {NEO4J_ERROR_MSG}: '{query}'\n    {str(e)}\n    Run a new query, correcting the errors.\n    \"\"\"\n\n    return error_message_template\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.read_query","title":"<code>read_query(query, parameters=None)</code>","text":"<p>Executes a given Cypher query with parameters on the Neo4j database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The Cypher query string to be executed.</p> required <code>parameters</code> <code>Optional[Dict[Any, Any]]</code> <p>A dictionary of parameters for                                     the query.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>QueryResult</code> <code>QueryResult</code> <p>An object representing the outcome of the query execution.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def read_query(\n    self, query: str, parameters: Optional[Dict[Any, Any]] = None\n) -&gt; QueryResult:\n    \"\"\"\n    Executes a given Cypher query with parameters on the Neo4j database.\n\n    Args:\n        query (str): The Cypher query string to be executed.\n        parameters (Optional[Dict[Any, Any]]): A dictionary of parameters for\n                                                the query.\n\n    Returns:\n        QueryResult: An object representing the outcome of the query execution.\n    \"\"\"\n    if not self.driver:\n        return QueryResult(\n            success=False, data=\"No database connection is established.\"\n        )\n\n    try:\n        assert isinstance(self.config, Neo4jChatAgentConfig)\n        with self.driver.session(\n            database=self.config.neo4j_settings.database\n        ) as session:\n            result = session.run(query, parameters)\n            if result.peek():\n                records = [record.data() for record in result]\n                return QueryResult(success=True, data=records)\n            else:\n                return QueryResult(success=True, data=[])\n    except Exception as e:\n        logger.error(f\"Failed to execute query: {query}\\n{e}\")\n        error_message = self.retry_query(e, query)\n        return QueryResult(success=False, data=error_message)\n    finally:\n        self.close()\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.write_query","title":"<code>write_query(query, parameters=None)</code>","text":"<p>Executes a write transaction using a given Cypher query on the Neo4j database. This method should be used for queries that modify the database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The Cypher query string to be executed.</p> required <code>parameters</code> <code>dict</code> <p>A dict of parameters for the Cypher query.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>QueryResult</code> <code>QueryResult</code> <p>An object representing the outcome of the query execution.          It contains a success flag and an optional error message.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def write_query(\n    self, query: str, parameters: Optional[Dict[Any, Any]] = None\n) -&gt; QueryResult:\n    \"\"\"\n    Executes a write transaction using a given Cypher query on the Neo4j database.\n    This method should be used for queries that modify the database.\n\n    Args:\n        query (str): The Cypher query string to be executed.\n        parameters (dict, optional): A dict of parameters for the Cypher query.\n\n    Returns:\n        QueryResult: An object representing the outcome of the query execution.\n                     It contains a success flag and an optional error message.\n    \"\"\"\n    # Check if query contains database/collection creation patterns\n    query_upper = query.upper()\n    is_creation_query = any(\n        [\n            \"CREATE\" in query_upper,\n            \"MERGE\" in query_upper,\n            \"CREATE CONSTRAINT\" in query_upper,\n            \"CREATE INDEX\" in query_upper,\n        ]\n    )\n\n    if is_creation_query:\n        self.config.database_created = True\n        logger.info(\"Detected database/collection creation query\")\n\n    if not self.driver:\n        return QueryResult(\n            success=False, data=\"No database connection is established.\"\n        )\n\n    try:\n        assert isinstance(self.config, Neo4jChatAgentConfig)\n        with self.driver.session(\n            database=self.config.neo4j_settings.database\n        ) as session:\n            session.write_transaction(lambda tx: tx.run(query, parameters))\n            return QueryResult(success=True)\n    except Exception as e:\n        logging.warning(f\"An error occurred: {e}\")\n        error_message = self.retry_query(e, query)\n        return QueryResult(success=False, data=error_message)\n    finally:\n        self.close()\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.remove_database","title":"<code>remove_database()</code>","text":"<p>Deletes all nodes and relationships from the current Neo4j database.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def remove_database(self) -&gt; None:\n    \"\"\"Deletes all nodes and relationships from the current Neo4j database.\"\"\"\n    delete_query = \"\"\"\n            MATCH (n)\n            DETACH DELETE n\n        \"\"\"\n    response = self.write_query(delete_query)\n\n    if response.success:\n        print(\"[green]Database is deleted!\")\n    else:\n        print(\"[red]Database is not deleted!\")\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.cypher_retrieval_tool","title":"<code>cypher_retrieval_tool(msg)</code>","text":"<p>\" Handle a CypherRetrievalTool message by executing a Cypher query and returning the result. Args:     msg (CypherRetrievalTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of executing the cypher_query.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def cypher_retrieval_tool(self, msg: CypherRetrievalTool) -&gt; str:\n    \"\"\" \"\n    Handle a CypherRetrievalTool message by executing a Cypher query and\n    returning the result.\n    Args:\n        msg (CypherRetrievalTool): The tool-message to handle.\n\n    Returns:\n        str: The result of executing the cypher_query.\n    \"\"\"\n    if not self.tried_schema:\n        return f\"\"\"\n        You did not yet use the `{graph_schema_tool_name}` tool to get the schema \n        of the neo4j knowledge-graph db. Use that tool first before using \n        the `{cypher_retrieval_tool_name}` tool, to ensure you know all the correct\n        node labels, relationship types, and property keys available in\n        the database.\n        \"\"\"\n    elif not self.config.database_created:\n        return f\"\"\"\n        You have not yet created the Neo4j database. \n        Use the `{cypher_creation_tool_name}`\n        tool to create the database first before using the \n        `{cypher_retrieval_tool_name}` tool.\n        \"\"\"\n    query = msg.cypher_query\n    self.current_retrieval_cypher_query = query\n    logger.info(f\"Executing Cypher query: {query}\")\n    response = self.read_query(query)\n    if isinstance(response.data, list) and len(response.data) == 0:\n        return \"\"\"\n        No results found; check if your query used the right label names -- \n        remember these are case sensitive, so you have to use the exact label\n        names you found in the schema. \n        Or retry using one of the  RETRY-SUGGESTIONS in your instructions. \n        \"\"\"\n    return str(response.data)\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.cypher_creation_tool","title":"<code>cypher_creation_tool(msg)</code>","text":"<p>\" Handle a CypherCreationTool message by executing a Cypher query and returning the result. Args:     msg (CypherCreationTool): The tool-message to handle.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of executing the cypher_query.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def cypher_creation_tool(self, msg: CypherCreationTool) -&gt; str:\n    \"\"\" \"\n    Handle a CypherCreationTool message by executing a Cypher query and\n    returning the result.\n    Args:\n        msg (CypherCreationTool): The tool-message to handle.\n\n    Returns:\n        str: The result of executing the cypher_query.\n    \"\"\"\n    query = msg.cypher_query\n\n    logger.info(f\"Executing Cypher query: {query}\")\n    response = self.write_query(query)\n    if response.success:\n        self.config.database_created = True\n        return \"Cypher query executed successfully\"\n    else:\n        return str(response.data)\n</code></pre>"},{"location":"reference/agent/special/neo4j/neo4j_chat_agent/#langroid.agent.special.neo4j.neo4j_chat_agent.Neo4jChatAgent.graph_schema_tool","title":"<code>graph_schema_tool(msg)</code>","text":"<p>Retrieves the schema of a Neo4j graph database.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>GraphSchemaTool</code> <p>An instance of GraphDatabaseSchema, typically</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str | Optional[Union[str, List[Dict[Any, Any]]]]</code> <p>The visual representation of the database schema as a string, or a</p> <code>str | Optional[Union[str, List[Dict[Any, Any]]]]</code> <p>message stating that the database schema is empty or not valid.</p> Source code in <code>langroid/agent/special/neo4j/neo4j_chat_agent.py</code> <pre><code>def graph_schema_tool(\n    self, msg: GraphSchemaTool | None\n) -&gt; str | Optional[Union[str, List[Dict[Any, Any]]]]:\n    \"\"\"\n    Retrieves the schema of a Neo4j graph database.\n\n    Args:\n        msg (GraphSchemaTool): An instance of GraphDatabaseSchema, typically\n        containing information or parameters needed for the database query.\n\n    Returns:\n        str: The visual representation of the database schema as a string, or a\n        message stating that the database schema is empty or not valid.\n\n    Raises:\n        This function does not explicitly raise exceptions but depends on the\n        behavior of 'self.read_query' method, which might raise exceptions related\n         to database connectivity or query execution.\n    \"\"\"\n    self.tried_schema = True\n    if self.config.kg_schema is not None and len(self.config.kg_schema) &gt; 0:\n        return self.config.kg_schema\n    schema_result = self.read_query(\"CALL db.schema.visualization()\")\n    if schema_result.success:\n        # there is a possibility that the schema is empty, which is a valid response\n        # the schema.data will be: [{\"nodes\": [], \"relationships\": []}]\n        self.config.kg_schema = schema_result.data  # type: ignore\n        return schema_result.data\n    else:\n        return f\"Failed to retrieve schema: {schema_result.data}\"\n</code></pre>"},{"location":"reference/agent/special/neo4j/system_messages/","title":"system_messages","text":"<p>langroid/agent/special/neo4j/system_messages.py </p>"},{"location":"reference/agent/special/neo4j/tools/","title":"tools","text":"<p>langroid/agent/special/neo4j/tools.py </p>"},{"location":"reference/agent/special/sql/","title":"sql","text":"<p>langroid/agent/special/sql/init.py </p>"},{"location":"reference/agent/special/sql/sql_chat_agent/","title":"sql_chat_agent","text":"<p>langroid/agent/special/sql/sql_chat_agent.py </p> <p>Agent that allows interaction with an SQL database using SQLAlchemy library.  The agent can execute SQL queries in the database and return the result. </p> <p>Functionality includes: - adding table and column context - asking a question about a SQL schema</p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgentConfig","title":"<code>SQLChatAgentConfig</code>","text":"<p>               Bases: <code>ChatAgentConfig</code></p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgentConfig.max_retained_tokens","title":"<code>max_retained_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional, but strongly recommended, context descriptions for tables, columns,  and relationships. It should be a dictionary where each key is a table name  and its value is another dictionary. </p> <p>In this inner dictionary: - The 'description' key corresponds to a string description of the table. - The 'columns' key corresponds to another dictionary where each key is a  column name and its value is a string description of that column. - The 'relationships' key corresponds to another dictionary where each key  is another table name and the value is a description of the relationship to  that table.</p> <p>If multi_schema support is enabled, the tables names in the description should be of the form 'schema_name.table_name'.</p> <p>For example: {     'table1': {         'description': 'description of table1',         'columns': {             'column1': 'description of column1 in table1',             'column2': 'description of column2 in table1'         }     },     'table2': {         'description': 'description of table2',         'columns': {             'column3': 'description of column3 in table2',             'column4': 'description of column4 in table2'         }     } }</p>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent","title":"<code>SQLChatAgent(config)</code>","text":"<p>               Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a SQL database</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If database information is not provided in the config.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def __init__(self, config: \"SQLChatAgentConfig\") -&gt; None:\n    \"\"\"Initialize the SQLChatAgent.\n\n    Raises:\n        ValueError: If database information is not provided in the config.\n    \"\"\"\n    self._validate_config(config)\n    self.config: SQLChatAgentConfig = config\n    self._init_database()\n    self._init_metadata()\n    self._init_table_metadata()\n    self.final_instructions = \"\"\n\n    # Caution - this updates the self.config.system_message!\n    self._init_system_message()\n    super().__init__(config)\n    self._init_tools()\n    if self.config.is_helper:\n        self.system_tool_format_instructions += self.final_instructions\n\n    if self.config.use_helper:\n        # helper_config.system_message is now the fully-populated sys msg of\n        # the main SQLAgent.\n        self.helper_config = self.config.model_copy()\n        self.helper_config.is_helper = True\n        self.helper_config.use_helper = False\n        self.helper_config.chat_mode = False\n        self.helper_agent = SQLHelperAgent(self.helper_config)\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.handle_message_fallback","title":"<code>handle_message_fallback(message)</code>","text":"<p>We'd end up here if the current msg has no tool. If this is from LLM, we may need to handle the scenario where it may have \"forgotten\" to generate a tool.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def handle_message_fallback(\n    self, message: str | ChatDocument\n) -&gt; str | ForwardTool | ChatDocument | None:\n    \"\"\"\n    We'd end up here if the current msg has no tool.\n    If this is from LLM, we may need to handle the scenario where\n    it may have \"forgotten\" to generate a tool.\n    \"\"\"\n    if (\n        not isinstance(message, ChatDocument)\n        or message.metadata.sender != Entity.LLM\n    ):\n        return None\n    if self.config.chat_mode:\n        # send any Non-tool msg to the user\n        return ForwardTool(agent=\"User\")\n    # Agent intent not clear =&gt; use the helper agent to\n    # do what this agent should have done, e.g. generate tool, etc.\n    # This is likelier to succeed since this agent has no \"baggage\" of\n    # prior conversation, other than the system msg, and special\n    # \"Intent-interpretation\" instructions.\n    if self._json_schema_available() and self.config.strict_recovery:\n        AnyTool = self._get_any_tool_message(optional=False)\n        self.set_output_format(\n            AnyTool,\n            force_tools=True,\n            use=True,\n            handle=True,\n            instructions=True,\n        )\n        recovery_message = self._strict_recovery_instructions(\n            AnyTool, optional=False\n        )\n        result = self.llm_response(recovery_message)\n        # remove the recovery_message (it has User role) from the chat history,\n        # else it may cause the LLM to directly use the AnyTool.\n        self.delete_last_message(role=Role.USER)  # delete last User-role msg\n        return result\n    elif self.config.use_helper:\n        response = self.helper_agent.llm_response(message)\n        tools = self.try_get_tool_messages(response)\n        if tools:\n            return response\n    # fall back on the clarification message\n    return self._clarifying_message()\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.retry_query","title":"<code>retry_query(e, query)</code>","text":"<p>Generate an error message for a failed SQL query and return it.</p> <p>Parameters: e (Exception): The exception raised during the SQL query execution. query (str): The SQL query that failed.</p> <p>Returns: str: The error message.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def retry_query(self, e: Exception, query: str) -&gt; str:\n    \"\"\"\n    Generate an error message for a failed SQL query and return it.\n\n    Parameters:\n    e (Exception): The exception raised during the SQL query execution.\n    query (str): The SQL query that failed.\n\n    Returns:\n    str: The error message.\n    \"\"\"\n    logger.error(f\"SQL Query failed: {query}\\nException: {e}\")\n\n    # Optional part to be included based on `use_schema_tools`\n    optional_schema_description = \"\"\n    if not self.config.use_schema_tools:\n        optional_schema_description = f\"\"\"\\\n        This JSON schema maps SQL database structure. It outlines tables, each \n        with a description and columns. Each table is identified by a key, and holds\n        a description and a dictionary of columns, with column \n        names as keys and their descriptions as values.\n\n        ```json\n        {self.config.context_descriptions}\n        ```\"\"\"\n\n    # Construct the error message\n    error_message_template = f\"\"\"\\\n    {SQL_ERROR_MSG}: '{query}'\n    {str(e)}\n    Run a new query, correcting the errors.\n    {optional_schema_description}\"\"\"\n\n    return error_message_template\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.run_query","title":"<code>run_query(msg)</code>","text":"<p>Handle a RunQueryTool message by executing a SQL query and returning the result.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>RunQueryTool</code> <p>The tool-message to handle.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The result of executing the SQL query.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def run_query(self, msg: RunQueryTool) -&gt; str:\n    \"\"\"\n    Handle a RunQueryTool message by executing a SQL query and returning the result.\n\n    Args:\n        msg (RunQueryTool): The tool-message to handle.\n\n    Returns:\n        str: The result of executing the SQL query.\n    \"\"\"\n    query = msg.query\n    session = self.Session\n    self.used_run_query = True\n    try:\n        logger.info(f\"Executing SQL query: {query}\")\n\n        query_result = session.execute(text(query))\n        session.commit()\n        try:\n            # attempt to fetch results: should work for normal SELECT queries\n            rows = query_result.fetchall()\n            n_rows = len(rows)\n            if self.config.max_result_rows and n_rows &gt; self.config.max_result_rows:\n                rows = rows[: self.config.max_result_rows]\n                logger.warning(\n                    f\"SQL query produced {n_rows} rows, \"\n                    f\"limiting to {self.config.max_result_rows}\"\n                )\n\n            response_message = self._format_rows(rows)\n        except ResourceClosedError:\n            # If we get here, it's a non-SELECT query (UPDATE, INSERT, DELETE)\n            affected_rows = query_result.rowcount  # type: ignore\n            response_message = f\"\"\"\n                Non-SELECT query executed successfully. \n                Rows affected: {affected_rows}\n                \"\"\"\n\n    except SQLAlchemyError as e:\n        session.rollback()\n        logger.error(f\"Failed to execute query: {query}\\n{e}\")\n        response_message = self.retry_query(e, query)\n    finally:\n        session.close()\n\n    final_message = f\"\"\"\n    Below is the result from your use of the TOOL `{RunQueryTool.name()}`:\n    ==== result ====\n    {response_message}\n    ================\n\n    If you are READY to ANSWER the ORIGINAL QUERY:\n    {self._tool_result_llm_answer_prompt()}\n    OTHERWISE:\n         continue using one of your available TOOLs:\n         {\",\".join(self.llm_tools_usable)}\n    \"\"\"\n    return final_message\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_table_names","title":"<code>get_table_names(msg)</code>","text":"<p>Handle a GetTableNamesTool message by returning the names of all tables in the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The names of all tables in the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_table_names(self, msg: GetTableNamesTool) -&gt; str:\n    \"\"\"\n    Handle a GetTableNamesTool message by returning the names of all tables in the\n    database.\n\n    Returns:\n        str: The names of all tables in the database.\n    \"\"\"\n    if isinstance(self.metadata, list):\n        table_names = [\", \".join(md.tables.keys()) for md in self.metadata]\n        return \", \".join(table_names)\n\n    return \", \".join(self.metadata.tables.keys())\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_table_schema","title":"<code>get_table_schema(msg)</code>","text":"<p>Handle a GetTableSchemaTool message by returning the schema of all provided tables in the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The schema of all provided tables in the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_table_schema(self, msg: GetTableSchemaTool) -&gt; str:\n    \"\"\"\n    Handle a GetTableSchemaTool message by returning the schema of all provided\n    tables in the database.\n\n    Returns:\n        str: The schema of all provided tables in the database.\n    \"\"\"\n    tables = msg.tables\n    result = \"\"\n    for table_name in tables:\n        table = self.table_metadata.get(table_name)\n        if table is not None:\n            result += f\"{table_name}: {table}\\n\"\n        else:\n            result += f\"{table_name} is not a valid table name.\\n\"\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLChatAgent.get_column_descriptions","title":"<code>get_column_descriptions(msg)</code>","text":"<p>Handle a GetColumnDescriptionsTool message by returning the descriptions of all provided columns from the database.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The descriptions of all provided columns from the database.</p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def get_column_descriptions(self, msg: GetColumnDescriptionsTool) -&gt; str:\n    \"\"\"\n    Handle a GetColumnDescriptionsTool message by returning the descriptions of all\n    provided columns from the database.\n\n    Returns:\n        str: The descriptions of all provided columns from the database.\n    \"\"\"\n    table = msg.table\n    columns = msg.columns.split(\", \")\n    result = f\"\\nTABLE: {table}\"\n    descriptions = self.config.context_descriptions.get(table)\n\n    for col in columns:\n        result += f\"\\n{col} =&gt; {descriptions['columns'][col]}\"  # type: ignore\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/sql_chat_agent/#langroid.agent.special.sql.sql_chat_agent.SQLHelperAgent","title":"<code>SQLHelperAgent(config)</code>","text":"<p>               Bases: <code>SQLChatAgent</code></p> Source code in <code>langroid/agent/special/sql/sql_chat_agent.py</code> <pre><code>def __init__(self, config: \"SQLChatAgentConfig\") -&gt; None:\n    \"\"\"Initialize the SQLChatAgent.\n\n    Raises:\n        ValueError: If database information is not provided in the config.\n    \"\"\"\n    self._validate_config(config)\n    self.config: SQLChatAgentConfig = config\n    self._init_database()\n    self._init_metadata()\n    self._init_table_metadata()\n    self.final_instructions = \"\"\n\n    # Caution - this updates the self.config.system_message!\n    self._init_system_message()\n    super().__init__(config)\n    self._init_tools()\n    if self.config.is_helper:\n        self.system_tool_format_instructions += self.final_instructions\n\n    if self.config.use_helper:\n        # helper_config.system_message is now the fully-populated sys msg of\n        # the main SQLAgent.\n        self.helper_config = self.config.model_copy()\n        self.helper_config.is_helper = True\n        self.helper_config.use_helper = False\n        self.helper_config.chat_mode = False\n        self.helper_agent = SQLHelperAgent(self.helper_config)\n</code></pre>"},{"location":"reference/agent/special/sql/utils/","title":"utils","text":"<p>langroid/agent/special/sql/utils/init.py </p>"},{"location":"reference/agent/special/sql/utils/description_extractors/","title":"description_extractors","text":"<p>langroid/agent/special/sql/utils/description_extractors.py </p>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_postgresql_descriptions","title":"<code>extract_postgresql_descriptions(engine, multi_schema=False)</code>","text":"<p>Extracts descriptions for tables and columns from a PostgreSQL database.</p> <p>This method retrieves the descriptions of tables and their columns from a PostgreSQL database using the provided SQLAlchemy engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a PostgreSQL database.</p> required <code>multi_schema</code> <code>bool</code> <p>Generate descriptions for all schemas in the database.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing the table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_postgresql_descriptions(\n    engine: Engine,\n    multi_schema: bool = False,\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Extracts descriptions for tables and columns from a PostgreSQL database.\n\n    This method retrieves the descriptions of tables and their columns\n    from a PostgreSQL database using the provided SQLAlchemy engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a PostgreSQL database.\n        multi_schema (bool): Generate descriptions for all schemas in the database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing the table description and a dictionary of\n        column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    result: Dict[str, Dict[str, Any]] = {}\n\n    def gen_schema_descriptions(schema: Optional[str] = None) -&gt; None:\n        table_names: List[str] = inspector.get_table_names(schema=schema)\n        with engine.connect() as conn:\n            for table in table_names:\n                if schema is None:\n                    table_name = table\n                else:\n                    table_name = f\"{schema}.{table}\"\n\n                table_comment = (\n                    conn.execute(\n                        text(f\"SELECT obj_description('{table_name}'::regclass)\")\n                    ).scalar()\n                    or \"\"\n                )\n\n                columns = {}\n                col_data = inspector.get_columns(table, schema=schema)\n                for idx, col in enumerate(col_data, start=1):\n                    col_comment = (\n                        conn.execute(\n                            text(\n                                f\"SELECT col_description('{table_name}'::regclass, \"\n                                f\"{idx})\"\n                            )\n                        ).scalar()\n                        or \"\"\n                    )\n                    columns[col[\"name\"]] = col_comment\n\n                result[table_name] = {\"description\": table_comment, \"columns\": columns}\n\n    if multi_schema:\n        for schema in inspector.get_schema_names():\n            gen_schema_descriptions(schema)\n    else:\n        gen_schema_descriptions()\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_mysql_descriptions","title":"<code>extract_mysql_descriptions(engine, multi_schema=False)</code>","text":"<p>Extracts descriptions for tables and columns from a MySQL database.</p> <p>This method retrieves the descriptions of tables and their columns from a MySQL database using the provided SQLAlchemy engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a MySQL database.</p> required <code>multi_schema</code> <code>bool</code> <p>Generate descriptions for all schemas in the database.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing the table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_mysql_descriptions(\n    engine: Engine,\n    multi_schema: bool = False,\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Extracts descriptions for tables and columns from a MySQL database.\n\n    This method retrieves the descriptions of tables and their columns\n    from a MySQL database using the provided SQLAlchemy engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a MySQL database.\n        multi_schema (bool): Generate descriptions for all schemas in the database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing the table description and a dictionary of\n        column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    result: Dict[str, Dict[str, Any]] = {}\n\n    def gen_schema_descriptions(schema: Optional[str] = None) -&gt; None:\n        table_names: List[str] = inspector.get_table_names(schema=schema)\n\n        with engine.connect() as conn:\n            for table in table_names:\n                if schema is None:\n                    table_name = table\n                else:\n                    table_name = f\"{schema}.{table}\"\n\n                query = text(\n                    \"SELECT table_comment FROM information_schema.tables WHERE\"\n                    \" table_schema = :schema AND table_name = :table\"\n                )\n                table_result = conn.execute(\n                    query, {\"schema\": engine.url.database, \"table\": table_name}\n                )\n                table_comment = table_result.scalar() or \"\"\n\n                columns = {}\n                for col in inspector.get_columns(table, schema=schema):\n                    columns[col[\"name\"]] = col.get(\"comment\", \"\")\n\n                result[table_name] = {\"description\": table_comment, \"columns\": columns}\n\n    if multi_schema:\n        for schema in inspector.get_schema_names():\n            gen_schema_descriptions(schema)\n    else:\n        gen_schema_descriptions()\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_default_descriptions","title":"<code>extract_default_descriptions(engine, multi_schema=False)</code>","text":"<p>Extracts default descriptions for tables and columns from a database.</p> <p>This method retrieves the table and column names from the given database and associates empty descriptions with them.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine connected to a database.</p> required <code>multi_schema</code> <code>bool</code> <p>Generate descriptions for all schemas in the database.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary mapping table names to a</p> <code>Dict[str, Dict[str, Any]]</code> <p>dictionary containing an empty table description and a dictionary of</p> <code>Dict[str, Dict[str, Any]]</code> <p>empty column descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_default_descriptions(\n    engine: Engine, multi_schema: bool = False\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Extracts default descriptions for tables and columns from a database.\n\n    This method retrieves the table and column names from the given database\n    and associates empty descriptions with them.\n\n    Args:\n        engine (Engine): SQLAlchemy engine connected to a database.\n        multi_schema (bool): Generate descriptions for all schemas in the database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary mapping table names to a\n        dictionary containing an empty table description and a dictionary of\n        empty column descriptions.\n    \"\"\"\n    inspector = inspect(engine)\n    result: Dict[str, Dict[str, Any]] = {}\n\n    def gen_schema_descriptions(schema: Optional[str] = None) -&gt; None:\n        table_names: List[str] = inspector.get_table_names(schema=schema)\n\n        for table in table_names:\n            columns = {}\n            for col in inspector.get_columns(table):\n                columns[col[\"name\"]] = \"\"\n\n            result[table] = {\"description\": \"\", \"columns\": columns}\n\n    if multi_schema:\n        for schema in inspector.get_schema_names():\n            gen_schema_descriptions(schema)\n    else:\n        gen_schema_descriptions()\n\n    return result\n</code></pre>"},{"location":"reference/agent/special/sql/utils/description_extractors/#langroid.agent.special.sql.utils.description_extractors.extract_schema_descriptions","title":"<code>extract_schema_descriptions(engine, multi_schema=False)</code>","text":"<p>Extracts the schema descriptions from the database connected to by the engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine instance.</p> required <code>multi_schema</code> <code>bool</code> <p>Generate descriptions for all schemas in the database.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary representation of table and column</p> <code>Dict[str, Dict[str, Any]]</code> <p>descriptions.</p> Source code in <code>langroid/agent/special/sql/utils/description_extractors.py</code> <pre><code>def extract_schema_descriptions(\n    engine: Engine, multi_schema: bool = False\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Extracts the schema descriptions from the database connected to by the engine.\n\n    Args:\n        engine (Engine): SQLAlchemy engine instance.\n        multi_schema (bool): Generate descriptions for all schemas in the database.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary representation of table and column\n        descriptions.\n    \"\"\"\n\n    extractors = {\n        \"postgresql\": extract_postgresql_descriptions,\n        \"mysql\": extract_mysql_descriptions,\n    }\n    return extractors.get(engine.dialect.name, extract_default_descriptions)(\n        engine, multi_schema=multi_schema\n    )\n</code></pre>"},{"location":"reference/agent/special/sql/utils/populate_metadata/","title":"populate_metadata","text":"<p>langroid/agent/special/sql/utils/populate_metadata.py </p>"},{"location":"reference/agent/special/sql/utils/populate_metadata/#langroid.agent.special.sql.utils.populate_metadata.populate_metadata_with_schema_tools","title":"<code>populate_metadata_with_schema_tools(metadata, info)</code>","text":"<p>Extracts information from an SQLAlchemy database's metadata and combines it with another dictionary with context descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>MetaData</code> <p>SQLAlchemy metadata object of the database.</p> required <code>info</code> <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary with table and column                                  descriptions.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Union[str, Dict[str, str]]]]</code> <p>Dict[str, Dict[str, Any]]: A dictionary with table and context information.</p> Source code in <code>langroid/agent/special/sql/utils/populate_metadata.py</code> <pre><code>def populate_metadata_with_schema_tools(\n    metadata: MetaData | List[MetaData],\n    info: Dict[str, Dict[str, Union[str, Dict[str, str]]]],\n) -&gt; Dict[str, Dict[str, Union[str, Dict[str, str]]]]:\n    \"\"\"\n    Extracts information from an SQLAlchemy database's metadata and combines it\n    with another dictionary with context descriptions.\n\n    Args:\n        metadata (MetaData): SQLAlchemy metadata object of the database.\n        info (Dict[str, Dict[str, Any]]): A dictionary with table and column\n                                             descriptions.\n\n    Returns:\n        Dict[str, Dict[str, Any]]: A dictionary with table and context information.\n    \"\"\"\n    db_info: Dict[str, Dict[str, Union[str, Dict[str, str]]]] = {}\n\n    def populate_metadata(md: MetaData) -&gt; None:\n        # Create empty metadata dictionary with column datatypes\n        for table_name, table in md.tables.items():\n            # Populate tables with empty descriptions\n            db_info[table_name] = {\n                \"description\": info[table_name][\"description\"] or \"\",\n                \"columns\": {},\n            }\n\n            for column in table.columns:\n                # Populate columns with datatype\n                db_info[table_name][\"columns\"][str(column.name)] = (  # type: ignore\n                    str(column.type)\n                )\n\n    if isinstance(metadata, list):\n        for md in metadata:\n            populate_metadata(md)\n    else:\n        populate_metadata(metadata)\n\n    return db_info\n</code></pre>"},{"location":"reference/agent/special/sql/utils/populate_metadata/#langroid.agent.special.sql.utils.populate_metadata.populate_metadata","title":"<code>populate_metadata(metadata, info)</code>","text":"<p>Populate metadata based on the provided database metadata and additional info.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>MetaData</code> <p>Metadata object from SQLAlchemy.</p> required <code>info</code> <code>Dict</code> <p>Additional information for database tables and columns.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, Dict[str, Union[str, Dict[str, str]]]]</code> <p>A dictionary containing populated metadata information.</p> Source code in <code>langroid/agent/special/sql/utils/populate_metadata.py</code> <pre><code>def populate_metadata(\n    metadata: MetaData | List[MetaData],\n    info: Dict[str, Dict[str, Union[str, Dict[str, str]]]],\n) -&gt; Dict[str, Dict[str, Union[str, Dict[str, str]]]]:\n    \"\"\"\n    Populate metadata based on the provided database metadata and additional info.\n\n    Args:\n        metadata (MetaData): Metadata object from SQLAlchemy.\n        info (Dict): Additional information for database tables and columns.\n\n    Returns:\n        Dict: A dictionary containing populated metadata information.\n    \"\"\"\n    # Fetch basic metadata info using available tools\n    db_info: Dict[str, Dict[str, Union[str, Dict[str, str]]]] = (\n        populate_metadata_with_schema_tools(metadata=metadata, info=info)\n    )\n\n    # Iterate over tables to update column metadata\n    for table_name in db_info.keys():\n        # Update only if additional info for the table exists\n        if table_name in info:\n            for column_name in db_info[table_name][\"columns\"]:\n                # Merge and update column description if available\n                if column_name in info[table_name][\"columns\"]:\n                    db_info[table_name][\"columns\"][column_name] = (  # type: ignore\n                        db_info[table_name][\"columns\"][column_name]  # type: ignore\n                        + \"; \"\n                        + info[table_name][\"columns\"][column_name]  # type: ignore\n                    )\n\n    return db_info\n</code></pre>"},{"location":"reference/agent/special/sql/utils/system_message/","title":"system_message","text":"<p>langroid/agent/special/sql/utils/system_message.py </p>"},{"location":"reference/agent/special/sql/utils/tools/","title":"tools","text":"<p>langroid/agent/special/sql/utils/tools.py </p>"},{"location":"reference/agent/tools/","title":"tools","text":"<p>langroid/agent/tools/init.py </p>"},{"location":"reference/agent/tools/#langroid.agent.tools.AddRecipientTool","title":"<code>AddRecipientTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Used by LLM to add a recipient to the previous message, when it has forgotten to specify a recipient. This avoids having to re-generate the previous message (and thus saves token-cost and time).</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.AddRecipientTool.response","title":"<code>response(agent)</code>","text":"<p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; ChatDocument:\n    \"\"\"\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.recipient.\n    \"\"\"\n    print(\n        \"[red]RecipientTool: \"\n        f\"Added recipient {self.intended_recipient} to message.\"\n    )\n    if self.__class__._saved_content == \"\":\n        recipient_request_name = RecipientTool.default_value(\"request\")\n        content = f\"\"\"\n            Recipient specified but content is empty!\n            This could be because the `{self.request}` tool/function was used \n            before using `{recipient_request_name}` tool/function.\n            Resend the message using `{recipient_request_name}` tool/function.\n            \"\"\"\n    else:\n        content = self.__class__._saved_content  # use class-level attrib value\n        # erase content since we just used it.\n        self.__class__._saved_content = \"\"\n    return ChatDocument(\n        content=content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as it msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool","title":"<code>RecipientTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Used by LLM to send a message to a specific recipient.</p> <p>Useful in cases where an LLM is talking to 2 or more agents (or an Agent and human user), and needs to specify which agent (task) its message is intended for. The recipient name should be the name of a task (which is normally the name of the agent that the task wraps, although the task can have its own name).</p> <p>To use this tool/function-call, LLM must generate a JSON structure with these fields: {     \"request\": \"recipient_message\", # also the function name when using fn-calling     \"intended_recipient\": ,     \"content\":  } The effect of this is that <code>content</code> will be sent to the <code>intended_recipient</code> task."},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool.create","title":"<code>create(recipients, default='')</code>  <code>classmethod</code>","text":"<p>Create a restricted version of RecipientTool that only allows certain recipients, and possibly sets a default recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef create(cls, recipients: List[str], default: str = \"\") -&gt; Type[\"RecipientTool\"]:\n    \"\"\"Create a restricted version of RecipientTool that\n    only allows certain recipients, and possibly sets a default recipient.\"\"\"\n\n    class RecipientToolRestricted(cls):  # type: ignore\n        allowed_recipients: ClassVar[List[str]] = recipients\n        default_recipient: ClassVar[str] = default\n\n    return RecipientToolRestricted\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool.instructions","title":"<code>instructions()</code>  <code>classmethod</code>","text":"<p>Generate instructions for using this tool/function. These are intended to be appended to the system message of the LLM.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    \"\"\"\n    Generate instructions for using this tool/function.\n    These are intended to be appended to the system message of the LLM.\n    \"\"\"\n    recipients = []\n    if has_field(cls, \"allowed_recipients\"):\n        recipients = cls.default_value(\"allowed_recipients\")\n    if len(recipients) &gt; 0:\n        recipients_str = \", \".join(recipients)\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to one of the following:\n        {recipients_str},\n        and setting the 'content' field to your message.\n        \"\"\"\n    else:\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to the name of the recipient, \n        and setting the 'content' field to your message.\n        \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool.response","title":"<code>response(agent)</code>","text":"<p>When LLM has correctly used this tool, construct a ChatDocument with an explicit recipient, and make it look like it is from the LLM.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.intended_recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    When LLM has correctly used this tool,\n    construct a ChatDocument with an explicit recipient,\n    and make it look like it is from the LLM.\n\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.intended_recipient.\n    \"\"\"\n    default_recipient = self.__class__.default_value(\"default_recipient\")\n    if self.intended_recipient == \"\" and default_recipient not in [\"\", None]:\n        self.intended_recipient = default_recipient\n    elif self.intended_recipient == \"\":\n        # save the content as a class-variable, so that\n        # we can construct the ChatDocument once the LLM specifies a recipient.\n        # This avoids having to re-generate the entire message, saving time + cost.\n        AddRecipientTool._saved_content = self.content\n        agent.enable_message(AddRecipientTool)\n        return ChatDocument(\n            content=\"\"\"\n            Empty recipient field!\n            Please use the 'add_recipient' tool/function-call to specify who your \n            message is intended for.\n            DO NOT REPEAT your original message; ONLY specify the recipient via this\n            tool/function-call.\n            \"\"\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                recipient=Entity.LLM,\n            ),\n        )\n\n    print(\"[red]RecipientTool: Validated properly addressed message\")\n\n    return ChatDocument(\n        content=self.content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as if msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RecipientTool.handle_message_fallback","title":"<code>handle_message_fallback(agent, msg)</code>  <code>staticmethod</code>","text":"<p>Response of agent if this tool is not used, e.g. the LLM simply sends a message without using this tool. This method has two purposes: (a) Alert the LLM that it has forgotten to specify a recipient, and prod it     to use the <code>add_recipient</code> tool to specify just the recipient     (and not re-generate the entire message). (b) Save the content of the message in the agent's <code>content</code> field,     so the agent can construct a ChatDocument with this content once LLM     later specifies a recipient using the <code>add_recipient</code> tool.</p> <p>This method is used to set the agent's handle_message_fallback() method.</p> <p>Returns:</p> Type Description <code>str</code> <p>reminder to LLM to use the <code>add_recipient</code> tool.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@staticmethod\ndef handle_message_fallback(\n    agent: ChatAgent, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Response of agent if this tool is not used, e.g.\n    the LLM simply sends a message without using this tool.\n    This method has two purposes:\n    (a) Alert the LLM that it has forgotten to specify a recipient, and prod it\n        to use the `add_recipient` tool to specify just the recipient\n        (and not re-generate the entire message).\n    (b) Save the content of the message in the agent's `content` field,\n        so the agent can construct a ChatDocument with this content once LLM\n        later specifies a recipient using the `add_recipient` tool.\n\n    This method is used to set the agent's handle_message_fallback() method.\n\n    Returns:\n        (str): reminder to LLM to use the `add_recipient` tool.\n    \"\"\"\n    # Note: once the LLM specifies a missing recipient, the task loop\n    # mechanism will not allow any of the \"native\" responders to respond,\n    # since the recipient will differ from the task name.\n    # So if this method is called, we can be sure that the recipient has not\n    # been specified.\n    if (\n        isinstance(msg, str)\n        or msg.metadata.sender != Entity.LLM\n        or msg.metadata.recipient != \"\"  # there IS an explicit recipient\n    ):\n        return None\n    content = msg if isinstance(msg, str) else msg.content\n    # save the content as a class-variable, so that\n    # we can construct the ChatDocument once the LLM specifies a recipient.\n    # This avoids having to re-generate the entire message, saving time + cost.\n    AddRecipientTool._saved_content = content\n    agent.enable_message(AddRecipientTool)\n    print(\"[red]RecipientTool: Recipient not specified, asking LLM to clarify.\")\n    return ChatDocument(\n        content=\"\"\"\n        Please use the 'add_recipient' tool/function-call to specify who your \n        `intended_recipient` is.\n        DO NOT REPEAT your original message; ONLY specify the \n        `intended_recipient` via this tool/function-call.\n        \"\"\",\n        metadata=ChatDocMetaData(\n            sender=Entity.AGENT,\n            recipient=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.RewindTool","title":"<code>RewindTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Used by LLM to rewind (i.e. backtrack) to the <code>n</code>th Assistant message and replace with a new msg.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.RewindTool.response","title":"<code>response(agent)</code>","text":"<p>Define the tool-handler method for this tool here itself, since it is a generic tool whose functionality should be the same for any agent.</p> <p>When LLM has correctly used this tool, rewind this agent's <code>message_history</code> to the <code>n</code>th assistant msg, and replace it with <code>content</code>. We need to mock it as if the LLM is sending this message.</p> <p>Within a multi-agent scenario, this also means that any other messages dependent on this message will need to be invalidated -- so go down the chain of child messages and clear each agent's history back to the <code>msg_idx</code> corresponding to the child message.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content.</p> Source code in <code>langroid/agent/tools/rewind_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    Define the tool-handler method for this tool here itself,\n    since it is a generic tool whose functionality should be the\n    same for any agent.\n\n    When LLM has correctly used this tool, rewind this agent's\n    `message_history` to the `n`th assistant msg, and replace it with `content`.\n    We need to mock it as if the LLM is sending this message.\n\n    Within a multi-agent scenario, this also means that any other messages dependent\n    on this message will need to be invalidated --\n    so go down the chain of child messages and clear each agent's history\n    back to the `msg_idx` corresponding to the child message.\n\n    Returns:\n        (ChatDocument): with content set to self.content.\n    \"\"\"\n    idx = agent.nth_message_idx_with_role(lm.Role.ASSISTANT, self.n)\n    if idx &lt; 0:\n        # set up a corrective message from AGENT\n        msg = f\"\"\"\n            Could not rewind to {self.n}th Assistant message!\n            Please check the value of `n` and try again.\n            Or it may be too early to use the `rewind_tool`.\n            \"\"\"\n        return agent.create_agent_response(msg)\n\n    parent = prune_messages(agent, idx)\n\n    # create ChatDocument with new content, to be returned as result of this tool\n    result_doc = agent.create_llm_response(self.content)\n    result_doc.metadata.parent_id = \"\" if parent is None else parent.id()\n    result_doc.metadata.agent_id = agent.id\n    result_doc.metadata.msg_idx = idx\n\n    # replace the message at idx with this new message\n    agent.message_history.extend(ChatDocument.to_LLMMessage(result_doc))\n\n    # set the replaced doc's parent's child to this result_doc\n    if parent is not None:\n        # first remove the this parent's child from registry\n        ChatDocument.delete_id(parent.metadata.child_id)\n        parent.metadata.child_id = result_doc.id()\n    return result_doc\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.AgentDoneTool","title":"<code>AgentDoneTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for AGENT entity (i.e. agent_response or downstream tool handling fns) to signal the current task is done.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.DoneTool","title":"<code>DoneTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for Agent Entity (i.e. agent_response) or LLM entity (i.e. llm_response) to signal the current task is done, with some content as the result.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.DoneTool.convert_content_to_string","title":"<code>convert_content_to_string(v)</code>  <code>classmethod</code>","text":"<p>Convert content to string if it's not already.</p> Source code in <code>langroid/agent/tools/orchestration.py</code> <pre><code>@field_validator(\"content\", mode=\"before\")\n@classmethod\ndef convert_content_to_string(cls, v: Any) -&gt; str:\n    \"\"\"Convert content to string if it's not already.\"\"\"\n    return str(v) if v is not None else \"\"\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.ForwardTool","title":"<code>ForwardTool</code>","text":"<p>               Bases: <code>PassTool</code></p> <p>Tool for forwarding the received msg (ChatDocument) to another agent or entity. Similar to PassTool, but with a specified recipient agent.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.ForwardTool.response","title":"<code>response(agent, chat_doc)</code>","text":"<p>When this tool is enabled for an Agent, this will result in a method added to the Agent with signature: <code>forward_tool(self, tool: ForwardTool, chat_doc: ChatDocument) -&gt; ChatDocument:</code></p> Source code in <code>langroid/agent/tools/orchestration.py</code> <pre><code>def response(self, agent: ChatAgent, chat_doc: ChatDocument) -&gt; ChatDocument:\n    \"\"\"When this tool is enabled for an Agent, this will result in a method\n    added to the Agent with signature:\n    `forward_tool(self, tool: ForwardTool, chat_doc: ChatDocument) -&gt; ChatDocument:`\n    \"\"\"\n    # if chat_doc contains ForwardTool, then we forward its parent ChatDocument;\n    # else forward chat_doc itself\n    new_doc = PassTool.response(self, agent, chat_doc)\n    new_doc.metadata.recipient = self.agent\n    return new_doc\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.PassTool","title":"<code>PassTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for \"passing\" on the received msg (ChatDocument), so that an as-yet-unspecified agent can handle it. Similar to ForwardTool, but without specifying the recipient agent.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.PassTool.response","title":"<code>response(agent, chat_doc)</code>","text":"<p>When this tool is enabled for an Agent, this will result in a method added to the Agent with signature: <code>pass_tool(self, tool: PassTool, chat_doc: ChatDocument) -&gt; ChatDocument:</code></p> Source code in <code>langroid/agent/tools/orchestration.py</code> <pre><code>def response(self, agent: ChatAgent, chat_doc: ChatDocument) -&gt; ChatDocument:\n    \"\"\"When this tool is enabled for an Agent, this will result in a method\n    added to the Agent with signature:\n    `pass_tool(self, tool: PassTool, chat_doc: ChatDocument) -&gt; ChatDocument:`\n    \"\"\"\n    # if PassTool is in chat_doc, pass its parent, else pass chat_doc itself\n    doc = chat_doc\n    while True:\n        tools = agent.get_tool_messages(doc)\n        if not any(isinstance(t, type(self)) for t in tools):\n            break\n        if doc.parent is None:\n            break\n        doc = doc.parent\n    assert doc is not None, \"PassTool: parent of chat_doc must not be None\"\n    new_doc = ChatDocument.deepcopy(doc)\n    new_doc.metadata.sender = Entity.AGENT\n    return new_doc\n</code></pre>"},{"location":"reference/agent/tools/#langroid.agent.tools.SendTool","title":"<code>SendTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for agent or LLM to send content to a specified agent. Similar to RecipientTool.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.AgentSendTool","title":"<code>AgentSendTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for Agent (i.e. agent_response) to send content or tool_messages to a specified agent. Similar to SendTool except that AgentSendTool is only usable by agent_response (or handler of another tool), to send content or tools to another agent. SendTool does not allow sending tools.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.DonePassTool","title":"<code>DonePassTool</code>","text":"<p>               Bases: <code>PassTool</code></p> <p>Tool to signal DONE, AND Pass incoming/current msg as result. Similar to PassTool, except we append a DoneTool to the result tool_messages.</p>"},{"location":"reference/agent/tools/#langroid.agent.tools.ResultTool","title":"<code>ResultTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Class to use as a wrapper for sending arbitrary results from an Agent's agent_response or tool handlers, to: (a) trigger completion of the current task (similar to (Agent)DoneTool), and (b) be returned as the result of the current task, i.e. this tool would appear      in the resulting ChatDocument's <code>tool_messages</code> list. See test_tool_handlers_and_results in test_tool_messages.py, and examples/basic/tool-extract-short-example.py.</p> Note <ul> <li>when defining a tool handler or agent_response, you can directly return     ResultTool(field1 = val1, ...),     where the values can be arbitrary data structures, including nested     Pydantic objs, or you can define a subclass of ResultTool with the     fields you want to return.</li> <li>This is a special ToolMessage that is NOT meant to be used or handled     by an agent.</li> <li>AgentDoneTool is more restrictive in that you can only send a <code>content</code>     or <code>tools</code> in the result.</li> </ul>"},{"location":"reference/agent/tools/#langroid.agent.tools.FinalResultTool","title":"<code>FinalResultTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Class to use as a wrapper for sending arbitrary results from an Agent's agent_response or tool handlers, to: (a) trigger completion of the current task as well as all parent tasks, and (b) be returned as the final result of the root task, i.e. this tool would appear      in the final ChatDocument's <code>tool_messages</code> list. See test_tool_handlers_and_results in test_tool_messages.py, and examples/basic/chat-tool-function.py.</p> Note <ul> <li>when defining a tool handler or agent_response, you can directly return     FinalResultTool(field1 = val1, ...),     where the values can be arbitrary data structures, including nested     Pydantic objs, or you can define a subclass of FinalResultTool with the     fields you want to return.</li> <li>This is a special ToolMessage that is NOT meant to be used by an agent's     llm_response, but only by agent_response or tool handlers.</li> <li>A subclass of this tool can be defined, with specific fields, and   with _allow_llm_use = True, to allow the LLM to generate this tool,   and have the effect of terminating the current and all parent tasks,   with the tool appearing in the final ChatDocument's <code>tool_messages</code> list.   See examples/basic/multi-agent-return-result.py.</li> </ul>"},{"location":"reference/agent/tools/duckduckgo_search_tool/","title":"duckduckgo_search_tool","text":"<p>langroid/agent/tools/duckduckgo_search_tool.py </p> <p>A tool to trigger a DuckDuckGo search for a given query, and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(DuckduckgoSearchTool)</code></p>"},{"location":"reference/agent/tools/duckduckgo_search_tool/#langroid.agent.tools.duckduckgo_search_tool.DuckduckgoSearchTool","title":"<code>DuckduckgoSearchTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/tools/duckduckgo_search_tool/#langroid.agent.tools.duckduckgo_search_tool.DuckduckgoSearchTool.handle","title":"<code>handle()</code>","text":"<p>Conducts a search using DuckDuckGo based on the provided query and number of results by triggering a duckduckgo_search.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing the titles, links, and summaries of each search result, separated by two newlines.</p> Source code in <code>langroid/agent/tools/duckduckgo_search_tool.py</code> <pre><code>def handle(self) -&gt; str:\n    \"\"\"\n    Conducts a search using DuckDuckGo based on the provided query\n    and number of results by triggering a duckduckgo_search.\n\n    Returns:\n        str: A formatted string containing the titles, links, and\n            summaries of each search result, separated by two newlines.\n    \"\"\"\n    search_results = duckduckgo_search(self.query, self.num_results)\n    # return Title, Link, Summary of each result, separated by two newlines\n    results_str = \"\\n\\n\".join(str(result) for result in search_results)\n    return f\"\"\"\n    BELOW ARE THE RESULTS FROM THE WEB SEARCH. USE THESE TO COMPOSE YOUR RESPONSE:\n    {results_str}\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/exa_search_tool/","title":"exa_search_tool","text":"<p>langroid/agent/tools/exa_search_tool.py </p> <p>A tool to trigger a Exa search for a given query, (https://docs.exa.ai/reference/getting-started) and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(ExaSearchTool)</code></p> <p>NOTE: To use this tool, you need to:</p> <ul> <li> <p>set the EXA_API_KEY environment variables in your <code>.env</code> file, e.g. <code>EXA_API_KEY=your_api_key_here</code> (Note as of 28 Jan 2023, Metaphor renamed to Exa, so you can also use <code>EXA_API_KEY=your_api_key_here</code>)</p> </li> <li> <p>install langroid with the <code>exa-py</code> extra, e.g. <code>pip install langroid[exa]</code> or <code>uv pip install langroid[exa]</code> or <code>poetry add langroid[exa]</code>  or <code>uv add langroid[exa]</code> (it installs the <code>exa_py</code> package from pypi).</p> </li> </ul> <p>For more information, please refer to the official docs: https://exa.ai/</p>"},{"location":"reference/agent/tools/exa_search_tool/#langroid.agent.tools.exa_search_tool.ExaSearchTool","title":"<code>ExaSearchTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/tools/exa_search_tool/#langroid.agent.tools.exa_search_tool.ExaSearchTool.handle","title":"<code>handle()</code>","text":"<p>Conducts a search using the exa API based on the provided query and number of results by triggering a exa_search.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing the titles, links, and summaries of each search result, separated by two newlines.</p> Source code in <code>langroid/agent/tools/exa_search_tool.py</code> <pre><code>def handle(self) -&gt; str:\n    \"\"\"\n    Conducts a search using the exa API based on the provided query\n    and number of results by triggering a exa_search.\n\n    Returns:\n        str: A formatted string containing the titles, links, and\n            summaries of each search result, separated by two newlines.\n    \"\"\"\n\n    search_results = exa_search(self.query, self.num_results)\n    # return Title, Link, Summary of each result, separated by two newlines\n    results_str = \"\\n\\n\".join(str(result) for result in search_results)\n    return f\"\"\"\n    BELOW ARE THE RESULTS FROM THE WEB SEARCH. USE THESE TO COMPOSE YOUR RESPONSE:\n    {results_str}\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/file_tools/","title":"file_tools","text":"<p>langroid/agent/tools/file_tools.py </p>"},{"location":"reference/agent/tools/file_tools/#langroid.agent.tools.file_tools.ReadFileTool","title":"<code>ReadFileTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/tools/file_tools/#langroid.agent.tools.file_tools.ReadFileTool.create","title":"<code>create(get_curr_dir)</code>  <code>classmethod</code>","text":"<p>Create a subclass of ReadFileTool for a specific directory</p> <p>Parameters:</p> Name Type Description Default <code>get_curr_dir</code> <code>callable</code> <p>A function that returns the current directory.</p> required <p>Returns:</p> Type Description <code>Type[ReadFileTool]</code> <p>Type[ReadFileTool]: A subclass of the ReadFileTool class, specifically for the current directory.</p> Source code in <code>langroid/agent/tools/file_tools.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    get_curr_dir: Callable[[], str] | None,\n) -&gt; Type[\"ReadFileTool\"]:\n    \"\"\"\n    Create a subclass of ReadFileTool for a specific directory\n\n    Args:\n        get_curr_dir (callable): A function that returns the current directory.\n\n    Returns:\n        Type[ReadFileTool]: A subclass of the ReadFileTool class, specifically\n            for the current directory.\n    \"\"\"\n\n    class CustomReadFileTool(cls):  # type: ignore\n        _curr_dir: Callable[[], str] | None = (\n            staticmethod(get_curr_dir) if get_curr_dir else None\n        )\n\n    return CustomReadFileTool\n</code></pre>"},{"location":"reference/agent/tools/file_tools/#langroid.agent.tools.file_tools.WriteFileTool","title":"<code>WriteFileTool</code>","text":"<p>               Bases: <code>XMLToolMessage</code></p>"},{"location":"reference/agent/tools/file_tools/#langroid.agent.tools.file_tools.WriteFileTool.create","title":"<code>create(get_curr_dir, get_git_repo)</code>  <code>classmethod</code>","text":"<p>Create a subclass of WriteFileTool with the current directory and git repo.</p> <p>Parameters:</p> Name Type Description Default <code>get_curr_dir</code> <code>callable</code> <p>A function that returns the current directory.</p> required <code>get_git_repo</code> <code>callable</code> <p>A function that returns the git repo.</p> required <p>Returns:</p> Type Description <code>Type[WriteFileTool]</code> <p>Type[WriteFileTool]: A subclass of the WriteFileTool class, specifically for the current directory and git repo.</p> Source code in <code>langroid/agent/tools/file_tools.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    get_curr_dir: Callable[[], str] | None,\n    get_git_repo: Callable[[], str] | None,\n) -&gt; Type[\"WriteFileTool\"]:\n    \"\"\"\n    Create a subclass of WriteFileTool with the current directory and git repo.\n\n    Args:\n        get_curr_dir (callable): A function that returns the current directory.\n        get_git_repo (callable): A function that returns the git repo.\n\n    Returns:\n        Type[WriteFileTool]: A subclass of the WriteFileTool class, specifically\n            for the current directory and git repo.\n    \"\"\"\n\n    class CustomWriteFileTool(cls):  # type: ignore\n        _curr_dir: Callable[[], str] | None = (\n            staticmethod(get_curr_dir) if get_curr_dir else None\n        )\n        _git_repo: Callable[[], str] | None = (\n            staticmethod(get_git_repo) if get_git_repo else None\n        )\n\n    return CustomWriteFileTool\n</code></pre>"},{"location":"reference/agent/tools/file_tools/#langroid.agent.tools.file_tools.ListDirTool","title":"<code>ListDirTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/tools/file_tools/#langroid.agent.tools.file_tools.ListDirTool.create","title":"<code>create(get_curr_dir)</code>  <code>classmethod</code>","text":"<p>Create a subclass of ListDirTool for a specific directory</p> <p>Parameters:</p> Name Type Description Default <code>get_curr_dir</code> <code>callable</code> <p>A function that returns the current directory.</p> required <p>Returns:</p> Type Description <code>Type[ReadFileTool]</code> <p>Type[ReadFileTool]: A subclass of the ReadFileTool class, specifically for the current directory.</p> Source code in <code>langroid/agent/tools/file_tools.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    get_curr_dir: Callable[[], str] | None,\n) -&gt; Type[\"ReadFileTool\"]:\n    \"\"\"\n    Create a subclass of ListDirTool for a specific directory\n\n    Args:\n        get_curr_dir (callable): A function that returns the current directory.\n\n    Returns:\n        Type[ReadFileTool]: A subclass of the ReadFileTool class, specifically\n            for the current directory.\n    \"\"\"\n\n    class CustomListDirTool(cls):  # type: ignore\n        _curr_dir: Callable[[], str] | None = (\n            staticmethod(get_curr_dir) if get_curr_dir else None\n        )\n\n    return CustomListDirTool\n</code></pre>"},{"location":"reference/agent/tools/google_search_tool/","title":"google_search_tool","text":"<p>langroid/agent/tools/google_search_tool.py </p> <p>A tool to trigger a Google search for a given query, and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(GoogleSearchTool)</code></p> <p>NOTE: Using this tool requires setting the GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables in your <code>.env</code> file, as explained in the README.</p>"},{"location":"reference/agent/tools/metaphor_search_tool/","title":"metaphor_search_tool","text":"<p>langroid/agent/tools/metaphor_search_tool.py </p> <p>A tool to trigger a Metaphor search for a given query, (https://docs.exa.ai/reference/getting-started) and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(MetaphorSearchTool)</code></p> <p>NOTE: To use this tool, you need to:</p> <ul> <li> <p>set the METAPHOR_API_KEY environment variables in your <code>.env</code> file, e.g. <code>METAPHOR_API_KEY=your_api_key_here</code> (Note as of 28 Jan 2023, Metaphor renamed to Exa, so you can also use <code>EXA_API_KEY=your_api_key_here</code>)</p> </li> <li> <p>install langroid with the <code>metaphor</code> extra, e.g. <code>pip install langroid[metaphor]</code> or <code>uv pip install langroid[metaphor]</code>  or <code>poetry add langroid[metaphor]</code>  or <code>uv add langroid[metaphor]</code> (it installs the <code>metaphor-python</code> package from pypi).</p> </li> </ul> <p>For more information, please refer to the official docs: https://metaphor.systems/</p>"},{"location":"reference/agent/tools/metaphor_search_tool/#langroid.agent.tools.metaphor_search_tool.MetaphorSearchTool","title":"<code>MetaphorSearchTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/tools/metaphor_search_tool/#langroid.agent.tools.metaphor_search_tool.MetaphorSearchTool.handle","title":"<code>handle()</code>","text":"<p>Conducts a search using the metaphor API based on the provided query and number of results by triggering a metaphor_search.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing the titles, links, and summaries of each search result, separated by two newlines.</p> Source code in <code>langroid/agent/tools/metaphor_search_tool.py</code> <pre><code>def handle(self) -&gt; str:\n    \"\"\"\n    Conducts a search using the metaphor API based on the provided query\n    and number of results by triggering a metaphor_search.\n\n    Returns:\n        str: A formatted string containing the titles, links, and\n            summaries of each search result, separated by two newlines.\n    \"\"\"\n\n    search_results = metaphor_search(self.query, self.num_results)\n    # return Title, Link, Summary of each result, separated by two newlines\n    results_str = \"\\n\\n\".join(str(result) for result in search_results)\n    return f\"\"\"\n    BELOW ARE THE RESULTS FROM THE WEB SEARCH. USE THESE TO COMPOSE YOUR RESPONSE:\n    {results_str}\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/orchestration/","title":"orchestration","text":"<p>langroid/agent/tools/orchestration.py </p> <p>Various tools to for agents to be able to control flow of Task, e.g. termination, routing to another agent, etc.</p>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.AgentDoneTool","title":"<code>AgentDoneTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for AGENT entity (i.e. agent_response or downstream tool handling fns) to signal the current task is done.</p>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.DoneTool","title":"<code>DoneTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for Agent Entity (i.e. agent_response) or LLM entity (i.e. llm_response) to signal the current task is done, with some content as the result.</p>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.DoneTool.convert_content_to_string","title":"<code>convert_content_to_string(v)</code>  <code>classmethod</code>","text":"<p>Convert content to string if it's not already.</p> Source code in <code>langroid/agent/tools/orchestration.py</code> <pre><code>@field_validator(\"content\", mode=\"before\")\n@classmethod\ndef convert_content_to_string(cls, v: Any) -&gt; str:\n    \"\"\"Convert content to string if it's not already.\"\"\"\n    return str(v) if v is not None else \"\"\n</code></pre>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.ResultTool","title":"<code>ResultTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Class to use as a wrapper for sending arbitrary results from an Agent's agent_response or tool handlers, to: (a) trigger completion of the current task (similar to (Agent)DoneTool), and (b) be returned as the result of the current task, i.e. this tool would appear      in the resulting ChatDocument's <code>tool_messages</code> list. See test_tool_handlers_and_results in test_tool_messages.py, and examples/basic/tool-extract-short-example.py.</p> Note <ul> <li>when defining a tool handler or agent_response, you can directly return     ResultTool(field1 = val1, ...),     where the values can be arbitrary data structures, including nested     Pydantic objs, or you can define a subclass of ResultTool with the     fields you want to return.</li> <li>This is a special ToolMessage that is NOT meant to be used or handled     by an agent.</li> <li>AgentDoneTool is more restrictive in that you can only send a <code>content</code>     or <code>tools</code> in the result.</li> </ul>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.FinalResultTool","title":"<code>FinalResultTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Class to use as a wrapper for sending arbitrary results from an Agent's agent_response or tool handlers, to: (a) trigger completion of the current task as well as all parent tasks, and (b) be returned as the final result of the root task, i.e. this tool would appear      in the final ChatDocument's <code>tool_messages</code> list. See test_tool_handlers_and_results in test_tool_messages.py, and examples/basic/chat-tool-function.py.</p> Note <ul> <li>when defining a tool handler or agent_response, you can directly return     FinalResultTool(field1 = val1, ...),     where the values can be arbitrary data structures, including nested     Pydantic objs, or you can define a subclass of FinalResultTool with the     fields you want to return.</li> <li>This is a special ToolMessage that is NOT meant to be used by an agent's     llm_response, but only by agent_response or tool handlers.</li> <li>A subclass of this tool can be defined, with specific fields, and   with _allow_llm_use = True, to allow the LLM to generate this tool,   and have the effect of terminating the current and all parent tasks,   with the tool appearing in the final ChatDocument's <code>tool_messages</code> list.   See examples/basic/multi-agent-return-result.py.</li> </ul>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.PassTool","title":"<code>PassTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for \"passing\" on the received msg (ChatDocument), so that an as-yet-unspecified agent can handle it. Similar to ForwardTool, but without specifying the recipient agent.</p>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.PassTool.response","title":"<code>response(agent, chat_doc)</code>","text":"<p>When this tool is enabled for an Agent, this will result in a method added to the Agent with signature: <code>pass_tool(self, tool: PassTool, chat_doc: ChatDocument) -&gt; ChatDocument:</code></p> Source code in <code>langroid/agent/tools/orchestration.py</code> <pre><code>def response(self, agent: ChatAgent, chat_doc: ChatDocument) -&gt; ChatDocument:\n    \"\"\"When this tool is enabled for an Agent, this will result in a method\n    added to the Agent with signature:\n    `pass_tool(self, tool: PassTool, chat_doc: ChatDocument) -&gt; ChatDocument:`\n    \"\"\"\n    # if PassTool is in chat_doc, pass its parent, else pass chat_doc itself\n    doc = chat_doc\n    while True:\n        tools = agent.get_tool_messages(doc)\n        if not any(isinstance(t, type(self)) for t in tools):\n            break\n        if doc.parent is None:\n            break\n        doc = doc.parent\n    assert doc is not None, \"PassTool: parent of chat_doc must not be None\"\n    new_doc = ChatDocument.deepcopy(doc)\n    new_doc.metadata.sender = Entity.AGENT\n    return new_doc\n</code></pre>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.DonePassTool","title":"<code>DonePassTool</code>","text":"<p>               Bases: <code>PassTool</code></p> <p>Tool to signal DONE, AND Pass incoming/current msg as result. Similar to PassTool, except we append a DoneTool to the result tool_messages.</p>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.ForwardTool","title":"<code>ForwardTool</code>","text":"<p>               Bases: <code>PassTool</code></p> <p>Tool for forwarding the received msg (ChatDocument) to another agent or entity. Similar to PassTool, but with a specified recipient agent.</p>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.ForwardTool.response","title":"<code>response(agent, chat_doc)</code>","text":"<p>When this tool is enabled for an Agent, this will result in a method added to the Agent with signature: <code>forward_tool(self, tool: ForwardTool, chat_doc: ChatDocument) -&gt; ChatDocument:</code></p> Source code in <code>langroid/agent/tools/orchestration.py</code> <pre><code>def response(self, agent: ChatAgent, chat_doc: ChatDocument) -&gt; ChatDocument:\n    \"\"\"When this tool is enabled for an Agent, this will result in a method\n    added to the Agent with signature:\n    `forward_tool(self, tool: ForwardTool, chat_doc: ChatDocument) -&gt; ChatDocument:`\n    \"\"\"\n    # if chat_doc contains ForwardTool, then we forward its parent ChatDocument;\n    # else forward chat_doc itself\n    new_doc = PassTool.response(self, agent, chat_doc)\n    new_doc.metadata.recipient = self.agent\n    return new_doc\n</code></pre>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.SendTool","title":"<code>SendTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for agent or LLM to send content to a specified agent. Similar to RecipientTool.</p>"},{"location":"reference/agent/tools/orchestration/#langroid.agent.tools.orchestration.AgentSendTool","title":"<code>AgentSendTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool for Agent (i.e. agent_response) to send content or tool_messages to a specified agent. Similar to SendTool except that AgentSendTool is only usable by agent_response (or handler of another tool), to send content or tools to another agent. SendTool does not allow sending tools.</p>"},{"location":"reference/agent/tools/recipient_tool/","title":"recipient_tool","text":"<p>langroid/agent/tools/recipient_tool.py </p> <p>The <code>recipient_tool</code> is used to send a message to a specific recipient. Various methods from the RecipientTool and AddRecipientTool class are inserted into the Agent as methods (see <code>langroid/agent/base.py</code>, the method <code>_get_tool_list()</code>).</p> <p>See usage examples in <code>tests/main/test_multi_agent_complex.py</code> and <code>tests/main/test_recipient_tool.py</code>.</p> <p>A simpler alternative to this tool is <code>SendTool</code>, see here: https://github.com/langroid/langroid/blob/main/langroid/agent/tools/orchestration.py</p> <p>You can also define your own XML-based variant of this tool: https://github.com/langroid/langroid/blob/main/examples/basic/xml-tool.py which uses XML rather than JSON, and can be more reliable than JSON, especially with weaker LLMs.</p>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.AddRecipientTool","title":"<code>AddRecipientTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Used by LLM to add a recipient to the previous message, when it has forgotten to specify a recipient. This avoids having to re-generate the previous message (and thus saves token-cost and time).</p>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.AddRecipientTool.response","title":"<code>response(agent)</code>","text":"<p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; ChatDocument:\n    \"\"\"\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.recipient.\n    \"\"\"\n    print(\n        \"[red]RecipientTool: \"\n        f\"Added recipient {self.intended_recipient} to message.\"\n    )\n    if self.__class__._saved_content == \"\":\n        recipient_request_name = RecipientTool.default_value(\"request\")\n        content = f\"\"\"\n            Recipient specified but content is empty!\n            This could be because the `{self.request}` tool/function was used \n            before using `{recipient_request_name}` tool/function.\n            Resend the message using `{recipient_request_name}` tool/function.\n            \"\"\"\n    else:\n        content = self.__class__._saved_content  # use class-level attrib value\n        # erase content since we just used it.\n        self.__class__._saved_content = \"\"\n    return ChatDocument(\n        content=content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as it msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool","title":"<code>RecipientTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Used by LLM to send a message to a specific recipient.</p> <p>Useful in cases where an LLM is talking to 2 or more agents (or an Agent and human user), and needs to specify which agent (task) its message is intended for. The recipient name should be the name of a task (which is normally the name of the agent that the task wraps, although the task can have its own name).</p> <p>To use this tool/function-call, LLM must generate a JSON structure with these fields: {     \"request\": \"recipient_message\", # also the function name when using fn-calling     \"intended_recipient\": ,     \"content\":  } The effect of this is that <code>content</code> will be sent to the <code>intended_recipient</code> task."},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.create","title":"<code>create(recipients, default='')</code>  <code>classmethod</code>","text":"<p>Create a restricted version of RecipientTool that only allows certain recipients, and possibly sets a default recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef create(cls, recipients: List[str], default: str = \"\") -&gt; Type[\"RecipientTool\"]:\n    \"\"\"Create a restricted version of RecipientTool that\n    only allows certain recipients, and possibly sets a default recipient.\"\"\"\n\n    class RecipientToolRestricted(cls):  # type: ignore\n        allowed_recipients: ClassVar[List[str]] = recipients\n        default_recipient: ClassVar[str] = default\n\n    return RecipientToolRestricted\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.instructions","title":"<code>instructions()</code>  <code>classmethod</code>","text":"<p>Generate instructions for using this tool/function. These are intended to be appended to the system message of the LLM.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@classmethod\ndef instructions(cls) -&gt; str:\n    \"\"\"\n    Generate instructions for using this tool/function.\n    These are intended to be appended to the system message of the LLM.\n    \"\"\"\n    recipients = []\n    if has_field(cls, \"allowed_recipients\"):\n        recipients = cls.default_value(\"allowed_recipients\")\n    if len(recipients) &gt; 0:\n        recipients_str = \", \".join(recipients)\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to one of the following:\n        {recipients_str},\n        and setting the 'content' field to your message.\n        \"\"\"\n    else:\n        return f\"\"\"\n        Since you will be talking to multiple recipients, \n        you must clarify who your intended recipient is, using \n        the `{cls.default_value(\"request\")}` tool/function-call, by setting the \n        'intended_recipient' field to the name of the recipient, \n        and setting the 'content' field to your message.\n        \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.response","title":"<code>response(agent)</code>","text":"<p>When LLM has correctly used this tool, construct a ChatDocument with an explicit recipient, and make it look like it is from the LLM.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content and metadata.recipient set to self.intended_recipient.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    When LLM has correctly used this tool,\n    construct a ChatDocument with an explicit recipient,\n    and make it look like it is from the LLM.\n\n    Returns:\n        (ChatDocument): with content set to self.content and\n            metadata.recipient set to self.intended_recipient.\n    \"\"\"\n    default_recipient = self.__class__.default_value(\"default_recipient\")\n    if self.intended_recipient == \"\" and default_recipient not in [\"\", None]:\n        self.intended_recipient = default_recipient\n    elif self.intended_recipient == \"\":\n        # save the content as a class-variable, so that\n        # we can construct the ChatDocument once the LLM specifies a recipient.\n        # This avoids having to re-generate the entire message, saving time + cost.\n        AddRecipientTool._saved_content = self.content\n        agent.enable_message(AddRecipientTool)\n        return ChatDocument(\n            content=\"\"\"\n            Empty recipient field!\n            Please use the 'add_recipient' tool/function-call to specify who your \n            message is intended for.\n            DO NOT REPEAT your original message; ONLY specify the recipient via this\n            tool/function-call.\n            \"\"\",\n            metadata=ChatDocMetaData(\n                sender=Entity.AGENT,\n                recipient=Entity.LLM,\n            ),\n        )\n\n    print(\"[red]RecipientTool: Validated properly addressed message\")\n\n    return ChatDocument(\n        content=self.content,\n        metadata=ChatDocMetaData(\n            recipient=self.intended_recipient,\n            # we are constructing this so it looks as if msg is from LLM\n            sender=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/recipient_tool/#langroid.agent.tools.recipient_tool.RecipientTool.handle_message_fallback","title":"<code>handle_message_fallback(agent, msg)</code>  <code>staticmethod</code>","text":"<p>Response of agent if this tool is not used, e.g. the LLM simply sends a message without using this tool. This method has two purposes: (a) Alert the LLM that it has forgotten to specify a recipient, and prod it     to use the <code>add_recipient</code> tool to specify just the recipient     (and not re-generate the entire message). (b) Save the content of the message in the agent's <code>content</code> field,     so the agent can construct a ChatDocument with this content once LLM     later specifies a recipient using the <code>add_recipient</code> tool.</p> <p>This method is used to set the agent's handle_message_fallback() method.</p> <p>Returns:</p> Type Description <code>str</code> <p>reminder to LLM to use the <code>add_recipient</code> tool.</p> Source code in <code>langroid/agent/tools/recipient_tool.py</code> <pre><code>@staticmethod\ndef handle_message_fallback(\n    agent: ChatAgent, msg: str | ChatDocument\n) -&gt; str | ChatDocument | None:\n    \"\"\"\n    Response of agent if this tool is not used, e.g.\n    the LLM simply sends a message without using this tool.\n    This method has two purposes:\n    (a) Alert the LLM that it has forgotten to specify a recipient, and prod it\n        to use the `add_recipient` tool to specify just the recipient\n        (and not re-generate the entire message).\n    (b) Save the content of the message in the agent's `content` field,\n        so the agent can construct a ChatDocument with this content once LLM\n        later specifies a recipient using the `add_recipient` tool.\n\n    This method is used to set the agent's handle_message_fallback() method.\n\n    Returns:\n        (str): reminder to LLM to use the `add_recipient` tool.\n    \"\"\"\n    # Note: once the LLM specifies a missing recipient, the task loop\n    # mechanism will not allow any of the \"native\" responders to respond,\n    # since the recipient will differ from the task name.\n    # So if this method is called, we can be sure that the recipient has not\n    # been specified.\n    if (\n        isinstance(msg, str)\n        or msg.metadata.sender != Entity.LLM\n        or msg.metadata.recipient != \"\"  # there IS an explicit recipient\n    ):\n        return None\n    content = msg if isinstance(msg, str) else msg.content\n    # save the content as a class-variable, so that\n    # we can construct the ChatDocument once the LLM specifies a recipient.\n    # This avoids having to re-generate the entire message, saving time + cost.\n    AddRecipientTool._saved_content = content\n    agent.enable_message(AddRecipientTool)\n    print(\"[red]RecipientTool: Recipient not specified, asking LLM to clarify.\")\n    return ChatDocument(\n        content=\"\"\"\n        Please use the 'add_recipient' tool/function-call to specify who your \n        `intended_recipient` is.\n        DO NOT REPEAT your original message; ONLY specify the \n        `intended_recipient` via this tool/function-call.\n        \"\"\",\n        metadata=ChatDocMetaData(\n            sender=Entity.AGENT,\n            recipient=Entity.LLM,\n        ),\n    )\n</code></pre>"},{"location":"reference/agent/tools/retrieval_tool/","title":"retrieval_tool","text":"<p>langroid/agent/tools/retrieval_tool.py </p>"},{"location":"reference/agent/tools/retrieval_tool/#langroid.agent.tools.retrieval_tool.RetrievalTool","title":"<code>RetrievalTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Retrieval tool, only to be used by a DocChatAgent. The handler method is defined in DocChatAgent.retrieval_tool</p>"},{"location":"reference/agent/tools/rewind_tool/","title":"rewind_tool","text":"<p>langroid/agent/tools/rewind_tool.py </p> <p>The <code>rewind_tool</code> is used to rewind to the <code>n</code>th previous Assistant message and replace it with a new <code>content</code>. This is useful in several scenarios and - saves token-cost + inference time, - reduces distracting clutter in chat history, which helps improve response quality.</p> <p>This is intended to mimic how a human user might use a chat interface, where they go down a conversation path, and want to go back in history to \"edit and re-submit\" a previous message, to get a better response.</p> <p>See usage examples in <code>tests/main/test_rewind_tool.py</code>.</p>"},{"location":"reference/agent/tools/rewind_tool/#langroid.agent.tools.rewind_tool.RewindTool","title":"<code>RewindTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Used by LLM to rewind (i.e. backtrack) to the <code>n</code>th Assistant message and replace with a new msg.</p>"},{"location":"reference/agent/tools/rewind_tool/#langroid.agent.tools.rewind_tool.RewindTool.response","title":"<code>response(agent)</code>","text":"<p>Define the tool-handler method for this tool here itself, since it is a generic tool whose functionality should be the same for any agent.</p> <p>When LLM has correctly used this tool, rewind this agent's <code>message_history</code> to the <code>n</code>th assistant msg, and replace it with <code>content</code>. We need to mock it as if the LLM is sending this message.</p> <p>Within a multi-agent scenario, this also means that any other messages dependent on this message will need to be invalidated -- so go down the chain of child messages and clear each agent's history back to the <code>msg_idx</code> corresponding to the child message.</p> <p>Returns:</p> Type Description <code>ChatDocument</code> <p>with content set to self.content.</p> Source code in <code>langroid/agent/tools/rewind_tool.py</code> <pre><code>def response(self, agent: ChatAgent) -&gt; str | ChatDocument:\n    \"\"\"\n    Define the tool-handler method for this tool here itself,\n    since it is a generic tool whose functionality should be the\n    same for any agent.\n\n    When LLM has correctly used this tool, rewind this agent's\n    `message_history` to the `n`th assistant msg, and replace it with `content`.\n    We need to mock it as if the LLM is sending this message.\n\n    Within a multi-agent scenario, this also means that any other messages dependent\n    on this message will need to be invalidated --\n    so go down the chain of child messages and clear each agent's history\n    back to the `msg_idx` corresponding to the child message.\n\n    Returns:\n        (ChatDocument): with content set to self.content.\n    \"\"\"\n    idx = agent.nth_message_idx_with_role(lm.Role.ASSISTANT, self.n)\n    if idx &lt; 0:\n        # set up a corrective message from AGENT\n        msg = f\"\"\"\n            Could not rewind to {self.n}th Assistant message!\n            Please check the value of `n` and try again.\n            Or it may be too early to use the `rewind_tool`.\n            \"\"\"\n        return agent.create_agent_response(msg)\n\n    parent = prune_messages(agent, idx)\n\n    # create ChatDocument with new content, to be returned as result of this tool\n    result_doc = agent.create_llm_response(self.content)\n    result_doc.metadata.parent_id = \"\" if parent is None else parent.id()\n    result_doc.metadata.agent_id = agent.id\n    result_doc.metadata.msg_idx = idx\n\n    # replace the message at idx with this new message\n    agent.message_history.extend(ChatDocument.to_LLMMessage(result_doc))\n\n    # set the replaced doc's parent's child to this result_doc\n    if parent is not None:\n        # first remove the this parent's child from registry\n        ChatDocument.delete_id(parent.metadata.child_id)\n        parent.metadata.child_id = result_doc.id()\n    return result_doc\n</code></pre>"},{"location":"reference/agent/tools/rewind_tool/#langroid.agent.tools.rewind_tool.prune_messages","title":"<code>prune_messages(agent, idx)</code>","text":"<p>Clear the message history of agent, starting at index <code>idx</code>, taking care to first clear all dependent messages (possibly from other agents' message histories) that are linked to the message at <code>idx</code>, via the <code>child_id</code> field of the <code>metadata</code> field of the ChatDocument linked from the message at <code>idx</code>.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>ChatAgent</code> <p>The agent whose message history is to be pruned.</p> required <code>idx</code> <code>int</code> <p>The index from which to start clearing the message history.</p> required <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>The parent ChatDocument of the ChatDocument linked from the message at <code>idx</code>,</p> <code>ChatDocument | None</code> <p>if it exists, else None.</p> Source code in <code>langroid/agent/tools/rewind_tool.py</code> <pre><code>def prune_messages(agent: ChatAgent, idx: int) -&gt; ChatDocument | None:\n    \"\"\"\n    Clear the message history of agent, starting at index `idx`,\n    taking care to first clear all dependent messages (possibly from other agents'\n    message histories) that are linked to the message at `idx`, via the `child_id` field\n    of the `metadata` field of the ChatDocument linked from the message at `idx`.\n\n    Args:\n        agent (ChatAgent): The agent whose message history is to be pruned.\n        idx (int): The index from which to start clearing the message history.\n\n    Returns:\n        The parent ChatDocument of the ChatDocument linked from the message at `idx`,\n        if it exists, else None.\n\n    \"\"\"\n    assert idx &gt;= 0, \"Invalid index for message history!\"\n    chat_doc_id = agent.message_history[idx].chat_document_id\n    chat_doc = ChatDocument.from_id(chat_doc_id)\n    assert chat_doc is not None, \"ChatDocument not found in registry!\"\n\n    parent = ChatDocument.from_id(chat_doc.metadata.parent_id)  # may be None\n    # We're invaliding the msg at idx,\n    # so starting with chat_doc, go down the child links\n    # and clear history of each agent, to the msg_idx\n    curr_doc = chat_doc\n    while child_doc := curr_doc.metadata.child:\n        if child_doc.metadata.msg_idx &gt;= 0:\n            child_agent = ChatAgent.from_id(child_doc.metadata.agent_id)\n            if child_agent is not None:\n                child_agent.clear_history(child_doc.metadata.msg_idx)\n        curr_doc = child_doc\n\n    # Clear out ObjectRegistry entries for this ChatDocuments\n    # and all descendants (in case they weren't already cleared above)\n    ChatDocument.delete_id(chat_doc.id())\n\n    # Finally, clear this agent's history back to idx,\n    # and replace the msg at idx with the new content\n    agent.clear_history(idx)\n    return parent\n</code></pre>"},{"location":"reference/agent/tools/segment_extract_tool/","title":"segment_extract_tool","text":"<p>langroid/agent/tools/segment_extract_tool.py </p> <p>A tool to extract segment numbers from the last user message, containing numbered segments.</p> <p>The idea is that when an LLM wants to (or is asked to) simply extract portions of a message verbatim, it should use this tool/function to SPECIFY what should be extracted, rather than actually extracting it. The output will be in the form of a list of segment numbers or ranges. This will usually be much cheaper and faster than actually writing out the extracted text. The handler of this tool/function will then extract the text and send it back.</p>"},{"location":"reference/agent/tools/task_tool/","title":"task_tool","text":"<p>langroid/agent/tools/task_tool.py </p> A tool that allows agents to delegate a task to a sub-agent with <p>specific tools enabled.</p>"},{"location":"reference/agent/tools/task_tool/#langroid.agent.tools.task_tool.TaskTool","title":"<code>TaskTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p> <p>Tool that spawns a sub-agent with specified tools to handle a task.</p> <p>The sub-agent can be given a custom name for identification in logs. If no name is provided, a random unique name starting with 'agent' will be generated.</p>"},{"location":"reference/agent/tools/task_tool/#langroid.agent.tools.task_tool.TaskTool.handle","title":"<code>handle(agent, chat_doc=None)</code>","text":"<p>Handle the TaskTool by creating a sub-agent with specified tools and running the task non-interactively.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>ChatAgent</code> <p>The parent ChatAgent that is handling this tool</p> required <code>chat_doc</code> <code>Optional[ChatDocument]</code> <p>The ChatDocument containing this tool message</p> <code>None</code> Source code in <code>langroid/agent/tools/task_tool.py</code> <pre><code>def handle(\n    self, agent: ChatAgent, chat_doc: Optional[ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n\n    Handle the TaskTool by creating a sub-agent with specified tools\n    and running the task non-interactively.\n\n    Args:\n        agent: The parent ChatAgent that is handling this tool\n        chat_doc: The ChatDocument containing this tool message\n    \"\"\"\n\n    task = self._set_up_task(agent)\n\n    # Create a ChatDocument for the prompt with parent pointer\n    prompt_doc = None\n    if chat_doc is not None:\n        from langroid.agent.chat_document import ChatDocMetaData\n\n        prompt_doc = ChatDocument(\n            content=self.prompt,\n            metadata=ChatDocMetaData(\n                parent_id=chat_doc.id(),\n                agent_id=agent.id,\n                sender=chat_doc.metadata.sender,\n            ),\n        )\n        # Set bidirectional parent-child relationship\n        chat_doc.metadata.child_id = prompt_doc.id()\n\n    # Run the task with the ChatDocument or string prompt\n    result = task.run(prompt_doc or self.prompt, turns=self.max_iterations or 10)\n    return result\n</code></pre>"},{"location":"reference/agent/tools/task_tool/#langroid.agent.tools.task_tool.TaskTool.handle_async","title":"<code>handle_async(agent, chat_doc=None)</code>  <code>async</code>","text":"<p>Async method to handle the TaskTool by creating a sub-agent with specified tools and running the task non-interactively.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>ChatAgent</code> <p>The parent ChatAgent that is handling this tool</p> required <code>chat_doc</code> <code>Optional[ChatDocument]</code> <p>The ChatDocument containing this tool message</p> <code>None</code> Source code in <code>langroid/agent/tools/task_tool.py</code> <pre><code>async def handle_async(\n    self, agent: ChatAgent, chat_doc: Optional[ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n    \"\"\"\n    Async method to handle the TaskTool by creating a sub-agent with specified tools\n    and running the task non-interactively.\n\n    Args:\n        agent: The parent ChatAgent that is handling this tool\n        chat_doc: The ChatDocument containing this tool message\n    \"\"\"\n    task = self._set_up_task(agent)\n\n    # Create a ChatDocument for the prompt with parent pointer\n    prompt_doc = None\n    if chat_doc is not None:\n        from langroid.agent.chat_document import ChatDocMetaData\n\n        prompt_doc = ChatDocument(\n            content=self.prompt,\n            metadata=ChatDocMetaData(\n                parent_id=chat_doc.id(),\n                agent_id=agent.id,\n                sender=chat_doc.metadata.sender,\n            ),\n        )\n        # Set bidirectional parent-child relationship\n        chat_doc.metadata.child_id = prompt_doc.id()\n\n    # Run the task with the ChatDocument or string prompt\n    # TODO eventually allow the various task setup configs,\n    #  including termination conditions\n    result = await task.run_async(\n        prompt_doc or self.prompt, turns=self.max_iterations or 10\n    )\n    return result\n</code></pre>"},{"location":"reference/agent/tools/tavily_search_tool/","title":"tavily_search_tool","text":"<p>langroid/agent/tools/tavily_search_tool.py </p> <p>A tool to trigger a Tavily search for a given query, and return the top results with their titles, links, summaries. Since the tool is stateless (i.e. does not need access to agent state), it can be enabled for any agent, without having to define a special method inside the agent: <code>agent.enable_message(TavilySearchTool)</code></p>"},{"location":"reference/agent/tools/tavily_search_tool/#langroid.agent.tools.tavily_search_tool.TavilySearchTool","title":"<code>TavilySearchTool</code>","text":"<p>               Bases: <code>ToolMessage</code></p>"},{"location":"reference/agent/tools/tavily_search_tool/#langroid.agent.tools.tavily_search_tool.TavilySearchTool.handle","title":"<code>handle()</code>","text":"<p>Conducts a search using Tavily based on the provided query and number of results by triggering a tavily_search.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted string containing the titles, links, and summaries of each search result, separated by two newlines.</p> Source code in <code>langroid/agent/tools/tavily_search_tool.py</code> <pre><code>def handle(self) -&gt; str:\n    \"\"\"\n    Conducts a search using Tavily based on the provided query\n    and number of results by triggering a tavily_search.\n\n    Returns:\n        str: A formatted string containing the titles, links, and\n            summaries of each search result, separated by two newlines.\n    \"\"\"\n    search_results = tavily_search(self.query, self.num_results)\n    # return Title, Link, Summary of each result, separated by two newlines\n    results_str = \"\\n\\n\".join(str(result) for result in search_results)\n    return f\"\"\"\n    BELOW ARE THE RESULTS FROM THE WEB SEARCH. USE THESE TO COMPOSE YOUR RESPONSE:\n    {results_str}\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/tools/mcp/","title":"mcp","text":"<p>langroid/agent/tools/mcp/init.py </p>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.FastMCPClient","title":"<code>FastMCPClient(server, persist_connection=False, forward_images=True, forward_text_resources=False, forward_blob_resources=False, sampling_handler=None, roots=None, log_handler=None, message_handler=None, read_timeout_seconds=None)</code>","text":"<p>A client for interacting with a FastMCP server.</p> <p>Provides async context manager functionality to safely manage resources.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>FastMCP server or path to such a server</p> required Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>def __init__(\n    self,\n    server: FastMCPServerSpec,\n    persist_connection: bool = False,\n    forward_images: bool = True,\n    forward_text_resources: bool = False,\n    forward_blob_resources: bool = False,\n    sampling_handler: SamplingHandler | None = None,  # type: ignore\n    roots: RootsList | RootsHandler | None = None,  # type: ignore\n    log_handler: LoggingFnT | None = None,\n    message_handler: MessageHandlerFnT | None = None,\n    read_timeout_seconds: datetime.timedelta | None = None,\n) -&gt; None:\n    \"\"\"Initialize the FastMCPClient.\n\n    Args:\n        server: FastMCP server or path to such a server\n    \"\"\"\n    self.server = server\n    self.client = None\n    self._cm = None\n    self.sampling_handler = sampling_handler\n    self.roots = roots\n    self.log_handler = log_handler\n    self.message_handler = message_handler\n    # Default a slightly larger read timeout for stdio transports on first\n    # connects. Allows flaky subprocess servers a bit more time to boot.\n    if read_timeout_seconds is None:\n        try:\n            default_secs = int(os.getenv(\"LANGROID_MCP_READ_TIMEOUT\", \"15\"))\n            self.read_timeout_seconds = datetime.timedelta(seconds=default_secs)\n        except Exception:\n            self.read_timeout_seconds = None\n    else:\n        self.read_timeout_seconds = read_timeout_seconds\n    self.persist_connection = persist_connection\n    self.forward_text_resources = forward_text_resources\n    self.forward_blob_resources = forward_blob_resources\n    self.forward_images = forward_images\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.FastMCPClient.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Open the underlying session.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"Open the underlying session.\"\"\"\n    await self.__aenter__()\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.FastMCPClient.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the underlying session.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the underlying session.\"\"\"\n    await self.__aexit__(None, None, None)\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.FastMCPClient.get_tool_async","title":"<code>get_tool_async(tool_name)</code>  <code>async</code>","text":"<p>Create a Langroid ToolMessage subclass from the MCP Tool with the given <code>tool_name</code>.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_tool_async(self, tool_name: str) -&gt; Type[ToolMessage]:\n    \"\"\"\n    Create a Langroid ToolMessage subclass from the MCP Tool\n    with the given `tool_name`.\n    \"\"\"\n    if not self.client:\n        if self.persist_connection:\n            await self.connect()\n            assert self.client\n        else:\n            raise RuntimeError(\n                \"Client not initialized. Use async with FastMCPClient.\"\n            )\n    target = await self.get_mcp_tool_async(tool_name)\n    if target is None:\n        raise ValueError(f\"No tool named {tool_name}\")\n    props = target.inputSchema.get(\"properties\", {})\n    # Get the list of required fields from JSON Schema\n    required_fields = set(target.inputSchema.get(\"required\", []))\n    fields: Dict[str, Tuple[type, Any]] = {}\n    for fname, schema in props.items():\n        ftype, fld = self._schema_to_field(\n            fname, schema, target.name, is_required=fname in required_fields\n        )\n        fields[fname] = (ftype, fld)\n\n    # Convert target.name to CamelCase and add Tool suffix\n    parts = target.name.replace(\"-\", \"_\").split(\"_\")\n    camel_case = \"\".join(part.capitalize() for part in parts)\n    model_name = f\"{camel_case}Tool\"\n\n    from langroid.agent.tool_message import ToolMessage as _BaseToolMessage\n\n    # IMPORTANT: Avoid clashes with reserved field names in Langroid ToolMessage!\n    # First figure out which field names are reserved\n    reserved = set(_BaseToolMessage.__annotations__.keys())\n    reserved.update([\"recipient\", \"_handler\", \"name\"])\n    renamed: Dict[str, str] = {}\n    new_fields: Dict[str, Tuple[type, Any]] = {}\n    for fname, (ftype, fld) in fields.items():\n        if fname in reserved:\n            new_name = fname + \"__\"\n            renamed[fname] = new_name\n            new_fields[new_name] = (ftype, fld)\n        else:\n            new_fields[fname] = (ftype, fld)\n    # now replace fields with our renamed\u2010aware mapping\n    fields = new_fields\n\n    # create Langroid ToolMessage subclass, with expected fields.\n    tool_model = cast(\n        Type[ToolMessage],\n        create_model(  # type: ignore[call-overload]\n            model_name,\n            request=(str, target.name),\n            purpose=(str, target.description or f\"Use the tool {target.name}\"),\n            __base__=ToolMessage,\n            **fields,\n        ),\n    )\n    # Store ALL client configuration needed to recreate a client\n    client_config = {\n        # Always store a SERVER FACTORY to ensure a fresh transport per call\n        \"server\": self._as_server_factory(self.server),\n        \"sampling_handler\": self.sampling_handler,\n        \"roots\": self.roots,\n        \"log_handler\": self.log_handler,\n        \"message_handler\": self.message_handler,\n        \"read_timeout_seconds\": self.read_timeout_seconds,\n    }\n\n    tool_model._client_config = client_config  # type: ignore [attr-defined]\n    tool_model._renamed_fields = renamed  # type: ignore[attr-defined]\n\n    # 2) define an arg-free call_tool_async()\n    async def call_tool_async(itself: ToolMessage) -&gt; Any:\n        from langroid.agent.tools.mcp.fastmcp_client import FastMCPClient\n\n        # pack up the payload\n        # Get exclude fields from model config with proper type checking\n        exclude_fields = set()\n        model_config = getattr(itself, \"model_config\", {})\n        if (\n            isinstance(model_config, dict)\n            and \"json_schema_extra\" in model_config\n            and model_config[\"json_schema_extra\"] is not None\n            and isinstance(model_config[\"json_schema_extra\"], dict)\n            and \"exclude\" in model_config[\"json_schema_extra\"]\n        ):\n            exclude_list = model_config[\"json_schema_extra\"][\"exclude\"]\n            if isinstance(exclude_list, (list, set, tuple)):\n                exclude_fields = set(exclude_list)\n\n        # Add standard excluded fields\n        exclude_fields.update([\"request\", \"purpose\"])\n\n        # Exclude None values - MCP servers don't expect None for optional params\n        payload = itself.model_dump(exclude=exclude_fields, exclude_none=True)\n\n        # restore any renamed fields\n        for orig, new in itself.__class__._renamed_fields.items():  # type: ignore\n            if new in payload:\n                payload[orig] = payload.pop(new)\n\n        client_cfg = getattr(itself.__class__, \"_client_config\", None)  # type: ignore\n        if not client_cfg:\n            # Fallback or error - ideally _client_config should always exist\n            raise RuntimeError(f\"Client config missing on {itself.__class__}\")\n\n        # Connect the client if not yet connected and keep the connection open\n        if self.persist_connection:\n            if not self.client:\n                await self.connect()\n\n            return await self.call_mcp_tool(itself.request, payload)\n\n        # open a fresh client, call the tool, then close\n        async with FastMCPClient(**client_cfg) as client:  # type: ignore\n            return await client.call_mcp_tool(itself.request, payload)\n\n    tool_model.call_tool_async = call_tool_async  # type: ignore\n\n    if not hasattr(tool_model, \"handle_async\"):\n        # 3) define handle_async() method with optional agent parameter\n        from typing import Union\n\n        async def handle_async(\n            self: ToolMessage, agent: Optional[Agent] = None\n        ) -&gt; Union[str, Optional[ChatDocument]]:\n            \"\"\"\n            Auto-generated handler for MCP tool. Returns ChatDocument with files\n            if files are present and agent is provided, otherwise returns text.\n\n            To override: define your own handle_async method with matching signature\n            if you need file handling, or simpler signature if you only need text.\n            \"\"\"\n            response = await self.call_tool_async()  # type: ignore[attr-defined]\n            if response is None:\n                return None\n\n            content, files = response\n\n            # If we have files and an agent is provided, return a ChatDocument\n            if files and agent is not None:\n                return agent.create_agent_response(\n                    content=content,\n                    files=files,\n                )\n            else:\n                # Otherwise, just return the text content\n                return str(content) if content is not None else None\n\n        # add the handle_async() method to the tool model\n        tool_model.handle_async = handle_async  # type: ignore\n\n    return tool_model\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.FastMCPClient.get_tools_async","title":"<code>get_tools_async()</code>  <code>async</code>","text":"<p>Get all available tools as Langroid ToolMessage classes, handling nested schemas, with <code>handle_async</code> methods</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_tools_async(self) -&gt; List[Type[ToolMessage]]:\n    \"\"\"\n    Get all available tools as Langroid ToolMessage classes,\n    handling nested schemas, with `handle_async` methods\n    \"\"\"\n    if not self.client:\n        if self.persist_connection:\n            await self.connect()\n            assert self.client\n        else:\n            raise RuntimeError(\n                \"Client not initialized. Use async with FastMCPClient.\"\n            )\n    resp = await self.client.list_tools()\n    return [await self.get_tool_async(t.name) for t in resp]\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.FastMCPClient.get_mcp_tool_async","title":"<code>get_mcp_tool_async(name)</code>  <code>async</code>","text":"<p>Find the \"original\" MCP Tool (i.e. of type mcp.types.Tool) on the server  matching <code>name</code>, or None if missing. This contains the metadata for the tool:  name, description, inputSchema, etc.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the tool to look up.</p> required <p>Returns:</p> Type Description <code>Optional[Tool]</code> <p>The raw Tool object from the server, or None.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_mcp_tool_async(self, name: str) -&gt; Optional[Tool]:\n    \"\"\"Find the \"original\" MCP Tool (i.e. of type mcp.types.Tool) on the server\n     matching `name`, or None if missing. This contains the metadata for the tool:\n     name, description, inputSchema, etc.\n\n    Args:\n        name: Name of the tool to look up.\n\n    Returns:\n        The raw Tool object from the server, or None.\n    \"\"\"\n    if not self.client:\n        if self.persist_connection:\n            await self.connect()\n            assert self.client\n        else:\n            raise RuntimeError(\n                \"Client not initialized. Use async with FastMCPClient.\"\n            )\n    resp: List[Tool] = await self.client.list_tools()\n    return next((t for t in resp if t.name == name), None)\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.FastMCPClient.call_mcp_tool","title":"<code>call_mcp_tool(tool_name, arguments)</code>  <code>async</code>","text":"<p>Call an MCP tool with the given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool to call.</p> required <code>arguments</code> <code>Dict[str, Any]</code> <p>Arguments to pass to the tool.</p> required <p>Returns:</p> Type Description <code>Optional[tuple[str, list[FileAttachment]]]</code> <p>The result of the tool call.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def call_mcp_tool(\n    self, tool_name: str, arguments: Dict[str, Any]\n) -&gt; Optional[tuple[str, list[FileAttachment]]]:\n    \"\"\"Call an MCP tool with the given arguments.\n\n    Args:\n        tool_name: Name of the tool to call.\n        arguments: Arguments to pass to the tool.\n\n    Returns:\n        The result of the tool call.\n    \"\"\"\n    if not self.client:\n        if self.persist_connection:\n            await self.connect()\n            assert self.client\n        else:\n            raise RuntimeError(\n                \"Client not initialized. Use async with FastMCPClient.\"\n            )\n    # Prefer validated call; if server fails to provide structured content\n    # despite declaring a schema, fall back to a raw request to bypass\n    # client-side validation and still surface the data.\n    try:\n        result: CallToolResult = await self.client.session.call_tool(\n            tool_name,\n            arguments,\n        )\n    except RuntimeError as e:\n        msg = str(e)\n        if \"has an output schema but did not return structured content\" not in msg:\n            raise\n        from mcp.types import (\n            CallToolRequest,\n            CallToolRequestParams,\n            ClientRequest,\n        )\n        from mcp.types import (\n            CallToolResult as _CallToolResult,\n        )\n\n        result = await self.client.session.send_request(  # type: ignore[assignment]\n            ClientRequest(\n                CallToolRequest(\n                    params=CallToolRequestParams(\n                        name=tool_name, arguments=arguments\n                    )\n                )\n            ),\n            _CallToolResult,\n        )\n    results = self._convert_tool_result(tool_name, result)\n\n    if isinstance(results, str):\n        return results, []\n\n    return results\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.mcp_tool","title":"<code>mcp_tool(server, tool_name)</code>","text":"<p>Decorator: declare a ToolMessage class bound to a FastMCP tool.</p> Usage <p>@mcp_tool(\"/path/to/server.py\", \"get_weather\") class WeatherTool:     def pretty(self) -&gt; str:         return f\"Temp is {self.temperature}\"</p> <p>The <code>server</code> may be a string/URL/FastMCP/ClientTransport, or a zero-arg callable returning one of those, e.g. <code>lambda: StdioTransport(...)</code>. Using a factory ensures a fresh transport per connection under fastmcp&gt;=2.13.</p> Source code in <code>langroid/agent/tools/mcp/decorators.py</code> <pre><code>def mcp_tool(\n    server: FastMCPServerSpec, tool_name: str\n) -&gt; Callable[[Type[ToolMessage]], Type[ToolMessage]]:\n    \"\"\"Decorator: declare a ToolMessage class bound to a FastMCP tool.\n\n    Usage:\n        @mcp_tool(\"/path/to/server.py\", \"get_weather\")\n        class WeatherTool:\n            def pretty(self) -&gt; str:\n                return f\"Temp is {self.temperature}\"\n\n    The `server` may be a string/URL/FastMCP/ClientTransport, or a zero-arg\n    callable returning one of those, e.g. `lambda: StdioTransport(...)`. Using a\n    factory ensures a fresh transport per connection under fastmcp&gt;=2.13.\n    \"\"\"\n\n    def decorator(user_cls: Type[ToolMessage]) -&gt; Type[ToolMessage]:\n        # build the \u201creal\u201d ToolMessage subclass for this server/tool\n        RealTool: Type[ToolMessage] = get_tool(server, tool_name)\n\n        # copy user\u2010defined methods / attributes onto RealTool\n        for name, attr in user_cls.__dict__.items():\n            if name.startswith(\"__\") and name.endswith(\"__\"):\n                continue\n            setattr(RealTool, name, attr)\n\n        # preserve the user\u2019s original name if you like:\n        RealTool.__name__ = user_cls.__name__\n        return RealTool\n\n    return decorator\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.get_tool","title":"<code>get_tool(server, tool_name, **client_kwargs)</code>","text":"<p>Get a single Langroid ToolMessage subclass for a specific MCP tool name (synchronous).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient and runs the async <code>get_tool_async</code> function using <code>asyncio.run()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>tool_name</code> <code>str</code> <p>The name of the tool to retrieve.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor (e.g., sampling_handler, roots).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Type[ToolMessage]</code> <p>A dynamically created Langroid ToolMessage subclass representing the</p> <code>Type[ToolMessage]</code> <p>requested tool.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>def get_tool(\n    server: FastMCPServerSpec,\n    tool_name: str,\n    **client_kwargs: Any,\n) -&gt; Type[ToolMessage]:\n    \"\"\"Get a single Langroid ToolMessage subclass\n    for a specific MCP tool name (synchronous).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient and runs the\n    async `get_tool_async` function using `asyncio.run()`.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        tool_name: The name of the tool to retrieve.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor (e.g., sampling_handler, roots).\n\n    Returns:\n        A dynamically created Langroid ToolMessage subclass representing the\n        requested tool.\n    \"\"\"\n    return asyncio.run(get_tool_async(server, tool_name, **client_kwargs))\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.get_tool_async","title":"<code>get_tool_async(server, tool_name, **client_kwargs)</code>  <code>async</code>","text":"<p>Get a single Langroid ToolMessage subclass for a specific MCP tool name (async).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>tool_name</code> <code>str</code> <p>The name of the tool to retrieve.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor (e.g., sampling_handler, roots).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Type[ToolMessage]</code> <p>A dynamically created Langroid ToolMessage subclass representing the</p> <code>Type[ToolMessage]</code> <p>requested tool.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_tool_async(\n    server: FastMCPServerSpec,\n    tool_name: str,\n    **client_kwargs: Any,\n) -&gt; Type[ToolMessage]:\n    \"\"\"Get a single Langroid ToolMessage subclass for a specific MCP tool name (async).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        tool_name: The name of the tool to retrieve.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor (e.g., sampling_handler, roots).\n\n    Returns:\n        A dynamically created Langroid ToolMessage subclass representing the\n        requested tool.\n    \"\"\"\n    async with FastMCPClient(server, **client_kwargs) as client:\n        return await client.get_tool_async(tool_name)\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.get_tools","title":"<code>get_tools(server, **client_kwargs)</code>","text":"<p>Get all available tools as Langroid ToolMessage subclasses (synchronous).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient and runs the async <code>get_tools_async</code> function using <code>asyncio.run()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor (e.g., sampling_handler, roots).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Type[ToolMessage]]</code> <p>A list of dynamically created Langroid ToolMessage subclasses</p> <code>List[Type[ToolMessage]]</code> <p>representing all available tools on the server.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>def get_tools(\n    server: FastMCPServerSpec,\n    **client_kwargs: Any,\n) -&gt; List[Type[ToolMessage]]:\n    \"\"\"Get all available tools as Langroid ToolMessage subclasses (synchronous).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient and runs the\n    async `get_tools_async` function using `asyncio.run()`.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor (e.g., sampling_handler, roots).\n\n    Returns:\n        A list of dynamically created Langroid ToolMessage subclasses\n        representing all available tools on the server.\n    \"\"\"\n    return asyncio.run(get_tools_async(server, **client_kwargs))\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.get_tools_async","title":"<code>get_tools_async(server, **client_kwargs)</code>  <code>async</code>","text":"<p>Get all available tools as Langroid ToolMessage subclasses (async).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor (e.g., sampling_handler, roots).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Type[ToolMessage]]</code> <p>A list of dynamically created Langroid ToolMessage subclasses</p> <code>List[Type[ToolMessage]]</code> <p>representing all available tools on the server.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_tools_async(\n    server: FastMCPServerSpec,\n    **client_kwargs: Any,\n) -&gt; List[Type[ToolMessage]]:\n    \"\"\"Get all available tools as Langroid ToolMessage subclasses (async).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor (e.g., sampling_handler, roots).\n\n    Returns:\n        A list of dynamically created Langroid ToolMessage subclasses\n        representing all available tools on the server.\n    \"\"\"\n    async with FastMCPClient(server, **client_kwargs) as client:\n        return await client.get_tools_async()\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.get_mcp_tool_async","title":"<code>get_mcp_tool_async(server, name, **client_kwargs)</code>  <code>async</code>","text":"<p>Get the raw MCP Tool object for a specific tool name (async).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient to retrieve the tool definition from the server.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>name</code> <code>str</code> <p>The name of the tool to look up.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Tool]</code> <p>The raw <code>mcp.types.Tool</code> object from the server, or <code>None</code> if the tool</p> <code>Optional[Tool]</code> <p>is not found.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_mcp_tool_async(\n    server: FastMCPServerSpec,\n    name: str,\n    **client_kwargs: Any,\n) -&gt; Optional[Tool]:\n    \"\"\"Get the raw MCP Tool object for a specific tool name (async).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient to\n    retrieve the tool definition from the server.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        name: The name of the tool to look up.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor.\n\n    Returns:\n        The raw `mcp.types.Tool` object from the server, or `None` if the tool\n        is not found.\n    \"\"\"\n    async with FastMCPClient(server, **client_kwargs) as client:\n        return await client.get_mcp_tool_async(name)\n</code></pre>"},{"location":"reference/agent/tools/mcp/#langroid.agent.tools.mcp.get_mcp_tools_async","title":"<code>get_mcp_tools_async(server, **client_kwargs)</code>  <code>async</code>","text":"<p>Get all available raw MCP Tool objects from the server (async).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient to retrieve the list of tool definitions from the server.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Tool]</code> <p>A list of raw <code>mcp.types.Tool</code> objects available on the server.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_mcp_tools_async(\n    server: FastMCPServerSpec,\n    **client_kwargs: Any,\n) -&gt; List[Tool]:\n    \"\"\"Get all available raw MCP Tool objects from the server (async).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient to\n    retrieve the list of tool definitions from the server.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor.\n\n    Returns:\n        A list of raw `mcp.types.Tool` objects available on the server.\n    \"\"\"\n    async with FastMCPClient(server, **client_kwargs) as client:\n        if not client.client:\n            raise RuntimeError(\"Client not initialized. Use async with FastMCPClient.\")\n        return await client.client.list_tools()\n</code></pre>"},{"location":"reference/agent/tools/mcp/decorators/","title":"decorators","text":"<p>langroid/agent/tools/mcp/decorators.py </p>"},{"location":"reference/agent/tools/mcp/decorators/#langroid.agent.tools.mcp.decorators.mcp_tool","title":"<code>mcp_tool(server, tool_name)</code>","text":"<p>Decorator: declare a ToolMessage class bound to a FastMCP tool.</p> Usage <p>@mcp_tool(\"/path/to/server.py\", \"get_weather\") class WeatherTool:     def pretty(self) -&gt; str:         return f\"Temp is {self.temperature}\"</p> <p>The <code>server</code> may be a string/URL/FastMCP/ClientTransport, or a zero-arg callable returning one of those, e.g. <code>lambda: StdioTransport(...)</code>. Using a factory ensures a fresh transport per connection under fastmcp&gt;=2.13.</p> Source code in <code>langroid/agent/tools/mcp/decorators.py</code> <pre><code>def mcp_tool(\n    server: FastMCPServerSpec, tool_name: str\n) -&gt; Callable[[Type[ToolMessage]], Type[ToolMessage]]:\n    \"\"\"Decorator: declare a ToolMessage class bound to a FastMCP tool.\n\n    Usage:\n        @mcp_tool(\"/path/to/server.py\", \"get_weather\")\n        class WeatherTool:\n            def pretty(self) -&gt; str:\n                return f\"Temp is {self.temperature}\"\n\n    The `server` may be a string/URL/FastMCP/ClientTransport, or a zero-arg\n    callable returning one of those, e.g. `lambda: StdioTransport(...)`. Using a\n    factory ensures a fresh transport per connection under fastmcp&gt;=2.13.\n    \"\"\"\n\n    def decorator(user_cls: Type[ToolMessage]) -&gt; Type[ToolMessage]:\n        # build the \u201creal\u201d ToolMessage subclass for this server/tool\n        RealTool: Type[ToolMessage] = get_tool(server, tool_name)\n\n        # copy user\u2010defined methods / attributes onto RealTool\n        for name, attr in user_cls.__dict__.items():\n            if name.startswith(\"__\") and name.endswith(\"__\"):\n                continue\n            setattr(RealTool, name, attr)\n\n        # preserve the user\u2019s original name if you like:\n        RealTool.__name__ = user_cls.__name__\n        return RealTool\n\n    return decorator\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/","title":"fastmcp_client","text":"<p>langroid/agent/tools/mcp/fastmcp_client.py </p>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.FastMCPClient","title":"<code>FastMCPClient(server, persist_connection=False, forward_images=True, forward_text_resources=False, forward_blob_resources=False, sampling_handler=None, roots=None, log_handler=None, message_handler=None, read_timeout_seconds=None)</code>","text":"<p>A client for interacting with a FastMCP server.</p> <p>Provides async context manager functionality to safely manage resources.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>FastMCP server or path to such a server</p> required Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>def __init__(\n    self,\n    server: FastMCPServerSpec,\n    persist_connection: bool = False,\n    forward_images: bool = True,\n    forward_text_resources: bool = False,\n    forward_blob_resources: bool = False,\n    sampling_handler: SamplingHandler | None = None,  # type: ignore\n    roots: RootsList | RootsHandler | None = None,  # type: ignore\n    log_handler: LoggingFnT | None = None,\n    message_handler: MessageHandlerFnT | None = None,\n    read_timeout_seconds: datetime.timedelta | None = None,\n) -&gt; None:\n    \"\"\"Initialize the FastMCPClient.\n\n    Args:\n        server: FastMCP server or path to such a server\n    \"\"\"\n    self.server = server\n    self.client = None\n    self._cm = None\n    self.sampling_handler = sampling_handler\n    self.roots = roots\n    self.log_handler = log_handler\n    self.message_handler = message_handler\n    # Default a slightly larger read timeout for stdio transports on first\n    # connects. Allows flaky subprocess servers a bit more time to boot.\n    if read_timeout_seconds is None:\n        try:\n            default_secs = int(os.getenv(\"LANGROID_MCP_READ_TIMEOUT\", \"15\"))\n            self.read_timeout_seconds = datetime.timedelta(seconds=default_secs)\n        except Exception:\n            self.read_timeout_seconds = None\n    else:\n        self.read_timeout_seconds = read_timeout_seconds\n    self.persist_connection = persist_connection\n    self.forward_text_resources = forward_text_resources\n    self.forward_blob_resources = forward_blob_resources\n    self.forward_images = forward_images\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.FastMCPClient.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Open the underlying session.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def connect(self) -&gt; None:\n    \"\"\"Open the underlying session.\"\"\"\n    await self.__aenter__()\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.FastMCPClient.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the underlying session.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the underlying session.\"\"\"\n    await self.__aexit__(None, None, None)\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.FastMCPClient.get_tool_async","title":"<code>get_tool_async(tool_name)</code>  <code>async</code>","text":"<p>Create a Langroid ToolMessage subclass from the MCP Tool with the given <code>tool_name</code>.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_tool_async(self, tool_name: str) -&gt; Type[ToolMessage]:\n    \"\"\"\n    Create a Langroid ToolMessage subclass from the MCP Tool\n    with the given `tool_name`.\n    \"\"\"\n    if not self.client:\n        if self.persist_connection:\n            await self.connect()\n            assert self.client\n        else:\n            raise RuntimeError(\n                \"Client not initialized. Use async with FastMCPClient.\"\n            )\n    target = await self.get_mcp_tool_async(tool_name)\n    if target is None:\n        raise ValueError(f\"No tool named {tool_name}\")\n    props = target.inputSchema.get(\"properties\", {})\n    # Get the list of required fields from JSON Schema\n    required_fields = set(target.inputSchema.get(\"required\", []))\n    fields: Dict[str, Tuple[type, Any]] = {}\n    for fname, schema in props.items():\n        ftype, fld = self._schema_to_field(\n            fname, schema, target.name, is_required=fname in required_fields\n        )\n        fields[fname] = (ftype, fld)\n\n    # Convert target.name to CamelCase and add Tool suffix\n    parts = target.name.replace(\"-\", \"_\").split(\"_\")\n    camel_case = \"\".join(part.capitalize() for part in parts)\n    model_name = f\"{camel_case}Tool\"\n\n    from langroid.agent.tool_message import ToolMessage as _BaseToolMessage\n\n    # IMPORTANT: Avoid clashes with reserved field names in Langroid ToolMessage!\n    # First figure out which field names are reserved\n    reserved = set(_BaseToolMessage.__annotations__.keys())\n    reserved.update([\"recipient\", \"_handler\", \"name\"])\n    renamed: Dict[str, str] = {}\n    new_fields: Dict[str, Tuple[type, Any]] = {}\n    for fname, (ftype, fld) in fields.items():\n        if fname in reserved:\n            new_name = fname + \"__\"\n            renamed[fname] = new_name\n            new_fields[new_name] = (ftype, fld)\n        else:\n            new_fields[fname] = (ftype, fld)\n    # now replace fields with our renamed\u2010aware mapping\n    fields = new_fields\n\n    # create Langroid ToolMessage subclass, with expected fields.\n    tool_model = cast(\n        Type[ToolMessage],\n        create_model(  # type: ignore[call-overload]\n            model_name,\n            request=(str, target.name),\n            purpose=(str, target.description or f\"Use the tool {target.name}\"),\n            __base__=ToolMessage,\n            **fields,\n        ),\n    )\n    # Store ALL client configuration needed to recreate a client\n    client_config = {\n        # Always store a SERVER FACTORY to ensure a fresh transport per call\n        \"server\": self._as_server_factory(self.server),\n        \"sampling_handler\": self.sampling_handler,\n        \"roots\": self.roots,\n        \"log_handler\": self.log_handler,\n        \"message_handler\": self.message_handler,\n        \"read_timeout_seconds\": self.read_timeout_seconds,\n    }\n\n    tool_model._client_config = client_config  # type: ignore [attr-defined]\n    tool_model._renamed_fields = renamed  # type: ignore[attr-defined]\n\n    # 2) define an arg-free call_tool_async()\n    async def call_tool_async(itself: ToolMessage) -&gt; Any:\n        from langroid.agent.tools.mcp.fastmcp_client import FastMCPClient\n\n        # pack up the payload\n        # Get exclude fields from model config with proper type checking\n        exclude_fields = set()\n        model_config = getattr(itself, \"model_config\", {})\n        if (\n            isinstance(model_config, dict)\n            and \"json_schema_extra\" in model_config\n            and model_config[\"json_schema_extra\"] is not None\n            and isinstance(model_config[\"json_schema_extra\"], dict)\n            and \"exclude\" in model_config[\"json_schema_extra\"]\n        ):\n            exclude_list = model_config[\"json_schema_extra\"][\"exclude\"]\n            if isinstance(exclude_list, (list, set, tuple)):\n                exclude_fields = set(exclude_list)\n\n        # Add standard excluded fields\n        exclude_fields.update([\"request\", \"purpose\"])\n\n        # Exclude None values - MCP servers don't expect None for optional params\n        payload = itself.model_dump(exclude=exclude_fields, exclude_none=True)\n\n        # restore any renamed fields\n        for orig, new in itself.__class__._renamed_fields.items():  # type: ignore\n            if new in payload:\n                payload[orig] = payload.pop(new)\n\n        client_cfg = getattr(itself.__class__, \"_client_config\", None)  # type: ignore\n        if not client_cfg:\n            # Fallback or error - ideally _client_config should always exist\n            raise RuntimeError(f\"Client config missing on {itself.__class__}\")\n\n        # Connect the client if not yet connected and keep the connection open\n        if self.persist_connection:\n            if not self.client:\n                await self.connect()\n\n            return await self.call_mcp_tool(itself.request, payload)\n\n        # open a fresh client, call the tool, then close\n        async with FastMCPClient(**client_cfg) as client:  # type: ignore\n            return await client.call_mcp_tool(itself.request, payload)\n\n    tool_model.call_tool_async = call_tool_async  # type: ignore\n\n    if not hasattr(tool_model, \"handle_async\"):\n        # 3) define handle_async() method with optional agent parameter\n        from typing import Union\n\n        async def handle_async(\n            self: ToolMessage, agent: Optional[Agent] = None\n        ) -&gt; Union[str, Optional[ChatDocument]]:\n            \"\"\"\n            Auto-generated handler for MCP tool. Returns ChatDocument with files\n            if files are present and agent is provided, otherwise returns text.\n\n            To override: define your own handle_async method with matching signature\n            if you need file handling, or simpler signature if you only need text.\n            \"\"\"\n            response = await self.call_tool_async()  # type: ignore[attr-defined]\n            if response is None:\n                return None\n\n            content, files = response\n\n            # If we have files and an agent is provided, return a ChatDocument\n            if files and agent is not None:\n                return agent.create_agent_response(\n                    content=content,\n                    files=files,\n                )\n            else:\n                # Otherwise, just return the text content\n                return str(content) if content is not None else None\n\n        # add the handle_async() method to the tool model\n        tool_model.handle_async = handle_async  # type: ignore\n\n    return tool_model\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.FastMCPClient.get_tools_async","title":"<code>get_tools_async()</code>  <code>async</code>","text":"<p>Get all available tools as Langroid ToolMessage classes, handling nested schemas, with <code>handle_async</code> methods</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_tools_async(self) -&gt; List[Type[ToolMessage]]:\n    \"\"\"\n    Get all available tools as Langroid ToolMessage classes,\n    handling nested schemas, with `handle_async` methods\n    \"\"\"\n    if not self.client:\n        if self.persist_connection:\n            await self.connect()\n            assert self.client\n        else:\n            raise RuntimeError(\n                \"Client not initialized. Use async with FastMCPClient.\"\n            )\n    resp = await self.client.list_tools()\n    return [await self.get_tool_async(t.name) for t in resp]\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.FastMCPClient.get_mcp_tool_async","title":"<code>get_mcp_tool_async(name)</code>  <code>async</code>","text":"<p>Find the \"original\" MCP Tool (i.e. of type mcp.types.Tool) on the server  matching <code>name</code>, or None if missing. This contains the metadata for the tool:  name, description, inputSchema, etc.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the tool to look up.</p> required <p>Returns:</p> Type Description <code>Optional[Tool]</code> <p>The raw Tool object from the server, or None.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_mcp_tool_async(self, name: str) -&gt; Optional[Tool]:\n    \"\"\"Find the \"original\" MCP Tool (i.e. of type mcp.types.Tool) on the server\n     matching `name`, or None if missing. This contains the metadata for the tool:\n     name, description, inputSchema, etc.\n\n    Args:\n        name: Name of the tool to look up.\n\n    Returns:\n        The raw Tool object from the server, or None.\n    \"\"\"\n    if not self.client:\n        if self.persist_connection:\n            await self.connect()\n            assert self.client\n        else:\n            raise RuntimeError(\n                \"Client not initialized. Use async with FastMCPClient.\"\n            )\n    resp: List[Tool] = await self.client.list_tools()\n    return next((t for t in resp if t.name == name), None)\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.FastMCPClient.call_mcp_tool","title":"<code>call_mcp_tool(tool_name, arguments)</code>  <code>async</code>","text":"<p>Call an MCP tool with the given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool to call.</p> required <code>arguments</code> <code>Dict[str, Any]</code> <p>Arguments to pass to the tool.</p> required <p>Returns:</p> Type Description <code>Optional[tuple[str, list[FileAttachment]]]</code> <p>The result of the tool call.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def call_mcp_tool(\n    self, tool_name: str, arguments: Dict[str, Any]\n) -&gt; Optional[tuple[str, list[FileAttachment]]]:\n    \"\"\"Call an MCP tool with the given arguments.\n\n    Args:\n        tool_name: Name of the tool to call.\n        arguments: Arguments to pass to the tool.\n\n    Returns:\n        The result of the tool call.\n    \"\"\"\n    if not self.client:\n        if self.persist_connection:\n            await self.connect()\n            assert self.client\n        else:\n            raise RuntimeError(\n                \"Client not initialized. Use async with FastMCPClient.\"\n            )\n    # Prefer validated call; if server fails to provide structured content\n    # despite declaring a schema, fall back to a raw request to bypass\n    # client-side validation and still surface the data.\n    try:\n        result: CallToolResult = await self.client.session.call_tool(\n            tool_name,\n            arguments,\n        )\n    except RuntimeError as e:\n        msg = str(e)\n        if \"has an output schema but did not return structured content\" not in msg:\n            raise\n        from mcp.types import (\n            CallToolRequest,\n            CallToolRequestParams,\n            ClientRequest,\n        )\n        from mcp.types import (\n            CallToolResult as _CallToolResult,\n        )\n\n        result = await self.client.session.send_request(  # type: ignore[assignment]\n            ClientRequest(\n                CallToolRequest(\n                    params=CallToolRequestParams(\n                        name=tool_name, arguments=arguments\n                    )\n                )\n            ),\n            _CallToolResult,\n        )\n    results = self._convert_tool_result(tool_name, result)\n\n    if isinstance(results, str):\n        return results, []\n\n    return results\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.get_tool_async","title":"<code>get_tool_async(server, tool_name, **client_kwargs)</code>  <code>async</code>","text":"<p>Get a single Langroid ToolMessage subclass for a specific MCP tool name (async).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>tool_name</code> <code>str</code> <p>The name of the tool to retrieve.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor (e.g., sampling_handler, roots).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Type[ToolMessage]</code> <p>A dynamically created Langroid ToolMessage subclass representing the</p> <code>Type[ToolMessage]</code> <p>requested tool.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_tool_async(\n    server: FastMCPServerSpec,\n    tool_name: str,\n    **client_kwargs: Any,\n) -&gt; Type[ToolMessage]:\n    \"\"\"Get a single Langroid ToolMessage subclass for a specific MCP tool name (async).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        tool_name: The name of the tool to retrieve.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor (e.g., sampling_handler, roots).\n\n    Returns:\n        A dynamically created Langroid ToolMessage subclass representing the\n        requested tool.\n    \"\"\"\n    async with FastMCPClient(server, **client_kwargs) as client:\n        return await client.get_tool_async(tool_name)\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.get_tool","title":"<code>get_tool(server, tool_name, **client_kwargs)</code>","text":"<p>Get a single Langroid ToolMessage subclass for a specific MCP tool name (synchronous).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient and runs the async <code>get_tool_async</code> function using <code>asyncio.run()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>tool_name</code> <code>str</code> <p>The name of the tool to retrieve.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor (e.g., sampling_handler, roots).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Type[ToolMessage]</code> <p>A dynamically created Langroid ToolMessage subclass representing the</p> <code>Type[ToolMessage]</code> <p>requested tool.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>def get_tool(\n    server: FastMCPServerSpec,\n    tool_name: str,\n    **client_kwargs: Any,\n) -&gt; Type[ToolMessage]:\n    \"\"\"Get a single Langroid ToolMessage subclass\n    for a specific MCP tool name (synchronous).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient and runs the\n    async `get_tool_async` function using `asyncio.run()`.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        tool_name: The name of the tool to retrieve.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor (e.g., sampling_handler, roots).\n\n    Returns:\n        A dynamically created Langroid ToolMessage subclass representing the\n        requested tool.\n    \"\"\"\n    return asyncio.run(get_tool_async(server, tool_name, **client_kwargs))\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.get_tools_async","title":"<code>get_tools_async(server, **client_kwargs)</code>  <code>async</code>","text":"<p>Get all available tools as Langroid ToolMessage subclasses (async).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor (e.g., sampling_handler, roots).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Type[ToolMessage]]</code> <p>A list of dynamically created Langroid ToolMessage subclasses</p> <code>List[Type[ToolMessage]]</code> <p>representing all available tools on the server.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_tools_async(\n    server: FastMCPServerSpec,\n    **client_kwargs: Any,\n) -&gt; List[Type[ToolMessage]]:\n    \"\"\"Get all available tools as Langroid ToolMessage subclasses (async).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor (e.g., sampling_handler, roots).\n\n    Returns:\n        A list of dynamically created Langroid ToolMessage subclasses\n        representing all available tools on the server.\n    \"\"\"\n    async with FastMCPClient(server, **client_kwargs) as client:\n        return await client.get_tools_async()\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.get_tools","title":"<code>get_tools(server, **client_kwargs)</code>","text":"<p>Get all available tools as Langroid ToolMessage subclasses (synchronous).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient and runs the async <code>get_tools_async</code> function using <code>asyncio.run()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor (e.g., sampling_handler, roots).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Type[ToolMessage]]</code> <p>A list of dynamically created Langroid ToolMessage subclasses</p> <code>List[Type[ToolMessage]]</code> <p>representing all available tools on the server.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>def get_tools(\n    server: FastMCPServerSpec,\n    **client_kwargs: Any,\n) -&gt; List[Type[ToolMessage]]:\n    \"\"\"Get all available tools as Langroid ToolMessage subclasses (synchronous).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient and runs the\n    async `get_tools_async` function using `asyncio.run()`.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor (e.g., sampling_handler, roots).\n\n    Returns:\n        A list of dynamically created Langroid ToolMessage subclasses\n        representing all available tools on the server.\n    \"\"\"\n    return asyncio.run(get_tools_async(server, **client_kwargs))\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.get_mcp_tool_async","title":"<code>get_mcp_tool_async(server, name, **client_kwargs)</code>  <code>async</code>","text":"<p>Get the raw MCP Tool object for a specific tool name (async).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient to retrieve the tool definition from the server.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>name</code> <code>str</code> <p>The name of the tool to look up.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Tool]</code> <p>The raw <code>mcp.types.Tool</code> object from the server, or <code>None</code> if the tool</p> <code>Optional[Tool]</code> <p>is not found.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_mcp_tool_async(\n    server: FastMCPServerSpec,\n    name: str,\n    **client_kwargs: Any,\n) -&gt; Optional[Tool]:\n    \"\"\"Get the raw MCP Tool object for a specific tool name (async).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient to\n    retrieve the tool definition from the server.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        name: The name of the tool to look up.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor.\n\n    Returns:\n        The raw `mcp.types.Tool` object from the server, or `None` if the tool\n        is not found.\n    \"\"\"\n    async with FastMCPClient(server, **client_kwargs) as client:\n        return await client.get_mcp_tool_async(name)\n</code></pre>"},{"location":"reference/agent/tools/mcp/fastmcp_client/#langroid.agent.tools.mcp.fastmcp_client.get_mcp_tools_async","title":"<code>get_mcp_tools_async(server, **client_kwargs)</code>  <code>async</code>","text":"<p>Get all available raw MCP Tool objects from the server (async).</p> <p>This is a convenience wrapper that creates a temporary FastMCPClient to retrieve the list of tool definitions from the server.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FastMCPServerSpec</code> <p>Specification of the FastMCP server to connect to.</p> required <code>**client_kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the FastMCPClient constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Tool]</code> <p>A list of raw <code>mcp.types.Tool</code> objects available on the server.</p> Source code in <code>langroid/agent/tools/mcp/fastmcp_client.py</code> <pre><code>async def get_mcp_tools_async(\n    server: FastMCPServerSpec,\n    **client_kwargs: Any,\n) -&gt; List[Tool]:\n    \"\"\"Get all available raw MCP Tool objects from the server (async).\n\n    This is a convenience wrapper that creates a temporary FastMCPClient to\n    retrieve the list of tool definitions from the server.\n\n    Args:\n        server: Specification of the FastMCP server to connect to.\n        **client_kwargs: Additional keyword arguments to pass to the\n            FastMCPClient constructor.\n\n    Returns:\n        A list of raw `mcp.types.Tool` objects available on the server.\n    \"\"\"\n    async with FastMCPClient(server, **client_kwargs) as client:\n        if not client.client:\n            raise RuntimeError(\"Client not initialized. Use async with FastMCPClient.\")\n        return await client.client.list_tools()\n</code></pre>"},{"location":"reference/cachedb/","title":"cachedb","text":"<p>langroid/cachedb/init.py </p>"},{"location":"reference/cachedb/base/","title":"base","text":"<p>langroid/cachedb/base.py </p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDBConfig","title":"<code>CacheDBConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration model for CacheDB.</p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB","title":"<code>CacheDB</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a cache database.</p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.store","title":"<code>store(key, value)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef store(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Abstract method to store a value associated with a key.\n\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.retrieve","title":"<code>retrieve(key)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any] | str | None</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, key: str) -&gt; Dict[str, Any] | str | None:\n    \"\"\"\n    Abstract method to retrieve the value associated with a key.\n\n    Args:\n        key (str): The key to retrieve the value for.\n\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.delete_keys","title":"<code>delete_keys(keys)</code>  <code>abstractmethod</code>","text":"<p>Delete the keys from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>The keys to delete.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef delete_keys(self, keys: List[str]) -&gt; None:\n    \"\"\"\n    Delete the keys from the cache.\n\n    Args:\n        keys (List[str]): The keys to delete.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.delete_keys_pattern","title":"<code>delete_keys_pattern(pattern)</code>  <code>abstractmethod</code>","text":"<p>Delete all keys with the given pattern</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The pattern to match.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef delete_keys_pattern(self, pattern: str) -&gt; None:\n    \"\"\"\n    Delete all keys with the given pattern\n\n    Args:\n        prefix (str): The pattern to match.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/","title":"redis_cachedb","text":"<p>langroid/cachedb/redis_cachedb.py </p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCacheConfig","title":"<code>RedisCacheConfig</code>","text":"<p>               Bases: <code>CacheDBConfig</code></p> <p>Configuration model for RedisCache.</p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache","title":"<code>RedisCache(config)</code>","text":"<p>               Bases: <code>CacheDB</code></p> <p>Redis implementation of the CacheDB.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RedisCacheConfig</code> <p>The configuration to use.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def __init__(self, config: RedisCacheConfig):\n    \"\"\"\n    Initialize a RedisCache with the given config.\n\n    Args:\n        config (RedisCacheConfig): The configuration to use.\n    \"\"\"\n    self.config = config\n    load_dotenv()\n\n    if self.config.fake:\n        self.pool = fakeredis.FakeStrictRedis()  # type: ignore\n    else:\n        redis_password = os.getenv(\"REDIS_PASSWORD\")\n        redis_host = os.getenv(\"REDIS_HOST\") or None\n        redis_port = os.getenv(\"REDIS_PORT\")\n        if None in [redis_password, redis_host, redis_port]:\n            if not RedisCache._warned_password:\n                logger.warning(\n                    \"\"\"REDIS_PASSWORD, REDIS_HOST, REDIS_PORT not set in .env file,\n                    using fake redis client\"\"\"\n                )\n                RedisCache._warned_password = True\n            self.pool = fakeredis.FakeStrictRedis()  # type: ignore\n        else:\n            self.pool = redis.ConnectionPool(  # type: ignore\n                host=redis_host,\n                port=redis_port,\n                password=redis_password,\n                max_connections=500,\n                socket_timeout=5,\n                socket_keepalive=True,\n                retry_on_timeout=True,\n                health_check_interval=30,\n            )\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.redis_client","title":"<code>redis_client()</code>","text":"<p>Cleanly open and close a redis client, avoids max clients exceeded error</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>@contextmanager  # type: ignore\ndef redis_client(self) -&gt; AbstractContextManager[T]:  # type: ignore\n    \"\"\"Cleanly open and close a redis client, avoids max clients exceeded error\"\"\"\n    if isinstance(self.pool, fakeredis.FakeStrictRedis):\n        yield self.pool\n    else:\n        client: T = redis.Redis(connection_pool=self.pool)\n        try:\n            yield client\n        finally:\n            client.close()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear","title":"<code>clear()</code>","text":"<p>Clear keys from current db.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear keys from current db.\"\"\"\n    with self.redis_client() as client:  # type: ignore\n        client.flushdb()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear_all","title":"<code>clear_all()</code>","text":"<p>Clear all keys from all dbs.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear_all(self) -&gt; None:\n    \"\"\"Clear all keys from all dbs.\"\"\"\n    with self.redis_client() as client:  # type: ignore\n        client.flushall()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.store","title":"<code>store(key, value)</code>","text":"<p>Store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def store(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Store a value associated with a key.\n\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\n    with self.redis_client() as client:  # type: ignore\n        try:\n            client.set(key, json.dumps(value))\n        except redis.exceptions.ConnectionError:\n            logger.warning(\"Redis connection error, not storing key/value\")\n            return None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.retrieve","title":"<code>retrieve(key)</code>","text":"<p>Retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any] | str | None</code> <p>dict|str|None: The value associated with the key.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def retrieve(self, key: str) -&gt; Dict[str, Any] | str | None:\n    \"\"\"\n    Retrieve the value associated with a key.\n\n    Args:\n        key (str): The key to retrieve the value for.\n\n    Returns:\n        dict|str|None: The value associated with the key.\n    \"\"\"\n    with self.redis_client() as client:  # type: ignore\n        try:\n            value = client.get(key)\n        except redis.exceptions.ConnectionError:\n            logger.warning(\"Redis connection error, returning None\")\n            return None\n        return json.loads(value) if value else None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.delete_keys","title":"<code>delete_keys(keys)</code>","text":"<p>Delete the keys from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>List[str]</code> <p>The keys to delete.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def delete_keys(self, keys: List[str]) -&gt; None:\n    \"\"\"\n    Delete the keys from the cache.\n\n    Args:\n        keys (List[str]): The keys to delete.\n    \"\"\"\n    with self.redis_client() as client:  # type: ignore\n        try:\n            client.delete(*keys)\n        except redis.exceptions.ConnectionError:\n            logger.warning(\"Redis connection error, not deleting keys\")\n            return None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.delete_keys_pattern","title":"<code>delete_keys_pattern(pattern)</code>","text":"<p>Delete the keys matching the pattern from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The pattern to match.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def delete_keys_pattern(self, pattern: str) -&gt; None:\n    \"\"\"\n    Delete the keys matching the pattern from the cache.\n\n    Args:\n        prefix (str): The pattern to match.\n    \"\"\"\n    with self.redis_client() as client:  # type: ignore\n        try:\n            keys = client.keys(pattern)\n            if len(keys) &gt; 0:\n                client.delete(*keys)\n        except redis.exceptions.ConnectionError:\n            logger.warning(\"Redis connection error, not deleting keys\")\n            return None\n</code></pre>"},{"location":"reference/embedding_models/","title":"embedding_models","text":"<p>langroid/embedding_models/init.py </p>"},{"location":"reference/embedding_models/#langroid.embedding_models.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for an embedding model.</p>"},{"location":"reference/embedding_models/#langroid.embedding_models.EmbeddingModel.clone","title":"<code>clone()</code>","text":"<p>Return a copy of this embedding model suitable for use in cloned agents. Default behaviour attempts to deep-copy the model configuration and instantiate a fresh model of the same type; if that is not possible, the original instance is reused.</p> Source code in <code>langroid/embedding_models/base.py</code> <pre><code>def clone(self) -&gt; \"EmbeddingModel\":\n    \"\"\"\n    Return a copy of this embedding model suitable for use in cloned agents.\n    Default behaviour attempts to deep-copy the model configuration and\n    instantiate a fresh model of the same type; if that is not possible,\n    the original instance is reused.\n    \"\"\"\n    config = getattr(self, \"config\", None)\n    if config is not None and hasattr(config, \"model_copy\"):\n        try:\n            return type(self)(config.model_copy(deep=True))  # type: ignore[call-arg]\n        except Exception:\n            pass\n    return self\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.EmbeddingModel.similarity","title":"<code>similarity(text1, text2)</code>","text":"<p>Compute cosine similarity between two texts.</p> Source code in <code>langroid/embedding_models/base.py</code> <pre><code>def similarity(self, text1: str, text2: str) -&gt; float:\n    \"\"\"Compute cosine similarity between two texts.\"\"\"\n    [emb1, emb2] = self.embedding_fn()([text1, text2])\n    return float(\n        np.array(emb1)\n        @ np.array(emb2)\n        / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n    )\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.OpenAIEmbeddings","title":"<code>OpenAIEmbeddings(config=OpenAIEmbeddingsConfig())</code>","text":"<p>               Bases: <code>EmbeddingModel</code></p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, config: OpenAIEmbeddingsConfig = OpenAIEmbeddingsConfig()):\n    super().__init__()\n    self.config = config\n    load_dotenv()\n\n    # Check if using LangDB\n    self.is_langdb = self.config.model_name.startswith(\"langdb/\")\n\n    if self.is_langdb:\n        self.config.model_name = self.config.model_name.replace(\"langdb/\", \"\")\n        self.config.api_base = self.config.langdb_params.base_url\n        project_id = self.config.langdb_params.project_id\n        if project_id:\n            self.config.api_base += \"/\" + project_id + \"/v1\"\n        self.config.api_key = self.config.langdb_params.api_key\n\n    if not self.config.api_key:\n        self.config.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n\n    self.config.organization = os.getenv(\"OPENAI_ORGANIZATION\", \"\")\n\n    if self.config.api_key == \"\":\n        if self.is_langdb:\n            raise ValueError(\n                \"\"\"\n                LANGDB_API_KEY must be set in .env or your environment \n                to use OpenAIEmbeddings via LangDB.\n                \"\"\"\n            )\n        else:\n            raise ValueError(\n                \"\"\"\n                OPENAI_API_KEY must be set in .env or your environment \n                to use OpenAIEmbeddings.\n                \"\"\"\n            )\n\n    self.client = OpenAI(\n        base_url=self.config.api_base,\n        api_key=self.config.api_key,\n        organization=self.config.organization,\n    )\n    model_for_tokenizer = self.config.model_name\n    if model_for_tokenizer.startswith(\"openai/\"):\n        self.config.model_name = model_for_tokenizer.replace(\"openai/\", \"\")\n    self.tokenizer = tiktoken.encoding_for_model(self.config.model_name)\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.OpenAIEmbeddings.truncate_texts","title":"<code>truncate_texts(texts)</code>","text":"<p>Truncate texts to the embedding model's context length. TODO: Maybe we should show warning, and consider doing T5 summarization?</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def truncate_texts(self, texts: List[str]) -&gt; List[str] | List[List[int]]:\n    \"\"\"\n    Truncate texts to the embedding model's context length.\n    TODO: Maybe we should show warning, and consider doing T5 summarization?\n    \"\"\"\n    truncated_tokens = [\n        self.tokenizer.encode(text, disallowed_special=())[\n            : self.config.context_length\n        ]\n        for text in texts\n    ]\n\n    if self.is_langdb:\n        # LangDB embedding endpt only works with strings, not tokens\n        return [self.tokenizer.decode(tokens) for tokens in truncated_tokens]\n    return truncated_tokens\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.LlamaCppServerEmbeddings","title":"<code>LlamaCppServerEmbeddings(config=LCSEC())</code>","text":"<p>               Bases: <code>EmbeddingModel</code></p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, config: LCSEC = LCSEC()):\n    super().__init__()\n    self.config = config\n\n    if self.config.api_base == \"\":\n        raise ValueError(\n            \"\"\"Api Base MUST be set for Llama Server Embeddings.\n            \"\"\"\n        )\n\n    self.tokenize_url = self.config.api_base + \"/tokenize\"\n    self.detokenize_url = self.config.api_base + \"/detokenize\"\n    self.embedding_url = self.config.api_base + \"/embeddings\"\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.GeminiEmbeddings","title":"<code>GeminiEmbeddings(config=GeminiEmbeddingsConfig())</code>","text":"<p>               Bases: <code>EmbeddingModel</code></p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, config: GeminiEmbeddingsConfig = GeminiEmbeddingsConfig()):\n    try:\n        from google import genai\n    except ImportError as e:\n        raise LangroidImportError(extra=\"google-genai\", error=str(e))\n    super().__init__()\n    self.config = config\n    load_dotenv()\n    self.config.api_key = os.getenv(\"GEMINI_API_KEY\", \"\")\n\n    if self.config.api_key == \"\":\n        raise ValueError(\n            \"\"\"\n            GEMINI_API_KEY env variable must be set to use GeminiEmbeddings.\n            \"\"\"\n        )\n    self.client = genai.Client(api_key=self.config.api_key)\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.GeminiEmbeddings.generate_embeddings","title":"<code>generate_embeddings(texts)</code>","text":"<p>Generates embeddings for a list of input texts.</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def generate_embeddings(self, texts: List[str]) -&gt; List[List[float]]:\n    \"\"\"Generates embeddings for a list of input texts.\"\"\"\n    all_embeddings: List[List[float]] = []\n\n    for batch in batched(texts, self.config.batch_size):\n        result = self.client.models.embed_content(  # type: ignore[attr-defined]\n            model=self.config.model_name,\n            contents=batch,  # type: ignore\n        )\n\n        if not hasattr(result, \"embeddings\") or not isinstance(\n            result.embeddings, list\n        ):\n            raise ValueError(\n                \"Unexpected format for embeddings: missing or incorrect type\"\n            )\n\n        # Extract .values from ContentEmbedding objects\n        all_embeddings.extend(\n            [emb.values for emb in result.embeddings]  # type: ignore\n        )\n\n    return all_embeddings\n</code></pre>"},{"location":"reference/embedding_models/#langroid.embedding_models.embedding_model","title":"<code>embedding_model(embedding_fn_type='openai')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_fn_type</code> <code>str</code> <p>Type of embedding model to use. Options are: - \"openai\", - \"azure-openai\", - \"sentencetransformer\", or - \"fastembed\". (others may be added in the future)</p> <code>'openai'</code> <p>Returns:     EmbeddingModel: The corresponding embedding model class.</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def embedding_model(embedding_fn_type: str = \"openai\") -&gt; EmbeddingModel:\n    \"\"\"\n    Args:\n        embedding_fn_type: Type of embedding model to use. Options are:\n         - \"openai\",\n         - \"azure-openai\",\n         - \"sentencetransformer\", or\n         - \"fastembed\".\n            (others may be added in the future)\n    Returns:\n        EmbeddingModel: The corresponding embedding model class.\n    \"\"\"\n    if embedding_fn_type == \"openai\":\n        return OpenAIEmbeddings  # type: ignore\n    elif embedding_fn_type == \"azure-openai\":\n        return AzureOpenAIEmbeddings  # type: ignore\n    elif embedding_fn_type == \"fastembed\":\n        return FastEmbedEmbeddings  # type: ignore\n    elif embedding_fn_type == \"llamacppserver\":\n        return LlamaCppServerEmbeddings  # type: ignore\n    elif embedding_fn_type == \"gemini\":\n        return GeminiEmbeddings  # type: ignore\n    else:  # default sentence transformer\n        return SentenceTransformerEmbeddings  # type: ignore\n</code></pre>"},{"location":"reference/embedding_models/base/","title":"base","text":"<p>langroid/embedding_models/base.py </p>"},{"location":"reference/embedding_models/base/#langroid.embedding_models.base.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for an embedding model.</p>"},{"location":"reference/embedding_models/base/#langroid.embedding_models.base.EmbeddingModel.clone","title":"<code>clone()</code>","text":"<p>Return a copy of this embedding model suitable for use in cloned agents. Default behaviour attempts to deep-copy the model configuration and instantiate a fresh model of the same type; if that is not possible, the original instance is reused.</p> Source code in <code>langroid/embedding_models/base.py</code> <pre><code>def clone(self) -&gt; \"EmbeddingModel\":\n    \"\"\"\n    Return a copy of this embedding model suitable for use in cloned agents.\n    Default behaviour attempts to deep-copy the model configuration and\n    instantiate a fresh model of the same type; if that is not possible,\n    the original instance is reused.\n    \"\"\"\n    config = getattr(self, \"config\", None)\n    if config is not None and hasattr(config, \"model_copy\"):\n        try:\n            return type(self)(config.model_copy(deep=True))  # type: ignore[call-arg]\n        except Exception:\n            pass\n    return self\n</code></pre>"},{"location":"reference/embedding_models/base/#langroid.embedding_models.base.EmbeddingModel.similarity","title":"<code>similarity(text1, text2)</code>","text":"<p>Compute cosine similarity between two texts.</p> Source code in <code>langroid/embedding_models/base.py</code> <pre><code>def similarity(self, text1: str, text2: str) -&gt; float:\n    \"\"\"Compute cosine similarity between two texts.\"\"\"\n    [emb1, emb2] = self.embedding_fn()([text1, text2])\n    return float(\n        np.array(emb1)\n        @ np.array(emb2)\n        / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n    )\n</code></pre>"},{"location":"reference/embedding_models/models/","title":"models","text":"<p>langroid/embedding_models/models.py </p>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.FastEmbedEmbeddingsConfig","title":"<code>FastEmbedEmbeddingsConfig</code>","text":"<p>               Bases: <code>EmbeddingModelsConfig</code></p> <p>Config for qdrant/fastembed embeddings, see here: https://github.com/qdrant/fastembed</p>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.EmbeddingFunctionCallable","title":"<code>EmbeddingFunctionCallable(embed_model, batch_size=512)</code>","text":"<p>A callable class designed to generate embeddings for a list of texts using the OpenAI or Azure OpenAI API, with automatic retries on failure.</p> <p>Attributes:</p> Name Type Description <code>embed_model</code> <code>EmbeddingModel</code> <p>An instance of EmbeddingModel that provides    configuration and utilities for generating embeddings.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>List[str]) -&gt; Embeddings: Generate embeddings for                     a list of input texts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code> OpenAIEmbeddings or AzureOpenAIEmbeddings</code> <p>An instance of             OpenAIEmbeddings or AzureOpenAIEmbeddings to use for             generating embeddings.</p> required <code>batch_size</code> <code>int</code> <p>Batch size</p> <code>512</code> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, embed_model: EmbeddingModel, batch_size: int = 512):\n    \"\"\"\n    Initialize the EmbeddingFunctionCallable with a specific model.\n\n    Args:\n        model ( OpenAIEmbeddings or AzureOpenAIEmbeddings): An instance of\n                        OpenAIEmbeddings or AzureOpenAIEmbeddings to use for\n                        generating embeddings.\n        batch_size (int): Batch size\n    \"\"\"\n    self.embed_model = embed_model\n    self.batch_size = batch_size\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.OpenAIEmbeddings","title":"<code>OpenAIEmbeddings(config=OpenAIEmbeddingsConfig())</code>","text":"<p>               Bases: <code>EmbeddingModel</code></p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, config: OpenAIEmbeddingsConfig = OpenAIEmbeddingsConfig()):\n    super().__init__()\n    self.config = config\n    load_dotenv()\n\n    # Check if using LangDB\n    self.is_langdb = self.config.model_name.startswith(\"langdb/\")\n\n    if self.is_langdb:\n        self.config.model_name = self.config.model_name.replace(\"langdb/\", \"\")\n        self.config.api_base = self.config.langdb_params.base_url\n        project_id = self.config.langdb_params.project_id\n        if project_id:\n            self.config.api_base += \"/\" + project_id + \"/v1\"\n        self.config.api_key = self.config.langdb_params.api_key\n\n    if not self.config.api_key:\n        self.config.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n\n    self.config.organization = os.getenv(\"OPENAI_ORGANIZATION\", \"\")\n\n    if self.config.api_key == \"\":\n        if self.is_langdb:\n            raise ValueError(\n                \"\"\"\n                LANGDB_API_KEY must be set in .env or your environment \n                to use OpenAIEmbeddings via LangDB.\n                \"\"\"\n            )\n        else:\n            raise ValueError(\n                \"\"\"\n                OPENAI_API_KEY must be set in .env or your environment \n                to use OpenAIEmbeddings.\n                \"\"\"\n            )\n\n    self.client = OpenAI(\n        base_url=self.config.api_base,\n        api_key=self.config.api_key,\n        organization=self.config.organization,\n    )\n    model_for_tokenizer = self.config.model_name\n    if model_for_tokenizer.startswith(\"openai/\"):\n        self.config.model_name = model_for_tokenizer.replace(\"openai/\", \"\")\n    self.tokenizer = tiktoken.encoding_for_model(self.config.model_name)\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.OpenAIEmbeddings.truncate_texts","title":"<code>truncate_texts(texts)</code>","text":"<p>Truncate texts to the embedding model's context length. TODO: Maybe we should show warning, and consider doing T5 summarization?</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def truncate_texts(self, texts: List[str]) -&gt; List[str] | List[List[int]]:\n    \"\"\"\n    Truncate texts to the embedding model's context length.\n    TODO: Maybe we should show warning, and consider doing T5 summarization?\n    \"\"\"\n    truncated_tokens = [\n        self.tokenizer.encode(text, disallowed_special=())[\n            : self.config.context_length\n        ]\n        for text in texts\n    ]\n\n    if self.is_langdb:\n        # LangDB embedding endpt only works with strings, not tokens\n        return [self.tokenizer.decode(tokens) for tokens in truncated_tokens]\n    return truncated_tokens\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.AzureOpenAIEmbeddings","title":"<code>AzureOpenAIEmbeddings(config=AzureOpenAIEmbeddingsConfig())</code>","text":"<p>               Bases: <code>EmbeddingModel</code></p> <p>Azure OpenAI embeddings model implementation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AzureOpenAIEmbeddingsConfig</code> <p>Configuration for Azure OpenAI embeddings model.</p> <code>AzureOpenAIEmbeddingsConfig()</code> <p>Raises:     ValueError: If required Azure config values are not set.</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(\n    self, config: AzureOpenAIEmbeddingsConfig = AzureOpenAIEmbeddingsConfig()\n):\n    \"\"\"\n    Initializes Azure OpenAI embeddings model.\n\n    Args:\n        config: Configuration for Azure OpenAI embeddings model.\n    Raises:\n        ValueError: If required Azure config values are not set.\n    \"\"\"\n    super().__init__()\n    self.config = config\n    load_dotenv()\n\n    if self.config.api_key == \"\":\n        raise ValueError(\n            \"\"\"AZURE_OPENAI_API_KEY env variable must be set to use \n        AzureOpenAIEmbeddings. Please set the AZURE_OPENAI_API_KEY value \n        in your .env file.\"\"\"\n        )\n\n    if self.config.api_base == \"\":\n        raise ValueError(\n            \"\"\"AZURE_OPENAI_API_BASE env variable must be set to use \n        AzureOpenAIEmbeddings. Please set the AZURE_OPENAI_API_BASE value \n        in your .env file.\"\"\"\n        )\n    self.client = AzureOpenAI(\n        api_key=self.config.api_key,\n        api_version=self.config.api_version,\n        azure_endpoint=self.config.api_base,\n        azure_deployment=self.config.deployment_name,\n    )\n    self.tokenizer = tiktoken.encoding_for_model(self.config.model_name)\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.AzureOpenAIEmbeddings.truncate_texts","title":"<code>truncate_texts(texts)</code>","text":"<p>Truncate texts to the embedding model's context length. TODO: Maybe we should show warning, and consider doing T5 summarization?</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def truncate_texts(self, texts: List[str]) -&gt; List[str] | List[List[int]]:\n    \"\"\"\n    Truncate texts to the embedding model's context length.\n    TODO: Maybe we should show warning, and consider doing T5 summarization?\n    \"\"\"\n    return [\n        self.tokenizer.encode(text, disallowed_special=())[\n            : self.config.context_length\n        ]\n        for text in texts\n    ]\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.AzureOpenAIEmbeddings.embedding_fn","title":"<code>embedding_fn()</code>","text":"<p>Get the embedding function for Azure OpenAI.</p> <p>Returns:</p> Type Description <code>Callable[[List[str]], Embeddings]</code> <p>Callable that generates embeddings for input texts.</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def embedding_fn(self) -&gt; Callable[[List[str]], Embeddings]:\n    \"\"\"Get the embedding function for Azure OpenAI.\n\n    Returns:\n        Callable that generates embeddings for input texts.\n    \"\"\"\n    return EmbeddingFunctionCallable(self, self.config.batch_size)\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.LlamaCppServerEmbeddings","title":"<code>LlamaCppServerEmbeddings(config=LCSEC())</code>","text":"<p>               Bases: <code>EmbeddingModel</code></p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, config: LCSEC = LCSEC()):\n    super().__init__()\n    self.config = config\n\n    if self.config.api_base == \"\":\n        raise ValueError(\n            \"\"\"Api Base MUST be set for Llama Server Embeddings.\n            \"\"\"\n        )\n\n    self.tokenize_url = self.config.api_base + \"/tokenize\"\n    self.detokenize_url = self.config.api_base + \"/detokenize\"\n    self.embedding_url = self.config.api_base + \"/embeddings\"\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.GeminiEmbeddings","title":"<code>GeminiEmbeddings(config=GeminiEmbeddingsConfig())</code>","text":"<p>               Bases: <code>EmbeddingModel</code></p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def __init__(self, config: GeminiEmbeddingsConfig = GeminiEmbeddingsConfig()):\n    try:\n        from google import genai\n    except ImportError as e:\n        raise LangroidImportError(extra=\"google-genai\", error=str(e))\n    super().__init__()\n    self.config = config\n    load_dotenv()\n    self.config.api_key = os.getenv(\"GEMINI_API_KEY\", \"\")\n\n    if self.config.api_key == \"\":\n        raise ValueError(\n            \"\"\"\n            GEMINI_API_KEY env variable must be set to use GeminiEmbeddings.\n            \"\"\"\n        )\n    self.client = genai.Client(api_key=self.config.api_key)\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.GeminiEmbeddings.generate_embeddings","title":"<code>generate_embeddings(texts)</code>","text":"<p>Generates embeddings for a list of input texts.</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def generate_embeddings(self, texts: List[str]) -&gt; List[List[float]]:\n    \"\"\"Generates embeddings for a list of input texts.\"\"\"\n    all_embeddings: List[List[float]] = []\n\n    for batch in batched(texts, self.config.batch_size):\n        result = self.client.models.embed_content(  # type: ignore[attr-defined]\n            model=self.config.model_name,\n            contents=batch,  # type: ignore\n        )\n\n        if not hasattr(result, \"embeddings\") or not isinstance(\n            result.embeddings, list\n        ):\n            raise ValueError(\n                \"Unexpected format for embeddings: missing or incorrect type\"\n            )\n\n        # Extract .values from ContentEmbedding objects\n        all_embeddings.extend(\n            [emb.values for emb in result.embeddings]  # type: ignore\n        )\n\n    return all_embeddings\n</code></pre>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.embedding_model","title":"<code>embedding_model(embedding_fn_type='openai')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_fn_type</code> <code>str</code> <p>Type of embedding model to use. Options are: - \"openai\", - \"azure-openai\", - \"sentencetransformer\", or - \"fastembed\". (others may be added in the future)</p> <code>'openai'</code> <p>Returns:     EmbeddingModel: The corresponding embedding model class.</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def embedding_model(embedding_fn_type: str = \"openai\") -&gt; EmbeddingModel:\n    \"\"\"\n    Args:\n        embedding_fn_type: Type of embedding model to use. Options are:\n         - \"openai\",\n         - \"azure-openai\",\n         - \"sentencetransformer\", or\n         - \"fastembed\".\n            (others may be added in the future)\n    Returns:\n        EmbeddingModel: The corresponding embedding model class.\n    \"\"\"\n    if embedding_fn_type == \"openai\":\n        return OpenAIEmbeddings  # type: ignore\n    elif embedding_fn_type == \"azure-openai\":\n        return AzureOpenAIEmbeddings  # type: ignore\n    elif embedding_fn_type == \"fastembed\":\n        return FastEmbedEmbeddings  # type: ignore\n    elif embedding_fn_type == \"llamacppserver\":\n        return LlamaCppServerEmbeddings  # type: ignore\n    elif embedding_fn_type == \"gemini\":\n        return GeminiEmbeddings  # type: ignore\n    else:  # default sentence transformer\n        return SentenceTransformerEmbeddings  # type: ignore\n</code></pre>"},{"location":"reference/embedding_models/remote_embeds/","title":"remote_embeds","text":"<p>langroid/embedding_models/remote_embeds.py </p> <p>If run as a script, starts an RPC server which handles remote embedding requests:</p> <p>For example: python3 -m langroid.embedding_models.remote_embeds --port <code>port</code></p> <p>where <code>port</code> is the port at which the service is exposed.  Currently, supports insecure connections only, and this should NOT be exposed to the internet.</p>"},{"location":"reference/embedding_models/remote_embeds/#langroid.embedding_models.remote_embeds.serve","title":"<code>serve(bind_address_base='localhost', port=50052, batch_size=512, data_parallel=False, device=None, devices=None, model_name='BAAI/bge-large-en-v1.5')</code>  <code>async</code>","text":"<p>Starts the RPC server.</p> Source code in <code>langroid/embedding_models/remote_embeds.py</code> <pre><code>async def serve(\n    bind_address_base: str = \"localhost\",\n    port: int = 50052,\n    batch_size: int = 512,\n    data_parallel: bool = False,\n    device: Optional[str] = None,\n    devices: Optional[list[str]] = None,\n    model_name: str = \"BAAI/bge-large-en-v1.5\",\n) -&gt; None:\n    \"\"\"Starts the RPC server.\"\"\"\n    server = grpc.aio.server()\n    embeddings_grpc.add_EmbeddingServicer_to_server(\n        RemoteEmbeddingRPCs(\n            model_name=model_name,\n            batch_size=batch_size,\n            data_parallel=data_parallel,\n            device=device,\n            devices=devices,\n        ),\n        server,\n    )  # type: ignore\n    url = f\"{bind_address_base}:{port}\"\n    server.add_insecure_port(url)\n    await server.start()\n    print(f\"Embedding server started, listening on {url}\")\n    await server.wait_for_termination()\n</code></pre>"},{"location":"reference/embedding_models/protoc/","title":"protoc","text":"<p>langroid/embedding_models/protoc/init.py </p>"},{"location":"reference/embedding_models/protoc/embeddings_pb2/","title":"embeddings_pb2","text":"<p>langroid/embedding_models/protoc/embeddings_pb2.py </p> <p>Generated protocol buffer code.</p>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/","title":"embeddings_pb2_grpc","text":"<p>langroid/embedding_models/protoc/embeddings_pb2_grpc.py </p> <p>Client and server classes corresponding to protobuf-defined services.</p>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/#langroid.embedding_models.protoc.embeddings_pb2_grpc.EmbeddingStub","title":"<code>EmbeddingStub(channel)</code>","text":"<p>               Bases: <code>object</code></p> <p>Missing associated documentation comment in .proto file.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <p>A grpc.Channel.</p> required Source code in <code>langroid/embedding_models/protoc/embeddings_pb2_grpc.py</code> <pre><code>def __init__(self, channel):\n    \"\"\"Constructor.\n\n    Args:\n        channel: A grpc.Channel.\n    \"\"\"\n    self.Embed = channel.unary_unary(\n        \"/Embedding/Embed\",\n        request_serializer=embeddings__pb2.EmbeddingRequest.SerializeToString,\n        response_deserializer=embeddings__pb2.BatchEmbeds.FromString,\n    )\n</code></pre>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/#langroid.embedding_models.protoc.embeddings_pb2_grpc.EmbeddingServicer","title":"<code>EmbeddingServicer</code>","text":"<p>               Bases: <code>object</code></p> <p>Missing associated documentation comment in .proto file.</p>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/#langroid.embedding_models.protoc.embeddings_pb2_grpc.EmbeddingServicer.Embed","title":"<code>Embed(request, context)</code>","text":"<p>Missing associated documentation comment in .proto file.</p> Source code in <code>langroid/embedding_models/protoc/embeddings_pb2_grpc.py</code> <pre><code>def Embed(self, request, context):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\"Method not implemented!\")\n    raise NotImplementedError(\"Method not implemented!\")\n</code></pre>"},{"location":"reference/embedding_models/protoc/embeddings_pb2_grpc/#langroid.embedding_models.protoc.embeddings_pb2_grpc.Embedding","title":"<code>Embedding</code>","text":"<p>               Bases: <code>object</code></p> <p>Missing associated documentation comment in .proto file.</p>"},{"location":"reference/language_models/","title":"language_models","text":"<p>langroid/language_models/init.py </p>"},{"location":"reference/language_models/#langroid.language_models.LLMConfig","title":"<code>LLMConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Common configuration for all language models.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMMessage","title":"<code>LLMMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class representing an entry in the msg-history sent to the LLM API. It could be one of these: - a user message - an LLM (\"Assistant\") response - a fn-call or tool-call-list from an OpenAI-compatible LLM API response - a result or results from executing a fn or tool-call(s)</p>"},{"location":"reference/language_models/#langroid.language_models.LLMMessage.api_dict","title":"<code>api_dict(model, has_system_role=True)</code>","text":"<p>Convert to dictionary for API request, keeping ONLY the fields that are expected in an API call! E.g., DROP the tool_id, since it is only for use in the Assistant API,     not the completion API.</p> <p>Parameters:</p> Name Type Description Default <code>has_system_role</code> <code>bool</code> <p>whether the message has a system role (if not, set to \"user\" role)</p> <code>True</code> <p>Returns:     dict: dictionary representation of LLM message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def api_dict(self, model: str, has_system_role: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to dictionary for API request, keeping ONLY\n    the fields that are expected in an API call!\n    E.g., DROP the tool_id, since it is only for use in the Assistant API,\n        not the completion API.\n\n    Args:\n        has_system_role: whether the message has a system role (if not,\n            set to \"user\" role)\n    Returns:\n        dict: dictionary representation of LLM message\n    \"\"\"\n    d = self.model_dump()\n    files: List[FileAttachment] = d.pop(\"files\")\n    if len(files) &gt; 0 and self.role == Role.USER:\n        # In there are files, then content is an array of\n        # different content-parts\n        d[\"content\"] = [\n            dict(\n                type=\"text\",\n                text=self.content,\n            )\n        ] + [f.to_dict(model) for f in self.files]\n\n    # if there is a key k = \"role\" with value \"system\", change to \"user\"\n    # in case has_system_role is False\n    if not has_system_role and \"role\" in d and d[\"role\"] == \"system\":\n        d[\"role\"] = \"user\"\n        if \"content\" in d:\n            d[\"content\"] = \"[ADDITIONAL SYSTEM MESSAGE:]\\n\\n\" + d[\"content\"]\n    # drop None values since API doesn't accept them\n    dict_no_none = {k: v for k, v in d.items() if v is not None}\n    if \"name\" in dict_no_none and dict_no_none[\"name\"] == \"\":\n        # OpenAI API does not like empty name\n        del dict_no_none[\"name\"]\n    if \"function_call\" in dict_no_none:\n        # arguments must be a string\n        if \"arguments\" in dict_no_none[\"function_call\"]:\n            dict_no_none[\"function_call\"][\"arguments\"] = json.dumps(\n                dict_no_none[\"function_call\"][\"arguments\"]\n            )\n    if \"tool_calls\" in dict_no_none:\n        # convert tool calls to API format\n        for tc in dict_no_none[\"tool_calls\"]:\n            if \"arguments\" in tc[\"function\"]:\n                # arguments must be a string\n                if tc[\"function\"][\"arguments\"] is None:\n                    tc[\"function\"][\"arguments\"] = \"{}\"\n                else:\n                    tc[\"function\"][\"arguments\"] = json.dumps(\n                        tc[\"function\"][\"arguments\"]\n                    )\n            if \"extra_content\" in tc and tc[\"extra_content\"] is None:\n                del tc[\"extra_content\"]\n    # IMPORTANT! drop fields that are not expected in API call\n    dict_no_none.pop(\"tool_id\", None)\n    dict_no_none.pop(\"timestamp\", None)\n    dict_no_none.pop(\"chat_document_id\", None)\n    return dict_no_none\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.LLMFunctionCall","title":"<code>LLMFunctionCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structure of LLM response indicating it \"wants\" to call a function. Modeled after OpenAI spec for <code>function_call</code> field in ChatCompletion API.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMFunctionCall.from_dict","title":"<code>from_dict(message)</code>  <code>staticmethod</code>","text":"<p>Initialize from dictionary. Args:     d: dictionary containing fields to initialize</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef from_dict(message: Dict[str, Any]) -&gt; \"LLMFunctionCall\":\n    \"\"\"\n    Initialize from dictionary.\n    Args:\n        d: dictionary containing fields to initialize\n    \"\"\"\n    fun_call = LLMFunctionCall(name=message[\"name\"])\n    fun_args_str = message[\"arguments\"]\n    # sometimes may be malformed with invalid indents,\n    # so we try to be safe by removing newlines.\n    if fun_args_str is not None:\n        fun_args_str = fun_args_str.replace(\"\\n\", \"\").strip()\n        dict_or_list = parse_imperfect_json(fun_args_str)\n\n        if not isinstance(dict_or_list, dict):\n            raise ValueError(\n                f\"\"\"\n                    Invalid function args: {fun_args_str}\n                    parsed as {dict_or_list},\n                    which is not a valid dict.\n                    \"\"\"\n            )\n        fun_args = dict_or_list\n    else:\n        fun_args = None\n    fun_call.arguments = fun_args\n\n    return fun_call\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.LLMFunctionSpec","title":"<code>LLMFunctionSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Description of a function available for the LLM to use. To be used when calling the LLM <code>chat()</code> method with the <code>functions</code> parameter. Modeled after OpenAI spec for <code>functions</code> fields in ChatCompletion API.</p>"},{"location":"reference/language_models/#langroid.language_models.Role","title":"<code>Role</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Possible roles for a message in a chat.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMTokenUsage","title":"<code>LLMTokenUsage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Usage of tokens by an LLM.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMResponse","title":"<code>LLMResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class representing response from LLM.</p>"},{"location":"reference/language_models/#langroid.language_models.LLMResponse.to_LLMMessage","title":"<code>to_LLMMessage()</code>","text":"<p>Convert LLM response to an LLMMessage, to be included in the message-list sent to the API. This is currently NOT used in any significant way in the library, and is only provided as a utility to construct a message list for the API when directly working with an LLM object.</p> <p>In a <code>ChatAgent</code>, an LLM response is first converted to a ChatDocument, which is in turn converted to an LLMMessage via <code>ChatDocument.to_LLMMessage()</code> See <code>ChatAgent._prep_llm_messages()</code> and <code>ChatAgent.llm_response_messages</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def to_LLMMessage(self) -&gt; LLMMessage:\n    \"\"\"Convert LLM response to an LLMMessage, to be included in the\n    message-list sent to the API.\n    This is currently NOT used in any significant way in the library, and is only\n    provided as a utility to construct a message list for the API when directly\n    working with an LLM object.\n\n    In a `ChatAgent`, an LLM response is first converted to a ChatDocument,\n    which is in turn converted to an LLMMessage via `ChatDocument.to_LLMMessage()`\n    See `ChatAgent._prep_llm_messages()` and `ChatAgent.llm_response_messages`\n    \"\"\"\n    return LLMMessage(\n        role=Role.ASSISTANT,\n        content=self.message,\n        name=None if self.function_call is None else self.function_call.name,\n        function_call=self.function_call,\n        tool_calls=self.oai_tool_calls,\n    )\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.LLMResponse.get_recipient_and_message","title":"<code>get_recipient_and_message(recognize_recipient_in_content=True)</code>","text":"<p>If <code>message</code> or <code>function_call</code> of an LLM response contains an explicit recipient name, return this recipient name and <code>message</code> stripped of the recipient name if specified.</p> <p>Two cases: (a) <code>message</code> contains addressing string <code>TO[&lt;name&gt;]:&lt;content&gt;</code>, or (b) <code>message</code> is empty and function_call/tool_call with explicit <code>recipient</code></p> <p>Parameters:</p> Name Type Description Default <code>recognize_recipient_in_content</code> <code>bool</code> <p>When True (default), parses message text for <code>TO[&lt;recipient&gt;]:&lt;content&gt;</code> patterns and top-level JSON <code>{\"recipient\": \"...\"}</code> fields. When False, only function_call/tool_call <code>recipient</code> fields are checked.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>name of recipient, which may be empty string if no recipient</p> <code>str</code> <p>content of message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_recipient_and_message(\n    self,\n    recognize_recipient_in_content: bool = True,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    If `message` or `function_call` of an LLM response contains an explicit\n    recipient name, return this recipient name and `message` stripped\n    of the recipient name if specified.\n\n    Two cases:\n    (a) `message` contains addressing string ``TO[&lt;name&gt;]:&lt;content&gt;``, or\n    (b) `message` is empty and function_call/tool_call with explicit `recipient`\n\n    Args:\n        recognize_recipient_in_content (bool): When True (default), parses\n            message text for ``TO[&lt;recipient&gt;]:&lt;content&gt;`` patterns and\n            top-level JSON ``{\"recipient\": \"...\"}`` fields. When False,\n            only function_call/tool_call ``recipient`` fields are checked.\n\n    Returns:\n        (str): name of recipient, which may be empty string if no recipient\n        (str): content of message\n\n    \"\"\"\n\n    if self.function_call is not None:\n        # in this case we ignore message, since all information is in function_call\n        msg = \"\"\n        args = self.function_call.arguments\n        recipient = \"\"\n        if isinstance(args, dict):\n            recipient = args.get(\"recipient\", \"\")\n        return recipient, msg\n    else:\n        msg = self.message\n        if self.oai_tool_calls is not None:\n            # get the first tool that has a recipient field, if any\n            for tc in self.oai_tool_calls:\n                if tc.function is not None and tc.function.arguments is not None:\n                    recipient = tc.function.arguments.get(\n                        \"recipient\"\n                    )  # type: ignore\n                    if recipient is not None and recipient != \"\":\n                        return recipient, \"\"\n\n    if not recognize_recipient_in_content:\n        return \"\", msg\n\n    # It's not a function or tool call, so continue looking to see\n    # if a recipient is specified in the message.\n\n    # First check if message contains \"TO: &lt;recipient&gt; &lt;content&gt;\"\n    recipient_name, content = parse_message(msg) if msg is not None else (\"\", \"\")\n    # check if there is a top level json that specifies 'recipient',\n    # and retain the entire message as content.\n    if recipient_name == \"\":\n        recipient_name = top_level_json_field(msg, \"recipient\") if msg else \"\"\n        content = msg\n    return recipient_name, content\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIChatModel","title":"<code>OpenAIChatModel</code>","text":"<p>               Bases: <code>ModelName</code></p> <p>Enum for OpenAI Chat models</p>"},{"location":"reference/language_models/#langroid.language_models.AnthropicModel","title":"<code>AnthropicModel</code>","text":"<p>               Bases: <code>ModelName</code></p> <p>Enum for Anthropic models</p>"},{"location":"reference/language_models/#langroid.language_models.GeminiModel","title":"<code>GeminiModel</code>","text":"<p>               Bases: <code>ModelName</code></p> <p>Enum for Gemini models</p>"},{"location":"reference/language_models/#langroid.language_models.OpenAICompletionModel","title":"<code>OpenAICompletionModel</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Completion models</p>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig","title":"<code>OpenAIGPTConfig(**kwargs)</code>","text":"<p>               Bases: <code>LLMConfig</code></p> <p>Class for any LLM with an OpenAI-like API: besides the OpenAI models this includes: (a) locally-served models behind an OpenAI-compatible API (b) non-local models, using a proxy adaptor lib like litellm that provides     an OpenAI-compatible API. (We could rename this class to OpenAILikeConfig, but we keep it as-is for now)</p> <p>Important Note: Due to the <code>env_prefix = \"OPENAI_\"</code> defined below, all of the fields below can be set AND OVERRIDDEN via env vars,</p>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig--by-upper-casing-the-name-and-prefixing-with-openai_-eg","title":"by upper-casing the name and prefixing with OPENAI_, e.g.","text":""},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig--openai_max_output_tokens1000","title":"OPENAI_MAX_OUTPUT_TOKENS=1000.","text":""},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig--if-any-of-these-is-defined-in-this-way-in-the-environment","title":"If any of these is defined in this way in the environment","text":""},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig--either-via-explicit-setenv-or-export-or-via-env-file-load_dotenv","title":"(either via explicit setenv or export or via .env file + load_dotenv()),","text":""},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig--the-environment-variable-takes-precedence-over-the-value-in-the-config","title":"the environment variable takes precedence over the value in the config.","text":"Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:  # type: ignore\n    local_model = \"api_base\" in kwargs and kwargs[\"api_base\"] is not None\n\n    chat_model = kwargs.get(\"chat_model\", \"\")\n    local_prefixes = [\"local/\", \"litellm/\", \"ollama/\", \"vllm/\", \"llamacpp/\"]\n    if any(chat_model.startswith(prefix) for prefix in local_prefixes):\n        local_model = True\n\n    warn_gpt_3_5 = (\n        \"chat_model\" not in kwargs.keys()\n        and not local_model\n        and default_openai_chat_model == OpenAIChatModel.GPT3_5_TURBO\n    )\n\n    if warn_gpt_3_5:\n        existing_hook = kwargs.get(\"run_on_first_use\", noop)\n\n        def with_warning() -&gt; None:\n            existing_hook()\n            gpt_3_5_warning()\n\n        kwargs[\"run_on_first_use\"] = with_warning\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig.model_copy","title":"<code>model_copy(*, update=None, deep=False)</code>","text":"<p>Copy config while preserving nested model instances and subclasses.</p> <p>Important: Avoid reconstructing via <code>model_dump</code> as that coerces nested models to their annotated base types (dropping subclass-only fields). Instead, defer to Pydantic's native <code>model_copy</code>, which keeps nested <code>BaseModel</code> instances (and their concrete subclasses) intact.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def model_copy(\n    self, *, update: Mapping[str, Any] | None = None, deep: bool = False\n) -&gt; \"OpenAIGPTConfig\":\n    \"\"\"\n    Copy config while preserving nested model instances and subclasses.\n\n    Important: Avoid reconstructing via `model_dump` as that coerces nested\n    models to their annotated base types (dropping subclass-only fields).\n    Instead, defer to Pydantic's native `model_copy`, which keeps nested\n    `BaseModel` instances (and their concrete subclasses) intact.\n    \"\"\"\n    # Delegate to BaseSettings/BaseModel implementation to preserve types\n    return super().model_copy(update=update, deep=deep)  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPTConfig.create","title":"<code>create(prefix)</code>  <code>classmethod</code>","text":"<p>Create a config class whose params can be set via a desired prefix from the .env file or env vars. E.g., using <pre><code>OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\nollama_config = OllamaConfig()\n</code></pre> you can have a group of params prefixed by \"OLLAMA_\", to be used with models served via <code>ollama</code>. This way, you can maintain several setting-groups in your .env file, one per model type.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>@classmethod\ndef create(cls, prefix: str) -&gt; Type[\"OpenAIGPTConfig\"]:\n    \"\"\"Create a config class whose params can be set via a desired\n    prefix from the .env file or env vars.\n    E.g., using\n    ```python\n    OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\n    ollama_config = OllamaConfig()\n    ```\n    you can have a group of params prefixed by \"OLLAMA_\", to be used\n    with models served via `ollama`.\n    This way, you can maintain several setting-groups in your .env file,\n    one per model type.\n    \"\"\"\n\n    class DynamicConfig(OpenAIGPTConfig):\n        pass\n\n    DynamicConfig.model_config = SettingsConfigDict(env_prefix=prefix.upper() + \"_\")\n    return DynamicConfig\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT","title":"<code>OpenAIGPT(config=OpenAIGPTConfig())</code>","text":"<p>               Bases: <code>LanguageModel</code></p> <p>Class for OpenAI LLMs</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, config: OpenAIGPTConfig = OpenAIGPTConfig()):\n    \"\"\"\n    Args:\n        config: configuration for openai-gpt model\n    \"\"\"\n    # copy the config to avoid modifying the original; deep to decouple\n    # nested models while preserving their concrete subclasses\n    config = config.model_copy(deep=True)\n    super().__init__(config)\n    self.config: OpenAIGPTConfig = config\n    # save original model name such as `provider/model` before\n    # we strip out the `provider` - we retain the original in\n    # case some params are specific to a provider.\n    self.chat_model_orig = self.config.chat_model_orig or self.config.chat_model\n\n    # Run the first time the model is used\n    self.run_on_first_use = cache(self.config.run_on_first_use)\n\n    # global override of chat_model,\n    # to allow quick testing with other models\n    if settings.chat_model != \"\":\n        self.config.chat_model = settings.chat_model\n        self.chat_model_orig = settings.chat_model\n        self.config.completion_model = settings.chat_model\n\n    if len(parts := self.config.chat_model.split(\"//\")) &gt; 1:\n        # there is a formatter specified, e.g.\n        # \"litellm/ollama/mistral//hf\" or\n        # \"local/localhost:8000/v1//mistral-instruct-v0.2\"\n        formatter = parts[1]\n        self.config.chat_model = parts[0]\n        if formatter == \"hf\":\n            # e.g. \"litellm/ollama/mistral//hf\" -&gt; \"litellm/ollama/mistral\"\n            formatter = find_hf_formatter(self.config.chat_model)\n            if formatter != \"\":\n                # e.g. \"mistral\"\n                self.config.formatter = formatter\n                logging.warning(\n                    f\"\"\"\n                    Using completions (not chat) endpoint with HuggingFace\n                    chat_template for {formatter} for\n                    model {self.config.chat_model}\n                    \"\"\"\n                )\n        else:\n            # e.g. \"local/localhost:8000/v1//mistral-instruct-v0.2\"\n            self.config.formatter = formatter\n\n    if self.config.formatter is not None:\n        self.config.hf_formatter = HFFormatter(\n            HFPromptFormatterConfig(model_name=self.config.formatter)\n        )\n\n    self.supports_json_schema: bool = self.config.supports_json_schema or False\n    self.supports_strict_tools: bool = self.config.supports_strict_tools or False\n\n    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", DUMMY_API_KEY)\n    self.api_key = config.api_key\n\n    # if model name starts with \"litellm\",\n    # set the actual model name by stripping the \"litellm/\" prefix\n    # and set the litellm flag to True\n    if self.config.chat_model.startswith(\"litellm/\") or self.config.litellm:\n        # e.g. litellm/ollama/mistral\n        self.config.litellm = True\n        self.api_base = self.config.api_base\n        if self.config.chat_model.startswith(\"litellm/\"):\n            # strip the \"litellm/\" prefix\n            # e.g. litellm/ollama/llama2 =&gt; ollama/llama2\n            self.config.chat_model = self.config.chat_model.split(\"/\", 1)[1]\n    elif self.config.chat_model.startswith(\"local/\"):\n        # expect this to be of the form \"local/localhost:8000/v1\",\n        # depending on how the model is launched locally.\n        # In this case the model served locally behind an OpenAI-compatible API\n        # so we can just use `openai.*` methods directly,\n        # and don't need a adaptor library like litellm\n        self.config.litellm = False\n        self.config.seed = None  # some models raise an error when seed is set\n        # Extract the api_base from the model name after the \"local/\" prefix\n        self.api_base = self.config.chat_model.split(\"/\", 1)[1]\n        if not self.api_base.startswith(\"http\"):\n            self.api_base = \"http://\" + self.api_base\n    elif self.config.chat_model.startswith(\"ollama/\"):\n        self.config.ollama = True\n\n        # use api_base from config if set, else fall back on OLLAMA_BASE_URL\n        self.api_base = self.config.api_base or OLLAMA_BASE_URL\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = OLLAMA_API_KEY\n        self.config.chat_model = self.config.chat_model.replace(\"ollama/\", \"\")\n    elif self.config.chat_model.startswith(\"vllm/\"):\n        self.supports_json_schema = True\n        self.config.chat_model = self.config.chat_model.replace(\"vllm/\", \"\")\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = os.environ.get(\"VLLM_API_KEY\", DUMMY_API_KEY)\n        self.api_base = self.config.api_base or \"http://localhost:8000/v1\"\n        if not self.api_base.startswith(\"http\"):\n            self.api_base = \"http://\" + self.api_base\n        if not self.api_base.endswith(\"/v1\"):\n            self.api_base = self.api_base + \"/v1\"\n    elif self.config.chat_model.startswith(\"llamacpp/\"):\n        self.supports_json_schema = True\n        self.api_base = self.config.chat_model.split(\"/\", 1)[1]\n        if not self.api_base.startswith(\"http\"):\n            self.api_base = \"http://\" + self.api_base\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = os.environ.get(\"LLAMA_API_KEY\", DUMMY_API_KEY)\n    else:\n        self.api_base = self.config.api_base\n        # If api_base is unset we use OpenAI's endpoint, which supports\n        # these features (with JSON schema restricted to a limited set of models)\n        self.supports_strict_tools = self.api_base is None\n        self.supports_json_schema = (\n            self.api_base is None and self.info().has_structured_output\n        )\n\n    if settings.chat_model != \"\":\n        # if we're overriding chat model globally, set completion model to same\n        self.config.completion_model = self.config.chat_model\n\n    if self.config.formatter is not None:\n        # we want to format chats -&gt; completions using this specific formatter\n        self.config.use_completion_for_chat = True\n        self.config.completion_model = self.config.chat_model\n\n    if self.config.use_completion_for_chat:\n        self.config.use_chat_for_completion = False\n\n    self.is_groq = self.config.chat_model.startswith(\"groq/\")\n    self.is_cerebras = self.config.chat_model.startswith(\"cerebras/\")\n    self.is_gemini = self.is_gemini_model()\n    self.is_deepseek = self.is_deepseek_model()\n    self.is_glhf = self.config.chat_model.startswith(\"glhf/\")\n    self.is_openrouter = self.config.chat_model.startswith(\"openrouter/\")\n    self.is_langdb = self.config.chat_model.startswith(\"langdb/\")\n    self.is_portkey = self.config.chat_model.startswith(\"portkey/\")\n    self.is_litellm_proxy = self.config.chat_model.startswith(\"litellm-proxy/\")\n\n    if self.is_groq:\n        # use groq-specific client\n        self.config.chat_model = self.config.chat_model.replace(\"groq/\", \"\")\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = os.getenv(\"GROQ_API_KEY\", DUMMY_API_KEY)\n        if self.config.use_cached_client:\n            self.client = get_groq_client(api_key=self.api_key)\n            self.async_client = get_async_groq_client(api_key=self.api_key)\n        else:\n            # Create new clients without caching\n            self.client = Groq(api_key=self.api_key)\n            self.async_client = AsyncGroq(api_key=self.api_key)\n    elif self.is_cerebras:\n        # use cerebras-specific client\n        self.config.chat_model = self.config.chat_model.replace(\"cerebras/\", \"\")\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = os.getenv(\"CEREBRAS_API_KEY\", DUMMY_API_KEY)\n        if self.config.use_cached_client:\n            self.client = get_cerebras_client(api_key=self.api_key)\n            # TODO there is not async client, so should we do anything here?\n            self.async_client = get_async_cerebras_client(api_key=self.api_key)\n        else:\n            # Create new clients without caching\n            self.client = Cerebras(api_key=self.api_key)\n            self.async_client = AsyncCerebras(api_key=self.api_key)\n    else:\n        # in these cases, there's no specific client: OpenAI python client suffices\n        if self.is_litellm_proxy:\n            self.config.chat_model = self.config.chat_model.replace(\n                \"litellm-proxy/\", \"\"\n            )\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = self.config.litellm_proxy.api_key or self.api_key\n            self.api_base = self.config.litellm_proxy.api_base or self.api_base\n        elif self.is_gemini:\n            self.config.chat_model = self.config.chat_model.replace(\"gemini/\", \"\")\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = os.getenv(\"GEMINI_API_KEY\", DUMMY_API_KEY)\n            # Use GEMINI_API_BASE env var if set (e.g. for Vertex AI),\n            # then config.api_base only if explicitly set by the user\n            # (not inherited from OPENAI_API_BASE via env_prefix),\n            # then fall back to the default Gemini endpoint.\n            gemini_api_base = os.getenv(\"GEMINI_API_BASE\", \"\")\n            openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n            explicit_api_base = (\n                self.config.api_base\n                if self.config.api_base and self.config.api_base != openai_api_base\n                else None\n            )\n            self.api_base = gemini_api_base or explicit_api_base or GEMINI_BASE_URL\n        elif self.is_glhf:\n            self.config.chat_model = self.config.chat_model.replace(\"glhf/\", \"\")\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = os.getenv(\"GLHF_API_KEY\", DUMMY_API_KEY)\n            self.api_base = GLHF_BASE_URL\n        elif self.is_openrouter:\n            self.config.chat_model = self.config.chat_model.replace(\n                \"openrouter/\", \"\"\n            )\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = os.getenv(\"OPENROUTER_API_KEY\", DUMMY_API_KEY)\n            self.api_base = OPENROUTER_BASE_URL\n        elif self.is_deepseek:\n            self.config.chat_model = self.config.chat_model.replace(\"deepseek/\", \"\")\n            self.api_base = DEEPSEEK_BASE_URL\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = os.getenv(\"DEEPSEEK_API_KEY\", DUMMY_API_KEY)\n        elif self.is_langdb:\n            self.config.chat_model = self.config.chat_model.replace(\"langdb/\", \"\")\n            self.api_base = self.config.langdb_params.base_url\n            project_id = self.config.langdb_params.project_id\n            if project_id:\n                self.api_base += \"/\" + project_id + \"/v1\"\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = self.config.langdb_params.api_key or DUMMY_API_KEY\n\n            if self.config.langdb_params:\n                params = self.config.langdb_params\n                if params.project_id:\n                    self.config.headers[\"x-project-id\"] = params.project_id\n                if params.label:\n                    self.config.headers[\"x-label\"] = params.label\n                if params.run_id:\n                    self.config.headers[\"x-run-id\"] = params.run_id\n                if params.thread_id:\n                    self.config.headers[\"x-thread-id\"] = params.thread_id\n        elif self.is_portkey:\n            # Parse the model string and extract provider/model\n            provider, model = self.config.portkey_params.parse_model_string(\n                self.config.chat_model\n            )\n            self.config.chat_model = model\n            if provider:\n                self.config.portkey_params.provider = provider\n\n            # Set Portkey base URL\n            self.api_base = self.config.portkey_params.base_url + \"/v1\"\n\n            # Set API key - use provider's API key from env if available\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = self.config.portkey_params.get_provider_api_key(\n                    self.config.portkey_params.provider, DUMMY_API_KEY\n                )\n\n            # Add Portkey-specific headers\n            self.config.headers.update(self.config.portkey_params.get_headers())\n\n        # Create http_client if needed - Priority order:\n        # 1. http_client_factory (most flexibility, not cacheable)\n        # 2. http_client_config (cacheable, moderate flexibility)\n        # 3. http_verify_ssl=False (cacheable, simple SSL bypass)\n        http_client = None\n        async_http_client = None\n        http_client_config_used = None\n\n        if self.config.http_client_factory is not None:\n            # Use the factory to create http_client (not cacheable)\n            http_client = self.config.http_client_factory()\n            if isinstance(http_client, (list, tuple)):\n                if len(http_client) != 2:\n                    raise ValueError(\n                        \"http_client_factory must return either a single \"\n                        \"httpx.Client or a tuple of \"\n                        \"(httpx.Client, httpx.AsyncClient)\"\n                    )\n                http_client, async_http_client = http_client\n            else:\n                # set async_http_client to None - so that it will\n                # be created later\n                async_http_client = None\n        elif self.config.http_client_config is not None:\n            # Use config dict (cacheable)\n            http_client_config_used = self.config.http_client_config\n        elif not self.config.http_verify_ssl:\n            # Simple SSL bypass (cacheable)\n            http_client_config_used = {\"verify\": False}\n            logging.warning(\n                \"SSL verification has been disabled. This is insecure and \"\n                \"should only be used in trusted environments (e.g., \"\n                \"corporate networks with self-signed certificates).\"\n            )\n\n        if self.config.use_cached_client:\n            self.client = get_openai_client(\n                api_key=self.api_key,\n                base_url=self.api_base,\n                organization=self.config.organization,\n                timeout=Timeout(self.config.timeout),\n                default_headers=self.config.headers,\n                http_client=http_client,\n                http_client_config=http_client_config_used,\n            )\n            self.async_client = get_async_openai_client(\n                api_key=self.api_key,\n                base_url=self.api_base,\n                organization=self.config.organization,\n                timeout=Timeout(self.config.timeout),\n                default_headers=self.config.headers,\n                http_client=async_http_client,\n                http_client_config=http_client_config_used,\n            )\n        else:\n            # Create new clients without caching\n            client_kwargs: Dict[str, Any] = dict(\n                api_key=self.api_key,\n                base_url=self.api_base,\n                organization=self.config.organization,\n                timeout=Timeout(self.config.timeout),\n                default_headers=self.config.headers,\n            )\n            if http_client is not None:\n                client_kwargs[\"http_client\"] = http_client\n            elif http_client_config_used is not None:\n                # Create http_client from config for non-cached scenario\n                try:\n                    from httpx import Client\n\n                    client_kwargs[\"http_client\"] = Client(**http_client_config_used)\n                except ImportError:\n                    raise ValueError(\n                        \"httpx is required to use http_client_config. \"\n                        \"Install it with: pip install httpx\"\n                    )\n            self.client = OpenAI(**client_kwargs)\n\n            async_client_kwargs: Dict[str, Any] = dict(\n                api_key=self.api_key,\n                base_url=self.api_base,\n                organization=self.config.organization,\n                timeout=Timeout(self.config.timeout),\n                default_headers=self.config.headers,\n            )\n            if async_http_client is not None:\n                async_client_kwargs[\"http_client\"] = async_http_client\n            elif http_client_config_used is not None:\n                # Create async http_client from config for non-cached scenario\n                try:\n                    from httpx import AsyncClient\n\n                    async_client_kwargs[\"http_client\"] = AsyncClient(\n                        **http_client_config_used\n                    )\n                except ImportError:\n                    raise ValueError(\n                        \"httpx is required to use http_client_config. \"\n                        \"Install it with: pip install httpx\"\n                    )\n            self.async_client = AsyncOpenAI(**async_client_kwargs)\n\n    self.cache: CacheDB | None = None\n    use_cache = self.config.cache_config is not None\n    if \"redis\" in settings.cache_type and use_cache:\n        if config.cache_config is None or not isinstance(\n            config.cache_config,\n            RedisCacheConfig,\n        ):\n            # switch to fresh redis config if needed\n            config.cache_config = RedisCacheConfig(\n                fake=\"fake\" in settings.cache_type\n            )\n        if \"fake\" in settings.cache_type:\n            # force use of fake redis if global cache_type is \"fakeredis\"\n            config.cache_config.fake = True\n        self.cache = RedisCache(config.cache_config)\n    elif settings.cache_type != \"none\" and use_cache:\n        raise ValueError(\n            f\"Invalid cache type {settings.cache_type}. \"\n            \"Valid types are redis, fakeredis, none\"\n        )\n\n    self.config._validate_litellm()\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.is_gemini_model","title":"<code>is_gemini_model()</code>","text":"<p>Are we using the gemini OpenAI-compatible API?</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def is_gemini_model(self) -&gt; bool:\n    \"\"\"Are we using the gemini OpenAI-compatible API?\"\"\"\n    return self.chat_model_orig.startswith(\"gemini/\")\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.unsupported_params","title":"<code>unsupported_params()</code>","text":"<p>List of params that are not supported by the current model</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def unsupported_params(self) -&gt; List[str]:\n    \"\"\"\n    List of params that are not supported by the current model\n    \"\"\"\n    unsupported = set(self.info().unsupported_params)\n    return list(unsupported)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.rename_params","title":"<code>rename_params()</code>","text":"<p>Map of param name -&gt; new name for specific models. Currently main troublemaker is o1* series.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def rename_params(self) -&gt; Dict[str, str]:\n    \"\"\"\n    Map of param name -&gt; new name for specific models.\n    Currently main troublemaker is o1* series.\n    \"\"\"\n    return self.info().rename_params\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.chat_context_length","title":"<code>chat_context_length()</code>","text":"<p>Context-length for chat-completion models/endpoints. Get it from the config if explicitly given,  otherwise use model_info based on model name, and fall back to  generic model_info if there's no match.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for chat-completion models/endpoints.\n    Get it from the config if explicitly given,\n     otherwise use model_info based on model name, and fall back to\n     generic model_info if there's no match.\n    \"\"\"\n    return self.config.chat_context_length or self.info().context_length\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.completion_context_length","title":"<code>completion_context_length()</code>","text":"<p>Context-length for completion models/endpoints. Get it from the config if explicitly given,  otherwise use model_info based on model name, and fall back to  generic model_info if there's no match.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def completion_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for completion models/endpoints.\n    Get it from the config if explicitly given,\n     otherwise use model_info based on model name, and fall back to\n     generic model_info if there's no match.\n    \"\"\"\n    return (\n        self.config.completion_context_length\n        or self.completion_info().context_length\n    )\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.chat_cost","title":"<code>chat_cost()</code>","text":"<p>(Prompt, Cached, Generation) cost per 1000 tokens, for chat-completion models/endpoints. Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_cost(self) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    (Prompt, Cached, Generation) cost per 1000 tokens, for chat-completion\n    models/endpoints.\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    info = self.info()\n    cached_cost_per_million = info.cached_cost_per_million\n    if not cached_cost_per_million:\n        cached_cost_per_million = info.input_cost_per_million\n    return (\n        info.input_cost_per_million / 1000,\n        cached_cost_per_million / 1000,\n        info.output_cost_per_million / 1000,\n    )\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.set_stream","title":"<code>set_stream(stream)</code>","text":"<p>Enable or disable streaming output from API. Args:     stream: enable streaming output from API Returns: previous value of stream</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def set_stream(self, stream: bool) -&gt; bool:\n    \"\"\"Enable or disable streaming output from API.\n    Args:\n        stream: enable streaming output from API\n    Returns: previous value of stream\n    \"\"\"\n    tmp = self.config.stream\n    self.config.stream = stream\n    return tmp\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.get_stream","title":"<code>get_stream()</code>","text":"<p>Get streaming status.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def get_stream(self) -&gt; bool:\n    \"\"\"Get streaming status.\"\"\"\n    return self.config.stream and settings.stream and self.info().allows_streaming\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAIGPT.tool_deltas_to_tools","title":"<code>tool_deltas_to_tools(tools)</code>  <code>staticmethod</code>","text":"<p>Convert accumulated tool-call deltas to OpenAIToolCall objects. Adapted from this excellent code:  https://community.openai.com/t/help-for-function-calls-with-streaming/627170/2</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>List[Dict[str, Any]]</code> <p>list of tool deltas received from streaming API</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>plain text corresponding to tool calls that failed to parse</p> <code>List[OpenAIToolCall]</code> <p>List[OpenAIToolCall]: list of OpenAIToolCall objects</p> <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: list of tool dicts (to reconstruct OpenAI API response, so it can be cached)</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>@staticmethod\ndef tool_deltas_to_tools(\n    tools: List[Dict[str, Any]],\n) -&gt; Tuple[\n    str,\n    List[OpenAIToolCall],\n    List[Dict[str, Any]],\n]:\n    \"\"\"\n    Convert accumulated tool-call deltas to OpenAIToolCall objects.\n    Adapted from this excellent code:\n     https://community.openai.com/t/help-for-function-calls-with-streaming/627170/2\n\n    Args:\n        tools: list of tool deltas received from streaming API\n\n    Returns:\n        str: plain text corresponding to tool calls that failed to parse\n        List[OpenAIToolCall]: list of OpenAIToolCall objects\n        List[Dict[str, Any]]: list of tool dicts\n            (to reconstruct OpenAI API response, so it can be cached)\n    \"\"\"\n    # Initialize a dictionary with default values\n\n    # idx -&gt; dict repr of tool\n    # (used to simulate OpenAIResponse object later, and also to\n    # accumulate function args as strings)\n    idx2tool_dict: Dict[str, Dict[str, Any]] = defaultdict(\n        lambda: {\n            \"id\": None,\n            \"function\": {\"arguments\": \"\", \"name\": None},\n            \"type\": None,\n            \"extra_content\": None,\n        }\n    )\n\n    for tool_delta in tools:\n        if tool_delta[\"id\"] is not None:\n            idx2tool_dict[tool_delta[\"index\"]][\"id\"] = tool_delta[\"id\"]\n\n        if tool_delta[\"function\"][\"name\"] is not None:\n            idx2tool_dict[tool_delta[\"index\"]][\"function\"][\"name\"] = tool_delta[\n                \"function\"\n            ][\"name\"]\n\n        idx2tool_dict[tool_delta[\"index\"]][\"function\"][\"arguments\"] += tool_delta[\n            \"function\"\n        ][\"arguments\"]\n\n        if tool_delta[\"type\"] is not None:\n            idx2tool_dict[tool_delta[\"index\"]][\"type\"] = tool_delta[\"type\"]\n\n        if tool_delta.get(\"extra_content\") is not None:\n            idx2tool_dict[tool_delta[\"index\"]][\"extra_content\"] = tool_delta[\n                \"extra_content\"\n            ]\n\n    # (try to) parse the fn args of each tool\n    contents: List[str] = []\n    good_indices = []\n    id2args: Dict[str, None | Dict[str, Any]] = {}\n    for idx, tool_dict in idx2tool_dict.items():\n        failed_content, args_dict = OpenAIGPT._parse_function_args(\n            tool_dict[\"function\"][\"arguments\"]\n        )\n        # used to build tool_calls_list below\n        id2args[tool_dict[\"id\"]] = args_dict or None  # if {}, store as None\n        if failed_content != \"\":\n            contents.append(failed_content)\n        else:\n            good_indices.append(idx)\n\n    # remove the failed tool calls\n    idx2tool_dict = {\n        idx: tool_dict\n        for idx, tool_dict in idx2tool_dict.items()\n        if idx in good_indices\n    }\n\n    # create OpenAIToolCall list\n    tool_calls_list = [\n        OpenAIToolCall(\n            id=tool_dict[\"id\"],\n            function=LLMFunctionCall(\n                name=tool_dict[\"function\"][\"name\"],\n                arguments=id2args.get(tool_dict[\"id\"]),\n            ),\n            type=tool_dict[\"type\"],\n            extra_content=tool_dict.get(\"extra_content\"),\n        )\n        for tool_dict in idx2tool_dict.values()\n    ]\n    return \"\\n\".join(contents), tool_calls_list, list(idx2tool_dict.values())\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.OpenAICallParams","title":"<code>OpenAICallParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Various params that can be sent to an OpenAI API chat-completion call. When specified, any param here overrides the one with same name in the OpenAIGPTConfig. See OpenAI API Reference for details on the params: https://platform.openai.com/docs/api-reference/chat</p>"},{"location":"reference/language_models/#langroid.language_models.MockLM","title":"<code>MockLM(config=MockLMConfig())</code>","text":"<p>               Bases: <code>LanguageModel</code></p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def __init__(self, config: MockLMConfig = MockLMConfig()):\n    super().__init__(config)\n    self.config: MockLMConfig = config\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM.chat","title":"<code>chat(messages, max_tokens=200, tools=None, tool_choice='auto', functions=None, function_call='auto', response_format=None)</code>","text":"<p>Mock chat function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def chat(\n    self,\n    messages: Union[str, List[lm.LLMMessage]],\n    max_tokens: int = 200,\n    tools: Optional[List[OpenAIToolSpec]] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n    functions: Optional[List[lm.LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n    response_format: Optional[OpenAIJsonSchemaSpec] = None,\n) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock chat function for testing\n    \"\"\"\n    last_msg = messages[-1].content if isinstance(messages, list) else messages\n    return self._response(last_msg)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM.achat","title":"<code>achat(messages, max_tokens=200, tools=None, tool_choice='auto', functions=None, function_call='auto', response_format=None)</code>  <code>async</code>","text":"<p>Mock chat function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>async def achat(\n    self,\n    messages: Union[str, List[lm.LLMMessage]],\n    max_tokens: int = 200,\n    tools: Optional[List[OpenAIToolSpec]] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n    functions: Optional[List[lm.LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n    response_format: Optional[OpenAIJsonSchemaSpec] = None,\n) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock chat function for testing\n    \"\"\"\n    last_msg = messages[-1].content if isinstance(messages, list) else messages\n    return await self._response_async(last_msg)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM.generate","title":"<code>generate(prompt, max_tokens=200)</code>","text":"<p>Mock generate function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def generate(self, prompt: str, max_tokens: int = 200) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock generate function for testing\n    \"\"\"\n    return self._response(prompt)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLM.agenerate","title":"<code>agenerate(prompt, max_tokens=200)</code>  <code>async</code>","text":"<p>Mock generate function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>async def agenerate(self, prompt: str, max_tokens: int = 200) -&gt; LLMResponse:\n    \"\"\"\n    Mock generate function for testing\n    \"\"\"\n    return await self._response_async(prompt)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.MockLMConfig","title":"<code>MockLMConfig</code>","text":"<p>               Bases: <code>LLMConfig</code></p> <p>Mock Language Model Configuration.</p> <p>Attributes:</p> Name Type Description <code>response_dict</code> <code>Dict[str, str]</code> <p>A \"response rule-book\", in the form of a dictionary; if last msg in dialog is x,then respond with response_dict[x]</p>"},{"location":"reference/language_models/#langroid.language_models.AzureConfig","title":"<code>AzureConfig(**kwargs)</code>","text":"<p>               Bases: <code>OpenAIGPTConfig</code></p> <p>Configuration for Azure OpenAI GPT.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>should be <code>azure.</code></p> <code>api_version</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_API_VERSION.</code></p> <code>deployment_name</code> <code>str | None</code> <p>can be optionally set in the <code>.env</code> file as <code>AZURE_OPENAI_DEPLOYMENT_NAME</code> and should be based the custom name you chose for your deployment when you deployed a model.</p> <code>model_name</code> <code>str</code> <p>[DEPRECATED] can be set in the <code>.env</code> file as <code>AZURE_OPENAI_MODEL_NAME</code> and should be based on the model name chosen during setup.</p> <code>chat_model</code> <code>str</code> <p>the chat model name to use. Can be set via the env variable <code>AZURE_OPENAI_CHAT_MODEL</code>. Recommended to use this instead of <code>model_name</code>.</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:  # type: ignore\n    if \"model_name\" in kwargs and \"chat_model\" not in kwargs:\n        kwargs[\"chat_model\"] = kwargs[\"model_name\"]\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/language_models/#langroid.language_models.AzureGPT","title":"<code>AzureGPT(config)</code>","text":"<p>               Bases: <code>OpenAIGPT</code></p> <p>Class to access OpenAI LLMs via Azure. These env variables can be obtained from the file <code>.azure_env</code>. Azure OpenAI doesn't support <code>completion</code></p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def __init__(self, config: AzureConfig):\n    # This will auto-populate config values from .env file\n    load_dotenv()\n    super().__init__(config)\n    self.config: AzureConfig = config\n\n    if (\n        self.config.azure_openai_client_provider\n        or self.config.azure_openai_async_client_provider\n    ):\n        if not self.config.azure_openai_client_provider:\n            self.client = None\n            logger.warning(\n                \"Using user-provided Azure OpenAI client, but only async \"\n                \"client has been provided. Synchronous calls will fail.\"\n            )\n        if not self.config.azure_openai_async_client_provider:\n            self.async_client = None\n            logger.warning(\n                \"Using user-provided Azure OpenAI client, but no async \"\n                \"client has been provided. Asynchronous calls will fail.\"\n            )\n\n        if self.config.azure_openai_client_provider:\n            self.client = self.config.azure_openai_client_provider()\n        if self.config.azure_openai_async_client_provider:\n            self.async_client = self.config.azure_openai_async_client_provider()\n            self.async_client.timeout = Timeout(self.config.timeout)\n    else:\n        if self.config.api_key == \"\":\n            raise ValueError(\n                \"\"\"\n                AZURE_OPENAI_API_KEY not set in .env file,\n                please set it to your Azure API key.\"\"\"\n            )\n\n        if self.config.api_base == \"\":\n            raise ValueError(\n                \"\"\"\n                AZURE_OPENAI_API_BASE not set in .env file,\n                please set it to your Azure API key.\"\"\"\n            )\n\n        self.client = AzureOpenAI(\n            api_key=self.config.api_key,\n            azure_endpoint=self.config.api_base,\n            api_version=self.config.api_version,\n            azure_deployment=self.config.deployment_name,\n        )\n        self.async_client = AsyncAzureOpenAI(\n            api_key=self.config.api_key,\n            azure_endpoint=self.config.api_base,\n            api_version=self.config.api_version,\n            azure_deployment=self.config.deployment_name,\n            timeout=Timeout(self.config.timeout),\n        )\n\n    self.supports_json_schema = (\n        self.config.api_version &gt;= azureStructuredOutputAPIMin\n        and self.config.api_version in azureStructuredOutputList\n    )\n</code></pre>"},{"location":"reference/language_models/azure_openai/","title":"azure_openai","text":"<p>langroid/language_models/azure_openai.py </p>"},{"location":"reference/language_models/azure_openai/#langroid.language_models.azure_openai.AzureConfig","title":"<code>AzureConfig(**kwargs)</code>","text":"<p>               Bases: <code>OpenAIGPTConfig</code></p> <p>Configuration for Azure OpenAI GPT.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>should be <code>azure.</code></p> <code>api_version</code> <code>str</code> <p>can be set in the <code>.env</code> file as <code>AZURE_OPENAI_API_VERSION.</code></p> <code>deployment_name</code> <code>str | None</code> <p>can be optionally set in the <code>.env</code> file as <code>AZURE_OPENAI_DEPLOYMENT_NAME</code> and should be based the custom name you chose for your deployment when you deployed a model.</p> <code>model_name</code> <code>str</code> <p>[DEPRECATED] can be set in the <code>.env</code> file as <code>AZURE_OPENAI_MODEL_NAME</code> and should be based on the model name chosen during setup.</p> <code>chat_model</code> <code>str</code> <p>the chat model name to use. Can be set via the env variable <code>AZURE_OPENAI_CHAT_MODEL</code>. Recommended to use this instead of <code>model_name</code>.</p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:  # type: ignore\n    if \"model_name\" in kwargs and \"chat_model\" not in kwargs:\n        kwargs[\"chat_model\"] = kwargs[\"model_name\"]\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/language_models/azure_openai/#langroid.language_models.azure_openai.AzureGPT","title":"<code>AzureGPT(config)</code>","text":"<p>               Bases: <code>OpenAIGPT</code></p> <p>Class to access OpenAI LLMs via Azure. These env variables can be obtained from the file <code>.azure_env</code>. Azure OpenAI doesn't support <code>completion</code></p> Source code in <code>langroid/language_models/azure_openai.py</code> <pre><code>def __init__(self, config: AzureConfig):\n    # This will auto-populate config values from .env file\n    load_dotenv()\n    super().__init__(config)\n    self.config: AzureConfig = config\n\n    if (\n        self.config.azure_openai_client_provider\n        or self.config.azure_openai_async_client_provider\n    ):\n        if not self.config.azure_openai_client_provider:\n            self.client = None\n            logger.warning(\n                \"Using user-provided Azure OpenAI client, but only async \"\n                \"client has been provided. Synchronous calls will fail.\"\n            )\n        if not self.config.azure_openai_async_client_provider:\n            self.async_client = None\n            logger.warning(\n                \"Using user-provided Azure OpenAI client, but no async \"\n                \"client has been provided. Asynchronous calls will fail.\"\n            )\n\n        if self.config.azure_openai_client_provider:\n            self.client = self.config.azure_openai_client_provider()\n        if self.config.azure_openai_async_client_provider:\n            self.async_client = self.config.azure_openai_async_client_provider()\n            self.async_client.timeout = Timeout(self.config.timeout)\n    else:\n        if self.config.api_key == \"\":\n            raise ValueError(\n                \"\"\"\n                AZURE_OPENAI_API_KEY not set in .env file,\n                please set it to your Azure API key.\"\"\"\n            )\n\n        if self.config.api_base == \"\":\n            raise ValueError(\n                \"\"\"\n                AZURE_OPENAI_API_BASE not set in .env file,\n                please set it to your Azure API key.\"\"\"\n            )\n\n        self.client = AzureOpenAI(\n            api_key=self.config.api_key,\n            azure_endpoint=self.config.api_base,\n            api_version=self.config.api_version,\n            azure_deployment=self.config.deployment_name,\n        )\n        self.async_client = AsyncAzureOpenAI(\n            api_key=self.config.api_key,\n            azure_endpoint=self.config.api_base,\n            api_version=self.config.api_version,\n            azure_deployment=self.config.deployment_name,\n            timeout=Timeout(self.config.timeout),\n        )\n\n    self.supports_json_schema = (\n        self.config.api_version &gt;= azureStructuredOutputAPIMin\n        and self.config.api_version in azureStructuredOutputList\n    )\n</code></pre>"},{"location":"reference/language_models/base/","title":"base","text":"<p>langroid/language_models/base.py </p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMConfig","title":"<code>LLMConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Common configuration for all language models.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionCall","title":"<code>LLMFunctionCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structure of LLM response indicating it \"wants\" to call a function. Modeled after OpenAI spec for <code>function_call</code> field in ChatCompletion API.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionCall.from_dict","title":"<code>from_dict(message)</code>  <code>staticmethod</code>","text":"<p>Initialize from dictionary. Args:     d: dictionary containing fields to initialize</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef from_dict(message: Dict[str, Any]) -&gt; \"LLMFunctionCall\":\n    \"\"\"\n    Initialize from dictionary.\n    Args:\n        d: dictionary containing fields to initialize\n    \"\"\"\n    fun_call = LLMFunctionCall(name=message[\"name\"])\n    fun_args_str = message[\"arguments\"]\n    # sometimes may be malformed with invalid indents,\n    # so we try to be safe by removing newlines.\n    if fun_args_str is not None:\n        fun_args_str = fun_args_str.replace(\"\\n\", \"\").strip()\n        dict_or_list = parse_imperfect_json(fun_args_str)\n\n        if not isinstance(dict_or_list, dict):\n            raise ValueError(\n                f\"\"\"\n                    Invalid function args: {fun_args_str}\n                    parsed as {dict_or_list},\n                    which is not a valid dict.\n                    \"\"\"\n            )\n        fun_args = dict_or_list\n    else:\n        fun_args = None\n    fun_call.arguments = fun_args\n\n    return fun_call\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionSpec","title":"<code>LLMFunctionSpec</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Description of a function available for the LLM to use. To be used when calling the LLM <code>chat()</code> method with the <code>functions</code> parameter. Modeled after OpenAI spec for <code>functions</code> fields in ChatCompletion API.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.OpenAIToolCall","title":"<code>OpenAIToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single tool call in a list of tool calls generated by OpenAI LLM API. See https://platform.openai.com/docs/api-reference/chat/create</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str | None</code> <p>The id of the tool call.</p> <code>type</code> <code>ToolTypes</code> <p>The type of the tool call; only \"function\" is currently possible (7/26/24).</p> <code>function</code> <code>LLMFunctionCall | None</code> <p>The function call.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.OpenAIToolCall.from_dict","title":"<code>from_dict(message)</code>  <code>staticmethod</code>","text":"<p>Initialize from dictionary. Args:     d: dictionary containing fields to initialize</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef from_dict(message: Dict[str, Any]) -&gt; \"OpenAIToolCall\":\n    \"\"\"\n    Initialize from dictionary.\n    Args:\n        d: dictionary containing fields to initialize\n    \"\"\"\n    id = message[\"id\"]\n    type = message[\"type\"]\n    function = LLMFunctionCall.from_dict(message[\"function\"])\n    extra_content = message.get(\"extra_content\")\n    return OpenAIToolCall(\n        id=id, type=type, function=function, extra_content=extra_content\n    )\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMTokenUsage","title":"<code>LLMTokenUsage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Usage of tokens by an LLM.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.Role","title":"<code>Role</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Possible roles for a message in a chat.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage","title":"<code>LLMMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class representing an entry in the msg-history sent to the LLM API. It could be one of these: - a user message - an LLM (\"Assistant\") response - a fn-call or tool-call-list from an OpenAI-compatible LLM API response - a result or results from executing a fn or tool-call(s)</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage.api_dict","title":"<code>api_dict(model, has_system_role=True)</code>","text":"<p>Convert to dictionary for API request, keeping ONLY the fields that are expected in an API call! E.g., DROP the tool_id, since it is only for use in the Assistant API,     not the completion API.</p> <p>Parameters:</p> Name Type Description Default <code>has_system_role</code> <code>bool</code> <p>whether the message has a system role (if not, set to \"user\" role)</p> <code>True</code> <p>Returns:     dict: dictionary representation of LLM message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def api_dict(self, model: str, has_system_role: bool = True) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to dictionary for API request, keeping ONLY\n    the fields that are expected in an API call!\n    E.g., DROP the tool_id, since it is only for use in the Assistant API,\n        not the completion API.\n\n    Args:\n        has_system_role: whether the message has a system role (if not,\n            set to \"user\" role)\n    Returns:\n        dict: dictionary representation of LLM message\n    \"\"\"\n    d = self.model_dump()\n    files: List[FileAttachment] = d.pop(\"files\")\n    if len(files) &gt; 0 and self.role == Role.USER:\n        # In there are files, then content is an array of\n        # different content-parts\n        d[\"content\"] = [\n            dict(\n                type=\"text\",\n                text=self.content,\n            )\n        ] + [f.to_dict(model) for f in self.files]\n\n    # if there is a key k = \"role\" with value \"system\", change to \"user\"\n    # in case has_system_role is False\n    if not has_system_role and \"role\" in d and d[\"role\"] == \"system\":\n        d[\"role\"] = \"user\"\n        if \"content\" in d:\n            d[\"content\"] = \"[ADDITIONAL SYSTEM MESSAGE:]\\n\\n\" + d[\"content\"]\n    # drop None values since API doesn't accept them\n    dict_no_none = {k: v for k, v in d.items() if v is not None}\n    if \"name\" in dict_no_none and dict_no_none[\"name\"] == \"\":\n        # OpenAI API does not like empty name\n        del dict_no_none[\"name\"]\n    if \"function_call\" in dict_no_none:\n        # arguments must be a string\n        if \"arguments\" in dict_no_none[\"function_call\"]:\n            dict_no_none[\"function_call\"][\"arguments\"] = json.dumps(\n                dict_no_none[\"function_call\"][\"arguments\"]\n            )\n    if \"tool_calls\" in dict_no_none:\n        # convert tool calls to API format\n        for tc in dict_no_none[\"tool_calls\"]:\n            if \"arguments\" in tc[\"function\"]:\n                # arguments must be a string\n                if tc[\"function\"][\"arguments\"] is None:\n                    tc[\"function\"][\"arguments\"] = \"{}\"\n                else:\n                    tc[\"function\"][\"arguments\"] = json.dumps(\n                        tc[\"function\"][\"arguments\"]\n                    )\n            if \"extra_content\" in tc and tc[\"extra_content\"] is None:\n                del tc[\"extra_content\"]\n    # IMPORTANT! drop fields that are not expected in API call\n    dict_no_none.pop(\"tool_id\", None)\n    dict_no_none.pop(\"timestamp\", None)\n    dict_no_none.pop(\"chat_document_id\", None)\n    return dict_no_none\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse","title":"<code>LLMResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class representing response from LLM.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse.to_LLMMessage","title":"<code>to_LLMMessage()</code>","text":"<p>Convert LLM response to an LLMMessage, to be included in the message-list sent to the API. This is currently NOT used in any significant way in the library, and is only provided as a utility to construct a message list for the API when directly working with an LLM object.</p> <p>In a <code>ChatAgent</code>, an LLM response is first converted to a ChatDocument, which is in turn converted to an LLMMessage via <code>ChatDocument.to_LLMMessage()</code> See <code>ChatAgent._prep_llm_messages()</code> and <code>ChatAgent.llm_response_messages</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def to_LLMMessage(self) -&gt; LLMMessage:\n    \"\"\"Convert LLM response to an LLMMessage, to be included in the\n    message-list sent to the API.\n    This is currently NOT used in any significant way in the library, and is only\n    provided as a utility to construct a message list for the API when directly\n    working with an LLM object.\n\n    In a `ChatAgent`, an LLM response is first converted to a ChatDocument,\n    which is in turn converted to an LLMMessage via `ChatDocument.to_LLMMessage()`\n    See `ChatAgent._prep_llm_messages()` and `ChatAgent.llm_response_messages`\n    \"\"\"\n    return LLMMessage(\n        role=Role.ASSISTANT,\n        content=self.message,\n        name=None if self.function_call is None else self.function_call.name,\n        function_call=self.function_call,\n        tool_calls=self.oai_tool_calls,\n    )\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse.get_recipient_and_message","title":"<code>get_recipient_and_message(recognize_recipient_in_content=True)</code>","text":"<p>If <code>message</code> or <code>function_call</code> of an LLM response contains an explicit recipient name, return this recipient name and <code>message</code> stripped of the recipient name if specified.</p> <p>Two cases: (a) <code>message</code> contains addressing string <code>TO[&lt;name&gt;]:&lt;content&gt;</code>, or (b) <code>message</code> is empty and function_call/tool_call with explicit <code>recipient</code></p> <p>Parameters:</p> Name Type Description Default <code>recognize_recipient_in_content</code> <code>bool</code> <p>When True (default), parses message text for <code>TO[&lt;recipient&gt;]:&lt;content&gt;</code> patterns and top-level JSON <code>{\"recipient\": \"...\"}</code> fields. When False, only function_call/tool_call <code>recipient</code> fields are checked.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>name of recipient, which may be empty string if no recipient</p> <code>str</code> <p>content of message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_recipient_and_message(\n    self,\n    recognize_recipient_in_content: bool = True,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    If `message` or `function_call` of an LLM response contains an explicit\n    recipient name, return this recipient name and `message` stripped\n    of the recipient name if specified.\n\n    Two cases:\n    (a) `message` contains addressing string ``TO[&lt;name&gt;]:&lt;content&gt;``, or\n    (b) `message` is empty and function_call/tool_call with explicit `recipient`\n\n    Args:\n        recognize_recipient_in_content (bool): When True (default), parses\n            message text for ``TO[&lt;recipient&gt;]:&lt;content&gt;`` patterns and\n            top-level JSON ``{\"recipient\": \"...\"}`` fields. When False,\n            only function_call/tool_call ``recipient`` fields are checked.\n\n    Returns:\n        (str): name of recipient, which may be empty string if no recipient\n        (str): content of message\n\n    \"\"\"\n\n    if self.function_call is not None:\n        # in this case we ignore message, since all information is in function_call\n        msg = \"\"\n        args = self.function_call.arguments\n        recipient = \"\"\n        if isinstance(args, dict):\n            recipient = args.get(\"recipient\", \"\")\n        return recipient, msg\n    else:\n        msg = self.message\n        if self.oai_tool_calls is not None:\n            # get the first tool that has a recipient field, if any\n            for tc in self.oai_tool_calls:\n                if tc.function is not None and tc.function.arguments is not None:\n                    recipient = tc.function.arguments.get(\n                        \"recipient\"\n                    )  # type: ignore\n                    if recipient is not None and recipient != \"\":\n                        return recipient, \"\"\n\n    if not recognize_recipient_in_content:\n        return \"\", msg\n\n    # It's not a function or tool call, so continue looking to see\n    # if a recipient is specified in the message.\n\n    # First check if message contains \"TO: &lt;recipient&gt; &lt;content&gt;\"\n    recipient_name, content = parse_message(msg) if msg is not None else (\"\", \"\")\n    # check if there is a top level json that specifies 'recipient',\n    # and retain the entire message as content.\n    if recipient_name == \"\":\n        recipient_name = top_level_json_field(msg, \"recipient\") if msg else \"\"\n        content = msg\n    return recipient_name, content\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel","title":"<code>LanguageModel(config=LLMConfig())</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for language models.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def __init__(self, config: LLMConfig = LLMConfig()):\n    self.config = config\n    self.chat_model_orig = config.chat_model\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.create","title":"<code>create(config)</code>  <code>staticmethod</code>","text":"<p>Create a language model. Args:     config: configuration for language model Returns: instance of language model</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef create(config: Optional[LLMConfig]) -&gt; Optional[\"LanguageModel\"]:\n    \"\"\"\n    Create a language model.\n    Args:\n        config: configuration for language model\n    Returns: instance of language model\n    \"\"\"\n    if type(config) is LLMConfig:\n        raise ValueError(\n            \"\"\"\n            Cannot create a Language Model object from LLMConfig.\n            Please specify a specific subclass of LLMConfig e.g.,\n            OpenAIGPTConfig. If you are creating a ChatAgent from\n            a ChatAgentConfig, please specify the `llm` field of this config\n            as a specific subclass of LLMConfig, e.g., OpenAIGPTConfig.\n            \"\"\"\n        )\n    from langroid.language_models.azure_openai import AzureGPT\n    from langroid.language_models.mock_lm import MockLM, MockLMConfig\n    from langroid.language_models.openai_gpt import OpenAIGPT\n\n    if config is None or config.type is None:\n        return None\n\n    if config.type == \"mock\":\n        return MockLM(cast(MockLMConfig, config))\n\n    openai: Union[Type[AzureGPT], Type[OpenAIGPT]]\n\n    if config.type == \"azure\":\n        openai = AzureGPT\n    else:\n        openai = OpenAIGPT\n    cls = dict(\n        openai=openai,\n    ).get(config.type, openai)\n    return cls(config)  # type: ignore\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.user_assistant_pairs","title":"<code>user_assistant_pairs(lst)</code>  <code>staticmethod</code>","text":"<p>Given an even-length sequence of strings, split into a sequence of pairs</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>List[str]</code> <p>sequence of strings</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, str]]</code> <p>List[Tuple[str,str]]: sequence of pairs of strings</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef user_assistant_pairs(lst: List[str]) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    Given an even-length sequence of strings, split into a sequence of pairs\n\n    Args:\n        lst (List[str]): sequence of strings\n\n    Returns:\n        List[Tuple[str,str]]: sequence of pairs of strings\n    \"\"\"\n    evens = lst[::2]\n    odds = lst[1::2]\n    return list(zip(evens, odds))\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_chat_history_components","title":"<code>get_chat_history_components(messages)</code>  <code>staticmethod</code>","text":"<p>From the chat history, extract system prompt, user-assistant turns, and final user msg.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>List of messages in the chat history</p> required <p>Returns:</p> Type Description <code>Tuple[str, List[Tuple[str, str]], str]</code> <p>Tuple[str, List[Tuple[str,str]], str]: system prompt, user-assistant turns, final user msg</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef get_chat_history_components(\n    messages: List[LLMMessage],\n) -&gt; Tuple[str, List[Tuple[str, str]], str]:\n    \"\"\"\n    From the chat history, extract system prompt, user-assistant turns, and\n    final user msg.\n\n    Args:\n        messages (List[LLMMessage]): List of messages in the chat history\n\n    Returns:\n        Tuple[str, List[Tuple[str,str]], str]:\n            system prompt, user-assistant turns, final user msg\n\n    \"\"\"\n    # Handle various degenerate cases\n    messages = [m for m in messages]  # copy\n    DUMMY_SYS_PROMPT = \"You are a helpful assistant.\"\n    DUMMY_USER_PROMPT = \"Follow the instructions above.\"\n    if len(messages) == 0 or messages[0].role != Role.SYSTEM:\n        logger.warning(\"No system msg, creating dummy system prompt\")\n        messages.insert(0, LLMMessage(content=DUMMY_SYS_PROMPT, role=Role.SYSTEM))\n    system_prompt = messages[0].content\n\n    # now we have messages = [Sys,...]\n    if len(messages) == 1:\n        logger.warning(\n            \"Got only system message in chat history, creating dummy user prompt\"\n        )\n        messages.append(LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, msg, ...]\n\n    if messages[1].role != Role.USER:\n        messages.insert(1, LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, user, ...]\n    if messages[-1].role != Role.USER:\n        logger.warning(\n            \"Last message in chat history is not a user message,\"\n            \" creating dummy user prompt\"\n        )\n        messages.append(LLMMessage(content=DUMMY_USER_PROMPT, role=Role.USER))\n\n    # now we have messages = [Sys, user, ..., user]\n    # so we omit the first and last elements and make pairs of user-asst messages\n    conversation = [m.content for m in messages[1:-1]]\n    user_prompt = messages[-1].content\n    pairs = LanguageModel.user_assistant_pairs(conversation)\n    return system_prompt, pairs, user_prompt\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.set_stream","title":"<code>set_stream(stream)</code>  <code>abstractmethod</code>","text":"<p>Enable or disable streaming output from API. Return previous value of stream.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef set_stream(self, stream: bool) -&gt; bool:\n    \"\"\"Enable or disable streaming output from API.\n    Return previous value of stream.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_stream","title":"<code>get_stream()</code>  <code>abstractmethod</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef get_stream(self) -&gt; bool:\n    \"\"\"Get streaming status\"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.chat","title":"<code>chat(messages, max_tokens=200, tools=None, tool_choice='auto', functions=None, function_call='auto', response_format=None)</code>  <code>abstractmethod</code>","text":"<p>Get chat-completion response from LLM.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, List[LLMMessage]]</code> <p>message-history to send to the LLM</p> required <code>max_tokens</code> <code>int</code> <p>max tokens to generate</p> <code>200</code> <code>tools</code> <code>Optional[List[OpenAIToolSpec]]</code> <p>tools available for the LLM to use in its response</p> <code>None</code> <code>tool_choice</code> <code>ToolChoiceTypes | Dict[str, str | Dict[str, str]]</code> <p>tool call mode, one of \"none\", \"auto\", \"required\", or a dict specifying a specific tool.</p> <code>'auto'</code> <code>functions</code> <code>Optional[List[LLMFunctionSpec]]</code> <p>functions available for LLM to call (deprecated)</p> <code>None</code> <code>function_call</code> <code>str | Dict[str, str]</code> <p>function calling mode, \"auto\", \"none\", or a specific fn     (deprecated)</p> <code>'auto'</code> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef chat(\n    self,\n    messages: Union[str, List[LLMMessage]],\n    max_tokens: int = 200,\n    tools: Optional[List[OpenAIToolSpec]] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n    functions: Optional[List[LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n    response_format: Optional[OpenAIJsonSchemaSpec] = None,\n) -&gt; LLMResponse:\n    \"\"\"\n    Get chat-completion response from LLM.\n\n    Args:\n        messages: message-history to send to the LLM\n        max_tokens: max tokens to generate\n        tools: tools available for the LLM to use in its response\n        tool_choice: tool call mode, one of \"none\", \"auto\", \"required\",\n            or a dict specifying a specific tool.\n        functions: functions available for LLM to call (deprecated)\n        function_call: function calling mode, \"auto\", \"none\", or a specific fn\n                (deprecated)\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.achat","title":"<code>achat(messages, max_tokens=200, tools=None, tool_choice='auto', functions=None, function_call='auto', response_format=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Async version of <code>chat</code>. See <code>chat</code> for details.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\nasync def achat(\n    self,\n    messages: Union[str, List[LLMMessage]],\n    max_tokens: int = 200,\n    tools: Optional[List[OpenAIToolSpec]] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n    functions: Optional[List[LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n    response_format: Optional[OpenAIJsonSchemaSpec] = None,\n) -&gt; LLMResponse:\n    \"\"\"Async version of `chat`. See `chat` for details.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.info","title":"<code>info()</code>","text":"<p>Info of relevant chat model</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def info(self) -&gt; ModelInfo:\n    \"\"\"Info of relevant chat model\"\"\"\n    orig_model = (\n        self.config.completion_model\n        if self.config.use_completion_for_chat\n        else self.chat_model_orig\n    )\n    return get_model_info(orig_model, self._fallback_model_names(orig_model))\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.completion_info","title":"<code>completion_info()</code>","text":"<p>Info of relevant completion model</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def completion_info(self) -&gt; ModelInfo:\n    \"\"\"Info of relevant completion model\"\"\"\n    orig_model = (\n        self.chat_model_orig\n        if self.config.use_chat_for_completion\n        else self.config.completion_model\n    )\n    return get_model_info(orig_model, self._fallback_model_names(orig_model))\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.supports_functions_or_tools","title":"<code>supports_functions_or_tools()</code>","text":"<p>Does this Model's API support \"native\" tool-calling, i.e. can we call the API with arguments that contain a list of available tools, and their schemas? Note that, given the plethora of LLM provider APIs this determination is imperfect at best, and leans towards returning True. When the API calls fails with an error indicating tools are not supported, then users are encouraged to use the Langroid-based prompt-based ToolMessage mechanism, which works with ANY LLM. To enable this, in your ChatAgentConfig, set <code>use_functions_api=False</code>, and <code>use_tools=True</code>.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def supports_functions_or_tools(self) -&gt; bool:\n    \"\"\"\n    Does this Model's API support \"native\" tool-calling, i.e.\n    can we call the API with arguments that contain a list of available tools,\n    and their schemas?\n    Note that, given the plethora of LLM provider APIs this determination is\n    imperfect at best, and leans towards returning True.\n    When the API calls fails with an error indicating tools are not supported,\n    then users are encouraged to use the Langroid-based prompt-based\n    ToolMessage mechanism, which works with ANY LLM. To enable this,\n    in your ChatAgentConfig, set `use_functions_api=False`, and `use_tools=True`.\n    \"\"\"\n    return self.info().has_tools\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.chat_cost","title":"<code>chat_cost()</code>","text":"<p>Return the cost per 1000 tokens for chat completions.</p> <p>Returns:</p> Type Description <code>Tuple[float, float, float]</code> <p>Tuple[float, float, float]: (input_cost, cached_cost, output_cost) per 1000 tokens</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def chat_cost(self) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Return the cost per 1000 tokens for chat completions.\n\n    Returns:\n        Tuple[float, float, float]: (input_cost, cached_cost, output_cost)\n            per 1000 tokens\n    \"\"\"\n    return (0.0, 0.0, 0.0)\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.update_usage_cost","title":"<code>update_usage_cost(chat, prompts, completions, cost)</code>","text":"<p>Update usage cost for this LLM. Args:     chat (bool): whether to update for chat or completion model     prompts (int): number of tokens used for prompts     completions (int): number of tokens used for completions     cost (float): total token cost in USD</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def update_usage_cost(\n    self, chat: bool, prompts: int, completions: int, cost: float\n) -&gt; None:\n    \"\"\"\n    Update usage cost for this LLM.\n    Args:\n        chat (bool): whether to update for chat or completion model\n        prompts (int): number of tokens used for prompts\n        completions (int): number of tokens used for completions\n        cost (float): total token cost in USD\n    \"\"\"\n    mdl = self.config.chat_model if chat else self.config.completion_model\n    if mdl is None:\n        return\n    if mdl not in self.usage_cost_dict:\n        self.usage_cost_dict[mdl] = LLMTokenUsage()\n    counter = self.usage_cost_dict[mdl]\n    counter.prompt_tokens += prompts\n    counter.completion_tokens += completions\n    counter.cost += cost\n    counter.calls += 1\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.tot_tokens_cost","title":"<code>tot_tokens_cost()</code>  <code>classmethod</code>","text":"<p>Return total tokens used and total cost across all models.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@classmethod\ndef tot_tokens_cost(cls) -&gt; Tuple[int, float]:\n    \"\"\"\n    Return total tokens used and total cost across all models.\n    \"\"\"\n    total_tokens = 0\n    total_cost = 0.0\n    for counter in cls.usage_cost_dict.values():\n        total_tokens += counter.total_tokens\n        total_cost += counter.cost\n    return total_tokens, total_cost\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_reasoning_final","title":"<code>get_reasoning_final(message)</code>","text":"<p>Extract \"reasoning\" and \"final answer\" from an LLM response, if the reasoning is found within configured delimiters, like , . E.g., ' Okay, let's see, the user wants...  2 + 3 = 5'</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>message from LLM</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple[str, str]: reasoning, final answer</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_reasoning_final(self, message: str) -&gt; Tuple[str, str]:\n    \"\"\"Extract \"reasoning\" and \"final answer\" from an LLM response, if the\n    reasoning is found within configured delimiters, like &lt;think&gt;, &lt;/think&gt;.\n    E.g.,\n    '&lt;think&gt; Okay, let's see, the user wants... &lt;/think&gt; 2 + 3 = 5'\n\n    Args:\n        message (str): message from LLM\n\n    Returns:\n        Tuple[str, str]: reasoning, final answer\n    \"\"\"\n    start, end = self.config.thought_delimiters\n    if start in message and end in message:\n        parts = message.split(start)\n        if len(parts) &gt; 1:\n            reasoning, final = parts[1].split(end)\n            return reasoning, final\n    return \"\", message\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.followup_to_standalone","title":"<code>followup_to_standalone(chat_history, question)</code>","text":"<p>Given a chat history and a question, convert it to a standalone question. Args:     chat_history: list of tuples of (question, answer)     query: follow-up question</p> <p>Returns: standalone version of the question</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def followup_to_standalone(\n    self, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n    \"\"\"\n    Given a chat history and a question, convert it to a standalone question.\n    Args:\n        chat_history: list of tuples of (question, answer)\n        query: follow-up question\n\n    Returns: standalone version of the question\n    \"\"\"\n    history = collate_chat_history(chat_history)\n\n    prompt = f\"\"\"\n    You are an expert at understanding a CHAT HISTORY between an AI Assistant\n    and a User, and you are highly skilled in rephrasing the User's FOLLOW-UP\n    QUESTION/REQUEST as a STANDALONE QUESTION/REQUEST that can be understood\n    WITHOUT the context of the chat history.\n\n    Below is the CHAT HISTORY. When the User asks you to rephrase a\n    FOLLOW-UP QUESTION/REQUEST, your ONLY task is to simply return the\n    question REPHRASED as a STANDALONE QUESTION/REQUEST, without any additional\n    text or context.\n\n    &lt;CHAT_HISTORY&gt;\n    {history}\n    &lt;/CHAT_HISTORY&gt;\n    \"\"\".strip()\n\n    follow_up_question = f\"\"\"\n    Please rephrase this as a stand-alone question or request:\n    &lt;FOLLOW-UP-QUESTION-OR-REQUEST&gt;\n    {question}\n    &lt;/FOLLOW-UP-QUESTION-OR-REQUEST&gt;\n    \"\"\".strip()\n\n    show_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-PROMPT= \")\n    standalone = self.chat(\n        messages=[\n            LLMMessage(role=Role.SYSTEM, content=prompt),\n            LLMMessage(role=Role.USER, content=follow_up_question),\n        ],\n        max_tokens=1024,\n    ).message.strip()\n\n    show_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-RESPONSE= \")\n    return standalone\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.StreamingIfAllowed","title":"<code>StreamingIfAllowed(llm, stream=True)</code>","text":"<p>Context to temporarily enable or disable streaming, if allowed globally via <code>settings.stream</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def __init__(self, llm: LanguageModel, stream: bool = True):\n    self.llm = llm\n    self.stream = stream\n</code></pre>"},{"location":"reference/language_models/client_cache/","title":"client_cache","text":"<p>langroid/language_models/client_cache.py </p> <p>Client caching/singleton pattern for LLM clients to prevent connection pool exhaustion.</p>"},{"location":"reference/language_models/client_cache/#langroid.language_models.client_cache.get_openai_client","title":"<code>get_openai_client(api_key, base_url=None, organization=None, timeout=120.0, default_headers=None, http_client=None, http_client_config=None)</code>","text":"<p>Get or create a singleton OpenAI client with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key</p> required <code>base_url</code> <code>Optional[str]</code> <p>Optional base URL for API</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>Optional organization ID</p> <code>None</code> <code>timeout</code> <code>Union[float, Timeout]</code> <p>Request timeout</p> <code>120.0</code> <code>default_headers</code> <code>Optional[Dict[str, str]]</code> <p>Optional default headers</p> <code>None</code> <code>http_client</code> <code>Optional[Any]</code> <p>Optional httpx.Client instance</p> <code>None</code> <code>http_client_config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional config dict for creating httpx.Client</p> <code>None</code> <p>Returns:</p> Type Description <code>OpenAI</code> <p>OpenAI client instance</p> Source code in <code>langroid/language_models/client_cache.py</code> <pre><code>def get_openai_client(\n    api_key: str,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    timeout: Union[float, Timeout] = 120.0,\n    default_headers: Optional[Dict[str, str]] = None,\n    http_client: Optional[Any] = None,\n    http_client_config: Optional[Dict[str, Any]] = None,\n) -&gt; OpenAI:\n    \"\"\"\n    Get or create a singleton OpenAI client with the given configuration.\n\n    Args:\n        api_key: OpenAI API key\n        base_url: Optional base URL for API\n        organization: Optional organization ID\n        timeout: Request timeout\n        default_headers: Optional default headers\n        http_client: Optional httpx.Client instance\n        http_client_config: Optional config dict for creating httpx.Client\n\n    Returns:\n        OpenAI client instance\n    \"\"\"\n    if isinstance(timeout, (int, float)):\n        timeout = Timeout(timeout)\n\n    # If http_client is provided directly, don't cache (complex object)\n    if http_client is not None:\n        client = OpenAI(\n            api_key=api_key,\n            base_url=base_url,\n            organization=organization,\n            timeout=timeout,\n            default_headers=default_headers,\n            http_client=http_client,\n        )\n        _all_clients.add(client)\n        return client\n\n    # If http_client_config is provided, create client from config and cache\n    created_http_client = None\n    if http_client_config is not None:\n        try:\n            from httpx import Client\n\n            created_http_client = Client(**http_client_config)\n        except ImportError:\n            raise ValueError(\n                \"httpx is required to use http_client_config. \"\n                \"Install it with: pip install httpx\"\n            )\n\n    cache_key = _get_cache_key(\n        \"openai\",\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        timeout=timeout,\n        default_headers=default_headers,\n        http_client_config=http_client_config,  # Include config in cache key\n    )\n\n    if cache_key in _client_cache:\n        return cast(OpenAI, _client_cache[cache_key])\n\n    client = OpenAI(\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        timeout=timeout,\n        default_headers=default_headers,\n        http_client=created_http_client,  # Use the client created from config\n    )\n\n    _client_cache[cache_key] = client\n    _all_clients.add(client)\n    return client\n</code></pre>"},{"location":"reference/language_models/client_cache/#langroid.language_models.client_cache.get_async_openai_client","title":"<code>get_async_openai_client(api_key, base_url=None, organization=None, timeout=120.0, default_headers=None, http_client=None, http_client_config=None)</code>","text":"<p>Get or create a singleton AsyncOpenAI client with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI API key</p> required <code>base_url</code> <code>Optional[str]</code> <p>Optional base URL for API</p> <code>None</code> <code>organization</code> <code>Optional[str]</code> <p>Optional organization ID</p> <code>None</code> <code>timeout</code> <code>Union[float, Timeout]</code> <p>Request timeout</p> <code>120.0</code> <code>default_headers</code> <code>Optional[Dict[str, str]]</code> <p>Optional default headers</p> <code>None</code> <code>http_client</code> <code>Optional[Any]</code> <p>Optional httpx.AsyncClient instance</p> <code>None</code> <code>http_client_config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional config dict for creating httpx.AsyncClient</p> <code>None</code> <p>Returns:</p> Type Description <code>AsyncOpenAI</code> <p>AsyncOpenAI client instance</p> Source code in <code>langroid/language_models/client_cache.py</code> <pre><code>def get_async_openai_client(\n    api_key: str,\n    base_url: Optional[str] = None,\n    organization: Optional[str] = None,\n    timeout: Union[float, Timeout] = 120.0,\n    default_headers: Optional[Dict[str, str]] = None,\n    http_client: Optional[Any] = None,\n    http_client_config: Optional[Dict[str, Any]] = None,\n) -&gt; AsyncOpenAI:\n    \"\"\"\n    Get or create a singleton AsyncOpenAI client with the given configuration.\n\n    Args:\n        api_key: OpenAI API key\n        base_url: Optional base URL for API\n        organization: Optional organization ID\n        timeout: Request timeout\n        default_headers: Optional default headers\n        http_client: Optional httpx.AsyncClient instance\n        http_client_config: Optional config dict for creating httpx.AsyncClient\n\n    Returns:\n        AsyncOpenAI client instance\n    \"\"\"\n    if isinstance(timeout, (int, float)):\n        timeout = Timeout(timeout)\n\n    # If http_client is provided directly, don't cache (complex object)\n    if http_client is not None:\n        client = AsyncOpenAI(\n            api_key=api_key,\n            base_url=base_url,\n            organization=organization,\n            timeout=timeout,\n            default_headers=default_headers,\n            http_client=http_client,\n        )\n        _all_clients.add(client)\n        return client\n\n    # If http_client_config is provided, create async client from config and cache\n    created_http_client = None\n    if http_client_config is not None:\n        try:\n            from httpx import AsyncClient\n\n            created_http_client = AsyncClient(**http_client_config)\n        except ImportError:\n            raise ValueError(\n                \"httpx is required to use http_client_config. \"\n                \"Install it with: pip install httpx\"\n            )\n\n    cache_key = _get_cache_key(\n        \"async_openai\",\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        timeout=timeout,\n        default_headers=default_headers,\n        http_client_config=http_client_config,  # Include config in cache key\n    )\n\n    if cache_key in _client_cache:\n        return cast(AsyncOpenAI, _client_cache[cache_key])\n\n    client = AsyncOpenAI(\n        api_key=api_key,\n        base_url=base_url,\n        organization=organization,\n        timeout=timeout,\n        default_headers=default_headers,\n        http_client=created_http_client,  # Use the client created from config\n    )\n\n    _client_cache[cache_key] = client\n    _all_clients.add(client)\n    return client\n</code></pre>"},{"location":"reference/language_models/client_cache/#langroid.language_models.client_cache.get_groq_client","title":"<code>get_groq_client(api_key)</code>","text":"<p>Get or create a singleton Groq client with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Groq API key</p> required <p>Returns:</p> Type Description <code>Groq</code> <p>Groq client instance</p> Source code in <code>langroid/language_models/client_cache.py</code> <pre><code>def get_groq_client(api_key: str) -&gt; Groq:\n    \"\"\"\n    Get or create a singleton Groq client with the given configuration.\n\n    Args:\n        api_key: Groq API key\n\n    Returns:\n        Groq client instance\n    \"\"\"\n    cache_key = _get_cache_key(\"groq\", api_key=api_key)\n\n    if cache_key in _client_cache:\n        return cast(Groq, _client_cache[cache_key])\n\n    client = Groq(api_key=api_key)\n    _client_cache[cache_key] = client\n    _all_clients.add(client)\n    return client\n</code></pre>"},{"location":"reference/language_models/client_cache/#langroid.language_models.client_cache.get_async_groq_client","title":"<code>get_async_groq_client(api_key)</code>","text":"<p>Get or create a singleton AsyncGroq client with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Groq API key</p> required <p>Returns:</p> Type Description <code>AsyncGroq</code> <p>AsyncGroq client instance</p> Source code in <code>langroid/language_models/client_cache.py</code> <pre><code>def get_async_groq_client(api_key: str) -&gt; AsyncGroq:\n    \"\"\"\n    Get or create a singleton AsyncGroq client with the given configuration.\n\n    Args:\n        api_key: Groq API key\n\n    Returns:\n        AsyncGroq client instance\n    \"\"\"\n    cache_key = _get_cache_key(\"async_groq\", api_key=api_key)\n\n    if cache_key in _client_cache:\n        return cast(AsyncGroq, _client_cache[cache_key])\n\n    client = AsyncGroq(api_key=api_key)\n    _client_cache[cache_key] = client\n    _all_clients.add(client)\n    return client\n</code></pre>"},{"location":"reference/language_models/client_cache/#langroid.language_models.client_cache.get_cerebras_client","title":"<code>get_cerebras_client(api_key)</code>","text":"<p>Get or create a singleton Cerebras client with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Cerebras API key</p> required <p>Returns:</p> Type Description <code>Cerebras</code> <p>Cerebras client instance</p> Source code in <code>langroid/language_models/client_cache.py</code> <pre><code>def get_cerebras_client(api_key: str) -&gt; Cerebras:\n    \"\"\"\n    Get or create a singleton Cerebras client with the given configuration.\n\n    Args:\n        api_key: Cerebras API key\n\n    Returns:\n        Cerebras client instance\n    \"\"\"\n    cache_key = _get_cache_key(\"cerebras\", api_key=api_key)\n\n    if cache_key in _client_cache:\n        return cast(Cerebras, _client_cache[cache_key])\n\n    client = Cerebras(api_key=api_key)\n    _client_cache[cache_key] = client\n    _all_clients.add(client)\n    return client\n</code></pre>"},{"location":"reference/language_models/client_cache/#langroid.language_models.client_cache.get_async_cerebras_client","title":"<code>get_async_cerebras_client(api_key)</code>","text":"<p>Get or create a singleton AsyncCerebras client with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Cerebras API key</p> required <p>Returns:</p> Type Description <code>AsyncCerebras</code> <p>AsyncCerebras client instance</p> Source code in <code>langroid/language_models/client_cache.py</code> <pre><code>def get_async_cerebras_client(api_key: str) -&gt; AsyncCerebras:\n    \"\"\"\n    Get or create a singleton AsyncCerebras client with the given configuration.\n\n    Args:\n        api_key: Cerebras API key\n\n    Returns:\n        AsyncCerebras client instance\n    \"\"\"\n    cache_key = _get_cache_key(\"async_cerebras\", api_key=api_key)\n\n    if cache_key in _client_cache:\n        return cast(AsyncCerebras, _client_cache[cache_key])\n\n    client = AsyncCerebras(api_key=api_key)\n    _client_cache[cache_key] = client\n    _all_clients.add(client)\n    return client\n</code></pre>"},{"location":"reference/language_models/config/","title":"config","text":"<p>langroid/language_models/config.py </p>"},{"location":"reference/language_models/mock_lm/","title":"mock_lm","text":"<p>langroid/language_models/mock_lm.py </p> <p>Mock Language Model for testing</p>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLMConfig","title":"<code>MockLMConfig</code>","text":"<p>               Bases: <code>LLMConfig</code></p> <p>Mock Language Model Configuration.</p> <p>Attributes:</p> Name Type Description <code>response_dict</code> <code>Dict[str, str]</code> <p>A \"response rule-book\", in the form of a dictionary; if last msg in dialog is x,then respond with response_dict[x]</p>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM","title":"<code>MockLM(config=MockLMConfig())</code>","text":"<p>               Bases: <code>LanguageModel</code></p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def __init__(self, config: MockLMConfig = MockLMConfig()):\n    super().__init__(config)\n    self.config: MockLMConfig = config\n</code></pre>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM.chat","title":"<code>chat(messages, max_tokens=200, tools=None, tool_choice='auto', functions=None, function_call='auto', response_format=None)</code>","text":"<p>Mock chat function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def chat(\n    self,\n    messages: Union[str, List[lm.LLMMessage]],\n    max_tokens: int = 200,\n    tools: Optional[List[OpenAIToolSpec]] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n    functions: Optional[List[lm.LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n    response_format: Optional[OpenAIJsonSchemaSpec] = None,\n) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock chat function for testing\n    \"\"\"\n    last_msg = messages[-1].content if isinstance(messages, list) else messages\n    return self._response(last_msg)\n</code></pre>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM.achat","title":"<code>achat(messages, max_tokens=200, tools=None, tool_choice='auto', functions=None, function_call='auto', response_format=None)</code>  <code>async</code>","text":"<p>Mock chat function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>async def achat(\n    self,\n    messages: Union[str, List[lm.LLMMessage]],\n    max_tokens: int = 200,\n    tools: Optional[List[OpenAIToolSpec]] = None,\n    tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = \"auto\",\n    functions: Optional[List[lm.LLMFunctionSpec]] = None,\n    function_call: str | Dict[str, str] = \"auto\",\n    response_format: Optional[OpenAIJsonSchemaSpec] = None,\n) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock chat function for testing\n    \"\"\"\n    last_msg = messages[-1].content if isinstance(messages, list) else messages\n    return await self._response_async(last_msg)\n</code></pre>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM.generate","title":"<code>generate(prompt, max_tokens=200)</code>","text":"<p>Mock generate function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>def generate(self, prompt: str, max_tokens: int = 200) -&gt; lm.LLMResponse:\n    \"\"\"\n    Mock generate function for testing\n    \"\"\"\n    return self._response(prompt)\n</code></pre>"},{"location":"reference/language_models/mock_lm/#langroid.language_models.mock_lm.MockLM.agenerate","title":"<code>agenerate(prompt, max_tokens=200)</code>  <code>async</code>","text":"<p>Mock generate function for testing</p> Source code in <code>langroid/language_models/mock_lm.py</code> <pre><code>async def agenerate(self, prompt: str, max_tokens: int = 200) -&gt; LLMResponse:\n    \"\"\"\n    Mock generate function for testing\n    \"\"\"\n    return await self._response_async(prompt)\n</code></pre>"},{"location":"reference/language_models/model_info/","title":"model_info","text":"<p>langroid/language_models/model_info.py </p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.ModelProvider","title":"<code>ModelProvider</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for model providers</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.ModelName","title":"<code>ModelName</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Parent class for all model name enums</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.OpenAIChatModel","title":"<code>OpenAIChatModel</code>","text":"<p>               Bases: <code>ModelName</code></p> <p>Enum for OpenAI Chat models</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.OpenAICompletionModel","title":"<code>OpenAICompletionModel</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Completion models</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.AnthropicModel","title":"<code>AnthropicModel</code>","text":"<p>               Bases: <code>ModelName</code></p> <p>Enum for Anthropic models</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.DeepSeekModel","title":"<code>DeepSeekModel</code>","text":"<p>               Bases: <code>ModelName</code></p> <p>Enum for DeepSeek models direct from DeepSeek API</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.GeminiModel","title":"<code>GeminiModel</code>","text":"<p>               Bases: <code>ModelName</code></p> <p>Enum for Gemini models</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.OpenAI_API_ParamInfo","title":"<code>OpenAI_API_ParamInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters exclusive to some models, when using OpenAI API</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.ModelInfo","title":"<code>ModelInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Consolidated information about LLM, related to capacity, cost and API idiosyncrasies. Reasonable defaults for all params in case there's no specific info available.</p>"},{"location":"reference/language_models/model_info/#langroid.language_models.model_info.get_model_info","title":"<code>get_model_info(model, fallback_models=[])</code>","text":"<p>Get model information by name or enum value</p> Source code in <code>langroid/language_models/model_info.py</code> <pre><code>def get_model_info(\n    model: str | ModelName,\n    fallback_models: List[str] = [],\n) -&gt; ModelInfo:\n    \"\"\"Get model information by name or enum value\"\"\"\n    # Sequence of models to try, starting with the primary model\n    models_to_try = [model] + fallback_models\n\n    # Find the first model in the sequence that has info defined using next()\n    # on a generator expression that filters out None results from _get_model_info\n    found_info = next(\n        (info for m in models_to_try if (info := _get_model_info(m)) is not None),\n        None,  # Default value if the iterator is exhausted (no valid info found)\n    )\n\n    # Return the found info, or a default ModelInfo if none was found\n    return found_info or ModelInfo()\n</code></pre>"},{"location":"reference/language_models/openai_gpt/","title":"openai_gpt","text":"<p>langroid/language_models/openai_gpt.py </p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAICallParams","title":"<code>OpenAICallParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Various params that can be sent to an OpenAI API chat-completion call. When specified, any param here overrides the one with same name in the OpenAIGPTConfig. See OpenAI API Reference for details on the params: https://platform.openai.com/docs/api-reference/chat</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.LiteLLMProxyConfig","title":"<code>LiteLLMProxyConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for LiteLLM proxy connection.</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig","title":"<code>OpenAIGPTConfig(**kwargs)</code>","text":"<p>               Bases: <code>LLMConfig</code></p> <p>Class for any LLM with an OpenAI-like API: besides the OpenAI models this includes: (a) locally-served models behind an OpenAI-compatible API (b) non-local models, using a proxy adaptor lib like litellm that provides     an OpenAI-compatible API. (We could rename this class to OpenAILikeConfig, but we keep it as-is for now)</p> <p>Important Note: Due to the <code>env_prefix = \"OPENAI_\"</code> defined below, all of the fields below can be set AND OVERRIDDEN via env vars,</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig--by-upper-casing-the-name-and-prefixing-with-openai_-eg","title":"by upper-casing the name and prefixing with OPENAI_, e.g.","text":""},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig--openai_max_output_tokens1000","title":"OPENAI_MAX_OUTPUT_TOKENS=1000.","text":""},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig--if-any-of-these-is-defined-in-this-way-in-the-environment","title":"If any of these is defined in this way in the environment","text":""},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig--either-via-explicit-setenv-or-export-or-via-env-file-load_dotenv","title":"(either via explicit setenv or export or via .env file + load_dotenv()),","text":""},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig--the-environment-variable-takes-precedence-over-the-value-in-the-config","title":"the environment variable takes precedence over the value in the config.","text":"Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:  # type: ignore\n    local_model = \"api_base\" in kwargs and kwargs[\"api_base\"] is not None\n\n    chat_model = kwargs.get(\"chat_model\", \"\")\n    local_prefixes = [\"local/\", \"litellm/\", \"ollama/\", \"vllm/\", \"llamacpp/\"]\n    if any(chat_model.startswith(prefix) for prefix in local_prefixes):\n        local_model = True\n\n    warn_gpt_3_5 = (\n        \"chat_model\" not in kwargs.keys()\n        and not local_model\n        and default_openai_chat_model == OpenAIChatModel.GPT3_5_TURBO\n    )\n\n    if warn_gpt_3_5:\n        existing_hook = kwargs.get(\"run_on_first_use\", noop)\n\n        def with_warning() -&gt; None:\n            existing_hook()\n            gpt_3_5_warning()\n\n        kwargs[\"run_on_first_use\"] = with_warning\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig.model_copy","title":"<code>model_copy(*, update=None, deep=False)</code>","text":"<p>Copy config while preserving nested model instances and subclasses.</p> <p>Important: Avoid reconstructing via <code>model_dump</code> as that coerces nested models to their annotated base types (dropping subclass-only fields). Instead, defer to Pydantic's native <code>model_copy</code>, which keeps nested <code>BaseModel</code> instances (and their concrete subclasses) intact.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def model_copy(\n    self, *, update: Mapping[str, Any] | None = None, deep: bool = False\n) -&gt; \"OpenAIGPTConfig\":\n    \"\"\"\n    Copy config while preserving nested model instances and subclasses.\n\n    Important: Avoid reconstructing via `model_dump` as that coerces nested\n    models to their annotated base types (dropping subclass-only fields).\n    Instead, defer to Pydantic's native `model_copy`, which keeps nested\n    `BaseModel` instances (and their concrete subclasses) intact.\n    \"\"\"\n    # Delegate to BaseSettings/BaseModel implementation to preserve types\n    return super().model_copy(update=update, deep=deep)  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPTConfig.create","title":"<code>create(prefix)</code>  <code>classmethod</code>","text":"<p>Create a config class whose params can be set via a desired prefix from the .env file or env vars. E.g., using <pre><code>OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\nollama_config = OllamaConfig()\n</code></pre> you can have a group of params prefixed by \"OLLAMA_\", to be used with models served via <code>ollama</code>. This way, you can maintain several setting-groups in your .env file, one per model type.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>@classmethod\ndef create(cls, prefix: str) -&gt; Type[\"OpenAIGPTConfig\"]:\n    \"\"\"Create a config class whose params can be set via a desired\n    prefix from the .env file or env vars.\n    E.g., using\n    ```python\n    OllamaConfig = OpenAIGPTConfig.create(\"ollama\")\n    ollama_config = OllamaConfig()\n    ```\n    you can have a group of params prefixed by \"OLLAMA_\", to be used\n    with models served via `ollama`.\n    This way, you can maintain several setting-groups in your .env file,\n    one per model type.\n    \"\"\"\n\n    class DynamicConfig(OpenAIGPTConfig):\n        pass\n\n    DynamicConfig.model_config = SettingsConfigDict(env_prefix=prefix.upper() + \"_\")\n    return DynamicConfig\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIResponse","title":"<code>OpenAIResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>OpenAI response model, either completion or chat.</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT","title":"<code>OpenAIGPT(config=OpenAIGPTConfig())</code>","text":"<p>               Bases: <code>LanguageModel</code></p> <p>Class for OpenAI LLMs</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, config: OpenAIGPTConfig = OpenAIGPTConfig()):\n    \"\"\"\n    Args:\n        config: configuration for openai-gpt model\n    \"\"\"\n    # copy the config to avoid modifying the original; deep to decouple\n    # nested models while preserving their concrete subclasses\n    config = config.model_copy(deep=True)\n    super().__init__(config)\n    self.config: OpenAIGPTConfig = config\n    # save original model name such as `provider/model` before\n    # we strip out the `provider` - we retain the original in\n    # case some params are specific to a provider.\n    self.chat_model_orig = self.config.chat_model_orig or self.config.chat_model\n\n    # Run the first time the model is used\n    self.run_on_first_use = cache(self.config.run_on_first_use)\n\n    # global override of chat_model,\n    # to allow quick testing with other models\n    if settings.chat_model != \"\":\n        self.config.chat_model = settings.chat_model\n        self.chat_model_orig = settings.chat_model\n        self.config.completion_model = settings.chat_model\n\n    if len(parts := self.config.chat_model.split(\"//\")) &gt; 1:\n        # there is a formatter specified, e.g.\n        # \"litellm/ollama/mistral//hf\" or\n        # \"local/localhost:8000/v1//mistral-instruct-v0.2\"\n        formatter = parts[1]\n        self.config.chat_model = parts[0]\n        if formatter == \"hf\":\n            # e.g. \"litellm/ollama/mistral//hf\" -&gt; \"litellm/ollama/mistral\"\n            formatter = find_hf_formatter(self.config.chat_model)\n            if formatter != \"\":\n                # e.g. \"mistral\"\n                self.config.formatter = formatter\n                logging.warning(\n                    f\"\"\"\n                    Using completions (not chat) endpoint with HuggingFace\n                    chat_template for {formatter} for\n                    model {self.config.chat_model}\n                    \"\"\"\n                )\n        else:\n            # e.g. \"local/localhost:8000/v1//mistral-instruct-v0.2\"\n            self.config.formatter = formatter\n\n    if self.config.formatter is not None:\n        self.config.hf_formatter = HFFormatter(\n            HFPromptFormatterConfig(model_name=self.config.formatter)\n        )\n\n    self.supports_json_schema: bool = self.config.supports_json_schema or False\n    self.supports_strict_tools: bool = self.config.supports_strict_tools or False\n\n    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", DUMMY_API_KEY)\n    self.api_key = config.api_key\n\n    # if model name starts with \"litellm\",\n    # set the actual model name by stripping the \"litellm/\" prefix\n    # and set the litellm flag to True\n    if self.config.chat_model.startswith(\"litellm/\") or self.config.litellm:\n        # e.g. litellm/ollama/mistral\n        self.config.litellm = True\n        self.api_base = self.config.api_base\n        if self.config.chat_model.startswith(\"litellm/\"):\n            # strip the \"litellm/\" prefix\n            # e.g. litellm/ollama/llama2 =&gt; ollama/llama2\n            self.config.chat_model = self.config.chat_model.split(\"/\", 1)[1]\n    elif self.config.chat_model.startswith(\"local/\"):\n        # expect this to be of the form \"local/localhost:8000/v1\",\n        # depending on how the model is launched locally.\n        # In this case the model served locally behind an OpenAI-compatible API\n        # so we can just use `openai.*` methods directly,\n        # and don't need a adaptor library like litellm\n        self.config.litellm = False\n        self.config.seed = None  # some models raise an error when seed is set\n        # Extract the api_base from the model name after the \"local/\" prefix\n        self.api_base = self.config.chat_model.split(\"/\", 1)[1]\n        if not self.api_base.startswith(\"http\"):\n            self.api_base = \"http://\" + self.api_base\n    elif self.config.chat_model.startswith(\"ollama/\"):\n        self.config.ollama = True\n\n        # use api_base from config if set, else fall back on OLLAMA_BASE_URL\n        self.api_base = self.config.api_base or OLLAMA_BASE_URL\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = OLLAMA_API_KEY\n        self.config.chat_model = self.config.chat_model.replace(\"ollama/\", \"\")\n    elif self.config.chat_model.startswith(\"vllm/\"):\n        self.supports_json_schema = True\n        self.config.chat_model = self.config.chat_model.replace(\"vllm/\", \"\")\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = os.environ.get(\"VLLM_API_KEY\", DUMMY_API_KEY)\n        self.api_base = self.config.api_base or \"http://localhost:8000/v1\"\n        if not self.api_base.startswith(\"http\"):\n            self.api_base = \"http://\" + self.api_base\n        if not self.api_base.endswith(\"/v1\"):\n            self.api_base = self.api_base + \"/v1\"\n    elif self.config.chat_model.startswith(\"llamacpp/\"):\n        self.supports_json_schema = True\n        self.api_base = self.config.chat_model.split(\"/\", 1)[1]\n        if not self.api_base.startswith(\"http\"):\n            self.api_base = \"http://\" + self.api_base\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = os.environ.get(\"LLAMA_API_KEY\", DUMMY_API_KEY)\n    else:\n        self.api_base = self.config.api_base\n        # If api_base is unset we use OpenAI's endpoint, which supports\n        # these features (with JSON schema restricted to a limited set of models)\n        self.supports_strict_tools = self.api_base is None\n        self.supports_json_schema = (\n            self.api_base is None and self.info().has_structured_output\n        )\n\n    if settings.chat_model != \"\":\n        # if we're overriding chat model globally, set completion model to same\n        self.config.completion_model = self.config.chat_model\n\n    if self.config.formatter is not None:\n        # we want to format chats -&gt; completions using this specific formatter\n        self.config.use_completion_for_chat = True\n        self.config.completion_model = self.config.chat_model\n\n    if self.config.use_completion_for_chat:\n        self.config.use_chat_for_completion = False\n\n    self.is_groq = self.config.chat_model.startswith(\"groq/\")\n    self.is_cerebras = self.config.chat_model.startswith(\"cerebras/\")\n    self.is_gemini = self.is_gemini_model()\n    self.is_deepseek = self.is_deepseek_model()\n    self.is_glhf = self.config.chat_model.startswith(\"glhf/\")\n    self.is_openrouter = self.config.chat_model.startswith(\"openrouter/\")\n    self.is_langdb = self.config.chat_model.startswith(\"langdb/\")\n    self.is_portkey = self.config.chat_model.startswith(\"portkey/\")\n    self.is_litellm_proxy = self.config.chat_model.startswith(\"litellm-proxy/\")\n\n    if self.is_groq:\n        # use groq-specific client\n        self.config.chat_model = self.config.chat_model.replace(\"groq/\", \"\")\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = os.getenv(\"GROQ_API_KEY\", DUMMY_API_KEY)\n        if self.config.use_cached_client:\n            self.client = get_groq_client(api_key=self.api_key)\n            self.async_client = get_async_groq_client(api_key=self.api_key)\n        else:\n            # Create new clients without caching\n            self.client = Groq(api_key=self.api_key)\n            self.async_client = AsyncGroq(api_key=self.api_key)\n    elif self.is_cerebras:\n        # use cerebras-specific client\n        self.config.chat_model = self.config.chat_model.replace(\"cerebras/\", \"\")\n        if self.api_key == OPENAI_API_KEY:\n            self.api_key = os.getenv(\"CEREBRAS_API_KEY\", DUMMY_API_KEY)\n        if self.config.use_cached_client:\n            self.client = get_cerebras_client(api_key=self.api_key)\n            # TODO there is not async client, so should we do anything here?\n            self.async_client = get_async_cerebras_client(api_key=self.api_key)\n        else:\n            # Create new clients without caching\n            self.client = Cerebras(api_key=self.api_key)\n            self.async_client = AsyncCerebras(api_key=self.api_key)\n    else:\n        # in these cases, there's no specific client: OpenAI python client suffices\n        if self.is_litellm_proxy:\n            self.config.chat_model = self.config.chat_model.replace(\n                \"litellm-proxy/\", \"\"\n            )\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = self.config.litellm_proxy.api_key or self.api_key\n            self.api_base = self.config.litellm_proxy.api_base or self.api_base\n        elif self.is_gemini:\n            self.config.chat_model = self.config.chat_model.replace(\"gemini/\", \"\")\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = os.getenv(\"GEMINI_API_KEY\", DUMMY_API_KEY)\n            # Use GEMINI_API_BASE env var if set (e.g. for Vertex AI),\n            # then config.api_base only if explicitly set by the user\n            # (not inherited from OPENAI_API_BASE via env_prefix),\n            # then fall back to the default Gemini endpoint.\n            gemini_api_base = os.getenv(\"GEMINI_API_BASE\", \"\")\n            openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n            explicit_api_base = (\n                self.config.api_base\n                if self.config.api_base and self.config.api_base != openai_api_base\n                else None\n            )\n            self.api_base = gemini_api_base or explicit_api_base or GEMINI_BASE_URL\n        elif self.is_glhf:\n            self.config.chat_model = self.config.chat_model.replace(\"glhf/\", \"\")\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = os.getenv(\"GLHF_API_KEY\", DUMMY_API_KEY)\n            self.api_base = GLHF_BASE_URL\n        elif self.is_openrouter:\n            self.config.chat_model = self.config.chat_model.replace(\n                \"openrouter/\", \"\"\n            )\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = os.getenv(\"OPENROUTER_API_KEY\", DUMMY_API_KEY)\n            self.api_base = OPENROUTER_BASE_URL\n        elif self.is_deepseek:\n            self.config.chat_model = self.config.chat_model.replace(\"deepseek/\", \"\")\n            self.api_base = DEEPSEEK_BASE_URL\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = os.getenv(\"DEEPSEEK_API_KEY\", DUMMY_API_KEY)\n        elif self.is_langdb:\n            self.config.chat_model = self.config.chat_model.replace(\"langdb/\", \"\")\n            self.api_base = self.config.langdb_params.base_url\n            project_id = self.config.langdb_params.project_id\n            if project_id:\n                self.api_base += \"/\" + project_id + \"/v1\"\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = self.config.langdb_params.api_key or DUMMY_API_KEY\n\n            if self.config.langdb_params:\n                params = self.config.langdb_params\n                if params.project_id:\n                    self.config.headers[\"x-project-id\"] = params.project_id\n                if params.label:\n                    self.config.headers[\"x-label\"] = params.label\n                if params.run_id:\n                    self.config.headers[\"x-run-id\"] = params.run_id\n                if params.thread_id:\n                    self.config.headers[\"x-thread-id\"] = params.thread_id\n        elif self.is_portkey:\n            # Parse the model string and extract provider/model\n            provider, model = self.config.portkey_params.parse_model_string(\n                self.config.chat_model\n            )\n            self.config.chat_model = model\n            if provider:\n                self.config.portkey_params.provider = provider\n\n            # Set Portkey base URL\n            self.api_base = self.config.portkey_params.base_url + \"/v1\"\n\n            # Set API key - use provider's API key from env if available\n            if self.api_key == OPENAI_API_KEY:\n                self.api_key = self.config.portkey_params.get_provider_api_key(\n                    self.config.portkey_params.provider, DUMMY_API_KEY\n                )\n\n            # Add Portkey-specific headers\n            self.config.headers.update(self.config.portkey_params.get_headers())\n\n        # Create http_client if needed - Priority order:\n        # 1. http_client_factory (most flexibility, not cacheable)\n        # 2. http_client_config (cacheable, moderate flexibility)\n        # 3. http_verify_ssl=False (cacheable, simple SSL bypass)\n        http_client = None\n        async_http_client = None\n        http_client_config_used = None\n\n        if self.config.http_client_factory is not None:\n            # Use the factory to create http_client (not cacheable)\n            http_client = self.config.http_client_factory()\n            if isinstance(http_client, (list, tuple)):\n                if len(http_client) != 2:\n                    raise ValueError(\n                        \"http_client_factory must return either a single \"\n                        \"httpx.Client or a tuple of \"\n                        \"(httpx.Client, httpx.AsyncClient)\"\n                    )\n                http_client, async_http_client = http_client\n            else:\n                # set async_http_client to None - so that it will\n                # be created later\n                async_http_client = None\n        elif self.config.http_client_config is not None:\n            # Use config dict (cacheable)\n            http_client_config_used = self.config.http_client_config\n        elif not self.config.http_verify_ssl:\n            # Simple SSL bypass (cacheable)\n            http_client_config_used = {\"verify\": False}\n            logging.warning(\n                \"SSL verification has been disabled. This is insecure and \"\n                \"should only be used in trusted environments (e.g., \"\n                \"corporate networks with self-signed certificates).\"\n            )\n\n        if self.config.use_cached_client:\n            self.client = get_openai_client(\n                api_key=self.api_key,\n                base_url=self.api_base,\n                organization=self.config.organization,\n                timeout=Timeout(self.config.timeout),\n                default_headers=self.config.headers,\n                http_client=http_client,\n                http_client_config=http_client_config_used,\n            )\n            self.async_client = get_async_openai_client(\n                api_key=self.api_key,\n                base_url=self.api_base,\n                organization=self.config.organization,\n                timeout=Timeout(self.config.timeout),\n                default_headers=self.config.headers,\n                http_client=async_http_client,\n                http_client_config=http_client_config_used,\n            )\n        else:\n            # Create new clients without caching\n            client_kwargs: Dict[str, Any] = dict(\n                api_key=self.api_key,\n                base_url=self.api_base,\n                organization=self.config.organization,\n                timeout=Timeout(self.config.timeout),\n                default_headers=self.config.headers,\n            )\n            if http_client is not None:\n                client_kwargs[\"http_client\"] = http_client\n            elif http_client_config_used is not None:\n                # Create http_client from config for non-cached scenario\n                try:\n                    from httpx import Client\n\n                    client_kwargs[\"http_client\"] = Client(**http_client_config_used)\n                except ImportError:\n                    raise ValueError(\n                        \"httpx is required to use http_client_config. \"\n                        \"Install it with: pip install httpx\"\n                    )\n            self.client = OpenAI(**client_kwargs)\n\n            async_client_kwargs: Dict[str, Any] = dict(\n                api_key=self.api_key,\n                base_url=self.api_base,\n                organization=self.config.organization,\n                timeout=Timeout(self.config.timeout),\n                default_headers=self.config.headers,\n            )\n            if async_http_client is not None:\n                async_client_kwargs[\"http_client\"] = async_http_client\n            elif http_client_config_used is not None:\n                # Create async http_client from config for non-cached scenario\n                try:\n                    from httpx import AsyncClient\n\n                    async_client_kwargs[\"http_client\"] = AsyncClient(\n                        **http_client_config_used\n                    )\n                except ImportError:\n                    raise ValueError(\n                        \"httpx is required to use http_client_config. \"\n                        \"Install it with: pip install httpx\"\n                    )\n            self.async_client = AsyncOpenAI(**async_client_kwargs)\n\n    self.cache: CacheDB | None = None\n    use_cache = self.config.cache_config is not None\n    if \"redis\" in settings.cache_type and use_cache:\n        if config.cache_config is None or not isinstance(\n            config.cache_config,\n            RedisCacheConfig,\n        ):\n            # switch to fresh redis config if needed\n            config.cache_config = RedisCacheConfig(\n                fake=\"fake\" in settings.cache_type\n            )\n        if \"fake\" in settings.cache_type:\n            # force use of fake redis if global cache_type is \"fakeredis\"\n            config.cache_config.fake = True\n        self.cache = RedisCache(config.cache_config)\n    elif settings.cache_type != \"none\" and use_cache:\n        raise ValueError(\n            f\"Invalid cache type {settings.cache_type}. \"\n            \"Valid types are redis, fakeredis, none\"\n        )\n\n    self.config._validate_litellm()\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.is_gemini_model","title":"<code>is_gemini_model()</code>","text":"<p>Are we using the gemini OpenAI-compatible API?</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def is_gemini_model(self) -&gt; bool:\n    \"\"\"Are we using the gemini OpenAI-compatible API?\"\"\"\n    return self.chat_model_orig.startswith(\"gemini/\")\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.unsupported_params","title":"<code>unsupported_params()</code>","text":"<p>List of params that are not supported by the current model</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def unsupported_params(self) -&gt; List[str]:\n    \"\"\"\n    List of params that are not supported by the current model\n    \"\"\"\n    unsupported = set(self.info().unsupported_params)\n    return list(unsupported)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.rename_params","title":"<code>rename_params()</code>","text":"<p>Map of param name -&gt; new name for specific models. Currently main troublemaker is o1* series.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def rename_params(self) -&gt; Dict[str, str]:\n    \"\"\"\n    Map of param name -&gt; new name for specific models.\n    Currently main troublemaker is o1* series.\n    \"\"\"\n    return self.info().rename_params\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.chat_context_length","title":"<code>chat_context_length()</code>","text":"<p>Context-length for chat-completion models/endpoints. Get it from the config if explicitly given,  otherwise use model_info based on model name, and fall back to  generic model_info if there's no match.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for chat-completion models/endpoints.\n    Get it from the config if explicitly given,\n     otherwise use model_info based on model name, and fall back to\n     generic model_info if there's no match.\n    \"\"\"\n    return self.config.chat_context_length or self.info().context_length\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.completion_context_length","title":"<code>completion_context_length()</code>","text":"<p>Context-length for completion models/endpoints. Get it from the config if explicitly given,  otherwise use model_info based on model name, and fall back to  generic model_info if there's no match.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def completion_context_length(self) -&gt; int:\n    \"\"\"\n    Context-length for completion models/endpoints.\n    Get it from the config if explicitly given,\n     otherwise use model_info based on model name, and fall back to\n     generic model_info if there's no match.\n    \"\"\"\n    return (\n        self.config.completion_context_length\n        or self.completion_info().context_length\n    )\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.chat_cost","title":"<code>chat_cost()</code>","text":"<p>(Prompt, Cached, Generation) cost per 1000 tokens, for chat-completion models/endpoints. Get it from the dict, otherwise fail-over to general method</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat_cost(self) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    (Prompt, Cached, Generation) cost per 1000 tokens, for chat-completion\n    models/endpoints.\n    Get it from the dict, otherwise fail-over to general method\n    \"\"\"\n    info = self.info()\n    cached_cost_per_million = info.cached_cost_per_million\n    if not cached_cost_per_million:\n        cached_cost_per_million = info.input_cost_per_million\n    return (\n        info.input_cost_per_million / 1000,\n        cached_cost_per_million / 1000,\n        info.output_cost_per_million / 1000,\n    )\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.set_stream","title":"<code>set_stream(stream)</code>","text":"<p>Enable or disable streaming output from API. Args:     stream: enable streaming output from API Returns: previous value of stream</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def set_stream(self, stream: bool) -&gt; bool:\n    \"\"\"Enable or disable streaming output from API.\n    Args:\n        stream: enable streaming output from API\n    Returns: previous value of stream\n    \"\"\"\n    tmp = self.config.stream\n    self.config.stream = stream\n    return tmp\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.get_stream","title":"<code>get_stream()</code>","text":"<p>Get streaming status.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def get_stream(self) -&gt; bool:\n    \"\"\"Get streaming status.\"\"\"\n    return self.config.stream and settings.stream and self.info().allows_streaming\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.tool_deltas_to_tools","title":"<code>tool_deltas_to_tools(tools)</code>  <code>staticmethod</code>","text":"<p>Convert accumulated tool-call deltas to OpenAIToolCall objects. Adapted from this excellent code:  https://community.openai.com/t/help-for-function-calls-with-streaming/627170/2</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>List[Dict[str, Any]]</code> <p>list of tool deltas received from streaming API</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>plain text corresponding to tool calls that failed to parse</p> <code>List[OpenAIToolCall]</code> <p>List[OpenAIToolCall]: list of OpenAIToolCall objects</p> <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: list of tool dicts (to reconstruct OpenAI API response, so it can be cached)</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>@staticmethod\ndef tool_deltas_to_tools(\n    tools: List[Dict[str, Any]],\n) -&gt; Tuple[\n    str,\n    List[OpenAIToolCall],\n    List[Dict[str, Any]],\n]:\n    \"\"\"\n    Convert accumulated tool-call deltas to OpenAIToolCall objects.\n    Adapted from this excellent code:\n     https://community.openai.com/t/help-for-function-calls-with-streaming/627170/2\n\n    Args:\n        tools: list of tool deltas received from streaming API\n\n    Returns:\n        str: plain text corresponding to tool calls that failed to parse\n        List[OpenAIToolCall]: list of OpenAIToolCall objects\n        List[Dict[str, Any]]: list of tool dicts\n            (to reconstruct OpenAI API response, so it can be cached)\n    \"\"\"\n    # Initialize a dictionary with default values\n\n    # idx -&gt; dict repr of tool\n    # (used to simulate OpenAIResponse object later, and also to\n    # accumulate function args as strings)\n    idx2tool_dict: Dict[str, Dict[str, Any]] = defaultdict(\n        lambda: {\n            \"id\": None,\n            \"function\": {\"arguments\": \"\", \"name\": None},\n            \"type\": None,\n            \"extra_content\": None,\n        }\n    )\n\n    for tool_delta in tools:\n        if tool_delta[\"id\"] is not None:\n            idx2tool_dict[tool_delta[\"index\"]][\"id\"] = tool_delta[\"id\"]\n\n        if tool_delta[\"function\"][\"name\"] is not None:\n            idx2tool_dict[tool_delta[\"index\"]][\"function\"][\"name\"] = tool_delta[\n                \"function\"\n            ][\"name\"]\n\n        idx2tool_dict[tool_delta[\"index\"]][\"function\"][\"arguments\"] += tool_delta[\n            \"function\"\n        ][\"arguments\"]\n\n        if tool_delta[\"type\"] is not None:\n            idx2tool_dict[tool_delta[\"index\"]][\"type\"] = tool_delta[\"type\"]\n\n        if tool_delta.get(\"extra_content\") is not None:\n            idx2tool_dict[tool_delta[\"index\"]][\"extra_content\"] = tool_delta[\n                \"extra_content\"\n            ]\n\n    # (try to) parse the fn args of each tool\n    contents: List[str] = []\n    good_indices = []\n    id2args: Dict[str, None | Dict[str, Any]] = {}\n    for idx, tool_dict in idx2tool_dict.items():\n        failed_content, args_dict = OpenAIGPT._parse_function_args(\n            tool_dict[\"function\"][\"arguments\"]\n        )\n        # used to build tool_calls_list below\n        id2args[tool_dict[\"id\"]] = args_dict or None  # if {}, store as None\n        if failed_content != \"\":\n            contents.append(failed_content)\n        else:\n            good_indices.append(idx)\n\n    # remove the failed tool calls\n    idx2tool_dict = {\n        idx: tool_dict\n        for idx, tool_dict in idx2tool_dict.items()\n        if idx in good_indices\n    }\n\n    # create OpenAIToolCall list\n    tool_calls_list = [\n        OpenAIToolCall(\n            id=tool_dict[\"id\"],\n            function=LLMFunctionCall(\n                name=tool_dict[\"function\"][\"name\"],\n                arguments=id2args.get(tool_dict[\"id\"]),\n            ),\n            type=tool_dict[\"type\"],\n            extra_content=tool_dict.get(\"extra_content\"),\n        )\n        for tool_dict in idx2tool_dict.values()\n    ]\n    return \"\\n\".join(contents), tool_calls_list, list(idx2tool_dict.values())\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.noop","title":"<code>noop()</code>","text":"<p>Does nothing.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def noop() -&gt; None:\n    \"\"\"Does nothing.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.litellm_logging_fn","title":"<code>litellm_logging_fn(model_call_dict)</code>","text":"<p>Logging function for litellm</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def litellm_logging_fn(model_call_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"Logging function for litellm\"\"\"\n    try:\n        api_input_dict = model_call_dict.get(\"additional_args\", {}).get(\n            \"complete_input_dict\"\n        )\n        if api_input_dict is not None:\n            text = escape(json.dumps(api_input_dict, indent=2))\n            print(\n                f\"[grey37]LITELLM: {text}[/grey37]\",\n            )\n    except Exception:\n        pass\n</code></pre>"},{"location":"reference/language_models/provider_params/","title":"provider_params","text":"<p>langroid/language_models/provider_params.py </p> <p>Provider-specific parameter configurations for various LLM providers.</p>"},{"location":"reference/language_models/provider_params/#langroid.language_models.provider_params.LangDBParams","title":"<code>LangDBParams</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Parameters specific to LangDB integration.</p>"},{"location":"reference/language_models/provider_params/#langroid.language_models.provider_params.PortkeyParams","title":"<code>PortkeyParams</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Parameters specific to Portkey integration.</p> <p>Portkey is an AI gateway that provides a unified API for multiple LLM providers, with features like automatic retries, fallbacks, load balancing, and observability.</p> Example usage"},{"location":"reference/language_models/provider_params/#langroid.language_models.provider_params.PortkeyParams--use-portkey-with-anthropic","title":"Use Portkey with Anthropic","text":"<p>config = OpenAIGPTConfig(     chat_model=\"portkey/anthropic/claude-3-sonnet-20240229\",     portkey_params=PortkeyParams(         api_key=\"your-portkey-api-key\",         provider=\"anthropic\"     ) )</p>"},{"location":"reference/language_models/provider_params/#langroid.language_models.provider_params.PortkeyParams.get_headers","title":"<code>get_headers()</code>","text":"<p>Generate Portkey-specific headers from parameters.</p> Source code in <code>langroid/language_models/provider_params.py</code> <pre><code>def get_headers(self) -&gt; Dict[str, str]:\n    \"\"\"Generate Portkey-specific headers from parameters.\"\"\"\n    import json\n    import os\n\n    headers = {}\n\n    if self.api_key and self.api_key != DUMMY_API_KEY:\n        headers[\"x-portkey-api-key\"] = self.api_key\n    else:\n        portkey_key = os.getenv(\"PORTKEY_API_KEY\", \"\")\n        if portkey_key:\n            headers[\"x-portkey-api-key\"] = portkey_key\n\n    if self.provider:\n        headers[\"x-portkey-provider\"] = self.provider\n\n    if self.virtual_key:\n        headers[\"x-portkey-virtual-key\"] = self.virtual_key\n\n    if self.trace_id:\n        headers[\"x-portkey-trace-id\"] = self.trace_id\n\n    if self.metadata:\n        headers[\"x-portkey-metadata\"] = json.dumps(self.metadata)\n\n    if self.retry:\n        headers[\"x-portkey-retry\"] = json.dumps(self.retry)\n\n    if self.cache:\n        headers[\"x-portkey-cache\"] = json.dumps(self.cache)\n\n    if self.cache_force_refresh is not None:\n        headers[\"x-portkey-cache-force-refresh\"] = str(\n            self.cache_force_refresh\n        ).lower()\n\n    if self.user:\n        headers[\"x-portkey-user\"] = self.user\n\n    if self.organization:\n        headers[\"x-portkey-organization\"] = self.organization\n\n    if self.custom_headers:\n        headers.update(self.custom_headers)\n\n    return headers\n</code></pre>"},{"location":"reference/language_models/provider_params/#langroid.language_models.provider_params.PortkeyParams.parse_model_string","title":"<code>parse_model_string(model_string)</code>","text":"<p>Parse a model string like \"portkey/anthropic/claude-3-sonnet\" and extract provider and model name.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[str, str]</code> <p>(provider, model_name)</p> Source code in <code>langroid/language_models/provider_params.py</code> <pre><code>def parse_model_string(self, model_string: str) -&gt; tuple[str, str]:\n    \"\"\"\n    Parse a model string like \"portkey/anthropic/claude-3-sonnet\"\n    and extract provider and model name.\n\n    Returns:\n        tuple: (provider, model_name)\n    \"\"\"\n    parts = model_string.split(\"/\", 2)\n    if len(parts) &gt;= 3 and parts[0] == \"portkey\":\n        _, provider, model = parts\n        return provider, model\n    else:\n        model = model_string.replace(\"portkey/\", \"\")\n        return \"\", model\n</code></pre>"},{"location":"reference/language_models/provider_params/#langroid.language_models.provider_params.PortkeyParams.get_provider_api_key","title":"<code>get_provider_api_key(provider, default_key=DUMMY_API_KEY)</code>","text":"<p>Get the API key for the provider from environment variables.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The provider name (e.g., \"anthropic\", \"openai\")</p> required <code>default_key</code> <code>str</code> <p>Default key to return if not found</p> <code>DUMMY_API_KEY</code> <p>Returns:</p> Type Description <code>str</code> <p>The API key for the provider</p> Source code in <code>langroid/language_models/provider_params.py</code> <pre><code>def get_provider_api_key(\n    self, provider: str, default_key: str = DUMMY_API_KEY\n) -&gt; str:\n    \"\"\"\n    Get the API key for the provider from environment variables.\n\n    Args:\n        provider: The provider name (e.g., \"anthropic\", \"openai\")\n        default_key: Default key to return if not found\n\n    Returns:\n        The API key for the provider\n    \"\"\"\n    import os\n\n    env_patterns = [\n        f\"{provider.upper()}_API_KEY\",\n        f\"{provider.upper()}_KEY\",\n    ]\n\n    for pattern in env_patterns:\n        key = os.getenv(pattern, \"\")\n        if key:\n            return key\n\n    return default_key\n</code></pre>"},{"location":"reference/language_models/utils/","title":"utils","text":"<p>langroid/language_models/utils.py </p>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.retry_with_exponential_backoff","title":"<code>retry_with_exponential_backoff(func, initial_delay=1, exponential_base=1.3, jitter=True, max_retries=5, errors=(requests.exceptions.RequestException, openai.APITimeoutError, openai.RateLimitError, openai.AuthenticationError, openai.APIError, aiohttp.ServerTimeoutError, asyncio.TimeoutError))</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def retry_with_exponential_backoff(\n    func: Callable[..., Any],\n    initial_delay: float = 1,\n    exponential_base: float = 1.3,\n    jitter: bool = True,\n    max_retries: int = 5,\n    errors: tuple = (  # type: ignore\n        requests.exceptions.RequestException,\n        openai.APITimeoutError,\n        openai.RateLimitError,\n        openai.AuthenticationError,\n        openai.APIError,\n        aiohttp.ServerTimeoutError,\n        asyncio.TimeoutError,\n    ),\n) -&gt; Callable[..., Any]:\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n\n    def wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n\n            except openai.BadRequestError as e:\n                # do not retry when the request itself is invalid,\n                # e.g. when context is too long\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n            except openai.AuthenticationError as e:\n                # do not retry when there's an auth error\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n\n            except openai.UnprocessableEntityError as e:\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n\n            # Retry on specified errors\n            except errors as e:\n\n                # For certain types of errors that slip through here\n                # (e.g. when using proxies like LiteLLM, do not retry)\n                if any(\n                    err in str(e)\n                    for err in [\n                        \"BadRequestError\",\n                        \"ConnectionError\",\n                        \"NotFoundError\",\n                    ]\n                ):\n                    logger.error(f\"OpenAI API request failed with error: {e}.\")\n                    raise e\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries &gt; max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                        f\" Last error: {str(e)}.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n                logger.warning(\n                    f\"\"\"OpenAI API request failed with error: \n                    {e}. \n                    Retrying in {delay} seconds...\"\"\"\n                )\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n</code></pre>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.async_retry_with_exponential_backoff","title":"<code>async_retry_with_exponential_backoff(func, initial_delay=1, exponential_base=1.3, jitter=True, max_retries=5, errors=(openai.APITimeoutError, openai.RateLimitError, openai.AuthenticationError, openai.APIError, aiohttp.ServerTimeoutError, asyncio.TimeoutError))</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def async_retry_with_exponential_backoff(\n    func: Callable[..., Any],\n    initial_delay: float = 1,\n    exponential_base: float = 1.3,\n    jitter: bool = True,\n    max_retries: int = 5,\n    errors: tuple = (  # type: ignore\n        openai.APITimeoutError,\n        openai.RateLimitError,\n        openai.AuthenticationError,\n        openai.APIError,\n        aiohttp.ServerTimeoutError,\n        asyncio.TimeoutError,\n    ),\n) -&gt; Callable[..., Any]:\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n\n    async def wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or exception is raised\n        while True:\n            try:\n                result = await func(*args, **kwargs)\n                return result\n\n            except openai.BadRequestError as e:\n                # do not retry when the request itself is invalid,\n                # e.g. when context is too long\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n            except openai.AuthenticationError as e:\n                # do not retry when there's an auth error\n                logger.error(f\"OpenAI API request failed with error: {e}.\")\n                raise e\n            # Retry on specified errors\n            except errors as e:\n                # For certain types of errors that slip through here\n                # (e.g. when using proxies like LiteLLM, do not retry)\n                if any(\n                    err in str(e)\n                    for err in [\n                        \"BadRequestError\",\n                        \"ConnectionError\",\n                        \"NotFoundError\",\n                    ]\n                ):\n                    logger.error(f\"OpenAI API request failed with error: {e}.\")\n                    raise e\n\n                # Increment retries\n                num_retries += 1\n\n                # Check if max retries has been reached\n                if num_retries &gt; max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                        f\" Last error: {str(e)}.\"\n                    )\n\n                # Increment the delay\n                delay *= exponential_base * (1 + jitter * random.random())\n                logger.warning(\n                    f\"\"\"OpenAI API request failed with error{e}. \n                    Retrying in {delay} seconds...\"\"\"\n                )\n                # Sleep for the delay\n                time.sleep(delay)\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/","title":"prompt_formatter","text":"<p>langroid/language_models/prompt_formatter/init.py </p>"},{"location":"reference/language_models/prompt_formatter/#langroid.language_models.prompt_formatter.PromptFormatter","title":"<code>PromptFormatter(config)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a prompt formatter</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/#langroid.language_models.prompt_formatter.PromptFormatter.format","title":"<code>format(messages)</code>  <code>abstractmethod</code>","text":"<p>Convert sequence of messages (system, user, assistant, user, assistant...user)     to a single prompt formatted according to the specific format type,     to be used in a /completions endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>chat history as a sequence of messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>formatted version of chat history</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>@abstractmethod\ndef format(self, messages: List[LLMMessage]) -&gt; str:\n    \"\"\"\n    Convert sequence of messages (system, user, assistant, user, assistant...user)\n        to a single prompt formatted according to the specific format type,\n        to be used in a /completions endpoint.\n\n    Args:\n        messages (List[LLMMessage]): chat history as a sequence of messages\n\n    Returns:\n        (str): formatted version of chat history\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/#langroid.language_models.prompt_formatter.Llama2Formatter","title":"<code>Llama2Formatter(config)</code>","text":"<p>               Bases: <code>PromptFormatter</code></p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/base/","title":"base","text":"<p>langroid/language_models/prompt_formatter/base.py </p>"},{"location":"reference/language_models/prompt_formatter/base/#langroid.language_models.prompt_formatter.base.PromptFormatter","title":"<code>PromptFormatter(config)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a prompt formatter</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/base/#langroid.language_models.prompt_formatter.base.PromptFormatter.format","title":"<code>format(messages)</code>  <code>abstractmethod</code>","text":"<p>Convert sequence of messages (system, user, assistant, user, assistant...user)     to a single prompt formatted according to the specific format type,     to be used in a /completions endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>chat history as a sequence of messages</p> required <p>Returns:</p> Type Description <code>str</code> <p>formatted version of chat history</p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>@abstractmethod\ndef format(self, messages: List[LLMMessage]) -&gt; str:\n    \"\"\"\n    Convert sequence of messages (system, user, assistant, user, assistant...user)\n        to a single prompt formatted according to the specific format type,\n        to be used in a /completions endpoint.\n\n    Args:\n        messages (List[LLMMessage]): chat history as a sequence of messages\n\n    Returns:\n        (str): formatted version of chat history\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/hf_formatter/","title":"hf_formatter","text":"<p>langroid/language_models/prompt_formatter/hf_formatter.py </p> <p>Prompt formatter based on HuggingFace <code>AutoTokenizer.apply_chat_template</code> method from their Transformers library. It searches the hub for a model matching the specified name, and uses the first one it finds. We assume that all matching models will have the same tokenizer, so we just use the first one.</p>"},{"location":"reference/language_models/prompt_formatter/hf_formatter/#langroid.language_models.prompt_formatter.hf_formatter.try_import_hf_modules","title":"<code>try_import_hf_modules()</code>","text":"<p>Attempts to import the AutoTokenizer class from the transformers package. Returns:     The AutoTokenizer class if successful. Raises:     ImportError: If the transformers package is not installed.</p> Source code in <code>langroid/language_models/prompt_formatter/hf_formatter.py</code> <pre><code>def try_import_hf_modules() -&gt; Tuple[Type[Any], Type[Any]]:\n    \"\"\"\n    Attempts to import the AutoTokenizer class from the transformers package.\n    Returns:\n        The AutoTokenizer class if successful.\n    Raises:\n        ImportError: If the transformers package is not installed.\n    \"\"\"\n    try:\n        from huggingface_hub import HfApi\n        from transformers import AutoTokenizer\n\n        return AutoTokenizer, HfApi\n    except ImportError:\n        raise ImportError(\n            \"\"\"\n            You are trying to use some/all of:\n            HuggingFace transformers.AutoTokenizer,\n            huggingface_hub.HfApi,\n            but these are not not installed \n            by default with Langroid. Please install langroid using the \n            `transformers` extra, like so:\n            pip install \"langroid[transformers]\"\n            or equivalent.\n            \"\"\"\n        )\n</code></pre>"},{"location":"reference/language_models/prompt_formatter/llama2_formatter/","title":"llama2_formatter","text":"<p>langroid/language_models/prompt_formatter/llama2_formatter.py </p>"},{"location":"reference/language_models/prompt_formatter/llama2_formatter/#langroid.language_models.prompt_formatter.llama2_formatter.Llama2Formatter","title":"<code>Llama2Formatter(config)</code>","text":"<p>               Bases: <code>PromptFormatter</code></p> Source code in <code>langroid/language_models/prompt_formatter/base.py</code> <pre><code>def __init__(self, config: PromptFormatterConfig):\n    self.config = config\n</code></pre>"},{"location":"reference/parsing/","title":"parsing","text":"<p>langroid/parsing/init.py </p>"},{"location":"reference/parsing/#langroid.parsing.PdfParsingConfig","title":"<code>PdfParsingConfig</code>","text":"<p>               Bases: <code>BaseParsingConfig</code></p>"},{"location":"reference/parsing/#langroid.parsing.PdfParsingConfig.enable_configs","title":"<code>enable_configs(values)</code>  <code>classmethod</code>","text":"<p>Ensure correct config is set based on library selection.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef enable_configs(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Ensure correct config is set based on library selection.\"\"\"\n    library = values.get(\"library\")\n\n    if library == \"llm-pdf-parser\":\n        values.setdefault(\"llm_parser_config\", LLMPdfParserConfig())\n    else:\n        values[\"llm_parser_config\"] = None\n\n    if library == \"marker\":\n        values.setdefault(\"marker_config\", MarkerConfig())\n    else:\n        values[\"marker_config\"] = None\n\n    return values\n</code></pre>"},{"location":"reference/parsing/#langroid.parsing.ParsingConfig","title":"<code>ParsingConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p>"},{"location":"reference/parsing/#langroid.parsing.ParsingConfig.convert_chunk_size_to_int","title":"<code>convert_chunk_size_to_int(v)</code>  <code>classmethod</code>","text":"<p>Convert chunk_size to int, maintaining backward compatibility with Pydantic V1.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>@field_validator(\"chunk_size\", mode=\"before\")\n@classmethod\ndef convert_chunk_size_to_int(cls, v: Any) -&gt; int:\n    \"\"\"Convert chunk_size to int, maintaining backward compatibility\n    with Pydantic V1.\n    \"\"\"\n    if isinstance(v, float):\n        return int(v)\n    return int(v)\n</code></pre>"},{"location":"reference/parsing/#langroid.parsing.Parser","title":"<code>Parser(config)</code>","text":"Source code in <code>langroid/parsing/parser.py</code> <pre><code>def __init__(self, config: ParsingConfig):\n    self.config = config\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n    except Exception:\n        self.tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n</code></pre>"},{"location":"reference/parsing/#langroid.parsing.Parser.add_window_ids","title":"<code>add_window_ids(chunks)</code>","text":"<p>Chunks may belong to multiple docs, but for each doc, they appear consecutively. Add window_ids in metadata</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def add_window_ids(self, chunks: List[Document]) -&gt; None:\n    \"\"\"Chunks may belong to multiple docs, but for each doc,\n    they appear consecutively. Add window_ids in metadata\"\"\"\n\n    # discard empty chunks\n    chunks = [c for c in chunks if c.content.strip() != \"\"]\n    if len(chunks) == 0:\n        return\n    # The original metadata.id (if any) is ignored since it will be same for all\n    # chunks and is useless. We want a distinct id for each chunk.\n    # ASSUMPTION: all chunks c of a doc have same c.metadata.id !\n    orig_ids = [c.metadata.id for c in chunks]\n    ids = [ObjectRegistry.new_id() for c in chunks]\n    id2chunk = {id: c for id, c in zip(ids, chunks)}\n\n    # group the ids by orig_id\n    # (each distinct orig_id refers to a different document)\n    orig_id_to_ids: Dict[str, List[str]] = {}\n    for orig_id, id in zip(orig_ids, ids):\n        if orig_id not in orig_id_to_ids:\n            orig_id_to_ids[orig_id] = []\n        orig_id_to_ids[orig_id].append(id)\n\n    # now each orig_id maps to a sequence of ids within a single doc\n\n    k = self.config.n_neighbor_ids\n    for orig, ids in orig_id_to_ids.items():\n        # ids are consecutive chunks in a single doc\n        n = len(ids)\n        window_ids = [ids[max(0, i - k) : min(n, i + k + 1)] for i in range(n)]\n        for i, _ in enumerate(ids):\n            c = id2chunk[ids[i]]\n            c.metadata.window_ids = window_ids[i]\n            c.metadata.id = ids[i]\n            c.metadata.is_chunk = True\n</code></pre>"},{"location":"reference/parsing/#langroid.parsing.Parser.chunk_tokens","title":"<code>chunk_tokens(text)</code>","text":"<p>Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries. Adapted from https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split into chunks.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of text chunks, each of which is a string of tokens</p> <code>List[str]</code> <p>roughly self.config.chunk_size tokens long.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def chunk_tokens(\n    self,\n    text: str,\n) -&gt; List[str]:\n    \"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens,\n    based on punctuation and newline boundaries.\n    Adapted from\n    https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py\n\n    Args:\n        text: The text to split into chunks.\n\n    Returns:\n        A list of text chunks, each of which is a string of tokens\n        roughly self.config.chunk_size tokens long.\n    \"\"\"\n    # Return an empty list if the text is empty or whitespace\n    if not text or text.isspace():\n        return []\n\n    # Tokenize the text\n    tokens = self.tokenizer.encode(text, disallowed_special=())\n\n    # Initialize an empty list of chunks\n    chunks = []\n\n    # Initialize a counter for the number of chunks\n    num_chunks = 0\n\n    # Loop until all tokens are consumed\n    while tokens and num_chunks &lt; self.config.max_chunks:\n        # Take the first chunk_size tokens as a chunk\n        chunk = tokens[: self.config.chunk_size]\n\n        # Decode the chunk into text\n        chunk_text = self.tokenizer.decode(chunk)\n\n        # Skip the chunk if it is empty or whitespace\n        if not chunk_text or chunk_text.isspace():\n            # Remove the tokens corresponding to the chunk text\n            # from remaining tokens\n            tokens = tokens[len(chunk) :]\n            # Continue to the next iteration of the loop\n            continue\n\n        # Find the last period or punctuation mark in the chunk\n        punctuation_matches = [\n            (m.start(), m.group())\n            for m in re.finditer(r\"(?:[.!?][\\s\\n]|\\n)\", chunk_text)\n        ]\n\n        last_punctuation = max([pos for pos, _ in punctuation_matches] + [-1])\n\n        # If there is a punctuation mark, and the last punctuation index is\n        # after MIN_CHUNK_SIZE_CHARS\n        if (\n            last_punctuation != -1\n            and last_punctuation &gt; self.config.min_chunk_chars\n        ):\n            # Truncate the chunk text at the punctuation mark\n            chunk_text = chunk_text[: last_punctuation + 1]\n\n        # Replace redundant (3 or more) newlines with 2 newlines to preser\n        # paragraph separation!\n        # But do NOT strip leading/trailing whitespace, to preserve formatting\n        # (e.g. code blocks, or in case we want to stitch chunks back together)\n        chunk_text_to_append = re.sub(r\"\\n{3,}\", \"\\n\\n\", chunk_text)\n\n        if len(chunk_text_to_append) &gt; self.config.discard_chunk_chars:\n            # Append the chunk text to the list of chunks\n            chunks.append(chunk_text_to_append)\n\n        # Remove the tokens corresponding to the chunk text\n        # from the remaining tokens\n        tokens = tokens[\n            len(self.tokenizer.encode(chunk_text, disallowed_special=())) :\n        ]\n\n        # Increment the number of chunks\n        num_chunks += 1\n\n    # There may be remaining tokens, but we discard them\n    # since we have already reached the maximum number of chunks\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/agent_chats/","title":"agent_chats","text":"<p>langroid/parsing/agent_chats.py </p>"},{"location":"reference/parsing/agent_chats/#langroid.parsing.agent_chats.parse_message","title":"<code>parse_message(msg)</code>","text":"<p>Parse the intended recipient and content of a message. Message format is assumed to be TO[]:. The TO[]: part is optional. <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to parse</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>str, str: task-name of intended recipient, and content of message (if recipient is not specified, task-name is empty string)</p> Source code in <code>langroid/parsing/agent_chats.py</code> <pre><code>@no_type_check\ndef parse_message(msg: str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Parse the intended recipient and content of a message.\n    Message format is assumed to be TO[&lt;recipient&gt;]:&lt;message&gt;.\n    The TO[&lt;recipient&gt;]: part is optional.\n\n    Args:\n        msg (str): message to parse\n\n    Returns:\n        str, str: task-name of intended recipient, and content of message\n            (if recipient is not specified, task-name is empty string)\n\n    \"\"\"\n    if msg is None:\n        return \"\", \"\"\n\n    # Grammar definition\n    name = Word(alphanums)\n    to_start = Literal(\"TO[\").suppress()\n    to_end = Literal(\"]:\").suppress()\n    to_field = (to_start + name(\"name\") + to_end) | Empty().suppress()\n    message = SkipTo(StringEnd())(\"text\")\n\n    # Parser definition\n    parser = to_field + message\n\n    try:\n        parsed = parser.parse_string(msg)\n        return parsed.name, parsed.text\n    except ParseException:\n        return \"\", msg\n</code></pre>"},{"location":"reference/parsing/code_parser/","title":"code_parser","text":"<p>langroid/parsing/code_parser.py </p>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser","title":"<code>CodeParser(config)</code>","text":"Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def __init__(self, config: CodeParsingConfig):\n    self.config = config\n    self.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.num_tokens","title":"<code>num_tokens(text)</code>","text":"<p>How many tokens are in the text, according to the tokenizer. This needs to be accurate, otherwise we may exceed the maximum number of tokens allowed by the model. Args:     text: string to tokenize Returns:     number of tokens in the text</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def num_tokens(self, text: str) -&gt; int:\n    \"\"\"\n    How many tokens are in the text, according to the tokenizer.\n    This needs to be accurate, otherwise we may exceed the maximum\n    number of tokens allowed by the model.\n    Args:\n        text: string to tokenize\n    Returns:\n        number of tokens in the text\n    \"\"\"\n    tokens = self.tokenizer.encode(text)\n    return len(tokens)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.split","title":"<code>split(docs)</code>","text":"<p>Split the documents into chunks, according to the config.splitter. Only the documents with a language in the config.extensions are split.</p> <p>Note</p> <p>We assume the metadata in each document has at least a <code>language</code> field, which is used to determine how to chunk the code.</p> <p>Args:     docs: list of documents to split Returns:     list of documents, where each document is a chunk; the metadata of the     original document is duplicated for each chunk, so that when we retrieve a     chunk, we immediately know info about the original document.</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def split(self, docs: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Split the documents into chunks, according to the config.splitter.\n    Only the documents with a language in the config.extensions are split.\n    !!! note\n        We assume the metadata in each document has at least a `language` field,\n        which is used to determine how to chunk the code.\n    Args:\n        docs: list of documents to split\n    Returns:\n        list of documents, where each document is a chunk; the metadata of the\n        original document is duplicated for each chunk, so that when we retrieve a\n        chunk, we immediately know info about the original document.\n    \"\"\"\n    chunked_docs = [\n        [\n            Document(content=chunk, metadata=d.metadata)\n            for chunk in chunk_code(\n                d.content,\n                d.metadata.language,  # type: ignore\n                self.config.chunk_size,\n                self.num_tokens,\n            )\n            if chunk.strip() != \"\"\n        ]\n        for d in docs\n        if d.metadata.language in self.config.extensions  # type: ignore\n    ]\n    if len(chunked_docs) == 0:\n        return []\n    # collapse the list of lists into a single list\n    return reduce(lambda x, y: x + y, chunked_docs)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.chunk_code","title":"<code>chunk_code(code, language, max_tokens, len_fn)</code>","text":"<p>Chunk code into smaller pieces, so that we don't exceed the maximum number of tokens allowed by the embedding model. Args:     code: string of code     language: str as a file extension, e.g. \"py\", \"yml\"     max_tokens: max tokens per chunk     len_fn: function to get the length of a string in token units Returns:</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def chunk_code(\n    code: str, language: str, max_tokens: int, len_fn: Callable[[str], int]\n) -&gt; List[str]:\n    \"\"\"\n    Chunk code into smaller pieces, so that we don't exceed the maximum\n    number of tokens allowed by the embedding model.\n    Args:\n        code: string of code\n        language: str as a file extension, e.g. \"py\", \"yml\"\n        max_tokens: max tokens per chunk\n        len_fn: function to get the length of a string in token units\n    Returns:\n\n    \"\"\"\n    lexer = get_lexer_by_name(language)\n    tokens = list(lex(code, lexer))\n\n    chunks = []\n    current_chunk = \"\"\n    for token_type, token_value in tokens:\n        if token_type in Token.Text.Whitespace:\n            current_chunk += token_value\n        else:\n            token_tokens = len_fn(token_value)\n            if len_fn(current_chunk) + token_tokens &lt;= max_tokens:\n                current_chunk += token_value\n            else:\n                chunks.append(current_chunk)\n                current_chunk = token_value\n\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/document_parser/","title":"document_parser","text":"<p>langroid/parsing/document_parser.py </p>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser","title":"<code>DocumentParser(source, config)</code>","text":"<p>               Bases: <code>Parser</code></p> <p>Abstract base class for extracting text from special types of docs such as PDFs or Docx.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The source, either a URL or a file path.</p> <code>doc_bytes</code> <code>BytesIO</code> <p>BytesIO object containing the doc data.</p> <pre><code>a path, a URL or a bytes object.\n</code></pre> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.create","title":"<code>create(source, config, doc_type=None)</code>  <code>classmethod</code>","text":"<p>Create a DocumentParser instance based on source type     and config..library specified. <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | bytes</code> <p>The source, could be a URL, file path, or bytes object.</p> required <code>config</code> <code>ParserConfig</code> <p>The parser configuration.</p> required <code>doc_type</code> <code>str | None</code> <p>The type of document, if known</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DocumentParser</code> <code>'DocumentParser'</code> <p>An instance of a DocumentParser subclass.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    source: str | bytes,\n    config: ParsingConfig,\n    doc_type: str | DocumentType | None = None,\n) -&gt; \"DocumentParser\":\n    \"\"\"\n    Create a DocumentParser instance based on source type\n        and config.&lt;source_type&gt;.library specified.\n\n    Args:\n        source (str|bytes): The source, could be a URL, file path,\n            or bytes object.\n        config (ParserConfig): The parser configuration.\n        doc_type (str|None): The type of document, if known\n\n    Returns:\n        DocumentParser: An instance of a DocumentParser subclass.\n    \"\"\"\n    inferred_doc_type = DocumentParser._document_type(source, doc_type)\n    if inferred_doc_type == DocumentType.PDF:\n        if config.pdf.library == \"fitz\":\n            return FitzPDFParser(source, config)\n        elif config.pdf.library == \"pymupdf4llm\":\n            return PyMuPDF4LLMParser(source, config)\n        elif config.pdf.library == \"docling\":\n            return DoclingParser(source, config)\n        elif config.pdf.library == \"pypdf\":\n            return PyPDFParser(source, config)\n        elif config.pdf.library == \"unstructured\":\n            return UnstructuredPDFParser(source, config)\n        elif config.pdf.library == \"pdf2image\":\n            return ImagePdfParser(source, config)\n        elif config.pdf.library == \"llm-pdf-parser\":\n            return LLMPdfParser(source, config)\n        elif config.pdf.library == \"marker\":\n            return MarkerPdfParser(source, config)\n        else:\n            raise ValueError(\n                f\"Unsupported PDF library specified: {config.pdf.library}\"\n            )\n    elif inferred_doc_type == DocumentType.DOCX:\n        if config.docx.library == \"unstructured\":\n            return UnstructuredDocxParser(source, config)\n        elif config.docx.library == \"python-docx\":\n            return PythonDocxParser(source, config)\n        elif config.docx.library == \"markitdown-docx\":\n            return MarkitdownDocxParser(source, config)\n        else:\n            raise ValueError(\n                f\"Unsupported DOCX library specified: {config.docx.library}\"\n            )\n    elif inferred_doc_type == DocumentType.DOC:\n        return UnstructuredDocParser(source, config)\n    elif inferred_doc_type == DocumentType.XLS:\n        return MarkitdownXLSXParser(source, config)\n    elif inferred_doc_type == DocumentType.XLSX:\n        return MarkitdownXLSXParser(source, config)\n    elif inferred_doc_type == DocumentType.PPTX:\n        return MarkitdownPPTXParser(source, config)\n    else:\n        source_name = source if isinstance(source, str) else \"bytes\"\n        raise ValueError(f\"Unsupported document type: {source_name}\")\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.chunks_from_path_or_bytes","title":"<code>chunks_from_path_or_bytes(source, parser, doc_type=None, lines=None)</code>  <code>staticmethod</code>","text":"<p>Get document chunks from a file path or bytes object. Args:     source (str|bytes): The source, which could be a URL, path or bytes object.     parser (Parser): The parser instance (for splitting the document).     doc_type (str|DocumentType|None): The type of document, if known.     lines (int|None): The number of lines to read from a plain text file. Returns:     List[Document]: A list of <code>Document</code> objects,         each containing a chunk of text, determined by the         chunking and splitting settings in the parser config.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>@staticmethod\ndef chunks_from_path_or_bytes(\n    source: str | bytes,\n    parser: Parser,\n    doc_type: str | DocumentType | None = None,\n    lines: int | None = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Get document chunks from a file path or bytes object.\n    Args:\n        source (str|bytes): The source, which could be a URL, path or bytes object.\n        parser (Parser): The parser instance (for splitting the document).\n        doc_type (str|DocumentType|None): The type of document, if known.\n        lines (int|None): The number of lines to read from a plain text file.\n    Returns:\n        List[Document]: A list of `Document` objects,\n            each containing a chunk of text, determined by the\n            chunking and splitting settings in the parser config.\n    \"\"\"\n    dtype: DocumentType = DocumentParser._document_type(source, doc_type)\n    if dtype in [\n        DocumentType.PDF,\n        DocumentType.DOC,\n        DocumentType.DOCX,\n        DocumentType.PPTX,\n        DocumentType.XLS,\n        DocumentType.XLSX,\n    ]:\n        doc_parser = DocumentParser.create(\n            source,\n            parser.config,\n            doc_type=doc_type,\n        )\n        chunks = doc_parser.get_doc_chunks()\n        if len(chunks) == 0 and dtype == DocumentType.PDF:\n            doc_parser = ImagePdfParser(source, parser.config)\n            chunks = doc_parser.get_doc_chunks()\n        return chunks\n    else:\n        # try getting as plain text; these will be chunked downstream\n        # -- could be a bytes object or a path\n        if isinstance(source, bytes):\n            content = source.decode()\n            if lines is not None:\n                file_lines = content.splitlines()[:lines]\n                content = \"\\n\".join(line.strip() for line in file_lines)\n        else:\n            with open(source, \"r\") as f:\n                if lines is not None:\n                    file_lines = list(itertools.islice(f, lines))\n                    content = \"\\n\".join(line.strip() for line in file_lines)\n                else:\n                    content = f.read()\n        soup = BeautifulSoup(content, \"html.parser\")\n        text = soup.get_text()\n        source_name = source if isinstance(source, str) else \"bytes\"\n        doc = Document(\n            content=text,\n            metadata=DocMetaData(source=str(source_name)),\n        )\n        return parser.split([doc])\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, Any], None, None]:\n    \"\"\"Yield each page in the PDF.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Langroid Document object (with possible metadata) corresponding to a given page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: Any) -&gt; Document:\n    \"\"\"\n    Get Langroid Document object (with possible metadata)\n    corresponding to a given page.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.fix_text","title":"<code>fix_text(text)</code>","text":"<p>Fix text extracted from a PDF.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The extracted text.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The fixed text.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def fix_text(self, text: str) -&gt; str:\n    \"\"\"\n    Fix text extracted from a PDF.\n\n    Args:\n        text (str): The extracted text.\n\n    Returns:\n        str: The fixed text.\n    \"\"\"\n    # Some pdf parsers introduce extra space before hyphen,\n    # so use regular expression to replace 'space-hyphen' with just 'hyphen'\n    return re.sub(r\" +\\-\", \"-\", text)\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.get_doc","title":"<code>get_doc()</code>","text":"<p>Get entire text from source as a single document.</p> <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the content of the pdf file, and metadata containing source name (URL or path)</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_doc(self) -&gt; Document:\n    \"\"\"\n    Get entire text from source as a single document.\n\n    Returns:\n        a `Document` object containing the content of the pdf file,\n            and metadata containing source name (URL or path)\n    \"\"\"\n\n    text = \"\".join(\n        [\n            self.get_document_from_page(page).content\n            for _, page in self.iterate_pages()\n        ]\n    )\n    return Document(content=text, metadata=DocMetaData(source=self.source))\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DocumentParser.get_doc_chunks","title":"<code>get_doc_chunks()</code>","text":"<p>Get document chunks from a pdf source, with page references in the document metadata.</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: a list of <code>Document</code> objects, each containing a chunk of text</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_doc_chunks(self) -&gt; List[Document]:\n    \"\"\"\n    Get document chunks from a pdf source,\n    with page references in the document metadata.\n\n    Returns:\n        List[Document]: a list of `Document` objects,\n            each containing a chunk of text\n    \"\"\"\n\n    split = []  # tokens in curr split\n    pages: List[str] = []\n    docs: List[Document] = []\n    # metadata.id to be shared by ALL chunks of this document\n    common_id = ObjectRegistry.new_id()\n    n_chunks = 0  # how many chunk so far\n    for i, page in self.iterate_pages():\n        # not used but could be useful, esp to blend the\n        # metadata from the pages into the chunks\n        page_doc = self.get_document_from_page(page)\n        page_text = page_doc.content\n        split += self.tokenizer.encode(page_text)\n        pages.append(str(i + 1))\n        # split could be so long it needs to be split\n        # into multiple chunks. Or it could be so short\n        # that it needs to be combined with the next chunk.\n        while len(split) &gt; self.config.chunk_size:\n            # pretty formatting of pages (e.g. 1-3, 4, 5-7)\n            p_0 = int(pages[0]) - self.config.page_number_offset\n            p_n = int(pages[-1]) - self.config.page_number_offset\n            page_str = f\"pages {p_0}-{p_n}\" if p_0 != p_n else f\"page {p_0}\"\n            text = self.tokenizer.decode(split[: self.config.chunk_size])\n            docs.append(\n                Document(\n                    content=text,\n                    metadata=DocMetaData(\n                        source=f\"{self.source} {page_str}\",\n                        is_chunk=True,\n                        id=common_id,\n                    ),\n                )\n            )\n            n_chunks += 1\n            split = split[self.config.chunk_size - self.config.overlap :]\n            pages = [str(i + 1)]\n    # there may be a last split remaining:\n    # if it's shorter than the overlap, we shouldn't make a chunk for it\n    # since it's already included in the prior chunk;\n    # the only exception is if there have been no chunks so far.\n    if len(split) &gt; self.config.overlap or n_chunks == 0:\n        p_0 = int(pages[0]) - self.config.page_number_offset\n        p_n = int(pages[-1]) - self.config.page_number_offset\n        page_str = f\"pages {p_0}-{p_n}\" if p_0 != p_n else f\"page {p_0}\"\n        text = self.tokenizer.decode(split[: self.config.chunk_size])\n        docs.append(\n            Document(\n                content=text,\n                metadata=DocMetaData(\n                    source=f\"{self.source} {page_str}\",\n                    is_chunk=True,\n                    id=common_id,\n                ),\n            )\n        )\n    self.add_window_ids(docs)\n    return docs\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser","title":"<code>FitzPDFParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>fitz</code> library.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>fitz</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, 'fitz.Page'], None, None]</code> <p>Generator[fitz.Page]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, \"fitz.Page\"], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `fitz`.\n\n    Returns:\n        Generator[fitz.Page]: Generator yielding each page.\n    \"\"\"\n    try:\n        import fitz\n    except ImportError:\n        LangroidImportError(\"fitz\", \"doc-chat\")\n    doc = fitz.open(stream=self.doc_bytes, filetype=\"pdf\")\n    for i, page in enumerate(doc):\n        yield i, page\n    doc.close()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.FitzPDFParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Document object from a given <code>fitz</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The <code>fitz</code> page object.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: \"fitz.Page\") -&gt; Document:\n    \"\"\"\n    Get Document object from a given `fitz` page.\n\n    Args:\n        page (fitz.Page): The `fitz` page object.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    return Document(\n        content=self.fix_text(page.get_text()),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyMuPDF4LLMParser","title":"<code>PyMuPDF4LLMParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>pymupdf4llm</code> library.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyMuPDF4LLMParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>fitz</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, 'fitz.Page'], None, None]</code> <p>Generator[fitz.Page]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, \"fitz.Page\"], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `fitz`.\n\n    Returns:\n        Generator[fitz.Page]: Generator yielding each page.\n    \"\"\"\n    try:\n        import pymupdf4llm  # noqa\n        import fitz\n    except ImportError:\n        raise LangroidImportError(\n            \"pymupdf4llm\", [\"pymupdf4llm\", \"all\", \"pdf-parsers\", \"doc-chat\"]\n        )\n    doc: fitz.Document = fitz.open(stream=self.doc_bytes, filetype=\"pdf\")\n    pages: List[Dict[str, Any]] = pymupdf4llm.to_markdown(doc, page_chunks=True)\n    for i, page in enumerate(pages):\n        yield i, page\n    doc.close()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyMuPDF4LLMParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Document object corresponding to a given \"page-chunk\" dictionary, see:  https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/api.html</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Dict[str, Any]</code> <p>The \"page-chunk\" dictionary.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: Dict[str, Any]) -&gt; Document:\n    \"\"\"\n    Get Document object corresponding to a given \"page-chunk\"\n    dictionary, see:\n     https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/api.html\n\n\n    Args:\n        page (Dict[str,Any]): The \"page-chunk\" dictionary.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    return Document(\n        content=self.fix_text(page.get(\"text\", \"\")),\n        # TODO could possible use other metadata from page, see above link.\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DoclingParser","title":"<code>DoclingParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>docling</code> library.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DoclingParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>docling</code>. Code largely from this example: https://github.com/DS4SD/docling/blob/4d41db3f7abb86c8c65386bf94e7eb0bf22bb82b/docs/examples/export_figures.py</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, Any], None, None]</code> <p>Generator[docling.Page]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, Any], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `docling`.\n    Code largely from this example:\n    https://github.com/DS4SD/docling/blob/4d41db3f7abb86c8c65386bf94e7eb0bf22bb82b/docs/examples/export_figures.py\n\n    Returns:\n        Generator[docling.Page]: Generator yielding each page.\n    \"\"\"\n    try:\n        import docling  # noqa\n    except ImportError:\n        raise LangroidImportError(\n            \"docling\", [\"docling\", \"pdf-parsers\", \"all\", \"doc-chat\"]\n        )\n\n    from docling.datamodel.base_models import InputFormat  # type: ignore\n    from docling.datamodel.pipeline_options import PdfPipelineOptions\n    from docling.document_converter import (  # type: ignore\n        ConversionResult,\n        DocumentConverter,\n        PdfFormatOption,\n    )\n    from docling_core.types.doc import ImageRefMode  # type: ignore\n\n    IMAGE_RESOLUTION_SCALE = 2.0\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n    pipeline_options.generate_page_images = True\n    pipeline_options.generate_picture_images = True\n\n    converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n    doc_path = self.source\n    if doc_path == \"bytes\":\n        # write to tmp file, then use that path\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_file:\n            temp_file.write(self.doc_bytes.getvalue())\n            doc_path = temp_file.name\n\n    output_dir = Path(str(Path(doc_path).with_suffix(\"\")) + \"-pages\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    result: ConversionResult = converter.convert(doc_path)\n\n    def n_page_elements(page) -&gt; int:  # type: ignore\n        if page.assembled is None:\n            return 0\n        return 1 + len(page.assembled.elements)\n\n    page_element_count = [n_page_elements(i) for i in result.pages]\n    element_page_cutoff = list(accumulate([1] + page_element_count))\n    for i, page in enumerate(result.pages):\n        page_start = element_page_cutoff[i]\n        page_end = element_page_cutoff[i + 1]\n        md_file = output_dir / f\"page_{i}.md\"\n        # we could have just directly exported to a markdown string,\n        # but we need to save to a file to force generation of image-files.\n        result.document.save_as_markdown(\n            md_file,\n            image_mode=ImageRefMode.REFERENCED,\n            from_element=page_start,\n            to_element=page_end,\n        )\n        yield i, md_file\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.DoclingParser.get_document_from_page","title":"<code>get_document_from_page(md_file)</code>","text":"<p>Get Document object from a given 1-page markdown file, possibly containing image refs.</p> <p>Parameters:</p> Name Type Description Default <code>md_file</code> <code>str</code> <p>The markdown file path for the page.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, md_file: str) -&gt; Document:\n    \"\"\"\n    Get Document object from a given 1-page markdown file,\n    possibly containing image refs.\n\n    Args:\n        md_file (str): The markdown file path for the page.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    with open(md_file, \"r\") as f:\n        text = f.read()\n    return Document(\n        content=self.fix_text(text),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser","title":"<code>PyPDFParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs using the <code>pypdf</code> library.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>pypdf</code>.</p> <p>Returns:</p> Type Description <code>Generator[Tuple[int, PageObject], None, None]</code> <p>Generator[pypdf.pdf.PageObject]: Generator yielding each page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, pypdf.PageObject], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `pypdf`.\n\n    Returns:\n        Generator[pypdf.pdf.PageObject]: Generator yielding each page.\n    \"\"\"\n    try:\n        import pypdf\n    except ImportError:\n        raise LangroidImportError(\"pypdf\", \"pdf-parsers\")\n    reader = pypdf.PdfReader(self.doc_bytes)\n    for i, page in enumerate(reader.pages):\n        yield i, page\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PyPDFParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Document object from a given <code>pypdf</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>PageObject</code> <p>The <code>pypdf</code> page object.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: pypdf.PageObject) -&gt; Document:\n    \"\"\"\n    Get Document object from a given `pypdf` page.\n\n    Args:\n        page (pypdf.pdf.PageObject): The `pypdf` page object.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    return Document(\n        content=self.fix_text(page.extract_text()),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.ImagePdfParser","title":"<code>ImagePdfParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parser for processing PDFs that are images, i.e. not \"true\" PDFs.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.ImagePdfParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Document object corresponding to a given <code>pdf2image</code> page.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Image</code> <p>The PIL Image object.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: \"Image\") -&gt; Document:  # type: ignore\n    \"\"\"\n    Get Document object corresponding to a given `pdf2image` page.\n\n    Args:\n        page (Image): The PIL Image object.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    try:\n        import pytesseract\n    except ImportError:\n        raise LangroidImportError(\"pytesseract\", \"pdf-parsers\")\n\n    text = pytesseract.image_to_string(page)\n    return Document(\n        content=self.fix_text(text),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredPDFParser","title":"<code>UnstructuredPDFParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parser for processing PDF files using the <code>unstructured</code> library.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredPDFParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Document object from a given <code>unstructured</code> element.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>unstructured element</code> <p>The <code>unstructured</code> element object.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: Any) -&gt; Document:\n    \"\"\"\n    Get Document object from a given `unstructured` element.\n\n    Args:\n        page (unstructured element): The `unstructured` element object.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    text = \" \".join(el.text for el in page)\n    return Document(\n        content=self.fix_text(text),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredDocxParser","title":"<code>UnstructuredDocxParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parser for processing DOCX files using the <code>unstructured</code> library.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.UnstructuredDocxParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Document object from a given <code>unstructured</code> element.</p> Note <p>The concept of \"pages\" doesn't actually exist in the .docx file format in the same way it does in formats like .pdf. A .docx file is made up of a series of elements like paragraphs and tables, but the division into pages is done dynamically based on the rendering settings (like the page size, margin size, font size, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>unstructured element</code> <p>The <code>unstructured</code> element object.</p> required <p>Returns:</p> Type Description <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: Any) -&gt; Document:\n    \"\"\"\n    Get Document object from a given `unstructured` element.\n\n    Note:\n        The concept of \"pages\" doesn't actually exist in the .docx file format in\n        the same way it does in formats like .pdf. A .docx file is made up of a\n        series of elements like paragraphs and tables, but the division into\n        pages is done dynamically based on the rendering settings (like the page\n        size, margin size, font size, etc.).\n\n    Args:\n        page (unstructured element): The `unstructured` element object.\n\n    Returns:\n        Document object, with content and possible metadata.\n    \"\"\"\n    text = \" \".join(el.text for el in page)\n    return Document(\n        content=self.fix_text(text),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PythonDocxParser","title":"<code>PythonDocxParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parser for processing DOCX files using the <code>python-docx</code> library.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PythonDocxParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Simulate iterating through pages. In a DOCX file, pages are not explicitly defined, so we consider each paragraph as a separate 'page' for simplicity.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, Any], None, None]:\n    \"\"\"\n    Simulate iterating through pages.\n    In a DOCX file, pages are not explicitly defined,\n    so we consider each paragraph as a separate 'page' for simplicity.\n    \"\"\"\n    try:\n        import docx\n    except ImportError:\n        raise LangroidImportError(\"python-docx\", \"docx\")\n\n    doc = docx.Document(self.doc_bytes)\n    for i, para in enumerate(doc.paragraphs, start=1):\n        yield i, [para]\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.PythonDocxParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Document object from a given 'page', which in this case is a single paragraph.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>list</code> <p>A list containing a single Paragraph object.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: Any) -&gt; Document:\n    \"\"\"\n    Get Document object from a given 'page', which in this case is a single\n    paragraph.\n\n    Args:\n        page (list): A list containing a single Paragraph object.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    paragraph = page[0]\n    return Document(\n        content=self.fix_text(paragraph.text),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkitdownDocxParser","title":"<code>MarkitdownDocxParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkitdownDocxParser.get_document_from_page","title":"<code>get_document_from_page(md_content)</code>","text":"<p>Get Document object from a given markdown section.</p> <p>Parameters:</p> Name Type Description Default <code>md_content</code> <code>str</code> <p>The markdown content for the section.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, md_content: str) -&gt; Document:\n    \"\"\"\n    Get Document object from a given markdown section.\n\n    Args:\n        md_content (str): The markdown content for the section.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    return Document(\n        content=self.fix_text(md_content),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkitdownXLSXParser","title":"<code>MarkitdownXLSXParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkitdownXLSXParser.get_document_from_page","title":"<code>get_document_from_page(md_content)</code>","text":"<p>Get Document object from a given 1-page markdown string.</p> <p>Parameters:</p> Name Type Description Default <code>md_content</code> <code>str</code> <p>The markdown content for the page.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, md_content: str) -&gt; Document:\n    \"\"\"\n    Get Document object from a given 1-page markdown string.\n\n    Args:\n        md_content (str): The markdown content for the page.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    return Document(\n        content=self.fix_text(md_content),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkitdownPPTXParser","title":"<code>MarkitdownPPTXParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: str | bytes, config: ParsingConfig):\n    \"\"\"\n    Args:\n        source (str|bytes): The source, which could be\n        a path, a URL or a bytes object.\n    \"\"\"\n    super().__init__(config)\n    self.config = config\n    if isinstance(source, bytes):\n        self.source = \"bytes\"\n        self.doc_bytes = BytesIO(source)\n    else:\n        self.source = source\n        self.doc_bytes = self._load_doc_as_bytesio()\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkitdownPPTXParser.get_document_from_page","title":"<code>get_document_from_page(md_content)</code>","text":"<p>Get Document object from a given 1-page markdown string.</p> <p>Parameters:</p> Name Type Description Default <code>md_content</code> <code>str</code> <p>The markdown content for the page.</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, md_content: str) -&gt; Document:\n    \"\"\"\n    Get Document object from a given 1-page markdown string.\n\n    Args:\n        md_content (str): The markdown content for the page.\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    return Document(\n        content=self.fix_text(md_content),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.LLMPdfParser","title":"<code>LLMPdfParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>This class converts PDFs to Markdown using multimodal LLMs.</p> <p>It extracts pages, converts them with the LLM (replacing images with detailed descriptions), and outputs Markdown page by page. The conversion follows <code>LLM_PDF_MD_SYSTEM_INSTRUCTION</code>. It employs multiprocessing for speed, async requests with rate limiting, and handles errors.</p> <p>It supports page-by-page splitting or chunking multiple pages into one, respecting page boundaries and a <code>max_token_limit</code>.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: Union[str, bytes], config: ParsingConfig):\n    super().__init__(source, config)\n    if not config.pdf.llm_parser_config:\n        raise ValueError(\n            \"LLMPdfParser requires a llm-based config in pdf parsing config\"\n        )\n    self.llm_parser_config: LLMPdfParserConfig = config.pdf.llm_parser_config\n    self.model_name = self.llm_parser_config.model_name\n\n    # Ensure output directory exists\n    self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n    prefix = (\n        Path(source).stem + \"_\"\n        if isinstance(source, str) and Path(source).exists()\n        else \"output_\"\n    )\n    temp_file = tempfile.NamedTemporaryFile(\n        suffix=\".md\",\n        prefix=prefix,\n        dir=str(self.OUTPUT_DIR),\n        delete=False,\n    )\n    temp_file.close()\n    self.output_filename = Path(temp_file.name)\n\n    self.max_tokens = self.llm_parser_config.max_tokens or self.DEFAULT_MAX_TOKENS\n\n    \"\"\"\n    If True, each PDF page is processed as a separate chunk,\n    resulting in one LLM request per page. If False, pages are\n    grouped into chunks based on `max_token_limit` before being sent\n    to the LLM.\n    \"\"\"\n    self.split_on_page = self.llm_parser_config.split_on_page or False\n\n    # Rate limiting parameters\n    import asyncio\n\n    self.requests_per_minute = self.llm_parser_config.requests_per_minute or 5\n\n    \"\"\"\n    A semaphore to control the number of concurrent requests to the LLM,\n    preventing rate limit errors.  A semaphore slot is acquired before\n    making an LLM request and released after the request is complete.\n    \"\"\"\n    self.semaphore = asyncio.Semaphore(self.requests_per_minute)\n    self.retry_delay = 5  # seconds, for exponential backoff\n    self.max_retries = 3\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.LLMPdfParser.max_tokens","title":"<code>max_tokens = self.llm_parser_config.max_tokens or self.DEFAULT_MAX_TOKENS</code>  <code>instance-attribute</code>","text":"<p>If True, each PDF page is processed as a separate chunk, resulting in one LLM request per page. If False, pages are grouped into chunks based on <code>max_token_limit</code> before being sent to the LLM.</p>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.LLMPdfParser.requests_per_minute","title":"<code>requests_per_minute = self.llm_parser_config.requests_per_minute or 5</code>  <code>instance-attribute</code>","text":"<p>A semaphore to control the number of concurrent requests to the LLM, preventing rate limit errors.  A semaphore slot is acquired before making an LLM request and released after the request is complete.</p>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.LLMPdfParser.process_chunks","title":"<code>process_chunks(chunks)</code>  <code>async</code>","text":"<p>Processes PDF chunks by sending them to the LLM API and collecting the results.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, where each dictionary represents a PDF chunk and contains the PDF data and page numbers.</p> required Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>async def process_chunks(self, chunks: List[Dict[str, Any]]) -&gt; List[str]:\n    \"\"\"\n    Processes PDF chunks by sending them to the LLM API and\n    collecting the results.\n\n    Args:\n        chunks: A list of dictionaries, where each dictionary represents\n            a PDF chunk and contains the PDF data and page numbers.\n    \"\"\"\n    # To show nice progress bar\n    from tqdm.asyncio import tqdm_asyncio\n\n    # Create a list of asynchronous tasks to send each chunk to the LLM.\n    # Chunk in this case might be single page or group of pages returned\n    # by prepare_pdf_chunks function\n    tasks = [self._send_chunk_to_llm(chunk) for chunk in chunks]\n\n    # Gather the results from all tasks, allowing exceptions to be returned.\n    # tqdm_asyncio is wrapper around asyncio.gather\n    gathered_results = await tqdm_asyncio.gather(\n        *tasks, desc=\"Processing chunks(pages)\", unit=\"chunk\"\n    )\n    results = []\n    for i, result in enumerate(gathered_results):\n        chunk = chunks[i]  # Get the corresponding chunk.\n\n        if isinstance(result, Exception):\n            # Handle exceptions that occurred during chunk processing.\n            logging.error(\n                \"Failed to process chunk %s: %s\",\n                chunk.get(\"page_numbers\", \"Unknown\"),\n                result,\n            )\n            results.append(\n                \"&lt;!----Error: Could not process chunk %s----&gt;\"\n                % chunk.get(\"page_numbers\", \"Unknown\")\n            )\n        else:\n            # Process successful results and append page/chunk markers.\n            markdown = str(result)\n            if self.split_on_page:\n                results.append(\n                    markdown + f\"&lt;!----Page-{chunk['page_numbers']}----&gt;\"\n                )\n            else:\n                results.append(\n                    markdown + f\"&lt;!----Chunk-{chunk['page_numbers']}----&gt;\"\n                )\n\n    return results  # Return the list of results.\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.LLMPdfParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Iterates over the document pages, extracting content using the LLM API, saves them to a markdown file, and yields page numbers along with their corresponding content.</p> <p>Yields:</p> Type Description <code>int</code> <p>A generator of tuples, where each tuple contains the page number</p> <code>Any</code> <p>(int) and the page content (Any).</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, Any], None, None]:\n    \"\"\"\n    Iterates over the document pages, extracting content using the\n    LLM API, saves them to a markdown file, and yields page numbers\n    along with their corresponding content.\n\n    Yields:\n        A generator of tuples, where each tuple contains the page number\n        (int) and the page content (Any).\n    \"\"\"\n    import asyncio\n\n    load_dotenv()\n    try:\n        # This involves extracting pages, grouping them according to the\n        # `max_tokens` limit (if `split_on_page` is False), and\n        # merging pages into larger PDF chunks. The result\n        # is a list of dictionaries, where each dictionary contains the\n        # PDF bytes and the associated page numbers or single page if\n        # `split_on_page` is true\n\n        pdf_chunks = self._prepare_pdf_chunks_for_llm(\n            num_workers=8,\n            max_tokens=self.max_tokens,\n            split_on_page=self.split_on_page,\n        )\n\n        # We asynchronously processes each chunk, sending it\n        # to the LLM and retrieving the Markdown output. It handles rate\n        # limiting and retries.\n        markdown_results = asyncio.run(self.process_chunks(pdf_chunks))\n\n        # This file serves as an intermediate storage location for the\n        # complete Markdown output.\n        with open(self.output_filename, \"w\", encoding=\"utf-8\") as outfile:\n            outfile.write(\"\\n\\n\".join(markdown_results))\n\n        # Read the full Markdown content from the temporary file.\n        with open(self.output_filename, \"r\", encoding=\"utf-8\") as infile:\n            full_markdown = infile.read()\n\n        # The splitting is based on the `split_on_page` setting. If True,\n        # the Markdown is split using the \"Page-\" marker. Otherwise, it's\n        # split using the \"Chunk-\" marker.\n        if self.split_on_page:\n            pages = full_markdown.split(\"&lt;!----Page-\")\n        else:\n            pages = full_markdown.split(\"&lt;!----Chunk-\")\n\n        # Remove the first element if it's empty (due to the split).\n        if pages and pages[0] == \"\":\n            pages = pages[1:]\n\n        # Iterate over the pages or chunks and yield their content.\n        for i, page in enumerate(pages):\n            # Check for errors during processing.\n            if \"&lt;!----Error:\" in page:\n                page_content = page\n                logging.warning(f\"Page {i}: Error processing chunk.\")\n            else:\n                # Extract the actual page content by removing the marker.\n                page_content = (\n                    page.split(\"----&gt;\", 1)[1]\n                    if len(page.split(\"----&gt;\", 1)) &gt; 1\n                    else page\n                )\n\n            # Yield the page number and content.\n            yield i, page_content\n\n    except Exception as e:\n        raise ValueError(f\"Error processing document: {e}\") from e\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.LLMPdfParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get a Document object from a given markdown page.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: str) -&gt; Document:\n    \"\"\"\n    Get a Document object from a given markdown page.\n    \"\"\"\n    return Document(\n        content=page,\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkerPdfParser","title":"<code>MarkerPdfParser(source, config)</code>","text":"<p>               Bases: <code>DocumentParser</code></p> <p>Parse PDF files using the <code>marker</code> library: https://github.com/VikParuchuri/marker</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def __init__(self, source: Union[str, bytes], config: ParsingConfig):\n    super().__init__(source, config)\n    user_config = (\n        config.pdf.marker_config.config_dict if config.pdf.marker_config else {}\n    )\n\n    self.config_dict = {**MarkerPdfParser.DEFAULT_CONFIG, **user_config}\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkerPdfParser.iterate_pages","title":"<code>iterate_pages()</code>","text":"<p>Yield each page in the PDF using <code>marker</code>.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def iterate_pages(self) -&gt; Generator[Tuple[int, Any], None, None]:\n    \"\"\"\n    Yield each page in the PDF using `marker`.\n    \"\"\"\n    try:\n        import marker  # noqa\n    except ImportError:\n        raise LangroidImportError(\n            \"marker-pdf\", [\"marker-pdf\", \"pdf-parsers\", \"all\", \"doc-chat\"]\n        )\n\n    import re\n\n    from marker.config.parser import ConfigParser\n    from marker.converters.pdf import PdfConverter\n    from marker.models import create_model_dict\n    from marker.output import save_output\n\n    config_parser = ConfigParser(self.config_dict)\n    converter = PdfConverter(\n        config=config_parser.generate_config_dict(),\n        artifact_dict=create_model_dict(),\n        processor_list=config_parser.get_processors(),\n        renderer=config_parser.get_renderer(),\n        llm_service=config_parser.get_llm_service(),\n    )\n    doc_path = self.source\n    if doc_path == \"bytes\":\n        # write to tmp file, then use that path\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_file:\n            temp_file.write(self.doc_bytes.getvalue())\n            doc_path = temp_file.name\n\n    output_dir = Path(str(Path(doc_path).with_suffix(\"\")) + \"-pages\")\n    os.makedirs(output_dir, exist_ok=True)\n    filename = Path(doc_path).stem + \"_converted\"\n\n    rendered = converter(doc_path)\n    save_output(rendered, output_dir=output_dir, fname_base=filename)\n    file_path = output_dir / f\"{filename}.md\"\n\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        full_markdown = f.read()\n\n    # Regex for splitting pages\n    pages = re.split(r\"\\{\\d+\\}----+\", full_markdown)\n\n    page_no = 0\n    for page in pages:\n        if page.strip():\n            yield page_no, page\n        page_no += 1\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.MarkerPdfParser.get_document_from_page","title":"<code>get_document_from_page(page)</code>","text":"<p>Get Document object from a given 1-page markdown file, possibly containing image refs.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>str</code> <p>The page we get by splitting large md file from</p> required <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>Document object, with content and possible metadata.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def get_document_from_page(self, page: str) -&gt; Document:\n    \"\"\"\n    Get Document object from a given 1-page markdown file,\n    possibly containing image refs.\n\n    Args:\n        page (str): The page we get by splitting large md file from\n        marker\n\n    Returns:\n        Document: Document object, with content and possible metadata.\n    \"\"\"\n    return Document(\n        content=self.fix_text(page),\n        metadata=DocMetaData(source=self.source),\n    )\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.find_last_full_char","title":"<code>find_last_full_char(possible_unicode)</code>","text":"<p>Find the index of the last full character in a byte string. Args:     possible_unicode (bytes): The bytes to check. Returns:     int: The index of the last full unicode character.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def find_last_full_char(possible_unicode: bytes) -&gt; int:\n    \"\"\"\n    Find the index of the last full character in a byte string.\n    Args:\n        possible_unicode (bytes): The bytes to check.\n    Returns:\n        int: The index of the last full unicode character.\n    \"\"\"\n\n    for i in range(len(possible_unicode) - 1, 0, -1):\n        if (possible_unicode[i] &amp; 0xC0) != 0x80:\n            return i\n    return 0\n</code></pre>"},{"location":"reference/parsing/document_parser/#langroid.parsing.document_parser.is_plain_text","title":"<code>is_plain_text(path_or_bytes)</code>","text":"<p>Check if a file is plain text by attempting to decode it as UTF-8. Args:     path_or_bytes (str|bytes): The file path or bytes object. Returns:     bool: True if the file is plain text, False otherwise.</p> Source code in <code>langroid/parsing/document_parser.py</code> <pre><code>def is_plain_text(path_or_bytes: str | bytes) -&gt; bool:\n    \"\"\"\n    Check if a file is plain text by attempting to decode it as UTF-8.\n    Args:\n        path_or_bytes (str|bytes): The file path or bytes object.\n    Returns:\n        bool: True if the file is plain text, False otherwise.\n    \"\"\"\n    if isinstance(path_or_bytes, str):\n        if path_or_bytes.startswith((\"http://\", \"https://\")):\n            response = requests.get(path_or_bytes)\n            response.raise_for_status()\n            content = response.content[:1024]\n        else:\n            with open(path_or_bytes, \"rb\") as f:\n                content = f.read(1024)\n    else:\n        content = path_or_bytes[:1024]\n    try:\n        # Use magic to detect the MIME type\n        import magic\n\n        mime_type = magic.from_buffer(content, mime=True)\n\n        # Check if the MIME type is not a text type\n        if not mime_type.startswith(\"text/\"):\n            return False\n\n        # Attempt to decode the content as UTF-8\n        content = content[: find_last_full_char(content)]\n\n        try:\n            _ = content.decode(\"utf-8\")\n            # Additional checks can go here, e.g., to verify that the content\n            # doesn't contain too many unusual characters for it to be considered text\n            return True\n        except UnicodeDecodeError:\n            return False\n    except UnicodeDecodeError:\n        # If decoding fails, it's likely not plain text (or not encoded in UTF-8)\n        return False\n</code></pre>"},{"location":"reference/parsing/file_attachment/","title":"file_attachment","text":"<p>langroid/parsing/file_attachment.py </p>"},{"location":"reference/parsing/file_attachment/#langroid.parsing.file_attachment.FileAttachment","title":"<code>FileAttachment(**data)</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a file attachment to be sent to an LLM API.</p> Source code in <code>langroid/parsing/file_attachment.py</code> <pre><code>def __init__(self, **data: Any) -&gt; None:\n    \"\"\"Initialize with sensible defaults for filename if not provided.\"\"\"\n    if \"filename\" not in data or data[\"filename\"] is None:\n        # Generate a more readable unique filename\n        unique_id = str(uuid.uuid4())[:8]\n        data[\"filename\"] = f\"attachment_{unique_id}.bin\"\n    super().__init__(**data)\n</code></pre>"},{"location":"reference/parsing/file_attachment/#langroid.parsing.file_attachment.FileAttachment.from_path","title":"<code>from_path(path, detail=None)</code>  <code>classmethod</code>","text":"<p>Create a FileAttachment from either a local file path or a URL.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_url</code> <p>Path to the file or URL to fetch</p> required <p>Returns:</p> Type Description <code>FileAttachment</code> <p>FileAttachment instance</p> Source code in <code>langroid/parsing/file_attachment.py</code> <pre><code>@classmethod\ndef from_path(\n    cls,\n    path: Union[str, Path],\n    detail: str | None = None,\n) -&gt; \"FileAttachment\":\n    \"\"\"Create a FileAttachment from either a local file path or a URL.\n\n    Args:\n        path_or_url: Path to the file or URL to fetch\n\n    Returns:\n        FileAttachment instance\n    \"\"\"\n    # Convert to string if Path object\n    path_str = str(path)\n\n    # Check if it's a URL\n    if path_str.startswith((\"http://\", \"https://\", \"ftp://\")):\n        return cls._from_url(url=path_str, detail=detail)\n    else:\n        # Assume it's a local file path\n        return cls._from_path(path_str, detail=detail)\n</code></pre>"},{"location":"reference/parsing/file_attachment/#langroid.parsing.file_attachment.FileAttachment.from_bytes","title":"<code>from_bytes(content, filename=None, mime_type=None)</code>  <code>classmethod</code>","text":"<p>Create a FileAttachment from bytes content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>bytes</code> <p>Raw bytes content</p> required <code>filename</code> <code>Optional[str]</code> <p>Optional name to use for the file</p> <code>None</code> <code>mime_type</code> <code>Optional[str]</code> <p>MIME type of the content, guessed from filename if provided</p> <code>None</code> <p>Returns:</p> Type Description <code>FileAttachment</code> <p>FileAttachment instance</p> Source code in <code>langroid/parsing/file_attachment.py</code> <pre><code>@classmethod\ndef from_bytes(\n    cls,\n    content: bytes,\n    filename: Optional[str] = None,\n    mime_type: Optional[str] = None,\n) -&gt; \"FileAttachment\":\n    \"\"\"Create a FileAttachment from bytes content.\n\n    Args:\n        content: Raw bytes content\n        filename: Optional name to use for the file\n        mime_type: MIME type of the content, guessed from filename if provided\n\n    Returns:\n        FileAttachment instance\n    \"\"\"\n    if mime_type is None and filename is not None:\n        mime_type, _ = mimetypes.guess_type(filename)\n\n    return cls(\n        content=content,\n        filename=filename,\n        mime_type=mime_type or \"application/octet-stream\",\n    )\n</code></pre>"},{"location":"reference/parsing/file_attachment/#langroid.parsing.file_attachment.FileAttachment.from_io","title":"<code>from_io(file_obj, filename=None, mime_type=None)</code>  <code>classmethod</code>","text":"<p>Create a FileAttachment from a file-like object.</p> <p>Parameters:</p> Name Type Description Default <code>file_obj</code> <code>BinaryIO</code> <p>File-like object with binary content</p> required <code>filename</code> <code>Optional[str]</code> <p>Optional name to use for the file</p> <code>None</code> <code>mime_type</code> <code>Optional[str]</code> <p>MIME type of the content, guessed from filename if provided</p> <code>None</code> <p>Returns:</p> Type Description <code>FileAttachment</code> <p>FileAttachment instance</p> Source code in <code>langroid/parsing/file_attachment.py</code> <pre><code>@classmethod\ndef from_io(\n    cls,\n    file_obj: BinaryIO,\n    filename: Optional[str] = None,\n    mime_type: Optional[str] = None,\n) -&gt; \"FileAttachment\":\n    \"\"\"Create a FileAttachment from a file-like object.\n\n    Args:\n        file_obj: File-like object with binary content\n        filename: Optional name to use for the file\n        mime_type: MIME type of the content, guessed from filename if provided\n\n    Returns:\n        FileAttachment instance\n    \"\"\"\n    content = file_obj.read()\n    return cls.from_bytes(content, filename, mime_type)\n</code></pre>"},{"location":"reference/parsing/file_attachment/#langroid.parsing.file_attachment.FileAttachment.from_text","title":"<code>from_text(text, filename=None, mime_type='text/plain', encoding='utf-8')</code>  <code>classmethod</code>","text":"<p>Create a FileAttachment from text content.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text content to include</p> required <code>filename</code> <code>Optional[str]</code> <p>Optional name to use for the file</p> <code>None</code> <code>mime_type</code> <code>str</code> <p>MIME type of the content</p> <code>'text/plain'</code> <code>encoding</code> <code>str</code> <p>Text encoding to use</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>FileAttachment</code> <p>FileAttachment instance</p> Source code in <code>langroid/parsing/file_attachment.py</code> <pre><code>@classmethod\ndef from_text(\n    cls,\n    text: str,\n    filename: Optional[str] = None,\n    mime_type: str = \"text/plain\",\n    encoding: str = \"utf-8\",\n) -&gt; \"FileAttachment\":\n    \"\"\"Create a FileAttachment from text content.\n\n    Args:\n        text: Text content to include\n        filename: Optional name to use for the file\n        mime_type: MIME type of the content\n        encoding: Text encoding to use\n\n    Returns:\n        FileAttachment instance\n    \"\"\"\n    content = text.encode(encoding)\n    return cls(content=content, filename=filename, mime_type=mime_type)\n</code></pre>"},{"location":"reference/parsing/file_attachment/#langroid.parsing.file_attachment.FileAttachment.to_base64","title":"<code>to_base64()</code>","text":"<p>Convert content to base64 encoding.</p> <p>Returns:</p> Type Description <code>str</code> <p>Base64 encoded string</p> Source code in <code>langroid/parsing/file_attachment.py</code> <pre><code>def to_base64(self) -&gt; str:\n    \"\"\"Convert content to base64 encoding.\n\n    Returns:\n        Base64 encoded string\n    \"\"\"\n    return base64.b64encode(self.content).decode(\"utf-8\")\n</code></pre>"},{"location":"reference/parsing/file_attachment/#langroid.parsing.file_attachment.FileAttachment.to_data_uri","title":"<code>to_data_uri()</code>","text":"<p>Convert content to a data URI.</p> <p>Returns:</p> Type Description <code>str</code> <p>A data URI string containing the base64-encoded content with MIME type</p> Source code in <code>langroid/parsing/file_attachment.py</code> <pre><code>def to_data_uri(self) -&gt; str:\n    \"\"\"Convert content to a data URI.\n\n    Returns:\n        A data URI string containing the base64-encoded content with MIME type\n    \"\"\"\n    base64_content = self.to_base64()\n    return f\"data:{self.mime_type};base64,{base64_content}\"\n</code></pre>"},{"location":"reference/parsing/file_attachment/#langroid.parsing.file_attachment.FileAttachment.to_dict","title":"<code>to_dict(model)</code>","text":"<p>Convert to a dictionary suitable for API requests. Tested only for PDF files.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with file data</p> Source code in <code>langroid/parsing/file_attachment.py</code> <pre><code>def to_dict(self, model: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to a dictionary suitable for API requests.\n    Tested only for PDF files.\n\n    Returns:\n        Dictionary with file data\n    \"\"\"\n    if (\n        self.mime_type\n        and self.mime_type.startswith(\"image/\")\n        or \"gemini\" in model.lower()\n    ):\n        # for gemini models, we use `image_url` for both pdf-files and images\n\n        image_url_dict = {}\n\n        # If we have a URL and it's a full http/https URL, use it directly\n        if self.url and (\n            self.url.startswith(\"http://\") or self.url.startswith(\"https://\")\n        ):\n            image_url_dict[\"url\"] = self.url\n        # Otherwise use base64 data URI\n        else:\n            image_url_dict[\"url\"] = self.to_data_uri()\n\n        # Add detail parameter if specified\n        if self.detail:\n            image_url_dict[\"detail\"] = self.detail\n\n        return dict(\n            type=\"image_url\",\n            image_url=image_url_dict,\n        )\n    else:\n        # For non-image files\n        return dict(\n            type=\"file\",\n            file=dict(\n                filename=self.filename,\n                file_data=self.to_data_uri(),\n            ),\n        )\n</code></pre>"},{"location":"reference/parsing/md_parser/","title":"md_parser","text":"<p>langroid/parsing/md_parser.py </p>"},{"location":"reference/parsing/md_parser/#langroid.parsing.md_parser.MarkdownChunkConfig","title":"<code>MarkdownChunkConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/parsing/md_parser/#langroid.parsing.md_parser.MarkdownChunkConfig.convert_chunk_size_to_int","title":"<code>convert_chunk_size_to_int(v)</code>  <code>classmethod</code>","text":"<p>Convert chunk_size to int, maintaining backward compatibility with Pydantic V1.</p> Source code in <code>langroid/parsing/md_parser.py</code> <pre><code>@field_validator(\"chunk_size\", mode=\"before\")\n@classmethod\ndef convert_chunk_size_to_int(cls, v: Any) -&gt; int:\n    \"\"\"Convert chunk_size to int, maintaining backward compatibility\n    with Pydantic V1.\n    \"\"\"\n    if isinstance(v, float):\n        return int(v)\n    return int(v)\n</code></pre>"},{"location":"reference/parsing/md_parser/#langroid.parsing.md_parser.parse_markdown_headings","title":"<code>parse_markdown_headings(md_text)</code>","text":"<p>Parse <code>md_text</code> to extract a heading-based hierarchy, skipping lines that look like headings inside fenced code blocks. Each heading node will have a child node for the text that appears between this heading and the next heading.</p> <p>Returns a list of top-level Node objects.</p> Example structure <p>Node(content='# Chapter 1', path=['# Chapter 1'], children=[     Node(content='Intro paragraph...', path=['# Chapter 1'], children=[]),     Node(content='## Section 1.1', path=['# Chapter 1', '## Section 1.1'],          children=[           Node(content='Some text in Section 1.1.', path=[...], children=[])     ]),     ... ])</p> Source code in <code>langroid/parsing/md_parser.py</code> <pre><code>def parse_markdown_headings(md_text: str) -&gt; List[Node]:\n    \"\"\"\n    Parse `md_text` to extract a heading-based hierarchy, skipping lines\n    that look like headings inside fenced code blocks. Each heading node\n    will have a child node for the text that appears between this heading\n    and the next heading.\n\n    Returns a list of top-level Node objects.\n\n    Example structure:\n        Node(content='# Chapter 1', path=['# Chapter 1'], children=[\n            Node(content='Intro paragraph...', path=['# Chapter 1'], children=[]),\n            Node(content='## Section 1.1', path=['# Chapter 1', '## Section 1.1'],\n                 children=[\n                  Node(content='Some text in Section 1.1.', path=[...], children=[])\n            ]),\n            ...\n        ])\n    \"\"\"\n    # If doc is empty or only whitespace, return []\n    if not md_text.strip():\n        return []\n\n    lines = md_text.splitlines(True)  # keep the newline characters\n\n    # We'll scan line-by-line, track code-fence status, collect headings\n    headings = []  # list of (level, heading_line, start_line_idx)\n    in_code_fence = False\n    fence_marker = None  # track which triple-backtick or ~~~ opened\n\n    for i, line in enumerate(lines):\n        # Check if we're toggling in/out of a fenced code block\n        # Typically triple backtick or triple tilde: ``` or ~~~\n        # We do a *loose* check: a line that starts with at least 3 backticks or tildes\n        # ignoring trailing text. You can refine as needed.\n        fence_match = re.match(r\"^(```+|~~~+)\", line.strip())\n        if fence_match:\n            # If we are not in a fence, we enter one;\n            # If we are in a fence, we exit if the marker matches\n            marker = fence_match.group(1)  # e.g. \"```\" or \"~~~~\"\n            if not in_code_fence:\n                in_code_fence = True\n                fence_marker = marker[:3]  # store triple backtick or triple tilde\n            else:\n                # only close if the fence_marker matches\n                # E.g. if we opened with ```, we close only on ```\n                if fence_marker and marker.startswith(fence_marker):\n                    in_code_fence = False\n                    fence_marker = None\n\n        if not in_code_fence:\n            # Check if the line is a heading\n            m = HEADING_RE.match(line)\n            if m:\n                hashes = m.group(1)  # e.g. \"##\"\n                heading_text = line.rstrip(\"\\n\")  # entire line, exact\n                level = len(hashes)\n                headings.append((level, heading_text, i))\n\n    # If no headings found, return a single root node with the entire text\n    if not headings:\n        return [Node(content=md_text.strip(), path=[], children=[])]\n\n    # Add a sentinel heading at the end-of-file, so we can slice the last block\n    # after the final real heading. We'll use level=0 so it doesn't form a real node.\n    headings.append((0, \"\", len(lines)))\n\n    # Now we build \"heading blocks\" with\n    # (level, heading_text, start_line, end_line, content)\n    heading_blocks = []\n    for idx in range(len(headings) - 1):\n        level, heading_line, start_i = headings[idx]\n        next_level, _, next_start_i = headings[idx + 1]\n\n        # Content is everything after the heading line until the next heading\n        # i.e. lines[start_i+1 : next_start_i]\n        block_content_lines = lines[start_i + 1 : next_start_i]\n        block_content = \"\".join(block_content_lines).rstrip(\"\\n\")\n\n        heading_blocks.append(\n            {\"level\": level, \"heading_text\": heading_line, \"content\": block_content}\n        )\n    # (We skip the sentinel heading in the final result.)\n\n    # We'll now convert heading_blocks into a tree using a stack-based approach\n    root_nodes: List[Node] = []\n    stack: List[Node] = []\n    header_path: List[str] = []\n\n    for hb in heading_blocks:\n        level = hb[\"level\"]  # type: ignore\n        heading_txt = hb[\"heading_text\"]\n        content_txt = hb[\"content\"]\n\n        # --- Pop stack first! ---\n        while stack and len(stack[-1].path) &gt;= level:\n            stack.pop()\n            header_path.pop()\n\n        # build new path, create a node for the heading\n        new_path = header_path + [heading_txt]\n        heading_node = Node(\n            content=heading_txt, path=new_path, children=[]  # type: ignore\n        )\n\n        # Possibly create a content child for whatever lines were below the heading\n        if content_txt.strip():  # type: ignore\n            content_node = Node(\n                content=content_txt, path=new_path, children=[]  # type: ignore\n            )\n            heading_node.children.append(content_node)\n\n        # Attach heading_node to the stack top or as a root\n        if stack:\n            stack[-1].children.append(heading_node)\n        else:\n            root_nodes.append(heading_node)\n\n        stack.append(heading_node)\n        header_path.append(heading_txt)  # type: ignore\n\n    return root_nodes\n</code></pre>"},{"location":"reference/parsing/md_parser/#langroid.parsing.md_parser.recursive_chunk","title":"<code>recursive_chunk(text, config)</code>","text":"<p>Enhanced chunker that:      1. Splits by paragraph (top-level).      2. Splits paragraphs by sentences if needed (never mid-sentence unless huge).      3. Allows going over the upper bound rather than splitting a single sentence.      4. Overlaps only once between consecutive chunks.      5. Looks ahead to avoid a \"dangling\" final chunk below the lower bound.      6. Preserves </p> <p>(and other original spacing) as best as possible.</p> Source code in <code>langroid/parsing/md_parser.py</code> <pre><code>def recursive_chunk(text: str, config: MarkdownChunkConfig) -&gt; List[str]:\n    \"\"\"\n    Enhanced chunker that:\n      1. Splits by paragraph (top-level).\n      2. Splits paragraphs by sentences if needed (never mid-sentence unless huge).\n      3. Allows going over the upper bound rather than splitting a single sentence.\n      4. Overlaps only once between consecutive chunks.\n      5. Looks ahead to avoid a \"dangling\" final chunk below the lower bound.\n      6. Preserves \\n\\n (and other original spacing) as best as possible.\n    \"\"\"\n\n    # -------------------------------------------------\n    # Helpers\n    # -------------------------------------------------\n    def count_words(text_block: str) -&gt; int:\n        return len(text_block.split())\n\n    lower_bound = int(config.chunk_size * (1 - config.variation_percent))\n    upper_bound = int(config.chunk_size * (1 + config.variation_percent))\n\n    # Quick check: if the entire text is short enough, return as-is.\n    if count_words(text) &lt;= upper_bound:\n        return [text.strip()]\n\n    # Split into paragraphs, preserving \\n\\n if it's there.\n    raw_paragraphs = text.split(\"\\n\\n\")\n    paragraphs = []\n    for i, p in enumerate(raw_paragraphs):\n        if p.strip():\n            # Re-append the double-newline if not the last piece\n            if i &lt; len(raw_paragraphs) - 1:\n                paragraphs.append(p + \"\\n\\n\")\n            else:\n                paragraphs.append(p)\n\n    # Split paragraphs into \"segments\": each segment is either\n    # a full short paragraph or (if too big) a list of sentences.\n    sentence_regex = r\"(?&lt;=[.!?])\\s+\"\n\n    def split_paragraph_into_sentences(paragraph: str) -&gt; List[str]:\n        \"\"\"\n        Return a list of sentence-sized segments. If a single sentence\n        is bigger than upper_bound, do a word-level fallback.\n        \"\"\"\n        if count_words(paragraph) &lt;= upper_bound:\n            return [paragraph]\n\n        sentences = re.split(sentence_regex, paragraph)\n        # Clean up stray whitespace\n        sentences = [s.strip() for s in sentences if s.strip()]\n\n        expanded = []\n        for s in sentences:\n            if count_words(s) &gt; upper_bound:\n                expanded.extend(_fallback_word_split(s, config))\n            else:\n                expanded.append(s)\n        return expanded\n\n    def _fallback_word_split(long_text: str, cfg: MarkdownChunkConfig) -&gt; List[str]:\n        \"\"\"\n        As a last resort, split extremely large 'sentence' by words.\n        \"\"\"\n        words = long_text.split()\n        pieces = []\n        start = 0\n        while start &lt; len(words):\n            end = start + cfg.chunk_size\n            chunk_words = words[start:end]\n            pieces.append(\" \".join(chunk_words))\n            start = end\n        return pieces\n\n    # Build a list of segments\n    segments = []\n    for para in paragraphs:\n        if count_words(para) &gt; upper_bound:\n            # split into sentences\n            segs = split_paragraph_into_sentences(para)\n            segments.extend(segs)\n        else:\n            segments.append(para)\n\n    # -------------------------------------------------\n    # Accumulate segments into final chunks\n    # -------------------------------------------------\n    chunks = []\n    current_chunk = \"\"\n    current_count = 0\n\n    def flush_chunk() -&gt; None:\n        nonlocal current_chunk, current_count\n        trimmed = current_chunk.strip()\n        if trimmed:\n            chunks.append(trimmed)\n        current_chunk = \"\"\n        current_count = 0\n\n    def remaining_tokens_in_future(all_segments: List[str], current_index: int) -&gt; int:\n        \"\"\"Sum of word counts from current_index onward.\"\"\"\n        return sum(count_words(s) for s in all_segments[current_index:])\n\n    for i, seg in enumerate(segments):\n        seg_count = count_words(seg)\n\n        # If this single segment alone exceeds upper_bound, we accept it as a big chunk.\n        if seg_count &gt; upper_bound:\n            # If we have something in the current chunk, flush it first\n            flush_chunk()\n            # Then store this large segment as its own chunk\n            chunks.append(seg.strip())\n            continue\n\n        # Attempt to add seg to the current chunk\n        if (current_count + seg_count) &gt; upper_bound and (current_count &gt;= lower_bound):\n            # We would normally flush here, but let's see if we are nearing the end:\n            # If the remaining tokens (including this one) is &lt; lower_bound,\n            # we just add it anyway to avoid creating a tiny final chunk.\n            future_tokens = remaining_tokens_in_future(segments, i)\n            if future_tokens &lt; lower_bound:\n                # Just add it (allowing to exceed upper bound)\n                if current_chunk:\n                    # Add space or preserve newline carefully\n                    # We'll do a basic approach here:\n                    if seg.startswith(\"\\n\\n\"):\n                        current_chunk += seg  # preserve double new line\n                    else:\n                        current_chunk += \" \" + seg\n                    current_count = count_words(current_chunk)\n                else:\n                    current_chunk = seg\n                    current_count = seg_count\n            else:\n                # Normal flush\n                old_chunk = current_chunk\n                flush_chunk()\n                # Overlap from old_chunk\n                overlap_tokens_list = (\n                    old_chunk.split()[-config.overlap_tokens :] if old_chunk else []\n                )\n                overlap_str = (\n                    \" \".join(overlap_tokens_list) if overlap_tokens_list else \"\"\n                )\n                if overlap_str:\n                    current_chunk = overlap_str + \" \" + seg\n                else:\n                    current_chunk = seg\n                current_count = count_words(current_chunk)\n        else:\n            # Just accumulate\n            if current_chunk:\n                if seg.startswith(\"\\n\\n\"):\n                    current_chunk += seg\n                else:\n                    current_chunk += \" \" + seg\n            else:\n                current_chunk = seg\n            current_count = count_words(current_chunk)\n\n    # Flush leftover\n    flush_chunk()\n\n    # Return non-empty\n    return [c for c in chunks if c.strip()]\n</code></pre>"},{"location":"reference/parsing/md_parser/#langroid.parsing.md_parser.aggregate_content","title":"<code>aggregate_content(node)</code>","text":"<p>Recursively aggregate the content from a node and all its descendants, excluding header-only nodes to avoid duplication.</p> Source code in <code>langroid/parsing/md_parser.py</code> <pre><code>def aggregate_content(node: Node) -&gt; str:\n    \"\"\"\n    Recursively aggregate the content from a node and all its descendants,\n    excluding header-only nodes to avoid duplication.\n    \"\"\"\n    parts = []\n\n    # Skip header-only nodes in content aggregation\n    is_header_only = node.path and node.content.strip() == node.path[-1].strip()\n    if not is_header_only and node.content.strip():\n        parts.append(node.content.strip())\n\n    # Recurse on children\n    for child in node.children:\n        child_text = aggregate_content(child)\n        if child_text.strip():\n            parts.append(child_text.strip())\n\n    return \"\\n\\n\".join(parts)\n</code></pre>"},{"location":"reference/parsing/md_parser/#langroid.parsing.md_parser.flatten_tree","title":"<code>flatten_tree(node, level=0)</code>","text":"<p>Flatten a node and its children back into proper markdown text.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to flatten</p> required <code>level</code> <code>int</code> <p>The current heading level (depth in the tree)</p> <code>0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Properly formatted markdown text</p> Source code in <code>langroid/parsing/md_parser.py</code> <pre><code>def flatten_tree(node: Node, level: int = 0) -&gt; str:\n    \"\"\"\n    Flatten a node and its children back into proper markdown text.\n\n    Args:\n        node: The node to flatten\n        level: The current heading level (depth in the tree)\n\n    Returns:\n        str: Properly formatted markdown text\n    \"\"\"\n    result = \"\"\n\n    # Check if this is a header node (content matches last item in path)\n    is_header = node.path and node.content.strip().startswith(\"#\")\n\n    # For header nodes, don't duplicate the hash marks\n    if is_header:\n        result = node.content.strip() + \"\\n\\n\"\n    elif node.content.strip():\n        result = node.content.strip() + \"\\n\\n\"\n\n    # Process all children\n    for child in node.children:\n        result += flatten_tree(child, level + 1)\n\n    return result\n</code></pre>"},{"location":"reference/parsing/md_parser/#langroid.parsing.md_parser.rollup_chunk_node","title":"<code>rollup_chunk_node(node, config, prefix='')</code>","text":"<p>Recursively produce rollup chunks from <code>node</code>, passing down a <code>prefix</code> (e.g., parent heading(s)).</p> <ul> <li>If a node is heading-only (content == last path item) and has children,   we skip creating a chunk for that node alone and instead add that heading   to the <code>prefix</code> for child nodes.</li> <li>If a node is NOT heading-only OR has no children, we try to fit all of its   flattened content into a single chunk. If it's too large, we chunk it.</li> <li>We pass the (possibly updated) prefix down to children, so each child's   chunk is enriched exactly once with all ancestor headings.</li> </ul> Source code in <code>langroid/parsing/md_parser.py</code> <pre><code>def rollup_chunk_node(\n    node: Node, config: MarkdownChunkConfig, prefix: str = \"\"\n) -&gt; List[Chunk]:\n    \"\"\"\n    Recursively produce rollup chunks from `node`, passing down a `prefix`\n    (e.g., parent heading(s)).\n\n    - If a node is heading-only (content == last path item) and has children,\n      we skip creating a chunk for that node alone and instead add that heading\n      to the `prefix` for child nodes.\n    - If a node is NOT heading-only OR has no children, we try to fit all of its\n      flattened content into a single chunk. If it's too large, we chunk it.\n    - We pass the (possibly updated) prefix down to children, so each child's\n      chunk is enriched exactly once with all ancestor headings.\n    \"\"\"\n\n    chunks: List[Chunk] = []\n\n    # Check if the node is \"heading-only\" and has children\n    # e.g. node.content==\"# Chapter 1\" and node.path[-1]==\"# Chapter 1\"\n    is_heading_only_with_children = (\n        node.path\n        and node.content.strip() == node.path[-1].strip()\n        and len(node.children) &gt; 0\n    )\n\n    if is_heading_only_with_children:\n        # We do NOT create a chunk for this node alone.\n        # Instead, we add its heading to the prefix for child chunks.\n        new_prefix = prefix + node.content.strip()\n        for i, child in enumerate(node.children):\n            sep = \"\\n\\n\" if i == 0 else config.header_context_sep\n            chunks.extend(rollup_chunk_node(child, config, prefix=new_prefix + sep))\n        return chunks\n\n    # If not heading-only-with-children, we handle this node's own content:\n    # Flatten the entire node (including sub-children) in standard Markdown form.\n    flattened = flatten_tree(node, level=len(node.path))\n    flattened_with_prefix = prefix + flattened\n    total_tokens = count_words(flattened_with_prefix)\n\n    # Check if we can roll up everything (node + children) in a single chunk\n    if total_tokens &lt;= config.chunk_size * (1 + config.variation_percent):\n        # One single chunk for the entire subtree\n        chunks.append(\n            Chunk(text=flattened_with_prefix, path=node.path, token_count=total_tokens)\n        )\n    else:\n        # It's too large overall. We'll chunk the node's own content first (if any),\n        # then recurse on children.\n        node_content = node.content.strip()\n\n        # If we have actual content that is not just a heading, chunk it with the prefix\n        # (like \"preamble\" text).\n        # Note: if this node is heading-only but has NO children,\n        # it will still land here\n        # (because is_heading_only_with_children was False due to zero children).\n        if node_content and (not node.path or node_content != node.path[-1].strip()):\n            # The node is actual content (not purely heading).\n            # We'll chunk it in paragraphs/sentences with the prefix.\n            content_chunks = recursive_chunk(node_content, config)\n            for text_block in content_chunks:\n                block_with_prefix = prefix + text_block\n                chunks.append(\n                    Chunk(\n                        text=block_with_prefix,\n                        path=node.path,\n                        token_count=count_words(block_with_prefix),\n                    )\n                )\n\n        # Now recurse on children, passing the same prefix so they get it too\n        for child in node.children:\n            chunks.extend(rollup_chunk_node(child, config, prefix=prefix))\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/para_sentence_split/","title":"para_sentence_split","text":"<p>langroid/parsing/para_sentence_split.py </p>"},{"location":"reference/parsing/parse_json/","title":"parse_json","text":"<p>langroid/parsing/parse_json.py </p>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.is_valid_json","title":"<code>is_valid_json(json_str)</code>","text":"<p>Check if the input string is a valid JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The input string to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input string is a valid JSON, False otherwise.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def is_valid_json(json_str: str) -&gt; bool:\n    \"\"\"Check if the input string is a valid JSON.\n\n    Args:\n        json_str (str): The input string to check.\n\n    Returns:\n        bool: True if the input string is a valid JSON, False otherwise.\n    \"\"\"\n    try:\n        json.loads(json_str)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.flatten","title":"<code>flatten(nested_list)</code>","text":"<p>Flatten a nested list into a single list of strings</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def flatten(nested_list) -&gt; Iterator[str]:  # type: ignore\n    \"\"\"Flatten a nested list into a single list of strings\"\"\"\n    for item in nested_list:\n        if isinstance(item, (list, tuple)):\n            for subitem in flatten(item):\n                yield subitem\n        else:\n            yield item\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.get_json_candidates","title":"<code>get_json_candidates(s)</code>","text":"<p>Get top-level JSON candidates, i.e. strings between curly braces.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def get_json_candidates(s: str) -&gt; List[str]:\n    \"\"\"Get top-level JSON candidates, i.e. strings between curly braces.\"\"\"\n    # Define the grammar for matching curly braces\n    curly_braces = original_text_for(nested_expr(\"{\", \"}\"))\n\n    # Parse the string\n    try:\n        results = curly_braces.search_string(s)\n        # Properly convert nested lists to strings\n        return [r[0] for r in results]\n    except Exception:\n        return []\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.try_repair_json_yaml","title":"<code>try_repair_json_yaml(s)</code>","text":"<p>Attempt to load as json, and if it fails, try repairing the JSON.    If that fails, replace any  with space as a last resort.    NOTE - replacing  with space will result in format loss,    which may matter in generated code (e.g. python, toml, etc)</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def try_repair_json_yaml(s: str) -&gt; str | None:\n    \"\"\"\n    Attempt to load as json, and if it fails, try repairing the JSON.\n    If that fails, replace any \\n with space as a last resort.\n    NOTE - replacing \\n with space will result in format loss,\n    which may matter in generated code (e.g. python, toml, etc)\n    \"\"\"\n    s_repaired_obj = repair_json(s, return_objects=True)\n    if isinstance(s_repaired_obj, list):\n        if len(s_repaired_obj) &gt; 0:\n            s_repaired_obj = s_repaired_obj[0]\n        else:\n            s_repaired_obj = None\n    if s_repaired_obj is not None:\n        return json.dumps(s_repaired_obj)  # type: ignore\n    else:\n        try:\n            yaml_result = yaml.safe_load(s)\n            if isinstance(yaml_result, dict):\n                return json.dumps(yaml_result)\n        except yaml.YAMLError:\n            pass\n        # If it still fails, replace any \\n with space as a last resort\n        s = s.replace(\"\\n\", \" \")\n        if is_valid_json(s):\n            return s\n        else:\n            return None  # all failed\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.extract_top_level_json","title":"<code>extract_top_level_json(s)</code>","text":"<p>Extract all top-level JSON-formatted substrings from a given string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string to search for JSON substrings.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of top-level JSON-formatted substrings.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def extract_top_level_json(s: str) -&gt; List[str]:\n    \"\"\"Extract all top-level JSON-formatted substrings from a given string.\n\n    Args:\n        s (str): The input string to search for JSON substrings.\n\n    Returns:\n        List[str]: A list of top-level JSON-formatted substrings.\n    \"\"\"\n    # Find JSON object and array candidates\n    json_candidates = get_json_candidates(s)\n    maybe_repaired_jsons = map(try_repair_json_yaml, json_candidates)\n\n    return [candidate for candidate in maybe_repaired_jsons if candidate is not None]\n</code></pre>"},{"location":"reference/parsing/parse_json/#langroid.parsing.parse_json.top_level_json_field","title":"<code>top_level_json_field(s, f)</code>","text":"<p>Extract the value of a field f from a top-level JSON object. If there are multiple, just return the first.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string to search for JSON substrings.</p> required <code>f</code> <code>str</code> <p>The field to extract from the JSON object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>The value of the field f in the top-level JSON object, if any. Otherwise, return an empty string.</p> Note <p>This function is designed to never crash. If any exception occurs during JSON parsing or field extraction, it gracefully returns an empty string.</p> Source code in <code>langroid/parsing/parse_json.py</code> <pre><code>def top_level_json_field(s: str, f: str) -&gt; Any:\n    \"\"\"\n    Extract the value of a field f from a top-level JSON object.\n    If there are multiple, just return the first.\n\n    Args:\n        s (str): The input string to search for JSON substrings.\n        f (str): The field to extract from the JSON object.\n\n    Returns:\n        str: The value of the field f in the top-level JSON object, if any.\n            Otherwise, return an empty string.\n\n    Note:\n        This function is designed to never crash. If any exception occurs during\n        JSON parsing or field extraction, it gracefully returns an empty string.\n    \"\"\"\n    try:\n        jsons = extract_top_level_json(s)\n        if len(jsons) == 0:\n            return \"\"\n        for j in jsons:\n            try:\n                json_data = json.loads(j)\n                if isinstance(json_data, dict):\n                    if f in json_data:\n                        return json_data[f]\n                elif isinstance(json_data, list):\n                    # Some responses wrap candidate JSON objects in a list; scan them.\n                    for item in json_data:\n                        if isinstance(item, dict) and f in item:\n                            return item[f]\n            except (json.JSONDecodeError, TypeError, KeyError):\n                # If this specific JSON fails to parse, continue to next candidate\n                continue\n    except Exception:\n        # Catch any unexpected errors to ensure we never crash\n        pass\n\n    return \"\"\n</code></pre>"},{"location":"reference/parsing/parser/","title":"parser","text":"<p>langroid/parsing/parser.py </p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.BaseParsingConfig","title":"<code>BaseParsingConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Base class for document parsing configurations.</p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.LLMPdfParserConfig","title":"<code>LLMPdfParserConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for LLM-based parsing.</p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.MarkerConfig","title":"<code>MarkerConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for Markitdown-based parsing.</p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.PdfParsingConfig","title":"<code>PdfParsingConfig</code>","text":"<p>               Bases: <code>BaseParsingConfig</code></p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.PdfParsingConfig.enable_configs","title":"<code>enable_configs(values)</code>  <code>classmethod</code>","text":"<p>Ensure correct config is set based on library selection.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef enable_configs(cls, values: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Ensure correct config is set based on library selection.\"\"\"\n    library = values.get(\"library\")\n\n    if library == \"llm-pdf-parser\":\n        values.setdefault(\"llm_parser_config\", LLMPdfParserConfig())\n    else:\n        values[\"llm_parser_config\"] = None\n\n    if library == \"marker\":\n        values.setdefault(\"marker_config\", MarkerConfig())\n    else:\n        values[\"marker_config\"] = None\n\n    return values\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.ParsingConfig","title":"<code>ParsingConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.ParsingConfig.convert_chunk_size_to_int","title":"<code>convert_chunk_size_to_int(v)</code>  <code>classmethod</code>","text":"<p>Convert chunk_size to int, maintaining backward compatibility with Pydantic V1.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>@field_validator(\"chunk_size\", mode=\"before\")\n@classmethod\ndef convert_chunk_size_to_int(cls, v: Any) -&gt; int:\n    \"\"\"Convert chunk_size to int, maintaining backward compatibility\n    with Pydantic V1.\n    \"\"\"\n    if isinstance(v, float):\n        return int(v)\n    return int(v)\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser","title":"<code>Parser(config)</code>","text":"Source code in <code>langroid/parsing/parser.py</code> <pre><code>def __init__(self, config: ParsingConfig):\n    self.config = config\n    try:\n        self.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n    except Exception:\n        self.tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser.add_window_ids","title":"<code>add_window_ids(chunks)</code>","text":"<p>Chunks may belong to multiple docs, but for each doc, they appear consecutively. Add window_ids in metadata</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def add_window_ids(self, chunks: List[Document]) -&gt; None:\n    \"\"\"Chunks may belong to multiple docs, but for each doc,\n    they appear consecutively. Add window_ids in metadata\"\"\"\n\n    # discard empty chunks\n    chunks = [c for c in chunks if c.content.strip() != \"\"]\n    if len(chunks) == 0:\n        return\n    # The original metadata.id (if any) is ignored since it will be same for all\n    # chunks and is useless. We want a distinct id for each chunk.\n    # ASSUMPTION: all chunks c of a doc have same c.metadata.id !\n    orig_ids = [c.metadata.id for c in chunks]\n    ids = [ObjectRegistry.new_id() for c in chunks]\n    id2chunk = {id: c for id, c in zip(ids, chunks)}\n\n    # group the ids by orig_id\n    # (each distinct orig_id refers to a different document)\n    orig_id_to_ids: Dict[str, List[str]] = {}\n    for orig_id, id in zip(orig_ids, ids):\n        if orig_id not in orig_id_to_ids:\n            orig_id_to_ids[orig_id] = []\n        orig_id_to_ids[orig_id].append(id)\n\n    # now each orig_id maps to a sequence of ids within a single doc\n\n    k = self.config.n_neighbor_ids\n    for orig, ids in orig_id_to_ids.items():\n        # ids are consecutive chunks in a single doc\n        n = len(ids)\n        window_ids = [ids[max(0, i - k) : min(n, i + k + 1)] for i in range(n)]\n        for i, _ in enumerate(ids):\n            c = id2chunk[ids[i]]\n            c.metadata.window_ids = window_ids[i]\n            c.metadata.id = ids[i]\n            c.metadata.is_chunk = True\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser.chunk_tokens","title":"<code>chunk_tokens(text)</code>","text":"<p>Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries. Adapted from https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split into chunks.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of text chunks, each of which is a string of tokens</p> <code>List[str]</code> <p>roughly self.config.chunk_size tokens long.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def chunk_tokens(\n    self,\n    text: str,\n) -&gt; List[str]:\n    \"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens,\n    based on punctuation and newline boundaries.\n    Adapted from\n    https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py\n\n    Args:\n        text: The text to split into chunks.\n\n    Returns:\n        A list of text chunks, each of which is a string of tokens\n        roughly self.config.chunk_size tokens long.\n    \"\"\"\n    # Return an empty list if the text is empty or whitespace\n    if not text or text.isspace():\n        return []\n\n    # Tokenize the text\n    tokens = self.tokenizer.encode(text, disallowed_special=())\n\n    # Initialize an empty list of chunks\n    chunks = []\n\n    # Initialize a counter for the number of chunks\n    num_chunks = 0\n\n    # Loop until all tokens are consumed\n    while tokens and num_chunks &lt; self.config.max_chunks:\n        # Take the first chunk_size tokens as a chunk\n        chunk = tokens[: self.config.chunk_size]\n\n        # Decode the chunk into text\n        chunk_text = self.tokenizer.decode(chunk)\n\n        # Skip the chunk if it is empty or whitespace\n        if not chunk_text or chunk_text.isspace():\n            # Remove the tokens corresponding to the chunk text\n            # from remaining tokens\n            tokens = tokens[len(chunk) :]\n            # Continue to the next iteration of the loop\n            continue\n\n        # Find the last period or punctuation mark in the chunk\n        punctuation_matches = [\n            (m.start(), m.group())\n            for m in re.finditer(r\"(?:[.!?][\\s\\n]|\\n)\", chunk_text)\n        ]\n\n        last_punctuation = max([pos for pos, _ in punctuation_matches] + [-1])\n\n        # If there is a punctuation mark, and the last punctuation index is\n        # after MIN_CHUNK_SIZE_CHARS\n        if (\n            last_punctuation != -1\n            and last_punctuation &gt; self.config.min_chunk_chars\n        ):\n            # Truncate the chunk text at the punctuation mark\n            chunk_text = chunk_text[: last_punctuation + 1]\n\n        # Replace redundant (3 or more) newlines with 2 newlines to preser\n        # paragraph separation!\n        # But do NOT strip leading/trailing whitespace, to preserve formatting\n        # (e.g. code blocks, or in case we want to stitch chunks back together)\n        chunk_text_to_append = re.sub(r\"\\n{3,}\", \"\\n\\n\", chunk_text)\n\n        if len(chunk_text_to_append) &gt; self.config.discard_chunk_chars:\n            # Append the chunk text to the list of chunks\n            chunks.append(chunk_text_to_append)\n\n        # Remove the tokens corresponding to the chunk text\n        # from the remaining tokens\n        tokens = tokens[\n            len(self.tokenizer.encode(chunk_text, disallowed_special=())) :\n        ]\n\n        # Increment the number of chunks\n        num_chunks += 1\n\n    # There may be remaining tokens, but we discard them\n    # since we have already reached the maximum number of chunks\n\n    return chunks\n</code></pre>"},{"location":"reference/parsing/pdf_utils/","title":"pdf_utils","text":"<p>langroid/parsing/pdf_utils.py </p>"},{"location":"reference/parsing/pdf_utils/#langroid.parsing.pdf_utils.pdf_split_pages","title":"<code>pdf_split_pages(input_pdf, splits=None)</code>","text":"<p>Splits a PDF into individual pages or chunks in a temporary directory.</p> <p>Parameters:</p> Name Type Description Default <code>input_pdf</code> <code>Union[BytesIO, BinaryIO, str]</code> <p>Input PDF file in bytes, binary mode, or a file path</p> required <code>splits</code> <code>Optional[List[int]]</code> <p>Optional list of page numbers to split at.     If provided, pages will be grouped into chunks ending at     these page numbers.     For example, if splits = [4, 9], the result will have pages 1-4, 5-9,     and 10-end.     If not provided, default to splitting into individual pages.</p> <code>None</code> <code>max_workers</code> <p>Maximum number of concurrent workers for parallel processing</p> required <p>Returns:</p> Type Description <code>Tuple[List[Path], TemporaryDirectory[Any]]</code> <p>Tuple containing: - List of paths to individual PDF pages or chunks - Temporary directory object (caller must call cleanup())</p> Example <p>paths, tmp_dir = split_pdf_temp(\"input.pdf\")</p> Source code in <code>langroid/parsing/pdf_utils.py</code> <pre><code>def pdf_split_pages(\n    input_pdf: Union[BytesIO, BinaryIO, str],\n    splits: Optional[List[int]] = None,\n) -&gt; Tuple[List[Path], TemporaryDirectory[Any]]:\n    \"\"\"Splits a PDF into individual pages or chunks in a temporary directory.\n\n    Args:\n        input_pdf: Input PDF file in bytes, binary mode, or a file path\n        splits: Optional list of page numbers to split at.\n                If provided, pages will be grouped into chunks ending at\n                these page numbers.\n                For example, if splits = [4, 9], the result will have pages 1-4, 5-9,\n                and 10-end.\n                If not provided, default to splitting into individual pages.\n        max_workers: Maximum number of concurrent workers for parallel processing\n\n    Returns:\n        Tuple containing:\n            - List of paths to individual PDF pages or chunks\n            - Temporary directory object (caller must call cleanup())\n\n    Example:\n        paths, tmp_dir = split_pdf_temp(\"input.pdf\")\n        # Use paths...\n        tmp_dir.cleanup()  # Clean up temp files when done\n    \"\"\"\n    tmp_dir = tempfile.TemporaryDirectory()\n    if isinstance(input_pdf, str):\n        doc = fitz.open(input_pdf)\n    else:\n        doc = fitz.open(stream=input_pdf, filetype=\"pdf\")\n    paths = []\n\n    total_pages = len(doc)\n\n    if splits is None:\n        # Split into individual pages (original behavior)\n        for page_num in range(total_pages):\n            new_doc = fitz.open()\n            new_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)\n            output = Path(tmp_dir.name) / f\"page_{page_num + 1}.pdf\"\n            new_doc.save(str(output))\n            new_doc.close()\n            paths.append(output)\n    else:\n        # Split according to specified page ranges\n        # Make sure the splits list is sorted and includes all valid splits\n        splits = sorted([s for s in splits if 1 &lt;= s &lt;= total_pages])\n\n        # Create the ranges to process\n        ranges = []\n        start_page = 0\n        for end_page in splits:\n            ranges.append((start_page, end_page - 1))\n            start_page = end_page\n\n        # Add the final range if there are pages after the last split\n        if start_page &lt; total_pages:\n            ranges.append((start_page, total_pages - 1))\n\n        # Process each range\n        for i, (from_page, to_page) in enumerate(ranges):\n            new_doc = fitz.open()\n            new_doc.insert_pdf(doc, from_page=from_page, to_page=to_page)\n            output = Path(tmp_dir.name) / f\"pages_{from_page + 1}_to_{to_page + 1}.pdf\"\n            new_doc.save(str(output))\n            new_doc.close()\n            paths.append(output)\n\n    doc.close()\n    return paths, tmp_dir\n</code></pre>"},{"location":"reference/parsing/pdf_utils/#langroid.parsing.pdf_utils.pdf_split_pages--use-paths","title":"Use paths...","text":"<p>tmp_dir.cleanup()  # Clean up temp files when done</p>"},{"location":"reference/parsing/repo_loader/","title":"repo_loader","text":"<p>langroid/parsing/repo_loader.py </p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoaderConfig","title":"<code>RepoLoaderConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for RepoLoader.</p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader","title":"<code>RepoLoader(url, config=RepoLoaderConfig())</code>","text":"<p>Class for recursively getting all file content in a repo.</p> <pre><code>config: configuration for RepoLoader\n</code></pre> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    config: RepoLoaderConfig = RepoLoaderConfig(),\n):\n    \"\"\"\n    Args:\n        url: full github url of repo, or just \"owner/repo\"\n        config: configuration for RepoLoader\n    \"\"\"\n    self.url = url\n    self.config = config\n    self.clone_path: Optional[str] = None\n    self.log_file = \".logs/repo_loader/download_log.json\"\n    self.repo: Optional[\"Repository\"] = None  # Initialize repo as Optional\n\n    os.makedirs(os.path.dirname(self.log_file), exist_ok=True)\n    if not os.path.exists(self.log_file):\n        with open(self.log_file, \"w\") as f:\n            json.dump({\"junk\": \"ignore\"}, f)\n    with open(self.log_file, \"r\") as f:\n        log = json.load(f)\n    if self.url in log and os.path.exists(log[self.url]):\n        logger.info(f\"Repo Already downloaded in {log[self.url]}\")\n        self.clone_path = log[self.url]\n\n    # it's a core dependency, so we don't need to enclose in try/except\n    from github import Github  # Late import\n\n    load_dotenv()\n    # authenticated calls to github api have higher rate limit\n    token = os.getenv(\"GITHUB_ACCESS_TOKEN\")\n\n    if \"github.com\" in self.url:\n        repo_name = self.url.split(\"github.com/\")[1]\n    else:\n        repo_name = self.url\n\n    g = Github(token)\n    self.repo = self._get_repo_with_retry(g, repo_name)\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.get_issues","title":"<code>get_issues(k=100)</code>","text":"<p>Get up to k issues from the GitHub repo.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def get_issues(self, k: int | None = 100) -&gt; List[IssueData]:\n    \"\"\"Get up to k issues from the GitHub repo.\"\"\"\n    if self.repo is None:\n        logger.warning(\"No repo found. Ensure the URL is correct.\")\n        return []  # Return an empty list rather than raise an error in this case\n\n    if k is None:\n        issues = self.repo.get_issues(state=\"all\")\n    else:\n        issues = self.repo.get_issues(state=\"all\")[:k]\n    issue_data_list = []\n    for issue in issues:\n        issue_data = IssueData(\n            state=issue.state,\n            year=issue.created_at.year,\n            month=issue.created_at.month,\n            day=issue.created_at.day,\n            assignee=issue.assignee.login if issue.assignee else None,\n            size=get_issue_size(issue.labels),\n            text=issue.body or \"No issue description body.\",\n        )\n        issue_data_list.append(issue_data)\n\n    return issue_data_list\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.clone","title":"<code>clone(path=None)</code>","text":"<p>Clone a GitHub repository to a local directory specified by <code>path</code>, if it has not already been cloned.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local directory where the repository should be cloned. If not specified, a temporary directory will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The path to the local directory where the repository was cloned.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def clone(self, path: Optional[str] = None) -&gt; Optional[str]:\n    \"\"\"\n    Clone a GitHub repository to a local directory specified by `path`,\n    if it has not already been cloned.\n\n    Args:\n        path (str): The local directory where the repository should be cloned.\n            If not specified, a temporary directory will be created.\n\n    Returns:\n        str: The path to the local directory where the repository was cloned.\n    \"\"\"\n    with open(self.log_file, \"r\") as f:\n        log: Dict[str, str] = json.load(f)\n\n    if (\n        self.url in log\n        and os.path.exists(log[self.url])\n        and _has_files(log[self.url])\n    ):\n        logger.warning(f\"Repo Already downloaded in {log[self.url]}\")\n        self.clone_path = log[self.url]\n        return self.clone_path\n\n    self.clone_path = path\n    if path is None:\n        path = self.default_clone_path()\n        self.clone_path = path\n\n    try:\n        subprocess.run([\"git\", \"clone\", self.url, path], check=True)\n        log[self.url] = path\n        with open(self.log_file, \"w\") as f:\n            json.dump(log, f)\n        return self.clone_path\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Git clone failed: {e}\")\n    except Exception as e:\n        logger.error(f\"An error occurred while trying to clone the repository:{e}\")\n\n    return self.clone_path\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_tree_from_github","title":"<code>load_tree_from_github(depth, lines=0)</code>","text":"<p>Get a nested dictionary of GitHub repository file and directory names up to a certain depth, with file contents.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>The depth level.</p> required <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]:</p> <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>A dictionary containing file and directory names, with file contents.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_tree_from_github(\n    self, depth: int, lines: int = 0\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n    \"\"\"\n    Get a nested dictionary of GitHub repository file and directory names\n    up to a certain depth, with file contents.\n\n    Args:\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n\n    Returns:\n        Dict[str, Union[str, List[Dict]]]:\n        A dictionary containing file and directory names, with file contents.\n    \"\"\"\n    if self.repo is None:\n        logger.warning(\"No repo found. Ensure the URL is correct.\")\n        return {}  # Return an empty dict rather than raise an error in this case\n\n    root_contents = self.repo.get_contents(\"\")\n    if not isinstance(root_contents, list):\n        root_contents = [root_contents]\n    repo_structure = {\n        \"type\": \"dir\",\n        \"name\": \"\",\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": \"\",\n    }\n\n    # A queue of tuples (current_node, current_depth, parent_structure)\n    queue = deque([(root_contents, 0, repo_structure)])\n\n    while queue:\n        current_node, current_depth, parent_structure = queue.popleft()\n\n        for content in current_node:\n            if not self._is_allowed(content):\n                continue\n            if content.type == \"dir\" and current_depth &lt; depth:\n                # Create a new sub-dictionary for this directory\n                new_dir = {\n                    \"type\": \"dir\",\n                    \"name\": content.name,\n                    \"dirs\": [],\n                    \"files\": [],\n                    \"path\": content.path,\n                }\n                parent_structure[\"dirs\"].append(new_dir)\n                contents = self.repo.get_contents(content.path)\n                if not isinstance(contents, list):\n                    contents = [contents]\n                queue.append(\n                    (\n                        contents,\n                        current_depth + 1,\n                        new_dir,\n                    )\n                )\n            elif content.type == \"file\":\n                file_content = \"\\n\".join(\n                    _get_decoded_content(content).splitlines()[:lines]\n                )\n                file_dict = {\n                    \"type\": \"file\",\n                    \"name\": content.name,\n                    \"content\": file_content,\n                    \"path\": content.path,\n                }\n                parent_structure[\"files\"].append(file_dict)\n\n    return repo_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load","title":"<code>load(path=None, depth=3, lines=0)</code>","text":"<p>From a local folder <code>path</code> (if None, the repo clone path), get:   a nested dictionary (tree) of dicts, files and contents   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path; if none, use self.clone_path()</p> <code>None</code> <code>depth</code> <code>int</code> <p>The depth level.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents, and a list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load(\n    self,\n    path: Optional[str] = None,\n    depth: int = 3,\n    lines: int = 0,\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n    \"\"\"\n    From a local folder `path` (if None, the repo clone path), get:\n      a nested dictionary (tree) of dicts, files and contents\n      a list of Document objects for each file.\n\n    Args:\n        path (str): The local folder path; if none, use self.clone_path()\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n\n    Returns:\n        Tuple of (dict, List_of_Documents):\n            A dictionary containing file and directory names, with file\n            contents, and a list of Document objects for each file.\n    \"\"\"\n    if path is None:\n        if self.clone_path is None or not _has_files(self.clone_path):\n            self.clone()\n        path = self.clone_path\n    if path is None:\n        raise ValueError(\"Unable to clone repo\")\n    return self.load_from_folder(\n        path=path,\n        depth=depth,\n        lines=lines,\n        file_types=self.config.file_types,\n        exclude_dirs=self.config.exclude_dirs,\n        url=self.url,\n    )\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_from_folder","title":"<code>load_from_folder(path, depth=3, lines=0, file_types=None, exclude_dirs=None, url='')</code>  <code>staticmethod</code>","text":"<p>From a local folder <code>path</code> (required), get:   a nested dictionary (tree) of dicts, files and contents, restricting to     desired file_types and excluding undesired directories.   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path, required.</p> required <code>depth</code> <code>int</code> <p>The depth level. Optional, default 3.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.     Optional, default 0 (no lines =&gt; empty string).</p> <code>0</code> <code>file_types</code> <code>List[str]</code> <p>The file types to include.     Optional, default None (all).</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>The directories to exclude.     Optional, default None (no exclusions).</p> <code>None</code> <code>url</code> <code>str</code> <p>Optional url, to be stored in docs as metadata. Default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents. A list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef load_from_folder(\n    path: str,\n    depth: int = 3,\n    lines: int = 0,\n    file_types: Optional[List[str]] = None,\n    exclude_dirs: Optional[List[str]] = None,\n    url: str = \"\",\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n    \"\"\"\n    From a local folder `path` (required), get:\n      a nested dictionary (tree) of dicts, files and contents, restricting to\n        desired file_types and excluding undesired directories.\n      a list of Document objects for each file.\n\n    Args:\n        path (str): The local folder path, required.\n        depth (int): The depth level. Optional, default 3.\n        lines (int): The number of lines of file contents to include.\n                Optional, default 0 (no lines =&gt; empty string).\n        file_types (List[str]): The file types to include.\n                Optional, default None (all).\n        exclude_dirs (List[str]): The directories to exclude.\n                Optional, default None (no exclusions).\n        url (str): Optional url, to be stored in docs as metadata. Default \"\".\n\n    Returns:\n        Tuple of (dict, List_of_Documents):\n            A dictionary containing file and directory names, with file contents.\n            A list of Document objects for each file.\n    \"\"\"\n\n    folder_structure = {\n        \"type\": \"dir\",\n        \"name\": \"\",\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": \"\",\n    }\n    # A queue of tuples (current_path, current_depth, parent_structure)\n    queue = deque([(path, 0, folder_structure)])\n    docs = []\n    exclude_dirs = exclude_dirs or []\n    while queue:\n        current_path, current_depth, parent_structure = queue.popleft()\n\n        for item in os.listdir(current_path):\n            item_path = os.path.join(current_path, item)\n            relative_path = os.path.relpath(item_path, path)\n            if (os.path.isdir(item_path) and item in exclude_dirs) or (\n                os.path.isfile(item_path)\n                and file_types is not None\n                and RepoLoader._file_type(item) not in file_types\n            ):\n                continue\n\n            if os.path.isdir(item_path) and current_depth &lt; depth:\n                # Create a new sub-dictionary for this directory\n                new_dir = {\n                    \"type\": \"dir\",\n                    \"name\": item,\n                    \"dirs\": [],\n                    \"files\": [],\n                    \"path\": relative_path,\n                }\n                parent_structure[\"dirs\"].append(new_dir)\n                queue.append((item_path, current_depth + 1, new_dir))\n            elif os.path.isfile(item_path):\n                # Add the file to the current dictionary\n                with open(item_path, \"r\") as f:\n                    file_lines = list(itertools.islice(f, lines))\n                file_content = \"\\n\".join(line.strip() for line in file_lines)\n                if file_content == \"\":\n                    continue\n\n                file_dict = {\n                    \"type\": \"file\",\n                    \"name\": item,\n                    \"content\": file_content,\n                    \"path\": relative_path,\n                }\n                parent_structure[\"files\"].append(file_dict)\n                docs.append(\n                    Document(\n                        content=file_content,\n                        metadata=DocMetaData(\n                            repo=url,\n                            source=relative_path,\n                            url=url,\n                            filename=item,\n                            extension=RepoLoader._file_type(item),\n                            language=RepoLoader._file_type(item),\n                        ),\n                    )\n                )\n    return folder_structure, docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.get_documents","title":"<code>get_documents(path, parser=Parser(ParsingConfig()), file_types=None, exclude_dirs=None, depth=-1, lines=None, doc_type=None)</code>  <code>staticmethod</code>","text":"<p>Recursively get all files under a path as Document objects.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | bytes</code> <p>The path to the directory or file, or bytes content. The bytes option is meant to support the case where the content has already been read from a file in an upstream process (e.g. from an API or a database), and we want to avoid having to write it to a temporary file just to read it again. (which can be very slow for large files, especially in a docker container)</p> required <code>parser</code> <code>Parser</code> <p>Parser to use to parse files.</p> <code>Parser(ParsingConfig())</code> <code>file_types</code> <code>List[str]</code> <p>List of file extensions OR filenames OR file_path_names to  include. Defaults to None, which includes all files.</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>List of directories to exclude. Defaults to None, which includes all directories.</p> <code>None</code> <code>depth</code> <code>int</code> <p>Max depth of recursion. Defaults to -1, which includes all depths.</p> <code>-1</code> <code>lines</code> <code>int</code> <p>Number of lines to read from each file. Defaults to None, which reads all lines.</p> <code>None</code> <code>doc_type</code> <code>str | DocumentType | None</code> <p>The type of document to parse.</p> <code>None</code> <p>Returns:     List[Document]: List of Document objects representing files.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef get_documents(\n    path: str | bytes,\n    parser: Parser = Parser(ParsingConfig()),\n    file_types: Optional[List[str]] = None,\n    exclude_dirs: Optional[List[str]] = None,\n    depth: int = -1,\n    lines: Optional[int] = None,\n    doc_type: str | DocumentType | None = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Recursively get all files under a path as Document objects.\n\n    Args:\n        path (str|bytes): The path to the directory or file, or bytes content.\n            The bytes option is meant to support the case where the content\n            has already been read from a file in an upstream process\n            (e.g. from an API or a database), and we want to avoid having to\n            write it to a temporary file just to read it again.\n            (which can be very slow for large files,\n            especially in a docker container)\n        parser (Parser): Parser to use to parse files.\n        file_types (List[str], optional): List of file extensions OR\n            filenames OR file_path_names to  include.\n            Defaults to None, which includes all files.\n        exclude_dirs (List[str], optional): List of directories to exclude.\n            Defaults to None, which includes all directories.\n        depth (int, optional): Max depth of recursion. Defaults to -1,\n            which includes all depths.\n        lines (int, optional): Number of lines to read from each file.\n            Defaults to None, which reads all lines.\n        doc_type (str|DocumentType | None, optional): The type of document to parse.\n    Returns:\n        List[Document]: List of Document objects representing files.\n\n    \"\"\"\n    docs = []\n    file_paths = []\n    if isinstance(path, bytes):\n        file_paths.append(path)\n    else:\n        path_obj = Path(path).resolve()\n\n        if path_obj.is_file():\n            file_paths.append(str(path_obj))\n        else:\n            path_depth = len(path_obj.parts)\n            for root, dirs, files in os.walk(path):\n                # Exclude directories if needed\n                if exclude_dirs:\n                    dirs[:] = [d for d in dirs if d not in exclude_dirs]\n\n                current_depth = len(Path(root).resolve().parts) - path_depth\n                if depth == -1 or current_depth &lt;= depth:\n                    for file in files:\n                        file_path = str(Path(root) / file)\n                        if (\n                            file_types is None\n                            or RepoLoader._file_type(file_path) in file_types\n                            or os.path.basename(file_path) in file_types\n                            or file_path in file_types\n                        ):\n                            file_paths.append(file_path)\n\n    for file_path in file_paths:\n        docs.extend(\n            DocumentParser.chunks_from_path_or_bytes(\n                file_path,\n                parser,\n                doc_type=doc_type,\n                lines=lines,\n            )\n        )\n    return docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_docs_from_github","title":"<code>load_docs_from_github(k=None, depth=None, lines=None)</code>","text":"<p>Directly from GitHub, recursively get all files in a repo that have one of the extensions, possibly up to a max number of files, max depth, and max number of lines per file (if any of these are specified).</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>max number of files to load, or None for all files</p> <code>None</code> <code>depth</code> <code>int</code> <p>max depth to recurse, or None for infinite depth</p> <code>None</code> <code>lines</code> <code>int</code> <p>max number of lines to get, from a file, or None for all lines</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects, each has fields <code>content</code> and <code>metadata</code>,</p> <code>List[Document]</code> <p>and <code>metadata</code> has fields <code>url</code>, <code>filename</code>, <code>extension</code>, <code>language</code></p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_docs_from_github(\n    self,\n    k: Optional[int] = None,\n    depth: Optional[int] = None,\n    lines: Optional[int] = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Directly from GitHub, recursively get all files in a repo that have one of the\n    extensions, possibly up to a max number of files, max depth, and max number\n    of lines per file (if any of these are specified).\n\n    Args:\n        k (int): max number of files to load, or None for all files\n        depth (int): max depth to recurse, or None for infinite depth\n        lines (int): max number of lines to get, from a file, or None for all lines\n\n    Returns:\n        list of Document objects, each has fields `content` and `metadata`,\n        and `metadata` has fields `url`, `filename`, `extension`, `language`\n    \"\"\"\n    if self.repo is None:\n        logger.warning(\"No repo found. Ensure the URL is correct.\")\n        return []  # Return an empty list rather than raise an error\n\n    contents = self.repo.get_contents(\"\")\n    if not isinstance(contents, list):\n        contents = [contents]\n    stack = list(zip(contents, [0] * len(contents)))  # stack of (content, depth)\n    # recursively get all files in repo that have one of the extensions\n    docs = []\n    i = 0\n\n    while stack:\n        if k is not None and i == k:\n            break\n        file_content, d = stack.pop()\n        if not self._is_allowed(file_content):\n            continue\n        if file_content.type == \"dir\":\n            if depth is None or d &lt;= depth:\n                items = self.repo.get_contents(file_content.path)\n                if not isinstance(items, list):\n                    items = [items]\n                stack.extend(list(zip(items, [d + 1] * len(items))))\n        else:\n            if depth is None or d &lt;= depth:\n                # need to decode the file content, which is in bytes\n                contents = self.repo.get_contents(file_content.path)\n                if isinstance(contents, list):\n                    contents = contents[0]\n                text = _get_decoded_content(contents)\n                if lines is not None:\n                    text = \"\\n\".join(text.split(\"\\n\")[:lines])\n                i += 1\n\n                # Note `source` is important, it may be used to cite\n                # evidence for an answer.\n                # See  URLLoader\n                # TODO we should use Pydantic to enforce/standardize this\n\n                docs.append(\n                    Document(\n                        content=text,\n                        metadata=DocMetaData(\n                            repo=self.url,\n                            source=file_content.html_url,\n                            url=file_content.html_url,\n                            filename=file_content.name,\n                            extension=self._file_type(file_content.name),\n                            language=self._file_type(file_content.name),\n                        ),\n                    )\n                )\n    return docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.select","title":"<code>select(structure, includes, excludes=[])</code>  <code>staticmethod</code>","text":"<p>Filter a structure dictionary for certain directories and files.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>includes</code> <code>List[str]</code> <p>A list of desired directories and files. For files, either full file names or \"file type\" can be specified. E.g.  \"toml\" will include all files with the \".toml\" extension, or \"Makefile\" will include all files named \"Makefile\".</p> required <code>excludes</code> <code>List[str]</code> <p>A list of directories and files to exclude. Similar to <code>includes</code>, full file/dir names or \"file type\" can be specified. Optional, defaults to empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef select(\n    structure: Dict[str, Union[str, List[Dict[str, Any]]]],\n    includes: List[str],\n    excludes: List[str] = [],\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n    \"\"\"\n    Filter a structure dictionary for certain directories and files.\n\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        includes (List[str]): A list of desired directories and files.\n            For files, either full file names or \"file type\" can be specified.\n            E.g.  \"toml\" will include all files with the \".toml\" extension,\n            or \"Makefile\" will include all files named \"Makefile\".\n        excludes (List[str]): A list of directories and files to exclude.\n            Similar to `includes`, full file/dir names or \"file type\" can be\n            specified. Optional, defaults to empty list.\n\n\n    Returns:\n        Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.\n    \"\"\"\n    filtered_structure = {\n        \"type\": structure[\"type\"],\n        \"name\": structure[\"name\"],\n        \"dirs\": [],\n        \"files\": [],\n        \"path\": structure[\"path\"],\n    }\n\n    for dir in structure[\"dirs\"]:\n        if (\n            dir[\"name\"] in includes\n            or RepoLoader._file_type(dir[\"name\"]) in includes\n        ) and (\n            dir[\"name\"] not in excludes\n            and RepoLoader._file_type(dir[\"name\"]) not in excludes\n        ):\n            # If the directory is in the select list, include the whole subtree\n            filtered_structure[\"dirs\"].append(dir)\n        else:\n            # Otherwise, filter the directory's contents\n            filtered_dir = RepoLoader.select(dir, includes)\n            if (\n                filtered_dir[\"dirs\"] or filtered_dir[\"files\"]\n            ):  # only add if not empty\n                filtered_structure[\"dirs\"].append(filtered_dir)\n\n    for file in structure[\"files\"]:\n        if (\n            file[\"name\"] in includes\n            or RepoLoader._file_type(file[\"name\"]) in includes\n        ) and (\n            file[\"name\"] not in excludes\n            and RepoLoader._file_type(file[\"name\"]) not in excludes\n        ):\n            filtered_structure[\"files\"].append(file)\n\n    return filtered_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.ls","title":"<code>ls(structure, depth=0)</code>  <code>staticmethod</code>","text":"<p>Get a list of names of files or directories up to a certain depth from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of names of files or directories.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef ls(structure: Dict[str, Union[str, List[Dict]]], depth: int = 0) -&gt; List[str]:\n    \"\"\"\n    Get a list of names of files or directories up to a certain depth from a\n    structure dictionary.\n\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        depth (int, optional): The depth level. Defaults to 0.\n\n    Returns:\n        List[str]: A list of names of files or directories.\n    \"\"\"\n    names = []\n\n    # A queue of tuples (current_structure, current_depth)\n    queue = deque([(structure, 0)])\n\n    while queue:\n        current_structure, current_depth = queue.popleft()\n\n        if current_depth &lt;= depth:\n            names.append(current_structure[\"name\"])\n\n            for dir in current_structure[\"dirs\"]:\n                queue.append((dir, current_depth + 1))\n\n            for file in current_structure[\"files\"]:\n                # add file names only if depth is less than the limit\n                if current_depth &lt; depth:\n                    names.append(file[\"name\"])\n    names = [n for n in names if n not in [\"\", None]]\n    return names\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.list_files","title":"<code>list_files(dir, depth=1, include_types=[], exclude_types=[])</code>  <code>staticmethod</code>","text":"<p>Recursively list all files in a directory, up to a certain depth.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>The directory path, relative to root.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 1.</p> <code>1</code> <code>include_types</code> <code>List[str]</code> <p>A list of file types to include. Defaults to empty list.</p> <code>[]</code> <code>exclude_types</code> <code>List[str]</code> <p>A list of file types to exclude. Defaults to empty list.</p> <code>[]</code> <p>Returns:     List[str]: A list of file names.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef list_files(\n    dir: str,\n    depth: int = 1,\n    include_types: List[str] = [],\n    exclude_types: List[str] = [],\n) -&gt; List[str]:\n    \"\"\"\n    Recursively list all files in a directory, up to a certain depth.\n\n    Args:\n        dir (str): The directory path, relative to root.\n        depth (int, optional): The depth level. Defaults to 1.\n        include_types (List[str], optional): A list of file types to include.\n            Defaults to empty list.\n        exclude_types (List[str], optional): A list of file types to exclude.\n            Defaults to empty list.\n    Returns:\n        List[str]: A list of file names.\n    \"\"\"\n    depth = depth if depth &gt;= 0 else 200\n    output = []\n\n    for root, dirs, files in os.walk(dir):\n        if root.count(os.sep) - dir.count(os.sep) &lt; depth:\n            level = root.count(os.sep) - dir.count(os.sep)\n            sub_indent = \" \" * 4 * (level + 1)\n            for d in dirs:\n                output.append(\"{}{}/\".format(sub_indent, d))\n            for f in files:\n                if include_types and RepoLoader._file_type(f) not in include_types:\n                    continue\n                if exclude_types and RepoLoader._file_type(f) in exclude_types:\n                    continue\n                output.append(\"{}{}\".format(sub_indent, f))\n    return output\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.show_file_contents","title":"<code>show_file_contents(tree)</code>  <code>staticmethod</code>","text":"<p>Print the contents of all files from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef show_file_contents(tree: Dict[str, Union[str, List[Dict[str, Any]]]]) -&gt; str:\n    \"\"\"\n    Print the contents of all files from a structure dictionary.\n\n    Args:\n        tree (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n    \"\"\"\n    contents = \"\"\n    for dir in tree[\"dirs\"]:\n        contents += RepoLoader.show_file_contents(dir)\n    for file in tree[\"files\"]:\n        path = file[\"path\"]\n        contents += f\"\"\"\n        {path}:\n        --------------------\n        {file[\"content\"]}\n\n        \"\"\"\n\n    return contents\n</code></pre>"},{"location":"reference/parsing/routing/","title":"routing","text":"<p>langroid/parsing/routing.py </p>"},{"location":"reference/parsing/routing/#langroid.parsing.routing.parse_addressed_message","title":"<code>parse_addressed_message(content, addressing='@')</code>","text":"<p>In a message-string containing possibly multiple @ occurrences, find the last addressee and extract their name, and the message content following it. <p>E.g. \"thank you @bob, now I will ask @alice again. @alice, where is the mirror?\" =&gt; (\"alice\", \"where is the mirror?\")</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The message content.</p> required <code>addressing</code> <code>str</code> <p>The addressing character. Defaults to \"@\".</p> <code>'@'</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Tuple[Optional[str], str]:</p> <code>str</code> <p>A tuple containing the last addressee and the subsequent message content.</p> Source code in <code>langroid/parsing/routing.py</code> <pre><code>def parse_addressed_message(\n    content: str, addressing: str = \"@\"\n) -&gt; Tuple[Optional[str], str]:\n    \"\"\"In a message-string containing possibly multiple @&lt;recipient&gt; occurrences,\n    find the last addressee and extract their name,\n    and the message content following it.\n\n    E.g. \"thank you @bob, now I will ask @alice again. @alice, where is the mirror?\" =&gt;\n    (\"alice\", \"where is the mirror?\")\n\n    Args:\n        content (str): The message content.\n        addressing (str, optional): The addressing character. Defaults to \"@\".\n\n    Returns:\n        Tuple[Optional[str], str]:\n        A tuple containing the last addressee and the subsequent message content.\n    \"\"\"\n    # Regex to find all occurrences of the pattern\n    pattern = re.compile(rf\"{re.escape(addressing)}(\\w+)[^\\w]\")\n    matches = list(pattern.finditer(content))\n\n    if not matches:\n        return None, content  # No addressee found, return None and original content\n\n    # Get the last match\n    last_match = matches[-1]\n    last_addressee = last_match.group(1)\n    # Extract content after the last addressee\n    content_after = content[last_match.end() :].strip()\n\n    return last_addressee, content_after\n</code></pre>"},{"location":"reference/parsing/search/","title":"search","text":"<p>langroid/parsing/search.py </p> <p>Utils to search for close matches in (a list of) strings. Useful for retrieval of docs/chunks relevant to a query, in the context of Retrieval-Augmented Generation (RAG), and SQLChat (e.g., to pull relevant parts of a large schema). See tests for examples: tests/main/test_string_search.py</p>"},{"location":"reference/parsing/search/#langroid.parsing.search.find_fuzzy_matches_in_docs","title":"<code>find_fuzzy_matches_in_docs(query, docs, docs_clean, k, words_before=None, words_after=None)</code>","text":"<p>Find approximate matches of the query in the docs and return surrounding characters.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search string.</p> required <code>docs</code> <code>List[Document]</code> <p>List of Document objects to search through.</p> required <code>docs_clean</code> <code>List[Document]</code> <p>List of Document objects with cleaned content.</p> required <code>k</code> <code>int</code> <p>Number of best matches to return.</p> required <code>words_before</code> <code>int | None</code> <p>Number of words to include before each match. Default None =&gt; return max</p> <code>None</code> <code>words_after</code> <code>int | None</code> <p>Number of words to include after each match. Default None =&gt; return max</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def find_fuzzy_matches_in_docs(\n    query: str,\n    docs: List[Document],\n    docs_clean: List[Document],\n    k: int,\n    words_before: int | None = None,\n    words_after: int | None = None,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Find approximate matches of the query in the docs and return surrounding\n    characters.\n\n    Args:\n        query (str): The search string.\n        docs (List[Document]): List of Document objects to search through.\n        docs_clean (List[Document]): List of Document objects with cleaned content.\n        k (int): Number of best matches to return.\n        words_before (int|None): Number of words to include before each match.\n            Default None =&gt; return max\n        words_after (int|None): Number of words to include after each match.\n            Default None =&gt; return max\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n    \"\"\"\n    if len(docs) == 0:\n        return []\n    best_matches = process.extract(\n        query,\n        [d.content for d in docs_clean],\n        limit=k,\n        scorer=fuzz.partial_ratio,\n    )\n\n    real_matches = [(m, score) for m, score in best_matches if score &gt; 50]\n    # find the original docs that corresponding to the matches\n    orig_doc_matches = []\n    for i, (m, s) in enumerate(real_matches):\n        for j, doc_clean in enumerate(docs_clean):\n            if m in doc_clean.content:\n                orig_doc_matches.append((docs[j], s))\n                break\n    if words_after is None and words_before is None:\n        return orig_doc_matches\n    if len(orig_doc_matches) == 0:\n        return []\n    if set(orig_doc_matches[0][0].model_fields) != {\"content\", \"metadata\"}:\n        # If there are fields beyond just content and metadata,\n        # we do NOT want to create new document objects with content fields\n        # based on words_before and words_after, since we don't know how to\n        # set those other fields.\n        return orig_doc_matches\n\n    contextual_matches = []\n    for match, score in orig_doc_matches:\n        choice_text = match.content\n        contexts = []\n        while choice_text != \"\":\n            context, start_pos, end_pos = get_context(\n                query, choice_text, words_before, words_after\n            )\n            if context == \"\" or end_pos == 0:\n                break\n            contexts.append(context)\n            words = choice_text.split()\n            end_pos = min(end_pos, len(words))\n            choice_text = \" \".join(words[end_pos:])\n        if len(contexts) &gt; 0:\n            contextual_matches.append(\n                (\n                    Document(\n                        content=\" ... \".join(contexts),\n                        metadata=match.metadata,\n                    ),\n                    score,\n                )\n            )\n\n    return contextual_matches\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.preprocess_text","title":"<code>preprocess_text(text)</code>","text":"<p>Preprocesses the given text by: 1. Lowercasing all words. 2. Tokenizing (splitting the text into words). 3. Removing punctuation. 4. Removing stopwords. 5. Lemmatizing words.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The preprocessed text.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def preprocess_text(text: str) -&gt; str:\n    \"\"\"\n    Preprocesses the given text by:\n    1. Lowercasing all words.\n    2. Tokenizing (splitting the text into words).\n    3. Removing punctuation.\n    4. Removing stopwords.\n    5. Lemmatizing words.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        str: The preprocessed text.\n    \"\"\"\n    # Ensure the NLTK resources are available\n    for resource in [\"tokenizers/punkt\", \"corpora/wordnet\", \"corpora/stopwords\"]:\n        download_nltk_resource(resource)\n    from nltk.corpus import stopwords\n    from nltk.stem import WordNetLemmatizer\n    from nltk.tokenize import RegexpTokenizer\n\n    # Lowercase the text\n    text = text.lower()\n\n    # Tokenize the text and remove punctuation\n    tokenizer = RegexpTokenizer(r\"\\w+\")\n    tokens = tokenizer.tokenize(text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    tokens = [t for t in tokens if t not in stop_words]\n\n    # Lemmatize words\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n\n    # Join the words back into a string\n    text = \" \".join(tokens)\n\n    return text\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.find_closest_matches_with_bm25","title":"<code>find_closest_matches_with_bm25(docs, docs_clean, query, k=5)</code>","text":"<p>Finds the k closest approximate matches using the BM25 algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Documents to search through.</p> required <code>docs_clean</code> <code>List[Document]</code> <p>List of cleaned Documents</p> required <code>query</code> <code>str</code> <p>The search query.</p> required <code>k</code> <code>int</code> <p>Number of matches to retrieve. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def find_closest_matches_with_bm25(\n    docs: List[Document],\n    docs_clean: List[Document],\n    query: str,\n    k: int = 5,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Finds the k closest approximate matches using the BM25 algorithm.\n\n    Args:\n        docs (List[Document]): List of Documents to search through.\n        docs_clean (List[Document]): List of cleaned Documents\n        query (str): The search query.\n        k (int, optional): Number of matches to retrieve. Defaults to 5.\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n    \"\"\"\n    if len(docs) == 0:\n        return []\n    texts = [doc.content for doc in docs_clean]\n    query = preprocess_text(query)\n\n    text_words = [text.split() for text in texts]\n\n    bm25 = BM25Okapi(text_words)\n    query_words = query.split()\n    doc_scores = bm25.get_scores(query_words)\n\n    # Get indices of top k scores\n    top_indices = sorted(range(len(doc_scores)), key=lambda i: -doc_scores[i])[:k]\n\n    # return the original docs, based on the scores from cleaned docs\n    return [(docs[i], doc_scores[i]) for i in top_indices]\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.get_context","title":"<code>get_context(query, text, words_before=100, words_after=100)</code>","text":"<p>Returns a portion of text containing the best approximate match of the query, including b words before and a words after the match.</p> <p>Args: query (str): The string to search for. text (str): The body of text in which to search. b (int): The number of words before the query to return. a (int): The number of words after the query to return.</p> <p>str: A string containing b words before, the match, and a words after     the best approximate match position of the query in the text.     The text is extracted from the original <code>text</code>, preserving formatting,     whitespace, etc, so it does not disturb any downstream processing.     If no match is found, returns empty string. int: The start position of the match in the text. int: The end position of the match in the text.</p> <p>Example:</p> <p>get_context(\"apple\", \"The quick brown fox jumps over the apple.\", 3, 2)</p>"},{"location":"reference/parsing/search/#langroid.parsing.search.get_context--fox-jumps-over-the-apple","title":"'fox jumps over the apple.'","text":"Source code in <code>langroid/parsing/search.py</code> <pre><code>def get_context(\n    query: str,\n    text: str,\n    words_before: int | None = 100,\n    words_after: int | None = 100,\n) -&gt; Tuple[str, int, int]:\n    \"\"\"\n    Returns a portion of text containing the best approximate match of the query,\n    including b words before and a words after the match.\n\n    Args:\n    query (str): The string to search for.\n    text (str): The body of text in which to search.\n    b (int): The number of words before the query to return.\n    a (int): The number of words after the query to return.\n\n    Returns:\n    str: A string containing b words before, the match, and a words after\n        the best approximate match position of the query in the text.\n        The text is extracted from the original `text`, preserving formatting,\n        whitespace, etc, so it does not disturb any downstream processing.\n        If no match is found, returns empty string.\n    int: The start position of the match in the text.\n    int: The end position of the match in the text.\n\n    Example:\n    &gt;&gt;&gt; get_context(\"apple\", \"The quick brown fox jumps over the apple.\", 3, 2)\n    # 'fox jumps over the apple.'\n    \"\"\"\n\n    # If no word limits specified, return full text\n    if words_after is None and words_before is None:\n        # return entire text since we're not asked to return a bounded context\n        return text, 0, 0\n\n    # make sure there is a good enough match to the query\n    if fuzz.partial_ratio(query, text) &lt; 40:\n        return \"\", 0, 0\n\n    # Find best matching position of query in text\n    sequence_matcher = difflib.SequenceMatcher(None, text, query)\n    match = sequence_matcher.find_longest_match(0, len(text), 0, len(query))\n\n    if match.size == 0:\n        return \"\", 0, 0\n\n    # Count words before match point\n    segments = text.split()\n    n_segs = len(segments)\n    start_segment_pos = len(text[: match.a].split())\n\n    # Calculate word window boundaries\n    words_before = words_before or n_segs\n    words_after = words_after or n_segs\n    start_pos = max(0, start_segment_pos - words_before)\n    end_pos = min(len(segments), start_segment_pos + words_after + len(query.split()))\n\n    # Find character positions where words start\n    word_positions = [m.start() for m in re.finditer(r\"\\S+\", text)]\n\n    # Convert word positions to character positions\n    start_char = word_positions[start_pos] if start_pos &lt; len(word_positions) else 0\n    end_char = word_positions[min(end_pos, len(word_positions) - 1)] + len(\n        text.split()[min(end_pos - 1, len(word_positions) - 1)]\n    )\n\n    # return exact substring with original formatting\n    return text[start_char:end_char], start_pos, end_pos\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.eliminate_near_duplicates","title":"<code>eliminate_near_duplicates(passages, threshold=0.8)</code>","text":"<p>Eliminate near duplicate text passages from a given list using MinHash and LSH. TODO: this has not been tested and the datasketch lib is not a dependency. Args:     passages (List[str]): A list of text passages.     threshold (float, optional): Jaccard similarity threshold to consider two                                  passages as near-duplicates. Default is 0.8.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of passages after eliminating near duplicates.</p> Example <p>passages = [\"Hello world\", \"Hello, world!\", \"Hi there\", \"Hello world!\"] print(eliminate_near_duplicates(passages))</p> Source code in <code>langroid/parsing/search.py</code> <pre><code>def eliminate_near_duplicates(passages: List[str], threshold: float = 0.8) -&gt; List[str]:\n    \"\"\"\n    Eliminate near duplicate text passages from a given list using MinHash and LSH.\n    TODO: this has not been tested and the datasketch lib is not a dependency.\n    Args:\n        passages (List[str]): A list of text passages.\n        threshold (float, optional): Jaccard similarity threshold to consider two\n                                     passages as near-duplicates. Default is 0.8.\n\n    Returns:\n        List[str]: A list of passages after eliminating near duplicates.\n\n    Example:\n        passages = [\"Hello world\", \"Hello, world!\", \"Hi there\", \"Hello world!\"]\n        print(eliminate_near_duplicates(passages))\n        # ['Hello world', 'Hi there']\n    \"\"\"\n\n    from datasketch import MinHash, MinHashLSH\n\n    # Create LSH index\n    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n\n    # Create MinHash objects for each passage and insert to LSH\n    minhashes = {}\n    for idx, passage in enumerate(passages):\n        m = MinHash(num_perm=128)\n        for word in passage.split():\n            m.update(word.encode(\"utf-8\"))\n        lsh.insert(idx, m)\n        minhashes[idx] = m\n\n    unique_idxs = set()\n    for idx in minhashes.keys():\n        # Query for similar passages (including itself)\n        result = lsh.query(minhashes[idx])\n\n        # If only the passage itself is returned, it's unique\n        if len(result) == 1 and idx in result:\n            unique_idxs.add(idx)\n\n    return [passages[idx] for idx in unique_idxs]\n</code></pre>"},{"location":"reference/parsing/search/#langroid.parsing.search.eliminate_near_duplicates--hello-world-hi-there","title":"['Hello world', 'Hi there']","text":""},{"location":"reference/parsing/spider/","title":"spider","text":"<p>langroid/parsing/spider.py </p>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.DomainSpecificSpider","title":"<code>DomainSpecificSpider(start_url, k=20, *args, **kwargs)</code>","text":"<p>               Bases: <code>CrawlSpider</code></p> <p>Parameters:</p> Name Type Description Default <code>start_url</code> <code>str</code> <p>The starting URL.</p> required <code>k</code> <code>int</code> <p>The max desired final URLs. Defaults to 20.</p> <code>20</code> Source code in <code>langroid/parsing/spider.py</code> <pre><code>def __init__(self, start_url: str, k: int = 20, *args, **kwargs):  # type: ignore\n    \"\"\"Initialize the spider with start_url and k.\n\n    Args:\n        start_url (str): The starting URL.\n        k (int, optional): The max desired final URLs. Defaults to 20.\n    \"\"\"\n    super(DomainSpecificSpider, self).__init__(*args, **kwargs)\n    self.start_urls = [start_url]\n    self.allowed_domains = [urlparse(start_url).netloc]\n    self.k = k\n    self.visited_urls: Set[str] = set()\n</code></pre>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.DomainSpecificSpider.parse_item","title":"<code>parse_item(response)</code>","text":"<p>Extracts URLs that are within the same domain.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>TextResponse</code> <p>The scrapy response object.</p> required Source code in <code>langroid/parsing/spider.py</code> <pre><code>def parse_item(self, response: TextResponse):  # type: ignore\n    \"\"\"Extracts URLs that are within the same domain.\n\n    Args:\n        response: The scrapy response object.\n    \"\"\"\n    for link in LxmlLinkExtractor(allow_domains=self.allowed_domains).extract_links(\n        response\n    ):\n        if len(self.visited_urls) &lt; self.k:\n            self.visited_urls.add(link.url)\n            yield {\"url\": link.url}\n</code></pre>"},{"location":"reference/parsing/spider/#langroid.parsing.spider.scrapy_fetch_urls","title":"<code>scrapy_fetch_urls(url, k=20)</code>","text":"<p>Fetches up to k URLs reachable from the input URL using Scrapy.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The starting URL.</p> required <code>k</code> <code>int</code> <p>The max desired final URLs. Defaults to 20.</p> <code>20</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of URLs within the same domain as the input URL.</p> Source code in <code>langroid/parsing/spider.py</code> <pre><code>@no_type_check\ndef scrapy_fetch_urls(url: str, k: int = 20) -&gt; List[str]:\n    \"\"\"Fetches up to k URLs reachable from the input URL using Scrapy.\n\n    Args:\n        url (str): The starting URL.\n        k (int, optional): The max desired final URLs. Defaults to 20.\n\n    Returns:\n        List[str]: List of URLs within the same domain as the input URL.\n    \"\"\"\n    urls = []\n\n    def _collect_urls(spider):\n        \"\"\"Handler for the spider_closed signal. Collects the visited URLs.\"\"\"\n        nonlocal urls\n        urls.extend(list(spider.visited_urls))\n\n    # Connect the spider_closed signal with our handler\n    dispatcher.connect(_collect_urls, signal=signals.spider_closed)\n\n    runner = CrawlerRunner(\n        {\n            \"USER_AGENT\": \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n        }\n    )\n\n    d = runner.crawl(DomainSpecificSpider, start_url=url, k=k)\n\n    # Block until crawling is done and then stop the reactor\n    crawl_deferred = defer.Deferred()\n\n    def _crawl_done(_):\n        reactor.stop()\n        crawl_deferred.callback(urls)\n\n    d.addBoth(_crawl_done)\n\n    # Start the reactor, it will stop once the crawl is done\n    reactor.run(installSignalHandlers=0)\n\n    # This will block until the deferred gets a result\n    return crawl_deferred.result\n</code></pre>"},{"location":"reference/parsing/table_loader/","title":"table_loader","text":"<p>langroid/parsing/table_loader.py </p>"},{"location":"reference/parsing/table_loader/#langroid.parsing.table_loader.read_tabular_data","title":"<code>read_tabular_data(path_or_url, sep=None)</code>","text":"<p>Reads tabular data from a file or URL and returns a pandas DataFrame. The separator is auto-detected if not specified.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_url</code> <code>str</code> <p>Path or URL to the file to be read.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Data from file or URL as a pandas DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data cannot be read or is misformatted.</p> Source code in <code>langroid/parsing/table_loader.py</code> <pre><code>def read_tabular_data(path_or_url: str, sep: None | str = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads tabular data from a file or URL and returns a pandas DataFrame.\n    The separator is auto-detected if not specified.\n\n    Args:\n        path_or_url (str): Path or URL to the file to be read.\n\n    Returns:\n        pd.DataFrame: Data from file or URL as a pandas DataFrame.\n\n    Raises:\n        ValueError: If the data cannot be read or is misformatted.\n    \"\"\"\n    try:\n        if sep is None:\n            # Read the first few lines to guess the separator\n            with pd.io.common.get_handle(path_or_url, \"r\") as file_handler:\n                first_lines = \"\".join(file_handler.handle.readlines(5))\n                sep = Sniffer().sniff(first_lines).delimiter\n                # If it's a local file, reset to the beginning\n                if hasattr(file_handler.handle, \"seek\"):\n                    file_handler.handle.seek(0)\n\n        # Read the data\n\n        # get non-blank column names\n        with pd.io.common.get_handle(path_or_url, \"r\") as f:\n            header_line = f.handle.readline().strip()\n            valid_cols = [col for col in header_line.split(sep) if col]\n            valid_cols = [c.replace('\"', \"\").replace(\"'\", \"\") for c in valid_cols]\n            if hasattr(f.handle, \"seek\"):\n                f.handle.seek(0)\n\n        # use only those columns\n        data = pd.read_csv(path_or_url, sep=sep, usecols=valid_cols)\n        data.columns = data.columns.str.strip()  # e.g. \"  column 1  \" -&gt; \"column 1\"\n\n        return data\n\n    except Exception as e:\n        raise ValueError(\n            \"Unable to read data. \"\n            \"Please ensure it is correctly formatted. Error: \" + str(e)\n        )\n</code></pre>"},{"location":"reference/parsing/table_loader/#langroid.parsing.table_loader.describe_dataframe","title":"<code>describe_dataframe(df, filter_fields=[], n_vals=10)</code>","text":"<p>Generates a description of the columns in the dataframe, along with a listing of up to <code>n_vals</code> unique values for each column. Intended to be used to insert into an LLM context so it can generate appropriate queries or filters on the df.</p> <p>Args: df (pd.DataFrame): The dataframe to describe. filter_fields (list): A list of fields that can be used for filtering.     When non-empty, the values-list will be restricted to these. n_vals (int): How many unique values to show for each column.</p> <p>Returns: str: A description of the dataframe.</p> Source code in <code>langroid/parsing/table_loader.py</code> <pre><code>def describe_dataframe(\n    df: pd.DataFrame, filter_fields: List[str] = [], n_vals: int = 10\n) -&gt; str:\n    \"\"\"\n    Generates a description of the columns in the dataframe,\n    along with a listing of up to `n_vals` unique values for each column.\n    Intended to be used to insert into an LLM context so it can generate\n    appropriate queries or filters on the df.\n\n    Args:\n    df (pd.DataFrame): The dataframe to describe.\n    filter_fields (list): A list of fields that can be used for filtering.\n        When non-empty, the values-list will be restricted to these.\n    n_vals (int): How many unique values to show for each column.\n\n    Returns:\n    str: A description of the dataframe.\n    \"\"\"\n    description = []\n    for column in df.columns.to_list():\n        unique_values = df[column].dropna().unique()\n        unique_count = len(unique_values)\n        if column not in filter_fields:\n            values_desc = f\"{unique_count} unique values\"\n        else:\n            if unique_count &gt; n_vals:\n                displayed_values = unique_values[:n_vals]\n                more_count = unique_count - n_vals\n                values_desc = f\" Values - {displayed_values}, ... {more_count} more\"\n            else:\n                values_desc = f\" Values - {unique_values}\"\n        col_type = \"string\" if df[column].dtype == \"object\" else df[column].dtype\n        col_desc = f\"* {column} ({col_type}); {values_desc}\"\n        description.append(col_desc)\n\n    all_cols = \"\\n\".join(description)\n\n    return f\"\"\"\n        Name of each field, its type and unique values (up to {n_vals}):\n        {all_cols}\n        \"\"\"\n</code></pre>"},{"location":"reference/parsing/url_loader/","title":"url_loader","text":"<p>langroid/parsing/url_loader.py </p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.BaseCrawlerConfig","title":"<code>BaseCrawlerConfig</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Base configuration for web crawlers.</p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.TrafilaturaConfig","title":"<code>TrafilaturaConfig</code>","text":"<p>               Bases: <code>BaseCrawlerConfig</code></p> <p>Configuration for Trafilatura crawler.</p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.FirecrawlConfig","title":"<code>FirecrawlConfig</code>","text":"<p>               Bases: <code>BaseCrawlerConfig</code></p> <p>Configuration for Firecrawl crawler.</p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.Crawl4aiConfig","title":"<code>Crawl4aiConfig</code>","text":"<p>               Bases: <code>BaseCrawlerConfig</code></p> <p>Configuration for the Crawl4aiCrawler.</p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.BaseCrawler","title":"<code>BaseCrawler(config)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for web crawlers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BaseCrawlerConfig</code> <p>Configuration for the crawler</p> required Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(self, config: BaseCrawlerConfig):\n    \"\"\"Initialize the base crawler.\n\n    Args:\n        config: Configuration for the crawler\n    \"\"\"\n    self.parser = config.parser if self.needs_parser else None\n    self.config: BaseCrawlerConfig = config\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.BaseCrawler.needs_parser","title":"<code>needs_parser</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Indicates whether the crawler requires a parser.</p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.CrawlerFactory","title":"<code>CrawlerFactory</code>","text":"<p>Factory for creating web crawlers.</p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.CrawlerFactory.create_crawler","title":"<code>create_crawler(config)</code>  <code>staticmethod</code>","text":"<p>Create a crawler instance based on configuration type.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BaseCrawlerConfig</code> <p>Configuration for the crawler</p> required <p>Returns:</p> Type Description <code>BaseCrawler</code> <p>A BaseCrawler instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If config type is not supported</p> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>@staticmethod\ndef create_crawler(config: BaseCrawlerConfig) -&gt; BaseCrawler:\n    \"\"\"Create a crawler instance based on configuration type.\n\n    Args:\n        config: Configuration for the crawler\n\n    Returns:\n        A BaseCrawler instance\n\n    Raises:\n        ValueError: If config type is not supported\n    \"\"\"\n    if isinstance(config, TrafilaturaConfig):\n        return TrafilaturaCrawler(config)\n    elif isinstance(config, FirecrawlConfig):\n        return FirecrawlCrawler(config)\n    elif isinstance(config, ExaCrawlerConfig):\n        return ExaCrawler(config)\n    elif isinstance(config, Crawl4aiConfig):\n        return Crawl4aiCrawler(config)\n    else:\n        raise ValueError(f\"Unsupported crawler configuration type: {type(config)}\")\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.TrafilaturaCrawler","title":"<code>TrafilaturaCrawler(config)</code>","text":"<p>               Bases: <code>BaseCrawler</code></p> <p>Crawler implementation using Trafilatura.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>TrafilaturaConfig</code> <p>Configuration for the crawler</p> required Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(self, config: TrafilaturaConfig):\n    \"\"\"Initialize the Trafilatura crawler.\n\n    Args:\n        config: Configuration for the crawler\n    \"\"\"\n    super().__init__(config)\n    self.config: TrafilaturaConfig = config\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.FirecrawlCrawler","title":"<code>FirecrawlCrawler(config)</code>","text":"<p>               Bases: <code>BaseCrawler</code></p> <p>Crawler implementation using Firecrawl.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>FirecrawlConfig</code> <p>Configuration for the crawler</p> required Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(self, config: FirecrawlConfig) -&gt; None:\n    \"\"\"Initialize the Firecrawl crawler.\n\n    Args:\n        config: Configuration for the crawler\n    \"\"\"\n    super().__init__(config)\n    self.config: FirecrawlConfig = config\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.ExaCrawler","title":"<code>ExaCrawler(config)</code>","text":"<p>               Bases: <code>BaseCrawler</code></p> <p>Crawler implementation using Exa API.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ExaCrawlerConfig</code> <p>Configuration for the crawler</p> required Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(self, config: ExaCrawlerConfig) -&gt; None:\n    \"\"\"Initialize the Exa crawler.\n\n    Args:\n        config: Configuration for the crawler\n    \"\"\"\n    super().__init__(config)\n    self.config: ExaCrawlerConfig = config\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.ExaCrawler.crawl","title":"<code>crawl(urls)</code>","text":"<p>Crawl the given URLs using Exa SDK.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>List[str]</code> <p>List of URLs to crawl</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Documents with content extracted from the URLs</p> <p>Raises:</p> Type Description <code>LangroidImportError</code> <p>If the exa package is not installed</p> <code>ValueError</code> <p>If the Exa API key is not set</p> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def crawl(self, urls: List[str]) -&gt; List[Document]:\n    \"\"\"Crawl the given URLs using Exa SDK.\n\n    Args:\n        urls: List of URLs to crawl\n\n    Returns:\n        List of Documents with content extracted from the URLs\n\n    Raises:\n        LangroidImportError: If the exa package is not installed\n        ValueError: If the Exa API key is not set\n    \"\"\"\n    try:\n        from exa_py import Exa\n    except ImportError:\n        raise LangroidImportError(\"exa\", \"exa\")\n\n    if not self.config.api_key:\n        raise ValueError(\"EXA_API_KEY key is required in your env or .env\")\n\n    exa = Exa(self.config.api_key)\n    docs = []\n\n    try:\n        for url in urls:\n            parsed_doc_chunks = self._process_document(url)\n            if parsed_doc_chunks:\n                docs.extend(parsed_doc_chunks)\n                continue\n            else:\n                results = exa.get_contents(\n                    [url],\n                    livecrawl=\"always\",\n                    text={\n                        \"include_html_tags\": True,\n                    },\n                )\n                result = results.results[0]\n                if result.text:\n                    md_text = md.markdownify(result.text, heading_style=\"ATX\")\n                    # append a NON-chunked document\n                    # (metadata.is_chunk = False, so will be chunked downstream)\n                    docs.append(\n                        Document(\n                            content=md_text,\n                            metadata=DocMetaData(\n                                source=url,\n                                title=getattr(result, \"title\", \"Unknown Title\"),\n                                published_date=getattr(\n                                    result, \"published_date\", \"Unknown Date\"\n                                ),\n                            ),\n                        )\n                    )\n\n    except Exception as e:\n        logging.error(f\"Error retrieving content from Exa API: {e}\")\n\n    return docs\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.Crawl4aiCrawler","title":"<code>Crawl4aiCrawler(config)</code>","text":"<p>               Bases: <code>BaseCrawler</code></p> <p>Crawler implementation using the crawl4ai library.</p> <p>This crawler intelligently dispatches URLs. Standard web pages are rendered and scraped using the crawl4ai browser engine. Direct links to documents (PDF, DOCX, etc.) are delegated to the framework's internal DocumentParser.</p> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(self, config: Crawl4aiConfig) -&gt; None:\n    \"\"\"Initialize the Crawl4ai crawler.\"\"\"\n    super().__init__(config)\n    self.config: Crawl4aiConfig = config\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.Crawl4aiCrawler.needs_parser","title":"<code>needs_parser</code>  <code>property</code>","text":"<p>Indicates that this crawler relies on the framework's DocumentParser for handling specific file types like PDF, DOCX, etc., which the browser engine cannot parse directly.</p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.Crawl4aiCrawler.crawl","title":"<code>crawl(urls)</code>","text":"<p>Executes the crawl by separating document URLs from web page URLs.</p> <ul> <li>Document URLs (.pdf, .docx, etc.) are processed using <code>_process_document</code>.</li> <li>Web page URLs are handled using the async crawl4ai engine.</li> </ul> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def crawl(self, urls: List[str]) -&gt; List[Document]:\n    \"\"\"\n    Executes the crawl by separating document URLs from web page URLs.\n\n    - Document URLs (.pdf, .docx, etc.) are processed using `_process_document`.\n    - Web page URLs are handled using the async crawl4ai engine.\n    \"\"\"\n    all_documents: List[Document] = []\n    webpage_urls: List[str] = []\n\n    # Step 1: Separate URLs into documents and web pages\n    for url in urls:\n        parsed_doc_chunks = self._process_document(url)\n        if parsed_doc_chunks:\n            all_documents.extend(parsed_doc_chunks)\n        else:\n            webpage_urls.append(url)\n\n    # Step 2: Process web page URLs asynchronously\n    if webpage_urls:\n        try:\n            loop = asyncio.get_running_loop()\n            if loop.is_running():\n                import nest_asyncio\n\n                nest_asyncio.apply()\n            web_docs = asyncio.run(self._async_crawl(webpage_urls))\n        except RuntimeError:\n            web_docs = asyncio.run(self._async_crawl(webpage_urls))\n\n        all_documents.extend(web_docs)\n\n    return all_documents\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.URLLoader","title":"<code>URLLoader(urls, parsing_config=ParsingConfig(), crawler_config=None)</code>","text":"<p>Loads URLs and extracts text using a specified crawler.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>List[Any]</code> <p>List of URLs to load</p> required <code>parsing_config</code> <code>ParsingConfig</code> <p>Configuration for parsing</p> <code>ParsingConfig()</code> <code>crawler_config</code> <code>Optional[BaseCrawlerConfig]</code> <p>Configuration for the crawler</p> <code>None</code> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(\n    self,\n    urls: List[Any],\n    parsing_config: ParsingConfig = ParsingConfig(),\n    crawler_config: Optional[BaseCrawlerConfig] = None,\n):\n    \"\"\"Initialize the URL loader.\n\n    Args:\n        urls: List of URLs to load\n        parsing_config: Configuration for parsing\n        crawler_config: Configuration for the crawler\n    \"\"\"\n    self.urls = urls\n    self.parsing_config = parsing_config\n\n    if crawler_config is None:\n        crawler_config = TrafilaturaConfig(parser=Parser(parsing_config))\n\n    self.crawler = CrawlerFactory.create_crawler(crawler_config)\n    if self.crawler.needs_parser:\n        self.crawler.parser = Parser(parsing_config)\n</code></pre>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.URLLoader.load","title":"<code>load()</code>","text":"<p>Load the URLs using the specified crawler.</p> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def load(self) -&gt; List[Document]:\n    \"\"\"Load the URLs using the specified crawler.\"\"\"\n    return self.crawler.crawl(self.urls)\n</code></pre>"},{"location":"reference/parsing/urls/","title":"urls","text":"<p>langroid/parsing/urls.py </p>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.url_to_tempfile","title":"<code>url_to_tempfile(url)</code>","text":"<p>Fetch content from the given URL and save it to a temporary local file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the content to fetch.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the temporary file where the content is saved.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If there's any issue fetching the content.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def url_to_tempfile(url: str) -&gt; str:\n    \"\"\"\n    Fetch content from the given URL and save it to a temporary local file.\n\n    Args:\n        url (str): The URL of the content to fetch.\n\n    Returns:\n        str: The path to the temporary file where the content is saved.\n\n    Raises:\n        HTTPError: If there's any issue fetching the content.\n    \"\"\"\n\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n\n    # Create a temporary file and write the content\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".tmp\") as temp_file:\n        temp_file.write(response.content)\n        return temp_file.name\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_user_input","title":"<code>get_user_input(msg, color='blue')</code>","text":"<p>Prompt the user for input. Args:     msg: printed prompt     color: color of the prompt Returns:     user input</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_user_input(msg: str, color: str = \"blue\") -&gt; str:\n    \"\"\"\n    Prompt the user for input.\n    Args:\n        msg: printed prompt\n        color: color of the prompt\n    Returns:\n        user input\n    \"\"\"\n    color_str = f\"[{color}]{msg} \" if color else msg + \" \"\n    print(color_str, end=\"\")\n    return input(\"\")\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_list_from_user","title":"<code>get_list_from_user(prompt=\"Enter input (type 'done' or hit return to finish)\", n=None)</code>","text":"<p>Prompt the user for inputs. Args:     prompt: printed prompt     n: how many inputs to prompt for. If None, then prompt until done, otherwise         quit after n inputs. Returns:     list of input strings</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_list_from_user(\n    prompt: str = \"Enter input (type 'done' or hit return to finish)\",\n    n: int | None = None,\n) -&gt; List[str]:\n    \"\"\"\n    Prompt the user for inputs.\n    Args:\n        prompt: printed prompt\n        n: how many inputs to prompt for. If None, then prompt until done, otherwise\n            quit after n inputs.\n    Returns:\n        list of input strings\n    \"\"\"\n    # Create an empty set to store the URLs.\n    input_set = set()\n\n    # Use a while loop to continuously ask the user for URLs.\n    for _ in range(n or 1000):\n        # Prompt the user for input.\n        input_str = Prompt.ask(f\"[blue]{prompt}\")\n\n        # Check if the user wants to exit the loop.\n        if input_str.lower() == \"done\" or input_str == \"\":\n            break\n\n        # if it is a URL, ask how many to crawl\n        if is_url(input_str):\n            url = input_str\n            input_str = Prompt.ask(\"[blue] How many new URLs to crawl?\", default=\"0\")\n            max_urls = int(input_str) + 1\n            tot_urls = list(find_urls(url, max_links=max_urls, max_depth=2))\n            tot_urls_str = \"\\n\".join(tot_urls)\n            print(\n                f\"\"\"\n                Found these {len(tot_urls)} links upto depth 2:\n                {tot_urls_str}\n                \"\"\"\n            )\n\n            input_set.update(tot_urls)\n        else:\n            input_set.add(input_str.strip())\n\n    return list(input_set)\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_urls_paths_bytes_indices","title":"<code>get_urls_paths_bytes_indices(inputs)</code>","text":"<p>Given a list of inputs, return a list of indices of URLs, list of indices of paths, list of indices of byte-contents. Args:     inputs: list of strings or bytes Returns:     list of Indices of URLs,     list of indices of paths,     list of indices of byte-contents</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_urls_paths_bytes_indices(\n    inputs: List[str | bytes],\n) -&gt; Tuple[List[int], List[int], List[int]]:\n    \"\"\"\n    Given a list of inputs, return a\n    list of indices of URLs, list of indices of paths, list of indices of byte-contents.\n    Args:\n        inputs: list of strings or bytes\n    Returns:\n        list of Indices of URLs,\n        list of indices of paths,\n        list of indices of byte-contents\n    \"\"\"\n    urls = []\n    paths = []\n    byte_list = []\n    for i, item in enumerate(inputs):\n        if isinstance(item, bytes):\n            byte_list.append(i)\n            continue\n        try:\n            url_adapter = TypeAdapter(HttpUrl)\n            Url(url=url_adapter.validate_python(item))\n            urls.append(i)\n        except ValidationError:\n            if os.path.exists(item):\n                paths.append(i)\n            else:\n                logger.warning(f\"{item} is neither a URL nor a path.\")\n    return urls, paths, byte_list\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.crawl_url","title":"<code>crawl_url(url, max_urls=1)</code>","text":"<p>Crawl starting at the url and return a list of URLs to be parsed, up to a maximum of <code>max_urls</code>. This has not been tested to work as intended. Ignore.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def crawl_url(url: str, max_urls: int = 1) -&gt; List[str]:\n    \"\"\"\n    Crawl starting at the url and return a list of URLs to be parsed,\n    up to a maximum of `max_urls`.\n    This has not been tested to work as intended. Ignore.\n    \"\"\"\n    from trafilatura.spider import focused_crawler\n\n    if max_urls == 1:\n        # no need to crawl, just return the original list\n        return [url]\n\n    to_visit = None\n    known_urls = None\n\n    # Create a RobotFileParser object\n    robots = urllib.robotparser.RobotFileParser()\n    while True:\n        if known_urls is not None and len(known_urls) &gt;= max_urls:\n            break\n        # Set the RobotFileParser object to the website's robots.txt file\n        robots.set_url(url + \"/robots.txt\")\n        robots.read()\n\n        if robots.can_fetch(\"*\", url):\n            # Start or resume the crawl\n            to_visit, known_urls = focused_crawler(\n                url,\n                max_seen_urls=max_urls,\n                max_known_urls=max_urls,\n                todo=to_visit,\n                known_links=known_urls,\n                rules=robots,\n            )\n        if to_visit is None:\n            break\n\n    if known_urls is None:\n        return [url]\n    final_urls = [s.strip() for s in known_urls]\n    return list(final_urls)[:max_urls]\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.find_urls","title":"<code>find_urls(url='https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', max_links=20, visited=None, depth=0, max_depth=2, match_domain=True)</code>","text":"<p>Recursively find all URLs on a given page.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to start from.</p> <code>'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer'</code> <code>max_links</code> <code>int</code> <p>The maximum number of links to find.</p> <code>20</code> <code>visited</code> <code>set</code> <p>A set of URLs that have already been visited.</p> <code>None</code> <code>depth</code> <code>int</code> <p>The current depth of the recursion.</p> <code>0</code> <code>max_depth</code> <code>int</code> <p>The maximum depth of the recursion.</p> <code>2</code> <code>match_domain</code> <code>bool</code> <p>Whether to only return URLs that are on the same domain.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>set</code> <code>Set[str]</code> <p>A set of URLs found on the page.</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def find_urls(\n    url: str = \"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\",\n    max_links: int = 20,\n    visited: Optional[Set[str]] = None,\n    depth: int = 0,\n    max_depth: int = 2,\n    match_domain: bool = True,\n) -&gt; Set[str]:\n    \"\"\"\n    Recursively find all URLs on a given page.\n\n    Args:\n        url (str): The URL to start from.\n        max_links (int): The maximum number of links to find.\n        visited (set): A set of URLs that have already been visited.\n        depth (int): The current depth of the recursion.\n        max_depth (int): The maximum depth of the recursion.\n        match_domain (bool): Whether to only return URLs that are on the same domain.\n\n    Returns:\n        set: A set of URLs found on the page.\n    \"\"\"\n\n    if visited is None:\n        visited = set()\n\n    if url in visited or depth &gt; max_depth:\n        return visited\n\n    visited.add(url)\n    base_domain = urlparse(url).netloc\n\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        links = [\n            urljoin(url, a[\"href\"])  # type: ignore\n            for a in soup.find_all(\"a\", href=True)\n        ]\n\n        # Defrag links: discard links that are to portions of same page\n        defragged_links = list(\n            set(urldefrag(link).url for link in links)  # type: ignore\n        )\n\n        # Filter links based on domain matching requirement\n        domain_matching_links = [\n            link for link in defragged_links if urlparse(link).netloc == base_domain\n        ]\n\n        # ensure url is first, since below we are taking first max_links urls\n        domain_matching_links = [url] + [x for x in domain_matching_links if x != url]\n\n        # If found links exceed max_links, return immediately\n        if len(domain_matching_links) &gt;= max_links:\n            return set(domain_matching_links[:max_links])\n\n        for link in domain_matching_links:\n            if len(visited) &gt;= max_links:\n                break\n\n            if link not in visited:\n                visited.update(\n                    find_urls(\n                        link,\n                        max_links,\n                        visited,\n                        depth + 1,\n                        max_depth,\n                        match_domain,\n                    )\n                )\n\n    except (requests.RequestException, Exception) as e:\n        print(f\"Error fetching {url}. Error: {e}\")\n\n    return set(list(visited)[:max_links])\n</code></pre>"},{"location":"reference/parsing/utils/","title":"utils","text":"<p>langroid/parsing/utils.py </p>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.batched","title":"<code>batched(iterable, n)</code>","text":"<p>Batch data into tuples of length n. The last batch may be shorter.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def batched(iterable: Iterable[T], n: int) -&gt; Iterable[Sequence[T]]:\n    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --&gt; ABC DEF G\n    if n &lt; 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := tuple(islice(it, n)):\n        yield batch\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.closest_string","title":"<code>closest_string(query, string_list)</code>","text":"<p>Find the closest match to the query in a list of strings.</p> <p>This function is case-insensitive and ignores leading and trailing whitespace. If no match is found, it returns 'No match found'.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The string to match.</p> required <code>string_list</code> <code>List[str]</code> <p>The list of strings to search.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The closest match to the query from the list, or 'No match found'  if no match is found.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def closest_string(query: str, string_list: List[str]) -&gt; str:\n    \"\"\"Find the closest match to the query in a list of strings.\n\n    This function is case-insensitive and ignores leading and trailing whitespace.\n    If no match is found, it returns 'No match found'.\n\n    Args:\n        query (str): The string to match.\n        string_list (List[str]): The list of strings to search.\n\n    Returns:\n        str: The closest match to the query from the list, or 'No match found'\n             if no match is found.\n    \"\"\"\n    # Create a dictionary where the keys are the standardized strings and\n    # the values are the original strings.\n    str_dict = {s.lower().strip(): s for s in string_list}\n\n    # Standardize the query and find the closest match in the list of keys.\n    closest_match = difflib.get_close_matches(\n        query.lower().strip(), str_dict.keys(), n=1\n    )\n\n    # Retrieve the original string from the value in the dictionary.\n    original_closest_match = (\n        str_dict[closest_match[0]] if closest_match else \"No match found\"\n    )\n\n    return original_closest_match\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.split_paragraphs","title":"<code>split_paragraphs(text)</code>","text":"<pre><code>Split the input text into paragraphs using \"\n</code></pre> <p>\" as the delimiter.</p> <pre><code>Args:\n    text (str): The input text.\n\nReturns:\n    list: A list of paragraphs.\n</code></pre> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def split_paragraphs(text: str) -&gt; List[str]:\n    \"\"\"\n    Split the input text into paragraphs using \"\\n\\n\" as the delimiter.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        list: A list of paragraphs.\n    \"\"\"\n    # Split based on a newline, followed by spaces/tabs, then another newline.\n    paras = re.split(r\"\\n[ \\t]*\\n\", text)\n    return [para.strip() for para in paras if para.strip()]\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.split_newlines","title":"<code>split_newlines(text)</code>","text":"<pre><code>Split the input text into lines using \"\n</code></pre> <p>\" as the delimiter.</p> <pre><code>Args:\n    text (str): The input text.\n\nReturns:\n    list: A list of lines.\n</code></pre> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def split_newlines(text: str) -&gt; List[str]:\n    \"\"\"\n    Split the input text into lines using \"\\n\" as the delimiter.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        list: A list of lines.\n    \"\"\"\n    lines = re.split(r\"\\n\", text)\n    return [line.strip() for line in lines if line.strip()]\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.number_segments","title":"<code>number_segments(s, granularity=1)</code>","text":"<p>Number the segments in a given text, preserving paragraph structure. A segment is a sequence of <code>len</code> consecutive \"sentences\", where a \"sentence\" is either a normal sentence, or if there isn't enough punctuation to properly identify sentences, then we use a pseudo-sentence via heuristics (split by newline or failing that, just split every 40 words). The goal here is simply to number segments at a reasonable granularity so the LLM can identify relevant segments, in the RelevanceExtractorAgent.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text.</p> required <code>granularity</code> <code>int</code> <p>The number of sentences in a segment. If this is -1, then the entire text is treated as a single segment, and is numbered as &lt;#1#&gt;.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with segments numbered in the style &lt;#1#&gt;, &lt;#2#&gt; etc.</p> Example <p>number_segments(\"Hello world! How are you? Have a good day.\") '&lt;#1#&gt; Hello world! &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.'</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def number_segments(s: str, granularity: int = 1) -&gt; str:\n    \"\"\"\n    Number the segments in a given text, preserving paragraph structure.\n    A segment is a sequence of `len` consecutive \"sentences\", where a \"sentence\"\n    is either a normal sentence, or if there isn't enough punctuation to properly\n    identify sentences, then we use a pseudo-sentence via heuristics (split by newline\n    or failing that, just split every 40 words). The goal here is simply to number\n    segments at a reasonable granularity so the LLM can identify relevant segments,\n    in the RelevanceExtractorAgent.\n\n    Args:\n        s (str): The input text.\n        granularity (int): The number of sentences in a segment.\n            If this is -1, then the entire text is treated as a single segment,\n            and is numbered as &lt;#1#&gt;.\n\n    Returns:\n        str: The text with segments numbered in the style &lt;#1#&gt;, &lt;#2#&gt; etc.\n\n    Example:\n        &gt;&gt;&gt; number_segments(\"Hello world! How are you? Have a good day.\")\n        '&lt;#1#&gt; Hello world! &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.'\n    \"\"\"\n    import nltk\n\n    if granularity &lt; 0:\n        return \"&lt;#1#&gt; \" + s\n    numbered_text = []\n    count = 0\n\n    paragraphs = split_paragraphs(s)\n    for paragraph in paragraphs:\n        sentences = nltk.sent_tokenize(paragraph)\n        # Some docs are problematic (e.g. resumes) and have no (or too few) periods,\n        # so we can't split usefully into sentences.\n        # We try a series of heuristics to split into sentences,\n        # until the avg num words per sentence is less than 40.\n        avg_words_per_sentence = sum(\n            len(nltk.word_tokenize(sentence)) for sentence in sentences\n        ) / len(sentences)\n        if avg_words_per_sentence &gt; 40:\n            sentences = split_newlines(paragraph)\n        avg_words_per_sentence = sum(\n            len(nltk.word_tokenize(sentence)) for sentence in sentences\n        ) / len(sentences)\n        if avg_words_per_sentence &gt; 40:\n            # Still too long, just split on every 40 words\n            sentences = []\n            for sentence in nltk.sent_tokenize(paragraph):\n                words = nltk.word_tokenize(sentence)\n                for i in range(0, len(words), 40):\n                    # if there are less than 20 words left after this,\n                    # just add them to the last sentence and break\n                    if len(words) - i &lt; 20:\n                        sentences.append(\" \".join(words[i:]))\n                        break\n                    else:\n                        sentences.append(\" \".join(words[i : i + 40]))\n        for i, sentence in enumerate(sentences):\n            num = count // granularity + 1\n            number_prefix = f\"&lt;#{num}#&gt;\" if count % granularity == 0 else \"\"\n            sentence = f\"{number_prefix} {sentence}\"\n            count += 1\n            sentences[i] = sentence\n        numbered_paragraph = \" \".join(sentences)\n        numbered_text.append(numbered_paragraph)\n\n    return \"  \\n\\n  \".join(numbered_text)\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.parse_number_range_list","title":"<code>parse_number_range_list(specs)</code>","text":"<p>Parse a specs string like \"3,5,7-10\" into a list of integers.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>str</code> <p>A string containing segment numbers and/or ranges          (e.g., \"3,5,7-10\").</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of segment numbers.</p> Example <p>parse_number_range_list(\"3,5,7-10\") [3, 5, 7, 8, 9, 10]</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def parse_number_range_list(specs: str) -&gt; List[int]:\n    \"\"\"\n    Parse a specs string like \"3,5,7-10\" into a list of integers.\n\n    Args:\n        specs (str): A string containing segment numbers and/or ranges\n                     (e.g., \"3,5,7-10\").\n\n    Returns:\n        List[int]: List of segment numbers.\n\n    Example:\n        &gt;&gt;&gt; parse_number_range_list(\"3,5,7-10\")\n        [3, 5, 7, 8, 9, 10]\n    \"\"\"\n    spec_indices = set()  # type: ignore\n    for part in specs.split(\",\"):\n        # some weak LLMs may generate &lt;#1#&gt; instead of 1, so extract just the digits\n        # or the \"-\"\n        part = \"\".join(char for char in part if char.isdigit() or char == \"-\")\n        if \"-\" in part:\n            start, end = map(int, part.split(\"-\"))\n            spec_indices.update(range(start, end + 1))\n        else:\n            spec_indices.add(int(part))\n\n    return sorted(list(spec_indices))\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.strip_k","title":"<code>strip_k(s, k=2)</code>","text":"<p>Strip any leading and trailing whitespaces from the input text beyond length k. This is useful for removing leading/trailing whitespaces from a text while preserving paragraph structure.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text.</p> required <code>k</code> <code>int</code> <p>The number of leading and trailing whitespaces to retain.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with leading and trailing whitespaces removed beyond length k.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def strip_k(s: str, k: int = 2) -&gt; str:\n    \"\"\"\n    Strip any leading and trailing whitespaces from the input text beyond length k.\n    This is useful for removing leading/trailing whitespaces from a text while\n    preserving paragraph structure.\n\n    Args:\n        s (str): The input text.\n        k (int): The number of leading and trailing whitespaces to retain.\n\n    Returns:\n        str: The text with leading and trailing whitespaces removed beyond length k.\n    \"\"\"\n\n    # Count leading and trailing whitespaces\n    leading_count = len(s) - len(s.lstrip())\n    trailing_count = len(s) - len(s.rstrip())\n\n    # Determine how many whitespaces to retain\n    leading_keep = min(leading_count, k)\n    trailing_keep = min(trailing_count, k)\n\n    # Use slicing to get the desired output\n    return s[leading_count - leading_keep : len(s) - (trailing_count - trailing_keep)]\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.clean_whitespace","title":"<code>clean_whitespace(text)</code>","text":"<p>Remove extra whitespace from the input text, while preserving paragraph structure.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def clean_whitespace(text: str) -&gt; str:\n    \"\"\"Remove extra whitespace from the input text, while preserving\n    paragraph structure.\n    \"\"\"\n    paragraphs = split_paragraphs(text)\n    cleaned_paragraphs = [\" \".join(p.split()) for p in paragraphs if p]\n    return \"\\n\\n\".join(cleaned_paragraphs)  # Join the cleaned paragraphs.\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.extract_numbered_segments","title":"<code>extract_numbered_segments(s, specs)</code>","text":"<p>Extract specified segments from a numbered text, preserving paragraph structure.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text containing numbered segments.</p> required <code>specs</code> <code>str</code> <p>A string containing segment numbers and/or ranges          (e.g., \"3,5,7-10\").</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Extracted segments, keeping original paragraph structures.</p> Example <p>text = \"(1) Hello world! (2) How are you? (3) Have a good day.\" extract_numbered_segments(text, \"1,3\") 'Hello world! Have a good day.'</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def extract_numbered_segments(s: str, specs: str) -&gt; str:\n    \"\"\"\n    Extract specified segments from a numbered text, preserving paragraph structure.\n\n    Args:\n        s (str): The input text containing numbered segments.\n        specs (str): A string containing segment numbers and/or ranges\n                     (e.g., \"3,5,7-10\").\n\n    Returns:\n        str: Extracted segments, keeping original paragraph structures.\n\n    Example:\n        &gt;&gt;&gt; text = \"(1) Hello world! (2) How are you? (3) Have a good day.\"\n        &gt;&gt;&gt; extract_numbered_segments(text, \"1,3\")\n        'Hello world! Have a good day.'\n    \"\"\"\n    # Use the helper function to get the list of indices from specs\n    if specs.strip() == \"\":\n        return \"\"\n    spec_indices = parse_number_range_list(specs)\n\n    # Regular expression to identify numbered segments like\n    # &lt;#1#&gt; Hello world! This is me. &lt;#2#&gt; How are you? &lt;#3#&gt; Have a good day.\n    # Note we match any character between segment markers, including newlines.\n    segment_pattern = re.compile(r\"&lt;#(\\d+)#&gt;([\\s\\S]*?)(?=&lt;#\\d+#&gt;|$)\")\n\n    # Split the text into paragraphs while preserving their boundaries\n    paragraphs = split_paragraphs(s)\n\n    extracted_paragraphs = []\n\n    for paragraph in paragraphs:\n        segments_with_numbers = segment_pattern.findall(paragraph)\n\n        # Extract the desired segments from this paragraph\n        extracted_segments = [\n            segment\n            for num, segment in segments_with_numbers\n            if int(num) in spec_indices\n        ]\n\n        # If we extracted any segments from this paragraph,\n        # join them with ellipsis (...) and append to results.\n        if extracted_segments:\n            extracted_paragraphs.append(\"...\".join(extracted_segments))\n\n    return \"\\n\\n\".join(extracted_paragraphs)\n</code></pre>"},{"location":"reference/parsing/utils/#langroid.parsing.utils.extract_content_from_path","title":"<code>extract_content_from_path(path, parsing, doc_type=None)</code>","text":"<p>Extract the content from a file path or URL, or a list of file paths or URLs.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>bytes | str | List[str]</code> <p>The file path or URL, or a list of file paths or URLs, or bytes content. The bytes option is meant to support cases where upstream code may have already loaded the content (e.g., from a database or API) and we want to avoid having to copy the content to a temporary file.</p> required <code>parsing</code> <code>ParsingConfig</code> <p>The parsing configuration.</p> required <code>doc_type</code> <code>str | DocumentType | None</code> <p>The document type if known. If multiple paths are given, this MUST apply to ALL docs.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | List[str]</code> <p>str | List[str]: The extracted content if a single file path or URL is provided,     or a list of extracted contents if a     list of file paths or URLs is provided.</p> Source code in <code>langroid/parsing/utils.py</code> <pre><code>def extract_content_from_path(\n    path: bytes | str | List[bytes | str],\n    parsing: ParsingConfig,\n    doc_type: str | DocumentType | None = None,\n) -&gt; str | List[str]:\n    \"\"\"\n    Extract the content from a file path or URL, or a list of file paths or URLs.\n\n    Args:\n        path (bytes | str | List[str]): The file path or URL, or a list of file paths or\n            URLs, or bytes content. The bytes option is meant to support cases\n            where upstream code may have already loaded the content (e.g., from a\n            database or API) and we want to avoid having to copy the content to a\n            temporary file.\n        parsing (ParsingConfig): The parsing configuration.\n        doc_type (str | DocumentType | None): The document type if known.\n            If multiple paths are given, this MUST apply to ALL docs.\n\n    Returns:\n        str | List[str]: The extracted content if a single file path or URL is provided,\n                or a list of extracted contents if a\n                list of file paths or URLs is provided.\n    \"\"\"\n    if isinstance(path, str) or isinstance(path, bytes):\n        paths = [path]\n    elif isinstance(path, list) and len(path) == 0:\n        return \"\"\n    else:\n        paths = path\n\n    url_idxs, path_idxs, byte_idxs = get_urls_paths_bytes_indices(paths)\n    urls = [paths[i] for i in url_idxs]\n    path_list = [paths[i] for i in path_idxs]\n    byte_list = [paths[i] for i in byte_idxs]\n    path_list.extend(byte_list)\n    parser = Parser(parsing)\n    docs: List[Document] = []\n    try:\n        if len(urls) &gt; 0:\n            loader = URLLoader(urls=urls, parser=parser)  # type: ignore\n            docs = loader.load()\n        if len(path_list) &gt; 0:\n            for p in path_list:\n                path_docs = RepoLoader.get_documents(\n                    p, parser=parser, doc_type=doc_type\n                )\n                docs.extend(path_docs)\n    except Exception as e:\n        logger.warning(f\"Error loading path {paths}: {e}\")\n        return \"\"\n    if len(docs) == 1:\n        return docs[0].content\n    else:\n        return [d.content for d in docs]\n</code></pre>"},{"location":"reference/parsing/web_search/","title":"web_search","text":"<p>langroid/parsing/web_search.py </p> <p>Utilities for web search.</p> <p>NOTE: Using Google Search requires setting the GOOGLE_API_KEY and GOOGLE_CSE_ID environment variables in your <code>.env</code> file, as explained in the README.</p>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.WebSearchResult","title":"<code>WebSearchResult(title, link, max_content_length=3500, max_summary_length=300)</code>","text":"<p>Class representing a Web Search result, containing the title, link, summary and full content of the result.</p> <pre><code>link (str): The link to the search result.\nmax_content_length (int): The maximum length of the full content.\nmax_summary_length (int): The maximum length of the summary.\n</code></pre> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def __init__(\n    self,\n    title: str,\n    link: str | None,\n    max_content_length: int = 3500,\n    max_summary_length: int = 300,\n):\n    \"\"\"\n    Args:\n        title (str): The title of the search result.\n        link (str): The link to the search result.\n        max_content_length (int): The maximum length of the full content.\n        max_summary_length (int): The maximum length of the summary.\n    \"\"\"\n    self.title = title\n    self.link = link\n    self.max_content_length = max_content_length\n    self.max_summary_length = max_summary_length\n    self.full_content = self.get_full_content()\n    self.summary = self.get_summary()\n</code></pre>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.metaphor_search","title":"<code>metaphor_search(query, num_results=5)</code>","text":"<p>Method that makes an API call by Metaphor client that queries the top num_results links that matches the query. Returns a list of WebSearchResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query body that users wants to make.</p> required <code>num_results</code> <code>int</code> <p>Number of top matching results that we want to grab</p> <code>5</code> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def metaphor_search(query: str, num_results: int = 5) -&gt; List[WebSearchResult]:\n    \"\"\"\n    Method that makes an API call by Metaphor client that queries\n    the top num_results links that matches the query. Returns a list\n    of WebSearchResult objects.\n\n    Args:\n        query (str): The query body that users wants to make.\n        num_results (int): Number of top matching results that we want\n            to grab\n    \"\"\"\n\n    load_dotenv()\n\n    api_key = os.getenv(\"METAPHOR_API_KEY\") or os.getenv(\"EXA_API_KEY\")\n    if not api_key:\n        raise ValueError(\n            \"\"\"\n            Neither METAPHOR_API_KEY nor EXA_API_KEY environment variables are set. \n            Please set one of them to your API key, and try again.\n            \"\"\"\n        )\n\n    try:\n        from metaphor_python import Metaphor\n    except ImportError:\n        raise LangroidImportError(\"metaphor-python\", \"metaphor\")\n\n    client = Metaphor(api_key=api_key)\n\n    response = client.search(\n        query=query,\n        num_results=num_results,\n    )\n    raw_results = response.results\n\n    return [\n        WebSearchResult(result.title, result.url, 3500, 300) for result in raw_results\n    ]\n</code></pre>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.exa_search","title":"<code>exa_search(query, num_results=5)</code>","text":"<p>Method that makes an API call by Exa client that queries the top num_results links that matches the query. Returns a list of WebSearchResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query body that users wants to make.</p> required <code>num_results</code> <code>int</code> <p>Number of top matching results that we want to grab</p> <code>5</code> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def exa_search(query: str, num_results: int = 5) -&gt; List[WebSearchResult]:\n    \"\"\"\n    Method that makes an API call by Exa client that queries\n    the top num_results links that matches the query. Returns a list\n    of WebSearchResult objects.\n\n    Args:\n        query (str): The query body that users wants to make.\n        num_results (int): Number of top matching results that we want\n            to grab\n    \"\"\"\n\n    load_dotenv()\n\n    api_key = os.getenv(\"EXA_API_KEY\")\n    if not api_key:\n        raise ValueError(\n            \"\"\"\n            EXA_API_KEY environment variables are not set. \n            Please set one of them to your API key, and try again.\n            \"\"\"\n        )\n\n    try:\n        from exa_py import Exa\n    except ImportError:\n        raise LangroidImportError(\"exa-py\", \"exa\")\n\n    client = Exa(api_key=api_key)\n\n    try:\n        response = client.search(\n            query=query,\n            num_results=num_results,\n        )\n        raw_results = response.results\n\n        return [\n            WebSearchResult(\n                title=result.title or \"\",\n                link=result.url,\n                max_content_length=3500,\n                max_summary_length=300,\n            )\n            for result in raw_results\n            if result.url is not None\n        ]\n    except Exception:\n        return [\n            WebSearchResult(\n                title=\"Error\",\n                link=None,\n                max_content_length=3500,\n                max_summary_length=300,\n            )\n        ]\n</code></pre>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.duckduckgo_search","title":"<code>duckduckgo_search(query, num_results=5)</code>","text":"<p>Method that makes an API call by DuckDuckGo client that queries the top <code>num_results</code> links that matche the query. Returns a list of WebSearchResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query body that users wants to make.</p> required <code>num_results</code> <code>int</code> <p>Number of top matching results that we want to grab</p> <code>5</code> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def duckduckgo_search(query: str, num_results: int = 5) -&gt; List[WebSearchResult]:\n    \"\"\"\n    Method that makes an API call by DuckDuckGo client that queries\n    the top `num_results` links that matche the query. Returns a list\n    of WebSearchResult objects.\n\n    Args:\n        query (str): The query body that users wants to make.\n        num_results (int): Number of top matching results that we want\n            to grab\n    \"\"\"\n\n    with DDGS() as ddgs:\n        search_results = [r for r in ddgs.text(query, max_results=num_results)]\n\n    return [\n        WebSearchResult(\n            title=result[\"title\"],\n            link=result[\"href\"],\n            max_content_length=3500,\n            max_summary_length=300,\n        )\n        for result in search_results\n    ]\n</code></pre>"},{"location":"reference/parsing/web_search/#langroid.parsing.web_search.tavily_search","title":"<code>tavily_search(query, num_results=5)</code>","text":"<p>Method that makes an API call to Tavily API that queries the top <code>num_results</code> links that match the query. Returns a list of WebSearchResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query body that users wants to make.</p> required <code>num_results</code> <code>int</code> <p>Number of top matching results that we want to grab</p> <code>5</code> Source code in <code>langroid/parsing/web_search.py</code> <pre><code>def tavily_search(query: str, num_results: int = 5) -&gt; List[WebSearchResult]:\n    \"\"\"\n    Method that makes an API call to Tavily API that queries\n    the top `num_results` links that match the query. Returns a list\n    of WebSearchResult objects.\n\n    Args:\n        query (str): The query body that users wants to make.\n        num_results (int): Number of top matching results that we want\n            to grab\n    \"\"\"\n\n    load_dotenv()\n\n    api_key = os.getenv(\"TAVILY_API_KEY\")\n    if not api_key:\n        raise ValueError(\n            \"TAVILY_API_KEY environment variable is not set. \"\n            \"Please set it to your API key and try again.\"\n        )\n\n    try:\n        from tavily import TavilyClient\n    except ImportError:\n        raise LangroidImportError(\"tavily-python\", \"tavily\")\n\n    client = TavilyClient(api_key=api_key)\n    response = client.search(query=query, max_results=num_results)\n    search_results = response[\"results\"]\n\n    return [\n        WebSearchResult(\n            title=result[\"title\"],\n            link=result[\"url\"],\n            max_content_length=3500,\n            max_summary_length=300,\n        )\n        for result in search_results\n    ]\n</code></pre>"},{"location":"reference/prompts/","title":"prompts","text":"<p>langroid/prompts/init.py </p>"},{"location":"reference/prompts/dialog/","title":"dialog","text":"<p>langroid/prompts/dialog.py </p>"},{"location":"reference/prompts/dialog/#langroid.prompts.dialog.collate_chat_history","title":"<code>collate_chat_history(inputs)</code>","text":"<p>Collate (human, ai) pairs into a single, string Args:     inputs: Returns:</p> Source code in <code>langroid/prompts/dialog.py</code> <pre><code>def collate_chat_history(inputs: List[tuple[str, str]]) -&gt; str:\n    \"\"\"\n    Collate (human, ai) pairs into a single, string\n    Args:\n        inputs:\n    Returns:\n    \"\"\"\n    pairs = [\n        f\"\"\"Human:{human}\n        AI:{ai}\n        \"\"\"\n        for human, ai in inputs\n    ]\n    return \"\\n\".join(pairs)\n</code></pre>"},{"location":"reference/prompts/prompts_config/","title":"prompts_config","text":"<p>langroid/prompts/prompts_config.py </p>"},{"location":"reference/prompts/templates/","title":"templates","text":"<p>langroid/prompts/templates.py </p>"},{"location":"reference/pydantic_v1/","title":"pydantic_v1","text":"<p>langroid/pydantic_v1/init.py </p> <p>Compatibility layer for Langroid's Pydantic migration.</p> <p>IMPORTANT: You are importing from langroid.pydantic_v1 but getting Pydantic v2 classes! Langroid has fully migrated to Pydantic v2, and this compatibility layer is deprecated.</p>"},{"location":"reference/pydantic_v1/main/","title":"main","text":"<p>langroid/pydantic_v1/main.py </p> <p>Compatibility layer for Pydantic v2 migration.</p> <p>This module now imports directly from Pydantic v2 since all internal code has been migrated to use Pydantic v2 patterns.</p>"},{"location":"reference/utils/","title":"utils","text":"<p>langroid/utils/init.py </p>"},{"location":"reference/utils/configuration/","title":"configuration","text":"<p>langroid/utils/configuration.py </p>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.SettingsProxy","title":"<code>SettingsProxy</code>","text":"<p>A proxy for the settings that returns a thread\u2010local override if set, or else falls back to the global settings.</p>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.update_global_settings","title":"<code>update_global_settings(cfg, keys)</code>","text":"<p>Update global settings so that modules can later access them via, e.g.,</p> <pre><code>from langroid.utils.configuration import settings\nif settings.debug: ...\n</code></pre> <p>This updates the global default.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def update_global_settings(cfg: BaseSettings, keys: List[str]) -&gt; None:\n    \"\"\"\n    Update global settings so that modules can later access them via, e.g.,\n\n        from langroid.utils.configuration import settings\n        if settings.debug: ...\n\n    This updates the global default.\n    \"\"\"\n    config_dict = cfg.model_dump()\n    filtered_config = {key: config_dict[key] for key in keys if key in config_dict}\n    new_settings = Settings(**filtered_config)\n    _global_settings.__dict__.update(new_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.set_global","title":"<code>set_global(key_vals)</code>","text":"<p>Update the global settings object.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def set_global(key_vals: Settings) -&gt; None:\n    \"\"\"\n    Update the global settings object.\n    \"\"\"\n    _global_settings.__dict__.update(key_vals.__dict__)\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.temporary_settings","title":"<code>temporary_settings(temp_settings)</code>","text":"<p>Temporarily override the settings for the calling thread.</p> <p>Within the context, any access to \"settings\" will use the provided temporary settings. Once the context is exited, the thread reverts to the global settings.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>@contextmanager\ndef temporary_settings(temp_settings: Settings) -&gt; Iterator[None]:\n    \"\"\"\n    Temporarily override the settings for the calling thread.\n\n    Within the context, any access to \"settings\" will use the provided temporary\n    settings. Once the context is exited, the thread reverts to the global settings.\n    \"\"\"\n    saved = getattr(_thread_local, \"override\", None)\n    _thread_local.override = temp_settings\n    try:\n        yield\n    finally:\n        if saved is not None:\n            _thread_local.override = saved\n        else:\n            del _thread_local.override\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.quiet_mode","title":"<code>quiet_mode(quiet=True)</code>","text":"<p>Temporarily override settings.quiet for the current thread. This implementation builds on the thread\u2011local temporary_settings context manager. The effective quiet mode is merged: if quiet is already True (from an outer context), then it remains True even if a nested context passes quiet=False.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>@contextmanager\ndef quiet_mode(quiet: bool = True) -&gt; Iterator[None]:\n    \"\"\"\n    Temporarily override settings.quiet for the current thread.\n    This implementation builds on the thread\u2011local temporary_settings context manager.\n    The effective quiet mode is merged:\n    if quiet is already True (from an outer context),\n    then it remains True even if a nested context passes quiet=False.\n    \"\"\"\n    current_effective = (\n        settings.model_dump()\n    )  # get the current thread's effective settings\n    # Create a new settings instance from the current effective state.\n    temp = Settings(**current_effective)\n    # Merge the new flag: once quiet is enabled, it stays enabled.\n    temp.quiet = settings.quiet or quiet\n    with temporary_settings(temp):\n        yield\n</code></pre>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.set_env","title":"<code>set_env(settings_instance)</code>","text":"<p>Set environment variables from a BaseSettings instance.</p> <p>Each field in the settings is written to os.environ.</p> Source code in <code>langroid/utils/configuration.py</code> <pre><code>def set_env(settings_instance: BaseSettings) -&gt; None:\n    \"\"\"\n    Set environment variables from a BaseSettings instance.\n\n    Each field in the settings is written to os.environ.\n    \"\"\"\n    for field_name, field in settings_instance.__class__.model_fields.items():\n        env_var_name = field.alias or field_name.upper()\n        os.environ[env_var_name] = str(settings_instance.model_dump()[field_name])\n</code></pre>"},{"location":"reference/utils/constants/","title":"constants","text":"<p>langroid/utils/constants.py </p>"},{"location":"reference/utils/git_utils/","title":"git_utils","text":"<p>langroid/utils/git_utils.py </p>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.git_read_file","title":"<code>git_read_file(repo, filepath)</code>","text":"<p>Read the contents of a file from a GitHub repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>str</code> <p>The GitHub repository in the format \"owner/repo\"</p> required <code>filepath</code> <code>str</code> <p>The file path relative to the repository root</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The contents of the file as a string</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def git_read_file(repo: str, filepath: str) -&gt; str:\n    \"\"\"\n    Read the contents of a file from a GitHub repository.\n\n    Args:\n        repo (str): The GitHub repository in the format \"owner/repo\"\n        filepath (str): The file path relative to the repository root\n\n    Returns:\n        str: The contents of the file as a string\n    \"\"\"\n    try:\n        g = Github()\n        github_repo = g.get_repo(repo)\n        file_content = github_repo.get_contents(filepath)\n        if isinstance(file_content, list) and len(file_content) &gt; 0:\n            return file_content[0].decoded_content.decode(\"utf-8\")\n        elif hasattr(file_content, \"decoded_content\"):\n            return file_content.decoded_content.decode(\"utf-8\")\n        else:\n            logger.error(f\"Unexpected file_content type: {type(file_content)}\")\n            return \"\"\n    except GithubException as e:\n        logger.error(f\"An error occurred while reading file {filepath}: {e}\")\n        return \"\"\n</code></pre>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.get_file_list","title":"<code>get_file_list(repo, dir, pat='')</code>","text":"<p>Get a list of files in a specified directory of a GitHub repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>str</code> <p>The GitHub repository in the format \"owner/repo\"</p> required <code>dir</code> <code>str</code> <p>The directory path relative to the repository root</p> required <code>pat</code> <code>str</code> <p>Optional wildcard pattern to filter file names (default: \"\")</p> <code>''</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file paths in the specified directory</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def get_file_list(repo: str, dir: str, pat: str = \"\") -&gt; List[str]:\n    \"\"\"\n    Get a list of files in a specified directory of a GitHub repository.\n\n    Args:\n        repo (str): The GitHub repository in the format \"owner/repo\"\n        dir (str): The directory path relative to the repository root\n        pat (str): Optional wildcard pattern to filter file names (default: \"\")\n\n    Returns:\n        List[str]: A list of file paths in the specified directory\n    \"\"\"\n    try:\n        g = Github()\n        github_repo = g.get_repo(repo)\n        contents = github_repo.get_contents(dir)\n\n        file_list = []\n        if isinstance(contents, list):\n            file_list = [content.path for content in contents if content.type == \"file\"]\n        elif hasattr(contents, \"path\") and hasattr(contents, \"type\"):\n            if contents.type == \"file\":\n                file_list = [contents.path]\n\n        if pat:\n            file_list = [file for file in file_list if fnmatch.fnmatch(file, pat)]\n        return sorted(file_list)\n\n    except GithubException as e:\n        logger.error(f\"An error occurred while fetching file list: {e}\")\n        return []\n</code></pre>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.git_init_repo","title":"<code>git_init_repo(dir)</code>","text":"<p>Set up a Git repository in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>Path to the directory where the Git repository should be initialized</p> required <p>Returns:</p> Type Description <code>Repo | None</code> <p>git.Repo: The initialized Git repository object</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def git_init_repo(dir: str) -&gt; git.Repo | None:\n    \"\"\"\n    Set up a Git repository in the specified directory.\n\n    Args:\n        dir (str): Path to the directory where the Git repository should be initialized\n\n    Returns:\n        git.Repo: The initialized Git repository object\n    \"\"\"\n    repo_path = Path(dir).expanduser()\n    try:\n        repo = git.Repo.init(repo_path)\n        logger.info(f\"Git repository initialized in {repo_path}\")\n\n        gitignore_content = textwrap.dedent(\n            \"\"\"\n        /target/\n        **/*.rs.bk\n        Cargo.lock\n        \"\"\"\n        ).strip()\n\n        gitignore_path = repo_path / \".gitignore\"\n        create_file(gitignore_path, gitignore_content)\n        logger.info(f\"Created .gitignore file in {repo_path}\")\n\n        # Ensure the default branch is 'main'\n        # Check if we're on the master branch\n        if repo.active_branch.name == \"master\":\n            # Rename the branch\n            repo.git.branch(\"-m\", \"master\", \"main\")\n            print(\"Branch renamed from 'master' to 'main'\")\n        else:\n            print(\"Current branch is not 'master'. No changes made.\")\n        return repo\n    except git.GitCommandError as e:\n        logger.error(f\"An error occurred while initializing the repository: {e}\")\n        return None\n</code></pre>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.git_commit_file","title":"<code>git_commit_file(repo, filepath, msg)</code>","text":"<p>Commit a file to a Git repository.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The Git repository object</p> required <code>filepath</code> <code>str</code> <p>Path to the file to be committed</p> required <code>msg</code> <code>str</code> <p>The commit message</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def git_commit_file(repo: git.Repo, filepath: str, msg: str) -&gt; None:\n    \"\"\"\n    Commit a file to a Git repository.\n\n    Args:\n        repo (git.Repo): The Git repository object\n        filepath (str): Path to the file to be committed\n        msg (str): The commit message\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        repo.index.add([filepath])\n        commit_msg = msg or f\"Updated {filepath}\"\n        repo.index.commit(commit_msg)\n        logger.info(f\"Successfully committed {filepath}: {commit_msg}\")\n    except git.GitCommandError as e:\n        logger.error(f\"An error occurred while committing: {e}\")\n</code></pre>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.git_commit_mods","title":"<code>git_commit_mods(repo, msg='commit all changes')</code>","text":"<p>Commit all modifications in the Git repository. Does not raise an error if there's nothing to commit.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The Git repository object</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def git_commit_mods(repo: git.Repo, msg: str = \"commit all changes\") -&gt; None:\n    \"\"\"\n    Commit all modifications in the Git repository.\n    Does not raise an error if there's nothing to commit.\n\n    Args:\n        repo (git.Repo): The Git repository object\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        if repo.is_dirty():\n            repo.git.add(update=True)\n            repo.index.commit(msg)\n            logger.info(\"Successfully committed all modifications\")\n        else:\n            logger.info(\"No changes to commit\")\n    except git.GitCommandError as e:\n        logger.error(f\"An error occurred while committing modifications: {e}\")\n</code></pre>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.git_restore_repo","title":"<code>git_restore_repo(repo)</code>","text":"<p>Restore all unstaged, uncommitted changes in the Git repository. This function undoes any dirty files to the last commit.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The Git repository object</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def git_restore_repo(repo: git.Repo) -&gt; None:\n    \"\"\"\n    Restore all unstaged, uncommitted changes in the Git repository.\n    This function undoes any dirty files to the last commit.\n\n    Args:\n        repo (git.Repo): The Git repository object\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        if repo.is_dirty():\n            repo.git.restore(\".\")\n            logger.info(\"Successfully restored all unstaged changes\")\n        else:\n            logger.info(\"No unstaged changes to restore\")\n    except git.GitCommandError as e:\n        logger.error(f\"An error occurred while restoring changes: {e}\")\n</code></pre>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.git_restore_file","title":"<code>git_restore_file(repo, file_path)</code>","text":"<p>Restore a specific file in the Git repository to its state in the last commit. This function undoes changes to the specified file.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The Git repository object</p> required <code>file_path</code> <code>str</code> <p>Path to the file to be restored</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def git_restore_file(repo: git.Repo, file_path: str) -&gt; None:\n    \"\"\"\n    Restore a specific file in the Git repository to its state in the last commit.\n    This function undoes changes to the specified file.\n\n    Args:\n        repo (git.Repo): The Git repository object\n        file_path (str): Path to the file to be restored\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        repo.git.restore(file_path)\n        logger.info(f\"Successfully restored file: {file_path}\")\n    except git.GitCommandError as e:\n        logger.error(f\"An error occurred while restoring file {file_path}: {e}\")\n</code></pre>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.git_create_checkout_branch","title":"<code>git_create_checkout_branch(repo, branch)</code>","text":"<p>Create and checkout a new branch in the given Git repository. If the branch already exists, it will be checked out. If we're already on the specified branch, no action is taken.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The Git repository object</p> required <code>branch</code> <code>str</code> <p>The name of the branch to create or checkout</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def git_create_checkout_branch(repo: git.Repo, branch: str) -&gt; None:\n    \"\"\"\n    Create and checkout a new branch in the given Git repository.\n    If the branch already exists, it will be checked out.\n    If we're already on the specified branch, no action is taken.\n\n    Args:\n        repo (git.Repo): The Git repository object\n        branch (str): The name of the branch to create or checkout\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        if repo.active_branch.name == branch:\n            logger.info(f\"Already on branch: {branch}\")\n            return\n\n        if branch in repo.heads:\n            repo.heads[branch].checkout()\n            logger.info(f\"Checked out existing branch: {branch}\")\n        else:\n            new_branch = repo.create_head(branch)\n            new_branch.checkout()\n            logger.info(f\"Created and checked out new branch: {branch}\")\n    except git.GitCommandError as e:\n        logger.error(f\"An error occurred while creating/checking out branch: {e}\")\n</code></pre>"},{"location":"reference/utils/git_utils/#langroid.utils.git_utils.git_diff_file","title":"<code>git_diff_file(repo, filepath)</code>","text":"<p>Show diffs of file between the latest commit and the previous one if any.</p> <p>Parameters:</p> Name Type Description Default <code>repo</code> <code>Repo</code> <p>The Git repository object</p> required <code>filepath</code> <code>str</code> <p>Path to the file to be diffed</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The diff output as a string</p> Source code in <code>langroid/utils/git_utils.py</code> <pre><code>def git_diff_file(repo: git.Repo, filepath: str) -&gt; str:\n    \"\"\"\n    Show diffs of file between the latest commit and the previous one if any.\n\n    Args:\n        repo (git.Repo): The Git repository object\n        filepath (str): Path to the file to be diffed\n\n    Returns:\n        str: The diff output as a string\n    \"\"\"\n    try:\n        # Get the two most recent commits\n        commits = list(repo.iter_commits(paths=filepath, max_count=2))\n\n        if len(commits) &lt; 2:\n            return \"No previous commit found for comparison.\"\n\n        # Get the diff between the two commits for the specific file\n        diff = repo.git.diff(commits[1].hexsha, commits[0].hexsha, filepath)\n\n        return str(diff)\n    except git.GitCommandError as e:\n        logger.error(f\"An error occurred while getting diff: {e}\")\n        return f\"Error: {str(e)}\"\n</code></pre>"},{"location":"reference/utils/globals/","title":"globals","text":"<p>langroid/utils/globals.py </p>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState","title":"<code>GlobalState</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base Pydantic model for global states.</p>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Get the global instance of the specific subclass.</p> <p>Returns:</p> Type Description <code>GlobalState</code> <p>The global instance of the subclass.</p> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef get_instance(cls: Type[\"GlobalState\"]) -&gt; \"GlobalState\":\n    \"\"\"\n    Get the global instance of the specific subclass.\n\n    Returns:\n        The global instance of the subclass.\n    \"\"\"\n    # Get the actual value from ModelPrivateAttr when accessing on class\n    instance_attr = getattr(cls, \"_instance\", None)\n    actual_instance: Optional[\"GlobalState\"]\n    if isinstance(instance_attr, ModelPrivateAttr):\n        default_value = instance_attr.default\n        if default_value is PydanticUndefined:\n            actual_instance = None\n        else:\n            actual_instance = cast(Optional[\"GlobalState\"], default_value)\n    else:\n        actual_instance = instance_attr\n\n    if actual_instance is None:\n        new_instance = cls()\n        cls._instance = new_instance\n        return new_instance\n    return actual_instance  # type: ignore\n</code></pre>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.set_values","title":"<code>set_values(**kwargs)</code>  <code>classmethod</code>","text":"<p>Set values on the global instance of the specific subclass.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Dict[str, Any]</code> <p>The fields and their values to set.</p> <code>{}</code> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef set_values(cls: Type[T], **kwargs: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Set values on the global instance of the specific subclass.\n\n    Args:\n        **kwargs: The fields and their values to set.\n    \"\"\"\n    instance = cls.get_instance()\n    for key, value in kwargs.items():\n        setattr(instance, key, value)\n</code></pre>"},{"location":"reference/utils/globals/#langroid.utils.globals.GlobalState.get_value","title":"<code>get_value(name)</code>  <code>classmethod</code>","text":"<p>Retrieve the value of a specific field from the global instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field to retrieve.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>The value of the specified field.</p> Source code in <code>langroid/utils/globals.py</code> <pre><code>@classmethod\ndef get_value(cls: Type[T], name: str) -&gt; Any:\n    \"\"\"\n    Retrieve the value of a specific field from the global instance.\n\n    Args:\n        name (str): The name of the field to retrieve.\n\n    Returns:\n        str: The value of the specified field.\n    \"\"\"\n    instance = cls.get_instance()\n    return getattr(instance, name)\n</code></pre>"},{"location":"reference/utils/html_logger/","title":"html_logger","text":"<p>langroid/utils/html_logger.py </p> <p>HTML Logger for Langroid Task System.</p> <p>This module provides an HTML logger that creates self-contained HTML files with collapsible log entries for better visualization of agent interactions.</p>"},{"location":"reference/utils/html_logger/#langroid.utils.html_logger.HTMLLogger","title":"<code>HTMLLogger(filename, log_dir='logs', model_info='', append=False)</code>","text":"<p>Logger that outputs task logs as interactive HTML files.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Base name for the log file (without extension)</p> required <code>log_dir</code> <code>str</code> <p>Directory to store log files</p> <code>'logs'</code> <code>model_info</code> <code>str</code> <p>Information about the model being used</p> <code>''</code> <code>append</code> <code>bool</code> <p>Whether to append to existing file</p> <code>False</code> Source code in <code>langroid/utils/html_logger.py</code> <pre><code>def __init__(\n    self,\n    filename: str,\n    log_dir: str = \"logs\",\n    model_info: str = \"\",\n    append: bool = False,\n):\n    \"\"\"Initialize the HTML logger.\n\n    Args:\n        filename: Base name for the log file (without extension)\n        log_dir: Directory to store log files\n        model_info: Information about the model being used\n        append: Whether to append to existing file\n    \"\"\"\n    self.filename = filename\n    self.log_dir = Path(log_dir)\n    self.log_dir.mkdir(parents=True, exist_ok=True)\n    self.file_path = self.log_dir / f\"{filename}.html\"\n    self.model_info = model_info\n    self.entries: List[Dict[str, Any]] = []\n    self.entry_counter = 0\n    self.tool_counter = 0\n\n    # Logger for errors\n    self.logger = setup_logger(__name__)\n\n    if not append or not self.file_path.exists():\n        self._write_header()\n</code></pre>"},{"location":"reference/utils/html_logger/#langroid.utils.html_logger.HTMLLogger.log","title":"<code>log(fields)</code>","text":"<p>Log a message entry.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>BaseModel</code> <p>ChatDocLoggerFields containing all log information</p> required Source code in <code>langroid/utils/html_logger.py</code> <pre><code>def log(self, fields: BaseModel) -&gt; None:\n    \"\"\"Log a message entry.\n\n    Args:\n        fields: ChatDocLoggerFields containing all log information\n    \"\"\"\n    try:\n        entry_html = self._format_entry(fields)\n        self._append_to_file(entry_html)\n        self.entry_counter += 1\n    except Exception as e:\n        self.logger.error(f\"Failed to log entry: {e}\")\n</code></pre>"},{"location":"reference/utils/html_logger/#langroid.utils.html_logger.HTMLLogger.close","title":"<code>close()</code>","text":"<p>Close the HTML file with footer.</p> Source code in <code>langroid/utils/html_logger.py</code> <pre><code>    def close(self) -&gt; None:\n        \"\"\"Close the HTML file with footer.\"\"\"\n        footer = \"\"\"\n    &lt;/div&gt;\n    &lt;script&gt;\n        // Update message count\n        const header = document.querySelector('.header-line div:last-child');\n        if (header) {\n            const messageCount = document.querySelectorAll('.entry').length;\n            header.textContent = header.textContent.replace(\n                /\\\\d+ messages/, messageCount + ' messages'\n            );\n        }\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\"\"\"\n        try:\n            with open(self.file_path, \"a\", encoding=\"utf-8\") as f:\n                f.write(footer)\n        except Exception as e:\n            self.logger.error(f\"Failed to write HTML footer: {e}\")\n</code></pre>"},{"location":"reference/utils/logging/","title":"logging","text":"<p>langroid/utils/logging.py </p>"},{"location":"reference/utils/logging/#langroid.utils.logging.RichFileLogger","title":"<code>RichFileLogger(log_file, append=False, color=True)</code>","text":"<p>Singleton-per-path, ref-counted, thread-safe file logger.</p> <p>\u2022 Any number of calls to <code>RichFileLogger(path)</code> yield the same object. \u2022 A per-instance lock guarantees that the underlying file is opened only   once, even when many threads construct the logger concurrently. \u2022 A reference counter tracks how many parts of the program are using the   logger; the FD is closed only when the counter reaches zero. \u2022 All writes are serialised with a dedicated write-lock.</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def __init__(self, log_file: str, append: bool = False, color: bool = True) -&gt; None:\n    # Double-checked locking: perform heavy init exactly once.\n    if getattr(self, \"_init_done\", False):\n        return\n\n    if not hasattr(self, \"_init_lock\"):\n        self._init_lock: threading.Lock = threading.Lock()\n\n    with self._init_lock:\n        if getattr(self, \"_init_done\", False):\n            return\n\n        os.makedirs(os.path.dirname(log_file), exist_ok=True)\n        mode = \"a\" if append else \"w\"\n        self._owns_file: bool = True\n        try:\n            self.file = open(log_file, mode, buffering=1, encoding=\"utf-8\")\n        except OSError as exc:  # EMFILE: too many open files\n            if exc.errno == 24:\n                # Fallback: reuse an already-open stream to avoid creating a new FD\n                self.file = sys.stderr\n                self._owns_file = False\n            else:\n                raise\n        self.log_file: str = log_file\n        self.color: bool = color\n        self.console: Console | None = (\n            Console(file=self.file, force_terminal=True, width=200)\n            if color\n            else None\n        )\n        self._write_lock = threading.Lock()\n        self._init_done = True  # set last\n</code></pre>"},{"location":"reference/utils/logging/#langroid.utils.logging.RichFileLogger.log","title":"<code>log(message)</code>","text":"<p>Thread-safe write to the log file.</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>@no_type_check\ndef log(self, message: str) -&gt; None:\n    \"\"\"Thread-safe write to the log file.\"\"\"\n    with self._write_lock:\n        if self.color and self.console is not None:\n            self.console.print(escape(message))\n        else:\n            print(message, file=self.file)\n        self.file.flush()\n</code></pre>"},{"location":"reference/utils/logging/#langroid.utils.logging.RichFileLogger.close","title":"<code>close()</code>","text":"<p>Decrease ref-count; close FD only when last user is done.</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Decrease ref-count; close FD only when last user is done.\"\"\"\n    with self._class_lock:\n        count = self._ref_counts.get(self.log_file, 0) - 1\n        if count &lt;= 0:\n            self._ref_counts.pop(self.log_file, None)\n            self._instances.pop(self.log_file, None)\n            with self._write_lock:\n                if self._owns_file and not self.file.closed:\n                    self.file.close()\n        else:\n            self._ref_counts[self.log_file] = count\n</code></pre>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_logger","title":"<code>setup_logger(name, level=logging.INFO, terminal=False)</code>","text":"<p>Set up a logger of module <code>name</code> at a desired level. Args:     name: module name     level: desired logging level Returns:     logger</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_logger(\n    name: str,\n    level: int = logging.INFO,\n    terminal: bool = False,\n) -&gt; logging.Logger:\n    \"\"\"\n    Set up a logger of module `name` at a desired level.\n    Args:\n        name: module name\n        level: desired logging level\n    Returns:\n        logger\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    if not logger.hasHandlers() and terminal:\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n        )\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n    return logger\n</code></pre>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_loggers_for_package","title":"<code>setup_loggers_for_package(package_name, level)</code>","text":"<p>Set up loggers for all modules in a package. This ensures that log-levels of modules outside the package are not affected. Args:     package_name: main package name     level: desired logging level Returns:</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_loggers_for_package(package_name: str, level: int) -&gt; None:\n    \"\"\"\n    Set up loggers for all modules in a package.\n    This ensures that log-levels of modules outside the package are not affected.\n    Args:\n        package_name: main package name\n        level: desired logging level\n    Returns:\n    \"\"\"\n    import importlib\n    import pkgutil\n\n    package = importlib.import_module(package_name)\n    for _, module_name, _ in pkgutil.walk_packages(\n        package.__path__, package.__name__ + \".\"\n    ):\n        module = importlib.import_module(module_name)\n        setup_logger(module.__name__, level)\n</code></pre>"},{"location":"reference/utils/object_registry/","title":"object_registry","text":"<p>langroid/utils/object_registry.py </p>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry","title":"<code>ObjectRegistry</code>","text":"<p>A global registry to hold id -&gt; object mappings.</p>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.add","title":"<code>add(obj)</code>  <code>classmethod</code>","text":"<p>Adds an object to the registry, returning the object's ID.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef add(cls, obj: ObjWithId) -&gt; str:\n    \"\"\"Adds an object to the registry, returning the object's ID.\"\"\"\n    object_id = obj.id() if callable(obj.id) else obj.id\n    cls.registry[object_id] = obj\n    return object_id\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.get","title":"<code>get(obj_id)</code>  <code>classmethod</code>","text":"<p>Retrieves an object by ID if it still exists.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef get(cls, obj_id: str) -&gt; Optional[ObjWithId]:\n    \"\"\"Retrieves an object by ID if it still exists.\"\"\"\n    return cls.registry.get(obj_id)\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.register_object","title":"<code>register_object(obj)</code>  <code>classmethod</code>","text":"<p>Registers an object in the registry, returning the object's ID.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef register_object(cls, obj: ObjWithId) -&gt; str:\n    \"\"\"Registers an object in the registry, returning the object's ID.\"\"\"\n    return cls.add(obj)\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.remove","title":"<code>remove(obj_id)</code>  <code>classmethod</code>","text":"<p>Removes an object from the registry.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef remove(cls, obj_id: str) -&gt; None:\n    \"\"\"Removes an object from the registry.\"\"\"\n    if obj_id in cls.registry:\n        del cls.registry[obj_id]\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.cleanup","title":"<code>cleanup()</code>  <code>classmethod</code>","text":"<p>Cleans up the registry by removing entries where the object is None.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@classmethod\ndef cleanup(cls) -&gt; None:\n    \"\"\"Cleans up the registry by removing entries where the object is None.\"\"\"\n    to_remove = [key for key, value in cls.registry.items() if value is None]\n    for key in to_remove:\n        del cls.registry[key]\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.ObjectRegistry.new_id","title":"<code>new_id()</code>  <code>staticmethod</code>","text":"<p>Generates a new unique ID.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>@staticmethod\ndef new_id() -&gt; str:\n    \"\"\"Generates a new unique ID.\"\"\"\n    return str(uuid4())\n</code></pre>"},{"location":"reference/utils/object_registry/#langroid.utils.object_registry.scheduled_cleanup","title":"<code>scheduled_cleanup(interval=600)</code>","text":"<p>Periodically cleans up the global registry every 'interval' seconds.</p> Source code in <code>langroid/utils/object_registry.py</code> <pre><code>def scheduled_cleanup(interval: int = 600) -&gt; None:\n    \"\"\"Periodically cleans up the global registry every 'interval' seconds.\"\"\"\n    while True:\n        ObjectRegistry.cleanup()\n        time.sleep(interval)\n</code></pre>"},{"location":"reference/utils/pandas_utils/","title":"pandas_utils","text":"<p>langroid/utils/pandas_utils.py </p>"},{"location":"reference/utils/pandas_utils/#langroid.utils.pandas_utils.UnsafeCommandError","title":"<code>UnsafeCommandError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when a command string violates security policy.</p>"},{"location":"reference/utils/pandas_utils/#langroid.utils.pandas_utils.CommandValidator","title":"<code>CommandValidator(df_name='df')</code>","text":"<p>               Bases: <code>NodeVisitor</code></p> <p>AST walker that enforces the security policy.</p> Source code in <code>langroid/utils/pandas_utils.py</code> <pre><code>def __init__(self, df_name: str = \"df\"):\n    self.df_name = df_name\n    self.depth = 0\n    self.chain = 0\n</code></pre>"},{"location":"reference/utils/pandas_utils/#langroid.utils.pandas_utils.sanitize_command","title":"<code>sanitize_command(expr, df_name='df')</code>","text":"<p>Validate expr; return it unchanged if it passes all rules, else raise UnsafeCommandError with the first violation encountered.</p> Source code in <code>langroid/utils/pandas_utils.py</code> <pre><code>def sanitize_command(expr: str, df_name: str = \"df\") -&gt; str:\n    \"\"\"\n    Validate *expr*; return it unchanged if it passes all rules,\n    else raise UnsafeCommandError with the first violation encountered.\n    \"\"\"\n    tree = ast.parse(expr, mode=\"eval\")\n    CommandValidator(df_name).visit(tree)\n    return expr\n</code></pre>"},{"location":"reference/utils/pydantic_utils/","title":"pydantic_utils","text":"<p>langroid/utils/pydantic_utils.py </p>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.flatten_dict","title":"<code>flatten_dict(d, parent_key='', sep='.')</code>","text":"<p>Flatten a nested dictionary, using a separator in the keys. Useful for pydantic_v1 models with nested fields -- first use     dct = mdl.model_dump() to get a nested dictionary, then use this function to flatten it.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def flatten_dict(\n    d: MutableMapping[str, Any], parent_key: str = \"\", sep: str = \".\"\n) -&gt; Dict[str, Any]:\n    \"\"\"Flatten a nested dictionary, using a separator in the keys.\n    Useful for pydantic_v1 models with nested fields -- first use\n        dct = mdl.model_dump()\n    to get a nested dictionary, then use this function to flatten it.\n    \"\"\"\n    items: List[Tuple[str, Any]] = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, MutableMapping):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.has_field","title":"<code>has_field(model_class, field_name)</code>","text":"<p>Check if a Pydantic model class has a field with the given name.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def has_field(model_class: Type[BaseModel], field_name: str) -&gt; bool:\n    \"\"\"Check if a Pydantic model class has a field with the given name.\"\"\"\n    return field_name in model_class.model_fields\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.flatten_pydantic_model","title":"<code>flatten_pydantic_model(model, base_model=BaseModel)</code>","text":"<p>Given a possibly nested Pydantic class, return a flattened version of it, by constructing top-level fields, whose names are formed from the path through the nested structure, separated by double underscores.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model to flatten.</p> required <code>base_model</code> <code>Type[BaseModel]</code> <p>The base model to use for the flattened model. Defaults to BaseModel.</p> <code>BaseModel</code> <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>Type[BaseModel]: The flattened Pydantic model.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def flatten_pydantic_model(\n    model: Type[BaseModel],\n    base_model: Type[BaseModel] = BaseModel,\n) -&gt; Type[BaseModel]:\n    \"\"\"\n    Given a possibly nested Pydantic class, return a flattened version of it,\n    by constructing top-level fields, whose names are formed from the path\n    through the nested structure, separated by double underscores.\n\n    Args:\n        model (Type[BaseModel]): The Pydantic model to flatten.\n        base_model (Type[BaseModel], optional): The base model to use for the\n            flattened model. Defaults to BaseModel.\n\n    Returns:\n        Type[BaseModel]: The flattened Pydantic model.\n    \"\"\"\n\n    flattened_fields: Dict[str, Any] = {}\n    models_to_process = [(model, \"\")]\n\n    while models_to_process:\n        current_model, current_prefix = models_to_process.pop()\n\n        for name, field in current_model.model_fields.items():\n            field_type = field.annotation if hasattr(field, \"annotation\") else field\n            if isinstance(field_type, type) and issubclass(field_type, BaseModel):\n                new_prefix = (\n                    f\"{current_prefix}{name}__\" if current_prefix else f\"{name}__\"\n                )\n                models_to_process.append((field_type, new_prefix))\n            else:\n                flattened_name = f\"{current_prefix}{name}\"\n\n                if (\n                    hasattr(field, \"default_factory\")\n                    and field.default_factory is not None\n                ):\n                    flattened_fields[flattened_name] = (\n                        field_type,\n                        field.default_factory,\n                    )\n                elif hasattr(field, \"default\") and field.default is not ...:\n                    flattened_fields[flattened_name] = (\n                        field_type,\n                        field.default,\n                    )\n                else:\n                    flattened_fields[flattened_name] = (field_type, ...)\n\n    return create_model(\"FlatModel\", __base__=base_model, **flattened_fields)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.get_field_names","title":"<code>get_field_names(model)</code>","text":"<p>Get all field names from a possibly nested Pydantic model.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def get_field_names(model: Type[BaseModel]) -&gt; List[str]:\n    \"\"\"Get all field names from a possibly nested Pydantic model.\"\"\"\n    mdl = flatten_pydantic_model(model)\n    fields = list(mdl.model_fields.keys())\n    # fields may be like a__b__c , so we only want the last part\n    return [f.split(\"__\")[-1] for f in fields]\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.generate_simple_schema","title":"<code>generate_simple_schema(model, exclude=[])</code>","text":"<p>Generates a JSON schema for a Pydantic model, with options to exclude specific fields.</p> <p>This function traverses the Pydantic model's fields, including nested models, to generate a dictionary representing the JSON schema. Fields specified in the exclude list will not be included in the generated schema.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model class to generate the schema for.</p> required <code>exclude</code> <code>List[str]</code> <p>A list of string field names to be excluded from the                  generated schema. Defaults to an empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary representing the JSON schema of the provided model,             with specified fields excluded.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def generate_simple_schema(\n    model: Type[BaseModel], exclude: List[str] = []\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generates a JSON schema for a Pydantic model,\n    with options to exclude specific fields.\n\n    This function traverses the Pydantic model's fields, including nested models,\n    to generate a dictionary representing the JSON schema. Fields specified in\n    the exclude list will not be included in the generated schema.\n\n    Args:\n        model (Type[BaseModel]): The Pydantic model class to generate the schema for.\n        exclude (List[str]): A list of string field names to be excluded from the\n                             generated schema. Defaults to an empty list.\n\n    Returns:\n        Dict[str, Any]: A dictionary representing the JSON schema of the provided model,\n                        with specified fields excluded.\n    \"\"\"\n    if hasattr(model, \"model_fields\"):\n        output: Dict[str, Any] = {}\n        for field_name, field in model.model_fields.items():\n            if field_name in exclude:\n                continue  # Skip excluded fields\n\n            field_type = field.annotation if hasattr(field, \"annotation\") else field\n            if isinstance(field_type, type) and issubclass(field_type, BaseModel):\n                # Recursively generate schema for nested models\n                output[field_name] = generate_simple_schema(field_type, exclude)\n            elif field_type is not None and hasattr(field_type, \"__name__\"):\n                # Represent the type as a string here\n                output[field_name] = {\"type\": field_type.__name__}\n            else:\n                # Fallback for complex types\n                output[field_name] = {\"type\": str(field_type)}\n        return output\n    else:\n        # Non-model type, return a simplified representation\n        return {\"type\": model.__name__}\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.flatten_pydantic_instance","title":"<code>flatten_pydantic_instance(instance, prefix='', force_str=False)</code>","text":"<p>Given a possibly nested Pydantic instance, return a flattened version of it, as a dict where nested traversal paths are translated to keys a__b__c.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>BaseModel</code> <p>The Pydantic instance to flatten.</p> required <code>prefix</code> <code>str</code> <p>The prefix to use for the top-level fields.</p> <code>''</code> <code>force_str</code> <code>bool</code> <p>Whether to force all values to be strings.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The flattened dict.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def flatten_pydantic_instance(\n    instance: BaseModel,\n    prefix: str = \"\",\n    force_str: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Given a possibly nested Pydantic instance, return a flattened version of it,\n    as a dict where nested traversal paths are translated to keys a__b__c.\n\n    Args:\n        instance (BaseModel): The Pydantic instance to flatten.\n        prefix (str, optional): The prefix to use for the top-level fields.\n        force_str (bool, optional): Whether to force all values to be strings.\n\n    Returns:\n        Dict[str, Any]: The flattened dict.\n\n    \"\"\"\n    flat_data: Dict[str, Any] = {}\n    for name, value in instance.model_dump().items():\n        # Assuming nested pydantic model will be a dict here\n        if isinstance(value, dict):\n            # Get field info from model_fields\n            field_info = instance.model_fields[name]\n            # Try to get the nested model type from field annotation\n            field_type = (\n                field_info.annotation if hasattr(field_info, \"annotation\") else None\n            )\n            if (\n                field_type\n                and isinstance(field_type, type)\n                and issubclass(field_type, BaseModel)\n            ):\n                nested_flat_data = flatten_pydantic_instance(\n                    field_type(**value),\n                    prefix=f\"{prefix}{name}__\",\n                    force_str=force_str,\n                )\n            else:\n                # Skip non-Pydantic nested fields for safety\n                continue\n            flat_data.update(nested_flat_data)\n        else:\n            flat_data[f\"{prefix}{name}\"] = str(value) if force_str else value\n    return flat_data\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.extract_fields","title":"<code>extract_fields(doc, fields)</code>","text":"<p>Extract specified fields from a Pydantic object. Supports dotted field names, e.g. \"metadata.author\". Dotted fields are matched exactly according to the corresponding path. Non-dotted fields are matched against the last part of the path. Clashes ignored. Args:     doc (BaseModel): The Pydantic object.     fields (List[str]): The list of fields to extract.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary of field names and values.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def extract_fields(doc: BaseModel, fields: List[str]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract specified fields from a Pydantic object.\n    Supports dotted field names, e.g. \"metadata.author\".\n    Dotted fields are matched exactly according to the corresponding path.\n    Non-dotted fields are matched against the last part of the path.\n    Clashes ignored.\n    Args:\n        doc (BaseModel): The Pydantic object.\n        fields (List[str]): The list of fields to extract.\n\n    Returns:\n        Dict[str, Any]: A dictionary of field names and values.\n\n    \"\"\"\n\n    def get_value(obj: BaseModel, path: str) -&gt; Any | None:\n        for part in path.split(\".\"):\n            if hasattr(obj, part):\n                obj = getattr(obj, part)\n            else:\n                return None\n        return obj\n\n    def traverse(obj: BaseModel, result: Dict[str, Any], prefix: str = \"\") -&gt; None:\n        for k, v in obj.__dict__.items():\n            key = f\"{prefix}.{k}\" if prefix else k\n            if isinstance(v, BaseModel):\n                traverse(v, result, key)\n            else:\n                result[key] = v\n\n    result: Dict[str, Any] = {}\n\n    # Extract values for dotted field names and use last part as key\n    for field in fields:\n        if \".\" in field:\n            value = get_value(doc, field)\n            if value is not None:\n                key = field.split(\".\")[-1]\n                result[key] = value\n\n    # Traverse the object to get non-dotted fields\n    all_fields: Dict[str, Any] = {}\n    traverse(doc, all_fields)\n\n    # Add non-dotted fields to the result.\n    # Prefer top-level attributes (e.g. doc.title) over nested ones\n    # (e.g. metadata.title) to avoid default metadata values overwriting\n    # real top-level fields.\n    for field in [f for f in fields if \".\" not in f]:\n        if hasattr(doc, field):\n            direct_val = getattr(doc, field)\n            if direct_val is not None:\n                result[field] = direct_val\n                continue\n        if field in result:\n            continue\n        for key, value in all_fields.items():\n            if key.split(\".\")[-1] == field and field not in result:\n                result[field] = value\n\n    return result\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.nested_dict_from_flat","title":"<code>nested_dict_from_flat(flat_data, sub_dict='')</code>","text":"<p>Given a flattened version of a nested dict, reconstruct the nested dict. Field names in the flattened dict are assumed to be of the form \"field1__field2__field3\", going from top level down.</p> <p>Parameters:</p> Name Type Description Default <code>flat_data</code> <code>Dict[str, Any]</code> <p>The flattened dict.</p> required <code>sub_dict</code> <code>str</code> <p>The name of the sub-dict to extract from the flattened dict. Defaults to \"\" (extract the whole dict).</p> <code>''</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The nested dict.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def nested_dict_from_flat(\n    flat_data: Dict[str, Any],\n    sub_dict: str = \"\",\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Given a flattened version of a nested dict, reconstruct the nested dict.\n    Field names in the flattened dict are assumed to be of the form\n    \"field1__field2__field3\", going from top level down.\n\n    Args:\n        flat_data (Dict[str, Any]): The flattened dict.\n        sub_dict (str, optional): The name of the sub-dict to extract from the\n            flattened dict. Defaults to \"\" (extract the whole dict).\n\n    Returns:\n        Dict[str, Any]: The nested dict.\n\n    \"\"\"\n    nested_data: Dict[str, Any] = {}\n    for key, value in flat_data.items():\n        if sub_dict != \"\" and not key.startswith(sub_dict + \"__\"):\n            continue\n        keys = key.split(\"__\")\n        d = nested_data\n        for k in keys[:-1]:\n            d = d.setdefault(k, {})\n        d[keys[-1]] = value\n    if sub_dict != \"\":  # e.g. \"payload\"\n        nested_data = nested_data[sub_dict]\n    return nested_data\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.pydantic_obj_from_flat_dict","title":"<code>pydantic_obj_from_flat_dict(flat_data, model, sub_dict='')</code>","text":"<p>Flattened dict with a__b__c style keys -&gt; nested dict -&gt; pydantic object</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def pydantic_obj_from_flat_dict(\n    flat_data: Dict[str, Any],\n    model: Type[BaseModel],\n    sub_dict: str = \"\",\n) -&gt; BaseModel:\n    \"\"\"Flattened dict with a__b__c style keys -&gt; nested dict -&gt; pydantic object\"\"\"\n    nested_data = nested_dict_from_flat(flat_data, sub_dict)\n    return model(**nested_data)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.temp_params","title":"<code>temp_params(config, field, temp)</code>","text":"<p>Context manager to temporarily override <code>field</code> in a <code>config</code></p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>@contextmanager\ndef temp_params(config: T, field: str, temp: T) -&gt; Generator[None, None, None]:\n    \"\"\"Context manager to temporarily override `field` in a `config`\"\"\"\n    original_vals = getattr(config, field)\n    try:\n        # Apply temporary settings\n        setattr(config, field, temp)\n        yield\n    finally:\n        # Revert to original settings\n        setattr(config, field, original_vals)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.numpy_to_python_type","title":"<code>numpy_to_python_type(numpy_type)</code>","text":"<p>Converts a numpy data type to its Python equivalent.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def numpy_to_python_type(numpy_type: Type[Any]) -&gt; Type[Any]:\n    \"\"\"Converts a numpy data type to its Python equivalent.\"\"\"\n    type_mapping = {\n        np.float64: float,\n        np.float32: float,\n        np.int64: int,\n        np.int32: int,\n        np.bool_: bool,\n        # Add other numpy types as necessary\n    }\n    return type_mapping.get(numpy_type, numpy_type)\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.dataframe_to_pydantic_model","title":"<code>dataframe_to_pydantic_model(df)</code>","text":"<p>Make a Pydantic model from a dataframe.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def dataframe_to_pydantic_model(df: pd.DataFrame) -&gt; Type[BaseModel]:\n    \"\"\"Make a Pydantic model from a dataframe.\"\"\"\n    fields = {col: (type(df[col].iloc[0]), ...) for col in df.columns}\n    return create_model(\"DataFrameModel\", __base__=BaseModel, **fields)  # type: ignore\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.dataframe_to_pydantic_objects","title":"<code>dataframe_to_pydantic_objects(df)</code>","text":"<p>Make a list of Pydantic objects from a dataframe.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def dataframe_to_pydantic_objects(df: pd.DataFrame) -&gt; List[BaseModel]:\n    \"\"\"Make a list of Pydantic objects from a dataframe.\"\"\"\n    Model = dataframe_to_pydantic_model(df)\n    return [Model(**row.to_dict()) for index, row in df.iterrows()]\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.first_non_null","title":"<code>first_non_null(series)</code>","text":"<p>Find the first non-null item in a pandas Series.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def first_non_null(series: pd.Series) -&gt; Any | None:\n    \"\"\"Find the first non-null item in a pandas Series.\"\"\"\n    for item in series:\n        if item is not None:\n            return item\n    return None\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.dataframe_to_document_model","title":"<code>dataframe_to_document_model(df, content='content', metadata=[], exclude=[])</code>","text":"<p>Make a subclass of Document from a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe.</p> required <code>content</code> <code>str</code> <p>The name of the column containing the content, which will map to the Document.content field.</p> <code>'content'</code> <code>metadata</code> <code>List[str]</code> <p>A list of column names containing metadata; these will be included in the Document.metadata field.</p> <code>[]</code> <code>exclude</code> <code>List[str]</code> <p>A list of column names to exclude from the model. (e.g. \"vector\" when lance is used to add an embedding vector to the df)</p> <code>[]</code> <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>Type[BaseModel]: A pydantic model subclassing Document.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def dataframe_to_document_model(\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n    exclude: List[str] = [],\n) -&gt; Type[BaseModel]:\n    \"\"\"\n    Make a subclass of Document from a dataframe.\n\n    Args:\n        df (pd.DataFrame): The dataframe.\n        content (str): The name of the column containing the content,\n            which will map to the Document.content field.\n        metadata (List[str]): A list of column names containing metadata;\n            these will be included in the Document.metadata field.\n        exclude (List[str]): A list of column names to exclude from the model.\n            (e.g. \"vector\" when lance is used to add an embedding vector to the df)\n\n    Returns:\n        Type[BaseModel]: A pydantic model subclassing Document.\n    \"\"\"\n\n    # Remove excluded columns\n    df = df.drop(columns=exclude, inplace=False)\n    # Check if metadata_cols is empty\n\n    if metadata:\n        # Define fields for the dynamic subclass of DocMetaData\n        metadata_fields = {\n            col: (\n                Optional[numpy_to_python_type(type(first_non_null(df[col])))],\n                None,  # Optional[numpy_to_python_type(type(first_non_null(df[col])))],\n            )\n            for col in metadata\n        }\n        DynamicMetaData = create_model(  # type: ignore\n            \"DynamicMetaData\", __base__=DocMetaData, **metadata_fields\n        )\n    else:\n        # Use the base DocMetaData class directly\n        DynamicMetaData = DocMetaData\n\n    # Define additional top-level fields for DynamicDocument\n    additional_fields = {\n        col: (\n            Optional[numpy_to_python_type(type(first_non_null(df[col])))],\n            None,  # Optional[numpy_to_python_type(type(first_non_null(df[col])))],\n        )\n        for col in df.columns\n        if col not in metadata and col != content\n    }\n\n    # Create a dynamic subclass of Document\n    DynamicDocumentFields = {\n        **{\"metadata\": (DynamicMetaData, ...)},\n        **additional_fields,\n    }\n    DynamicDocument = create_model(  # type: ignore\n        \"DynamicDocument\", __base__=Document, **DynamicDocumentFields\n    )\n\n    def from_df_row(\n        cls: type[BaseModel],\n        row: pd.Series,\n        content: str = \"content\",\n        metadata: List[str] = [],\n    ) -&gt; BaseModel | None:\n        content_val = row[content] if (content and content in row) else \"\"\n        metadata_values = (\n            {col: row[col] for col in metadata if col in row} if metadata else {}\n        )\n        additional_values = {\n            col: row[col] for col in additional_fields if col in row and col != content\n        }\n        metadata = DynamicMetaData(**metadata_values)\n        return cls(content=content_val, metadata=metadata, **additional_values)\n\n    # Bind the method to the class\n    DynamicDocument.from_df_row = classmethod(from_df_row)\n\n    return DynamicDocument  # type: ignore\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.dataframe_to_documents","title":"<code>dataframe_to_documents(df, content='content', metadata=[], doc_cls=None)</code>","text":"<p>Make a list of Document objects from a dataframe. Args:     df (pd.DataFrame): The dataframe.     content (str): The name of the column containing the content,         which will map to the Document.content field.     metadata (List[str]): A list of column names containing metadata;         these will be included in the Document.metadata field.     doc_cls (Type[BaseModel], optional): A Pydantic model subclassing         Document. Defaults to None. Returns:     List[Document]: The list of Document objects.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def dataframe_to_documents(\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n    doc_cls: Type[BaseModel] | None = None,\n) -&gt; List[Document]:\n    \"\"\"\n    Make a list of Document objects from a dataframe.\n    Args:\n        df (pd.DataFrame): The dataframe.\n        content (str): The name of the column containing the content,\n            which will map to the Document.content field.\n        metadata (List[str]): A list of column names containing metadata;\n            these will be included in the Document.metadata field.\n        doc_cls (Type[BaseModel], optional): A Pydantic model subclassing\n            Document. Defaults to None.\n    Returns:\n        List[Document]: The list of Document objects.\n    \"\"\"\n    Model = doc_cls or dataframe_to_document_model(df, content, metadata)\n    docs = [\n        Model.from_df_row(row, content, metadata)  # type: ignore\n        for _, row in df.iterrows()\n    ]\n    return [m for m in docs if m is not None]\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.extra_metadata","title":"<code>extra_metadata(document, doc_cls=Document)</code>","text":"<p>Checks for extra fields in a document's metadata that are not defined in the original metadata schema.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>The document instance to check for extra fields.</p> required <code>doc_cls</code> <code>Type[Document]</code> <p>The class type derived from Document, used as a reference to identify extra fields in the document's metadata.</p> <code>Document</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings representing the keys of the extra fields found</p> <code>List[str]</code> <p>in the document's metadata.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def extra_metadata(document: Document, doc_cls: Type[Document] = Document) -&gt; List[str]:\n    \"\"\"\n    Checks for extra fields in a document's metadata that are not defined in the\n    original metadata schema.\n\n    Args:\n        document (Document): The document instance to check for extra fields.\n        doc_cls (Type[Document]): The class type derived from Document, used\n            as a reference to identify extra fields in the document's metadata.\n\n    Returns:\n        List[str]: A list of strings representing the keys of the extra fields found\n        in the document's metadata.\n    \"\"\"\n    # Convert metadata to dict, including extra fields.\n    metadata_fields = set(document.metadata.model_dump().keys())\n\n    # Get defined fields in the metadata of doc_cls\n    metadata_field = doc_cls.model_fields[\"metadata\"]\n    metadata_type = (\n        metadata_field.annotation\n        if hasattr(metadata_field, \"annotation\")\n        else metadata_field\n    )\n    if isinstance(metadata_type, type) and hasattr(metadata_type, \"model_fields\"):\n        defined_fields = set(metadata_type.model_fields.keys())\n    else:\n        defined_fields = set()\n\n    # Identify extra fields not in defined fields.\n    extra_fields = list(metadata_fields - defined_fields)\n\n    return extra_fields\n</code></pre>"},{"location":"reference/utils/pydantic_utils/#langroid.utils.pydantic_utils.extend_document_class","title":"<code>extend_document_class(d)</code>","text":"<p>Generates a new pydantic class based on a given document instance.</p> <p>This function dynamically creates a new pydantic class with additional fields based on the \"extra\" metadata fields present in the given document instance. The new class is a subclass of the original Document class, with the original metadata fields retained and extra fields added as normal fields to the metadata.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>Document</code> <p>An instance of the Document class.</p> required <p>Returns:</p> Type Description <code>Type[Document]</code> <p>A new subclass of the Document class that includes the additional fields</p> <code>Type[Document]</code> <p>found in the metadata of the given document instance.</p> Source code in <code>langroid/utils/pydantic_utils.py</code> <pre><code>def extend_document_class(d: Document) -&gt; Type[Document]:\n    \"\"\"Generates a new pydantic class based on a given document instance.\n\n    This function dynamically creates a new pydantic class with additional\n    fields based on the \"extra\" metadata fields present in the given document\n    instance. The new class is a subclass of the original Document class, with\n    the original metadata fields retained and extra fields added as normal\n    fields to the metadata.\n\n    Args:\n        d: An instance of the Document class.\n\n    Returns:\n        A new subclass of the Document class that includes the additional fields\n        found in the metadata of the given document instance.\n    \"\"\"\n    # Extract the fields from the original metadata class, including types,\n    # correctly handling special types like List[str].\n    original_metadata_fields = {\n        k: (v.annotation, ...) for k, v in DocMetaData.model_fields.items()\n    }\n    # Extract extra fields from the metadata instance with their types\n    extra_fields = {\n        k: (type(v), ...)\n        for k, v in d.metadata.__dict__.items()\n        if k not in DocMetaData.model_fields\n    }\n\n    # Combine original and extra fields for the new metadata class\n    combined_fields = {**original_metadata_fields, **extra_fields}\n\n    # Create a new metadata class with combined fields\n    NewMetadataClass = create_model(  # type: ignore\n        \"ExtendedDocMetadata\", **combined_fields, __base__=DocMetaData\n    )\n    # NewMetadataClass.__config__.arbitrary_types_allowed = True\n\n    # Create a new document class using the new metadata class\n    NewDocumentClass = create_model(\n        \"ExtendedDocument\",\n        content=(str, ...),\n        metadata=(NewMetadataClass, ...),\n        __base__=Document,\n    )\n\n    return NewDocumentClass\n</code></pre>"},{"location":"reference/utils/system/","title":"system","text":"<p>langroid/utils/system.py </p>"},{"location":"reference/utils/system/#langroid.utils.system.LazyLoad","title":"<code>LazyLoad(import_path)</code>","text":"<p>Lazy loading of modules or classes.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def __init__(self, import_path: str) -&gt; None:\n    self.import_path = import_path\n    self._target = None\n    self._is_target_loaded = False\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.rmdir","title":"<code>rmdir(path)</code>","text":"<p>Remove a directory recursively. Args:     path (str): path to directory to remove Returns:     True if a dir was removed, false otherwise. Raises error if failed to remove.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def rmdir(path: str) -&gt; bool:\n    \"\"\"\n    Remove a directory recursively.\n    Args:\n        path (str): path to directory to remove\n    Returns:\n        True if a dir was removed, false otherwise. Raises error if failed to remove.\n    \"\"\"\n    if not any([path.startswith(p) for p in DELETION_ALLOWED_PATHS]):\n        raise ValueError(\n            f\"\"\"\n        Removing Dir '{path}' not allowed. \n        Must start with one of {DELETION_ALLOWED_PATHS}\n        This is a safety measure to prevent accidental deletion of files.\n        If you are sure you want to delete this directory, please add it \n        to the `DELETION_ALLOWED_PATHS` list in langroid/utils/system.py and \n        re-run the command.\n        \"\"\"\n        )\n\n    try:\n        shutil.rmtree(path)\n    except FileNotFoundError:\n        logger.warning(f\"Directory '{path}' does not exist. No action taken.\")\n        return False\n    except Exception as e:\n        logger.error(f\"Error while removing directory '{path}': {e}\")\n    return True\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.caller_name","title":"<code>caller_name()</code>","text":"<p>Who called the function?</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def caller_name() -&gt; str:\n    \"\"\"\n    Who called the function?\n    \"\"\"\n    frame = inspect.currentframe()\n    if frame is None:\n        return \"\"\n\n    caller_frame = frame.f_back\n\n    # If there's no caller frame, the function was called from the global scope\n    if caller_frame is None:\n        return \"\"\n\n    return caller_frame.f_code.co_name\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.generate_user_id","title":"<code>generate_user_id(org='')</code>","text":"<p>Generate a unique user ID based on the username and machine name. Returns:</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def generate_user_id(org: str = \"\") -&gt; str:\n    \"\"\"\n    Generate a unique user ID based on the username and machine name.\n    Returns:\n    \"\"\"\n    # Get the username\n    username = getpass.getuser()\n\n    # Get the machine's name\n    machine_name = socket.gethostname()\n\n    org_pfx = f\"{org}_\" if org else \"\"\n\n    # Create a consistent unique ID based on the username and machine name\n    unique_string = f\"{org_pfx}{username}@{machine_name}\"\n\n    # Generate a SHA-256 hash of the unique string\n    user_id = hashlib.sha256(unique_string.encode()).hexdigest()\n\n    return user_id\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.update_hash","title":"<code>update_hash(hash=None, s='')</code>","text":"<p>Takes a SHA256 hash string and a new string, updates the hash with the new string, and returns the updated hash string.</p> <p>Parameters:</p> Name Type Description Default <code>hash</code> <code>str</code> <p>A SHA256 hash string.</p> <code>None</code> <code>s</code> <code>str</code> <p>A new string to update the hash with.</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>The updated hash in hexadecimal format.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def update_hash(hash: str | None = None, s: str = \"\") -&gt; str:\n    \"\"\"\n    Takes a SHA256 hash string and a new string, updates the hash with the new string,\n    and returns the updated hash string.\n\n    Args:\n        hash (str): A SHA256 hash string.\n        s (str): A new string to update the hash with.\n\n    Returns:\n        The updated hash in hexadecimal format.\n    \"\"\"\n    # Create a new hash object if no hash is provided\n    if hash is None:\n        hash_obj = hashlib.sha256()\n    else:\n        # Convert the hexadecimal hash string to a byte object\n        hash_bytes = bytes.fromhex(hash)\n        hash_obj = hashlib.sha256(hash_bytes)\n\n    # Update the hash with the new string\n    hash_obj.update(s.encode(\"utf-8\"))\n\n    # Return the updated hash in hexadecimal format and the original string\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.hash","title":"<code>hash(s)</code>","text":"<p>Generate a SHA256 hash of a string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to hash.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The SHA256 hash of the string.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def hash(s: str) -&gt; str:\n    \"\"\"\n    Generate a SHA256 hash of a string.\n\n    Args:\n        s (str): The string to hash.\n\n    Returns:\n        str: The SHA256 hash of the string.\n    \"\"\"\n    return update_hash(s=s)\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.generate_unique_id","title":"<code>generate_unique_id()</code>","text":"<p>Generate a unique ID using UUID4.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def generate_unique_id() -&gt; str:\n    \"\"\"Generate a unique ID using UUID4.\"\"\"\n    return str(uuid.uuid4())\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.create_file","title":"<code>create_file(filepath, content='', if_exists='overwrite')</code>","text":"<p>Create, overwrite or append to a file, with the given content at the specified filepath. If content is empty, it will simply touch to create an empty file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>The relative path of the file to be created</p> required <code>content</code> <code>str</code> <p>The content to be written to the file</p> <code>''</code> <code>if_exists</code> <code>Literal['overwrite', 'skip', 'error', 'append']</code> <p>Action to take if file exists</p> <code>'overwrite'</code> Source code in <code>langroid/utils/system.py</code> <pre><code>def create_file(\n    filepath: str | Path,\n    content: str = \"\",\n    if_exists: Literal[\"overwrite\", \"skip\", \"error\", \"append\"] = \"overwrite\",\n) -&gt; None:\n    \"\"\"\n    Create, overwrite or append to a file, with the given content\n    at the specified filepath.\n    If content is empty, it will simply touch to create an empty file.\n\n    Args:\n        filepath (str|Path): The relative path of the file to be created\n        content (str): The content to be written to the file\n        if_exists (Literal[\"overwrite\", \"skip\", \"error\", \"append\"]):\n            Action to take if file exists\n    \"\"\"\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    if filepath.exists():\n        if if_exists == \"skip\":\n            logger.warning(f\"File already exists, skipping: {filepath}\")\n            return\n        elif if_exists == \"error\":\n            raise FileExistsError(f\"File already exists: {filepath}\")\n        elif if_exists == \"append\":\n            mode = \"a\"\n        else:  # overwrite\n            mode = \"w\"\n    else:\n        mode = \"w\"\n\n    if content == \"\" and mode in [\"a\", \"w\"]:\n        filepath.touch()\n        logger.warning(f\"Empty file created: {filepath}\")\n    else:\n        # the newline = '\\n` argument is used to ensure that\n        # newlines in the content are written as actual line breaks\n        with open(filepath, mode, newline=\"\\n\") as f:\n            f.write(content)\n        action = \"appended to\" if mode == \"a\" else \"created/updated in\"\n        logger.warning(f\"Content {action}: {filepath}\")\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.read_file","title":"<code>read_file(path, line_numbers=False)</code>","text":"<p>Read the contents of a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file to be read.</p> required <code>line_numbers</code> <code>bool</code> <p>If True, prepend line numbers to each line. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The contents of the file, optionally with line numbers.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def read_file(path: str, line_numbers: bool = False) -&gt; str:\n    \"\"\"\n    Read the contents of a file.\n\n    Args:\n        path (str): The path to the file to be read.\n        line_numbers (bool, optional): If True, prepend line numbers to each line.\n            Defaults to False.\n\n    Returns:\n        str: The contents of the file, optionally with line numbers.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n    \"\"\"\n    # raise an error if the file does not exist\n    if not Path(path).exists():\n        raise FileNotFoundError(f\"File not found: {path}\")\n    file = Path(path).expanduser()\n    content = file.read_text()\n    if line_numbers:\n        lines = content.splitlines()\n        numbered_lines = [f\"{i+1}: {line}\" for i, line in enumerate(lines)]\n        return \"\\n\".join(numbered_lines)\n    return content\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.diff_files","title":"<code>diff_files(file1, file2)</code>","text":"<p>Find the diffs between two files, in unified diff format.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def diff_files(file1: str, file2: str) -&gt; str:\n    \"\"\"\n    Find the diffs between two files, in unified diff format.\n    \"\"\"\n    with open(file1, \"r\") as f1, open(file2, \"r\") as f2:\n        lines1 = f1.readlines()\n        lines2 = f2.readlines()\n\n    differ = difflib.unified_diff(lines1, lines2, fromfile=file1, tofile=file2)\n    diff_result = \"\".join(differ)\n    return diff_result\n</code></pre>"},{"location":"reference/utils/system/#langroid.utils.system.list_dir","title":"<code>list_dir(path)</code>","text":"<p>List the contents of a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of the files and directories in the specified directory.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def list_dir(path: str | Path) -&gt; list[str]:\n    \"\"\"\n    List the contents of a directory.\n\n    Args:\n        path (str): The path to the directory.\n\n    Returns:\n        list[str]: A list of the files and directories in the specified directory.\n    \"\"\"\n    dir_path = Path(path)\n    if not dir_path.is_dir():\n        raise NotADirectoryError(f\"Path is not a directory: {dir_path}\")\n    return [str(p) for p in dir_path.iterdir()]\n</code></pre>"},{"location":"reference/utils/types/","title":"types","text":"<p>langroid/utils/types.py </p>"},{"location":"reference/utils/types/#langroid.utils.types.is_instance_of","title":"<code>is_instance_of(obj, type_hint)</code>","text":"<p>Check if an object is an instance of a type hint, e.g. to check whether x is of type <code>List[ToolMessage]</code> or type <code>int</code></p> Source code in <code>langroid/utils/types.py</code> <pre><code>def is_instance_of(obj: Any, type_hint: Type[T] | Any) -&gt; bool:\n    \"\"\"\n    Check if an object is an instance of a type hint, e.g.\n    to check whether x is of type `List[ToolMessage]` or type `int`\n    \"\"\"\n    if type_hint == Any:\n        return True\n\n    if type_hint is type(obj):\n        return True\n\n    origin = get_origin(type_hint)\n    args = get_args(type_hint)\n\n    if origin is Union:\n        return any(is_instance_of(obj, arg) for arg in args)\n\n    if origin:  # e.g. List, Dict, Tuple, Set\n        if isinstance(obj, origin):\n            # check if all items in obj are of the required types\n            if args:\n                if isinstance(obj, (list, tuple, set)):\n                    return all(is_instance_of(item, args[0]) for item in obj)\n                if isinstance(obj, dict):\n                    return all(\n                        is_instance_of(k, args[0]) and is_instance_of(v, args[1])\n                        for k, v in obj.items()\n                    )\n            return True\n        else:\n            return False\n\n    return isinstance(obj, type_hint)\n</code></pre>"},{"location":"reference/utils/types/#langroid.utils.types.to_string","title":"<code>to_string(msg)</code>","text":"<p>Best-effort conversion of arbitrary msg to str. Return empty string if conversion fails.</p> Source code in <code>langroid/utils/types.py</code> <pre><code>def to_string(msg: Any) -&gt; str:\n    \"\"\"\n    Best-effort conversion of arbitrary msg to str.\n    Return empty string if conversion fails.\n    \"\"\"\n    if msg is None:\n        return \"\"\n    if isinstance(msg, str):\n        return msg\n    if isinstance(msg, BaseModel):\n        return msg.model_dump_json()\n    # last resort: use json.dumps() or str() to make it a str\n    try:\n        return json.dumps(msg)\n    except Exception:\n        try:\n            return str(msg)\n        except Exception as e:\n            logger.error(\n                f\"\"\"\n                Error converting msg to str: {e}\", \n                \"\"\",\n                exc_info=True,\n            )\n            return \"\"\n</code></pre>"},{"location":"reference/utils/types/#langroid.utils.types.is_callable","title":"<code>is_callable(obj, k=1)</code>","text":"<p>Check if object is callable and accepts exactly k args.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Object to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if object is callable with k args, False otherwise</p> Source code in <code>langroid/utils/types.py</code> <pre><code>def is_callable(obj: Any, k: int = 1) -&gt; bool:\n    \"\"\"Check if object is callable and accepts exactly k args.\n\n    Args:\n        obj: Object to check\n\n    Returns:\n        bool: True if object is callable with k args, False otherwise\n    \"\"\"\n    if not callable(obj):\n        return False\n    try:\n        sig = signature(obj)\n        params = list(sig.parameters.values())\n        return len(params) == k\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/utils/algorithms/","title":"algorithms","text":"<p>langroid/utils/algorithms/init.py </p>"},{"location":"reference/utils/algorithms/graph/","title":"graph","text":"<p>langroid/utils/algorithms/graph.py </p> <p>Graph algos.</p>"},{"location":"reference/utils/algorithms/graph/#langroid.utils.algorithms.graph.topological_sort","title":"<code>topological_sort(order)</code>","text":"<p>Given a directed adjacency matrix, return a topological sort of the nodes. order[i,j] = -1 means there is an edge from i to j. order[i,j] = 0 means there is no edge from i to j. order[i,j] = 1 means there is an edge from j to i.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>array</code> <p>The adjacency matrix.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: The topological sort of the nodes.</p> Source code in <code>langroid/utils/algorithms/graph.py</code> <pre><code>@no_type_check\ndef topological_sort(order: np.array) -&gt; List[int]:\n    \"\"\"\n    Given a directed adjacency matrix, return a topological sort of the nodes.\n    order[i,j] = -1 means there is an edge from i to j.\n    order[i,j] = 0 means there is no edge from i to j.\n    order[i,j] = 1 means there is an edge from j to i.\n\n    Args:\n        order (np.array): The adjacency matrix.\n\n    Returns:\n        List[int]: The topological sort of the nodes.\n\n    \"\"\"\n    n = order.shape[0]\n\n    # Calculate the in-degrees\n    in_degree = [0] * n\n    for i in range(n):\n        for j in range(n):\n            if order[i, j] == -1:\n                in_degree[j] += 1\n\n    # Initialize the queue with nodes of in-degree 0\n    queue = [i for i in range(n) if in_degree[i] == 0]\n    result = []\n\n    while queue:\n        node = queue.pop(0)\n        result.append(node)\n\n        for i in range(n):\n            if order[node, i] == -1:\n                in_degree[i] -= 1\n                if in_degree[i] == 0:\n                    queue.append(i)\n\n    assert len(result) == n, \"Cycle detected\"\n    return result\n</code></pre>"},{"location":"reference/utils/algorithms/graph/#langroid.utils.algorithms.graph.components","title":"<code>components(order)</code>","text":"<p>Find the connected components in an undirected graph represented by a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>ndarray</code> <p>A matrix with values 0 or 1 indicating undirected graph edges. <code>order[i][j] = 1</code> means an edge between <code>i</code> and <code>j</code>, and <code>0</code> means no edge.</p> required <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List[List[int]]: A list of List where each List contains the indices of nodes in the same connected component.</p> Example <p>order = np.array([     [1, 1, 0, 0],     [1, 1, 1, 0],     [0, 1, 1, 0],     [0, 0, 0, 1] ]) components(order)</p> Source code in <code>langroid/utils/algorithms/graph.py</code> <pre><code>@no_type_check\ndef components(order: np.ndarray) -&gt; List[List[int]]:\n    \"\"\"\n    Find the connected components in an undirected graph represented by a matrix.\n\n    Args:\n        order (np.ndarray): A matrix with values 0 or 1 indicating\n            undirected graph edges. `order[i][j] = 1` means an edge between `i`\n            and `j`, and `0` means no edge.\n\n    Returns:\n        List[List[int]]: A list of List where each List contains the indices of\n            nodes in the same connected component.\n\n    Example:\n        order = np.array([\n            [1, 1, 0, 0],\n            [1, 1, 1, 0],\n            [0, 1, 1, 0],\n            [0, 0, 0, 1]\n        ])\n        components(order)\n        # [[0, 1, 2], [3]]\n    \"\"\"\n\n    i2g: Dict[int, int] = {}  # index to group mapping\n    next_group = 0\n    n = order.shape[0]\n    for i in range(n):\n        connected_groups = {i2g[j] for j in np.nonzero(order[i, :])[0] if j in i2g}\n\n        # If the node is not part of any group\n        # and is not connected to any groups, assign a new group\n        if not connected_groups:\n            i2g[i] = next_group\n            next_group += 1\n        else:\n            # If the node is connected to multiple groups, we merge them\n            main_group = min(connected_groups)\n            for j in np.nonzero(order[i, :])[0]:\n                if i2g.get(j) in connected_groups:\n                    i2g[j] = main_group\n            i2g[i] = main_group\n\n    # Convert i2g to a list of Lists\n    groups: Dict[int, List[int]] = {}\n    for index, group in i2g.items():\n        if group not in groups:\n            groups[group] = []\n        groups[group].append(index)\n\n    return list(groups.values())\n</code></pre>"},{"location":"reference/utils/algorithms/graph/#langroid.utils.algorithms.graph.components--0-1-2-3","title":"[[0, 1, 2], [3]]","text":""},{"location":"reference/utils/output/","title":"output","text":"<p>langroid/utils/output/init.py </p>"},{"location":"reference/utils/output/#langroid.utils.output.PrintColored","title":"<code>PrintColored(color)</code>","text":"<p>Context to temporarily print in a desired color</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>def __init__(self, color: str):\n    self.color = color\n</code></pre>"},{"location":"reference/utils/output/citations/","title":"citations","text":"<p>langroid/utils/output/citations.py </p>"},{"location":"reference/utils/output/citations/#langroid.utils.output.citations.extract_markdown_references","title":"<code>extract_markdown_references(md_string)</code>","text":"<p>Extracts markdown references (e.g., [^1], [^2]) from a string and returns them as a sorted list of integers.</p> <p>Parameters:</p> Name Type Description Default <code>md_string</code> <code>str</code> <p>The markdown string containing references.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>list[int]: A sorted list of unique integers from the markdown references.</p> Source code in <code>langroid/utils/output/citations.py</code> <pre><code>def extract_markdown_references(md_string: str) -&gt; List[int]:\n    \"\"\"\n    Extracts markdown references (e.g., [^1], [^2]) from a string and returns\n    them as a sorted list of integers.\n\n    Args:\n        md_string (str): The markdown string containing references.\n\n    Returns:\n        list[int]: A sorted list of unique integers from the markdown references.\n    \"\"\"\n    import re\n\n    # Regex to find all occurrences of [^&lt;number&gt;]\n    matches = re.findall(r\"\\[\\^(\\d+)\\]\", md_string)\n    # Convert matches to integers, remove duplicates with set, and sort\n    return sorted(set(int(match) for match in matches))\n</code></pre>"},{"location":"reference/utils/output/citations/#langroid.utils.output.citations.invalid_markdown_citations","title":"<code>invalid_markdown_citations(md_string)</code>","text":"<p>Finds non-numeric markdown citations (e.g., [^a], [^xyz]) in a string.</p> <p>Parameters:</p> Name Type Description Default <code>md_string</code> <code>str</code> <p>The markdown string to search for invalid citations.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of invalid citation strings (without brackets/caret).</p> Source code in <code>langroid/utils/output/citations.py</code> <pre><code>def invalid_markdown_citations(md_string: str) -&gt; List[str]:\n    \"\"\"\n    Finds non-numeric markdown citations (e.g., [^a], [^xyz]) in a string.\n\n    Args:\n        md_string (str): The markdown string to search for invalid citations.\n\n    Returns:\n        List[str]: List of invalid citation strings (without brackets/caret).\n    \"\"\"\n    import re\n\n    # Find all citation references first\n    matches = re.findall(r\"\\[\\^([^\\]\\s]+)\\]\", md_string)\n\n    # Filter out purely numeric citations\n    invalid_citations = [match for match in matches if not match.isdigit()]\n\n    return sorted(set(invalid_citations))\n</code></pre>"},{"location":"reference/utils/output/citations/#langroid.utils.output.citations.format_footnote_text","title":"<code>format_footnote_text(content, width=0)</code>","text":"<p>Formats the content so that each original line is individually processed. - If width=0, no wrapping is done (lines remain as is). - If width&gt;0, lines are wrapped to that width. - Blank lines remain blank (with indentation). - Everything is indented by 4 spaces (for markdown footnotes).</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text of the footnote to be formatted.</p> required <code>width</code> <code>int</code> <p>Maximum width of the text lines. If 0, lines are not wrapped.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Properly formatted markdown footnote text.</p> Source code in <code>langroid/utils/output/citations.py</code> <pre><code>def format_footnote_text(content: str, width: int = 0) -&gt; str:\n    \"\"\"\n    Formats the content so that each original line is individually processed.\n    - If width=0, no wrapping is done (lines remain as is).\n    - If width&gt;0, lines are wrapped to that width.\n    - Blank lines remain blank (with indentation).\n    - Everything is indented by 4 spaces (for markdown footnotes).\n\n    Args:\n        content (str): The text of the footnote to be formatted.\n        width (int): Maximum width of the text lines. If 0, lines are not wrapped.\n\n    Returns:\n        str: Properly formatted markdown footnote text.\n    \"\"\"\n    import textwrap\n\n    indent = \"    \"  # 4 spaces for markdown footnotes\n    lines = content.split(\"\\n\")  # keep original line structure\n\n    output_lines = []\n    for line in lines:\n        # If the line is empty (or just spaces), keep it blank (but indented)\n        if not line.strip():\n            output_lines.append(indent)\n            continue\n\n        if width &gt; 0:\n            # Wrap each non-empty line to the specified width\n            wrapped = textwrap.wrap(line, width=width)\n            if not wrapped:\n                # If textwrap gives nothing, add a blank (indented) line\n                output_lines.append(indent)\n            else:\n                for subline in wrapped:\n                    output_lines.append(indent + subline)\n        else:\n            # No wrapping: just indent the original line\n            output_lines.append(indent + line)\n\n    # Join them with newline so we preserve the paragraph/blank line structure\n    return \"\\n\".join(output_lines)\n</code></pre>"},{"location":"reference/utils/output/citations/#langroid.utils.output.citations.format_cited_references","title":"<code>format_cited_references(citations, passages)</code>","text":"<p>Given a list of (integer) citations, and a list of passages, return a string that can be added as a footer to the main text, to show sources cited.</p> <p>Parameters:</p> Name Type Description Default <code>citations</code> <code>list[int]</code> <p>list of citations, presumably from main text</p> required <code>passages</code> <code>list[Document]</code> <p>list of passages (Document objects)</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatted string of FULL citations (i.e. reference AND content) for footnote in markdown;</p> <code>str</code> <code>str</code> <p>formatted string of BRIEF citations (i.e. reference only) for footnote in markdown.</p> Source code in <code>langroid/utils/output/citations.py</code> <pre><code>def format_cited_references(\n    citations: List[int], passages: list[Document]\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Given a list of (integer) citations, and a list of passages, return a string\n    that can be added as a footer to the main text, to show sources cited.\n\n    Args:\n        citations (list[int]): list of citations, presumably from main text\n        passages (list[Document]): list of passages (Document objects)\n\n    Returns:\n        str: formatted string of FULL citations (i.e. reference AND content)\n            for footnote in markdown;\n        str: formatted string of BRIEF citations (i.e. reference only)\n            for footnote in markdown.\n    \"\"\"\n    citations_str = \"\"\n    full_citations_str = \"\"\n    if len(citations) &gt; 0:\n        # append [i] source, content for each citation\n        good_citations = [c for c in citations if c &gt; 0 and c &lt;= len(passages)]\n        if len(good_citations) &lt; len(citations):\n            logger.warning(f\"Invalid citations: {set(citations) - set(good_citations)}\")\n\n        # source and content for each citation\n        full_citations_str = \"\\n\".join(\n            [\n                f\"[^{c}] {str(passages[c-1].metadata)}\"\n                f\"\\n{format_footnote_text(passages[c-1].content)}\"\n                for c in good_citations\n            ]\n        )\n\n        # source for each citation\n        citations_str = \"\\n\".join(\n            [f\"[^{c}] {str(passages[c-1].metadata)}\" for c in good_citations]\n        )\n    return full_citations_str, citations_str\n</code></pre>"},{"location":"reference/utils/output/printing/","title":"printing","text":"<p>langroid/utils/output/printing.py </p>"},{"location":"reference/utils/output/printing/#langroid.utils.output.printing.PrintColored","title":"<code>PrintColored(color)</code>","text":"<p>Context to temporarily print in a desired color</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>def __init__(self, color: str):\n    self.color = color\n</code></pre>"},{"location":"reference/utils/output/printing/#langroid.utils.output.printing.silence_stdout","title":"<code>silence_stdout()</code>","text":"<p>Temporarily silence all output to stdout and from rich.print.</p> <p>This context manager redirects all output written to stdout (which includes outputs from the built-in print function and rich.print) to /dev/null on UNIX-like systems or NUL on Windows. Once the context block exits, stdout is restored to its original state.</p> Example <p>with silence_stdout_and_rich():     print(\"This won't be printed\")     rich.print(\"This also won't be printed\")</p> Note <p>This suppresses both standard print functions and the rich library outputs.</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>@contextmanager\ndef silence_stdout() -&gt; Iterator[None]:\n    \"\"\"\n    Temporarily silence all output to stdout and from rich.print.\n\n    This context manager redirects all output written to stdout (which includes\n    outputs from the built-in print function and rich.print) to /dev/null on\n    UNIX-like systems or NUL on Windows. Once the context block exits, stdout is\n    restored to its original state.\n\n    Example:\n        with silence_stdout_and_rich():\n            print(\"This won't be printed\")\n            rich.print(\"This also won't be printed\")\n\n    Note:\n        This suppresses both standard print functions and the rich library outputs.\n    \"\"\"\n    platform_null = \"/dev/null\" if sys.platform != \"win32\" else \"NUL\"\n    original_stdout = sys.stdout\n    fnull = open(platform_null, \"w\")\n    sys.stdout = fnull\n    try:\n        yield\n    finally:\n        sys.stdout = original_stdout\n        fnull.close()\n</code></pre>"},{"location":"reference/utils/output/status/","title":"status","text":"<p>langroid/utils/output/status.py </p>"},{"location":"reference/utils/output/status/#langroid.utils.output.status.status","title":"<code>status(msg, log_if_quiet=True)</code>","text":"<p>Displays a rich spinner if not in quiet mode, else optionally logs the message.</p> Source code in <code>langroid/utils/output/status.py</code> <pre><code>def status(\n    msg: str,\n    log_if_quiet: bool = True,\n) -&gt; AbstractContextManager[Any]:\n    \"\"\"\n    Displays a rich spinner if not in quiet mode, else optionally logs the message.\n    \"\"\"\n    stack = ExitStack()\n    logged = False\n    if settings.quiet and log_if_quiet:\n        logged = True\n        logger.info(msg)\n\n    if not settings.quiet:\n        try:\n            stack.enter_context(console.status(msg))\n        except LiveError:\n            if not logged:\n                logger.info(msg)\n\n    # When using rich spinner, we enforce quiet mode\n    # (since output will be messy otherwise);\n    # We make an exception to this when debug is enabled.\n    stack.enter_context(quiet_mode(not settings.debug))\n\n    return stack\n</code></pre>"},{"location":"reference/vector_store/","title":"vector_store","text":"<p>langroid/vector_store/init.py </p>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore","title":"<code>VectorStore(config)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a vector store.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def __init__(self, config: VectorStoreConfig):\n    self.config = config\n    if config.embedding_model is None:\n        self.embedding_model = EmbeddingModel.create(config.embedding)\n    else:\n        self.embedding_model = config.embedding_model\n    if hasattr(self.config, \"embedding_model\"):\n        self.config.embedding_model = None\n    self.embedding_fn: EmbeddingFunction = self.embedding_model.embedding_fn()\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.clone","title":"<code>clone()</code>","text":"<p>Return a vector-store clone suitable for agent cloning.</p> <p>The default implementation deep-copies the configuration, reuses any existing embedding model, and instantiates a fresh store of the same type. Subclasses can override when sharing the instance is required (e.g., embedded/local stores that rely on file locks).</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def clone(self) -&gt; \"VectorStore\":\n    \"\"\"Return a vector-store clone suitable for agent cloning.\n\n    The default implementation deep-copies the configuration, reuses any\n    existing embedding model, and instantiates a fresh store of the same\n    type. Subclasses can override when sharing the instance is required\n    (e.g., embedded/local stores that rely on file locks).\n    \"\"\"\n\n    config_class = self.config.__class__\n    config_data = self.config.model_dump(mode=\"python\")\n    config_data[\"embedding_model\"] = None\n    config_copy = config_class.model_validate(config_data)\n    logger.debug(\n        \"Cloning VectorStore %s: original collection=%s, copied collection=%s\",\n        type(self).__name__,\n        getattr(self.config, \"collection_name\", None),\n        getattr(config_copy, \"collection_name\", None),\n    )\n    # Preserve the calculated collection contents without forcing replaces\n    if hasattr(config_copy, \"replace_collection\"):\n        config_copy.replace_collection = False  # type: ignore[attr-defined]\n    cloned_embedding: Optional[EmbeddingModel] = None\n    if (\n        hasattr(self, \"embedding_model\")\n        and getattr(self, \"embedding_model\") is not None\n    ):\n        cloned_embedding = self.embedding_model.clone()  # type: ignore[attr-defined]\n        if hasattr(config_copy, \"embedding_model\"):\n            config_copy.embedding_model = cloned_embedding\n\n    cloned_store = type(self)(config_copy)  # type: ignore[call-arg]\n    if hasattr(cloned_store.config, \"embedding_model\"):\n        cloned_store.config.embedding_model = None\n    logger.debug(\n        \"Cloned VectorStore %s: cloned collection=%s\",\n        type(self).__name__,\n        getattr(cloned_store.config, \"collection_name\", None),\n    )\n    if hasattr(cloned_store.config, \"replace_collection\"):\n        cloned_store.config.replace_collection = False\n    # Some stores might not honour replace_collection; ensure same collection\n    if getattr(self.config, \"collection_name\", None) is not None:\n        setattr(\n            cloned_store.config,\n            \"collection_name\",\n            getattr(self.config, \"collection_name\", None),\n        )\n    return cloned_store\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.clear_empty_collections","title":"<code>clear_empty_collections()</code>  <code>abstractmethod</code>","text":"<p>Clear all empty collections in the vector store. Returns the number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_empty_collections(self) -&gt; int:\n    \"\"\"Clear all empty collections in the vector store.\n    Returns the number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>  <code>abstractmethod</code>","text":"<p>Clear all collections in the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>really</code> <code>bool</code> <p>Whether to really clear all collections. Defaults to False.</p> <code>False</code> <code>prefix</code> <code>str</code> <p>Prefix of collections to clear.</p> <code>''</code> <p>Returns:     int: Number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"\n    Clear all collections in the vector store.\n\n    Args:\n        really (bool, optional): Whether to really clear all collections.\n            Defaults to False.\n        prefix (str, optional): Prefix of collections to clear.\n    Returns:\n        int: Number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.list_collections","title":"<code>list_collections(empty=False)</code>  <code>abstractmethod</code>","text":"<p>List all collections in the vector store (only non empty collections if empty=False).</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"List all collections in the vector store\n    (only non empty collections if empty=False).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.set_collection","title":"<code>set_collection(collection_name, replace=False)</code>","text":"<p>Set the current collection to the given collection name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the collection if it         already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def set_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Set the current collection to the given collection name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the collection if it\n            already exists. Defaults to False.\n    \"\"\"\n\n    self.config.collection_name = collection_name\n    self.config.replace_collection = replace\n    if replace:\n        self.create_collection(collection_name, replace=True)\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>  <code>abstractmethod</code>","text":"<p>Create a collection with the given name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the         collection if it already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"Create a collection with the given name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the\n            collection if it already exists. Defaults to False.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.compute_from_docs","title":"<code>compute_from_docs(docs, calc)</code>","text":"<p>Compute a result on a set of documents, using a dataframe calc string like <code>df.groupby('state')['income'].mean()</code>.</p> <p>If full_eval is False (default), the input expression is sanitized to prevent most common code injection attack vectors. If full_eval is True, sanitization is bypassed - use only with trusted input!</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def compute_from_docs(self, docs: List[Document], calc: str) -&gt; str:\n    \"\"\"Compute a result on a set of documents,\n    using a dataframe calc string like `df.groupby('state')['income'].mean()`.\n\n    If full_eval is False (default), the input expression is sanitized to prevent\n    most common code injection attack vectors.\n    If full_eval is True, sanitization is bypassed - use only with trusted input!\n    \"\"\"\n    # convert each doc to a dict, using dotted paths for nested fields\n    dicts = [flatten_dict(doc.model_dump(by_alias=True)) for doc in docs]\n    df = pd.DataFrame(dicts)\n\n    try:\n        # SECURITY MITIGATION: Eval input is sanitized to prevent most common\n        # code injection attack vectors when full_eval is False.\n        vars = {\"df\": df}\n        if not self.config.full_eval:\n            calc = sanitize_command(calc)\n        code = compile(calc, \"&lt;calc&gt;\", \"eval\")\n        result = eval(code, vars, {})\n    except Exception as e:\n        # return error message so LLM can fix the calc string if needed\n        err = f\"\"\"\n        Error encountered in pandas eval: {str(e)}\n        \"\"\"\n        if isinstance(e, KeyError) and \"not in index\" in str(e):\n            # Pd.eval sometimes fails on a perfectly valid exprn like\n            # df.loc[..., 'column'] with a KeyError.\n            err += \"\"\"\n            Maybe try a different way, e.g. \n            instead of df.loc[..., 'column'], try df.loc[...]['column']\n            \"\"\"\n        return err\n    return stringify(result)\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.maybe_add_ids","title":"<code>maybe_add_ids(documents)</code>","text":"<p>Add ids to metadata if absent, since some vecdbs don't like having blank ids.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def maybe_add_ids(self, documents: Sequence[Document]) -&gt; None:\n    \"\"\"Add ids to metadata if absent, since some\n    vecdbs don't like having blank ids.\"\"\"\n    for d in documents:\n        if d.metadata.id in [None, \"\"]:\n            d.metadata.id = ObjectRegistry.new_id()\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.similar_texts_with_scores","title":"<code>similar_texts_with_scores(text, k=1, where=None)</code>  <code>abstractmethod</code>","text":"<p>Find k most similar texts to the given text, in terms of vector distance metric (e.g., cosine similarity).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to find similar texts for.</p> required <code>k</code> <code>int</code> <p>Number of similar texts to retrieve. Defaults to 1.</p> <code>1</code> <code>where</code> <code>Optional[str]</code> <p>Where clause to filter the search.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef similar_texts_with_scores(\n    self,\n    text: str,\n    k: int = 1,\n    where: Optional[str] = None,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Find k most similar texts to the given text, in terms of vector distance metric\n    (e.g., cosine similarity).\n\n    Args:\n        text (str): The text to find similar texts for.\n        k (int, optional): Number of similar texts to retrieve. Defaults to 1.\n        where (Optional[str], optional): Where clause to filter the search.\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.add_context_window","title":"<code>add_context_window(docs_scores, neighbors=0)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. These window_ids may overlap, so we - coalesce each overlapping groups into a single window (maintaining ordering), - create a new document for each part, preserving metadata,</p> <p>We may have stored a longer set of window_ids than we need during chunking. Now, we just want <code>neighbors</code> on each side of the center of the window_ids list.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <code>neighbors</code> <code>int</code> <p>Number of neighbors on \"each side\" of match to retrieve. Defaults to 0. \"Each side\" here means before and after the match, in the original text.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def add_context_window(\n    self, docs_scores: List[Tuple[Document, float]], neighbors: int = 0\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk.\n    These window_ids may overlap, so we\n    - coalesce each overlapping groups into a single window (maintaining ordering),\n    - create a new document for each part, preserving metadata,\n\n    We may have stored a longer set of window_ids than we need during chunking.\n    Now, we just want `neighbors` on each side of the center of the window_ids list.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n        neighbors (int, optional): Number of neighbors on \"each side\" of match to\n            retrieve. Defaults to 0.\n            \"Each side\" here means before and after the match,\n            in the original text.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    # We return a larger context around each match, i.e.\n    # a window of `neighbors` on each side of the match.\n    docs = [d for d, s in docs_scores]\n    scores = [s for d, s in docs_scores]\n    if neighbors == 0:\n        return docs_scores\n    doc_chunks = [d for d in docs if d.metadata.is_chunk]\n    if len(doc_chunks) == 0:\n        return docs_scores\n    window_ids_list = []\n    id2metadata = {}\n    # id -&gt; highest score of a doc it appears in\n    id2max_score: Dict[int | str, float] = {}\n    for i, d in enumerate(docs):\n        window_ids = d.metadata.window_ids\n        if len(window_ids) == 0:\n            window_ids = [d.id()]\n        id2metadata.update({id: d.metadata for id in window_ids})\n\n        id2max_score.update(\n            {id: max(id2max_score.get(id, 0), scores[i]) for id in window_ids}\n        )\n        n = len(window_ids)\n        chunk_idx = window_ids.index(d.id())\n        neighbor_ids = window_ids[\n            max(0, chunk_idx - neighbors) : min(n, chunk_idx + neighbors + 1)\n        ]\n        window_ids_list += [neighbor_ids]\n\n    # window_ids could be from different docs,\n    # and they may overlap, so we coalesce overlapping groups into\n    # separate windows.\n    window_ids_list = self.remove_overlaps(window_ids_list)\n    final_docs = []\n    final_scores = []\n    for w in window_ids_list:\n        metadata = copy.deepcopy(id2metadata[w[0]])\n        metadata.window_ids = w\n        document = Document(\n            content=\"\".join([d.content for d in self.get_documents_by_ids(w)]),\n            metadata=metadata,\n        )\n        # make a fresh id since content is in general different\n        document.metadata.id = ObjectRegistry.new_id()\n        final_docs += [document]\n        final_scores += [max(id2max_score[id] for id in w)]\n    return list(zip(final_docs, final_scores))\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.remove_overlaps","title":"<code>remove_overlaps(windows)</code>  <code>staticmethod</code>","text":"<p>Given a collection of windows, where each window is a sequence of ids, identify groups of overlapping windows, and for each overlapping group, order the chunk-ids using topological sort so they appear in the original order in the text.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>List[int | str]</code> <p>List of windows, where each window is a sequence of ids.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List[int|str]: List of windows, where each window is a sequence of ids, and no two windows overlap.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@staticmethod\ndef remove_overlaps(windows: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"\n    Given a collection of windows, where each window is a sequence of ids,\n    identify groups of overlapping windows, and for each overlapping group,\n    order the chunk-ids using topological sort so they appear in the original\n    order in the text.\n\n    Args:\n        windows (List[int|str]): List of windows, where each window is a\n            sequence of ids.\n\n    Returns:\n        List[int|str]: List of windows, where each window is a sequence of ids,\n            and no two windows overlap.\n    \"\"\"\n    ids = set(id for w in windows for id in w)\n    # id -&gt; {win -&gt; # pos}\n    id2win2pos: Dict[str, Dict[int, int]] = {id: {} for id in ids}\n\n    for i, w in enumerate(windows):\n        for j, id in enumerate(w):\n            id2win2pos[id][i] = j\n\n    n = len(windows)\n    # relation between windows:\n    order = np.zeros((n, n), dtype=np.int8)\n    for i, w in enumerate(windows):\n        for j, x in enumerate(windows):\n            if i == j:\n                continue\n            if len(set(w).intersection(x)) == 0:\n                continue\n            id = list(set(w).intersection(x))[0]  # any common id\n            if id2win2pos[id][i] &gt; id2win2pos[id][j]:\n                order[i, j] = -1  # win i is before win j\n            else:\n                order[i, j] = 1  # win i is after win j\n\n    # find groups of windows that overlap, like connected components in a graph\n    groups = components(np.abs(order))\n\n    # order the chunk-ids in each group using topological sort\n    new_windows = []\n    for g in groups:\n        # find total ordering among windows in group based on order matrix\n        # (this is a topological sort)\n        _g = np.array(g)\n        order_matrix = order[_g][:, _g]\n        ordered_window_indices = topological_sort(order_matrix)\n        ordered_window_ids = [windows[i] for i in _g[ordered_window_indices]]\n        flattened = [id for w in ordered_window_ids for id in w]\n        flattened_deduped = list(dict.fromkeys(flattened))\n        # Note we are not going to split these, and instead we'll return\n        # larger windows from concatenating the connected groups.\n        # This ensures context is retained for LLM q/a\n        new_windows += [flattened_deduped]\n\n    return new_windows\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.get_all_documents","title":"<code>get_all_documents(where='')</code>  <code>abstractmethod</code>","text":"<p>Get all documents in the current collection, possibly filtered by <code>where</code>.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_all_documents(self, where: str = \"\") -&gt; List[Document]:\n    \"\"\"\n    Get all documents in the current collection, possibly filtered by `where`.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.VectorStore.get_documents_by_ids","title":"<code>get_documents_by_ids(ids)</code>  <code>abstractmethod</code>","text":"<p>Get documents by their ids. Args:     ids (List[str]): List of document ids.</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\n    \"\"\"\n    Get documents by their ids.\n    Args:\n        ids (List[str]): List of document ids.\n\n    Returns:\n        List[Document]: List of documents\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB","title":"<code>QdrantDB(config=QdrantDBConfig())</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def __init__(self, config: QdrantDBConfig = QdrantDBConfig()):\n    super().__init__(config)\n    self.config: QdrantDBConfig = config\n    from qdrant_client import QdrantClient\n\n    if self.config.use_sparse_embeddings:\n        try:\n            from transformers import AutoModelForMaskedLM, AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"\"\"\n                To use sparse embeddings,\n                you must install langroid with the [transformers] extra, e.g.:\n                pip install \"langroid[transformers]\"\n                \"\"\"\n            )\n\n        self.sparse_tokenizer = AutoTokenizer.from_pretrained(\n            self.config.sparse_embedding_model\n        )\n        self.sparse_model = AutoModelForMaskedLM.from_pretrained(\n            self.config.sparse_embedding_model\n        )\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    key = os.getenv(\"QDRANT_API_KEY\")\n    url = os.getenv(\"QDRANT_API_URL\")\n    if config.docker:\n        if url is None:\n            logger.warning(\n                f\"\"\"The QDRANT_API_URL env variable must be set to use\n                QdrantDB in local docker mode. Please set this\n                value in your .env file.\n                Switching to local storage at {config.storage_path}\n                \"\"\"\n            )\n            config.cloud = False\n        else:\n            config.cloud = True\n    elif config.cloud and None in [key, url]:\n        logger.warning(\n            f\"\"\"QDRANT_API_KEY, QDRANT_API_URL env variable must be set to use\n            QdrantDB in cloud mode. Please set these values\n            in your .env file.\n            Switching to local storage at {config.storage_path}\n            \"\"\"\n        )\n        config.cloud = False\n\n    if config.cloud:\n        self.client = QdrantClient(\n            url=url,\n            api_key=key,\n            timeout=config.timeout,\n        )\n    else:\n        try:\n            self.client = QdrantClient(\n                path=config.storage_path,\n            )\n        except Exception as e:\n            new_storage_path = config.storage_path + \".new\"\n            logger.warning(\n                f\"\"\"\n                Error connecting to local QdrantDB at {config.storage_path}:\n                {e}\n                Switching to {new_storage_path}\n                \"\"\"\n            )\n            self.client = QdrantClient(\n                path=new_storage_path,\n            )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB.clone","title":"<code>clone()</code>","text":"<p>Create an independent Qdrant client when running against Qdrant Cloud.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def clone(self) -&gt; \"QdrantDB\":\n    \"\"\"Create an independent Qdrant client when running against Qdrant Cloud.\"\"\"\n    if not self.config.cloud:\n        return self\n    cloned = super().clone()\n    assert isinstance(cloned, QdrantDB)\n    return cloned\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB.close","title":"<code>close()</code>","text":"<p>Close the QdrantDB client and release any resources (e.g., file locks). This is especially important for local storage to release the .lock file.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Close the QdrantDB client and release any resources (e.g., file locks).\n    This is especially important for local storage to release the .lock file.\n    \"\"\"\n    if hasattr(self.client, \"close\"):\n        # QdrantLocal has a close method that releases the lock\n        self.client.close()\n        logger.info(f\"Closed QdrantDB connection for {self.config.storage_path}\")\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [\n        c for c in self.list_collections(empty=True) if c.startswith(prefix)\n    ]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for name in coll_names:\n        info = self.client.get_collection(collection_name=name)\n        points_count = from_optional(info.points_count, 0)\n\n        n_empty_deletes += points_count == 0\n        n_non_empty_deletes += points_count &gt; 0\n        self.client.delete_collection(collection_name=name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and\n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n\n    colls = list(self.client.get_collections())[0][1]\n    if empty:\n        return [coll.name for coll in colls]\n    counts = []\n    for coll in colls:\n        try:\n            counts.append(\n                from_optional(\n                    self.client.get_collection(\n                        collection_name=coll.name\n                    ).points_count,\n                    0,\n                )\n            )\n        except Exception:\n            logger.warning(f\"Error getting collection {coll.name}\")\n            counts.append(0)\n    return [coll.name for coll, count in zip(colls, counts) if (count or 0) &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/#langroid.vector_store.QdrantDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    from qdrant_client.http.models import (\n        CollectionStatus,\n        Distance,\n        SparseIndexParams,\n        SparseVectorParams,\n        VectorParams,\n    )\n\n    self.config.collection_name = collection_name\n    if self.client.collection_exists(collection_name=collection_name):\n        coll = self.client.get_collection(collection_name=collection_name)\n        if (\n            coll.status == CollectionStatus.GREEN\n            and from_optional(coll.points_count, 0) &gt; 0\n        ):\n            logger.warning(f\"Non-empty Collection {collection_name} already exists\")\n            if not replace:\n                logger.warning(\"Not replacing collection\")\n                return\n            else:\n                logger.warning(\"Recreating fresh collection\")\n        self.client.delete_collection(collection_name=collection_name)\n\n    vectors_config = {\n        \"\": VectorParams(\n            size=self.embedding_dim,\n            distance=Distance.COSINE,\n        )\n    }\n    sparse_vectors_config = None\n    if self.config.use_sparse_embeddings:\n        sparse_vectors_config = {\n            \"text-sparse\": SparseVectorParams(index=SparseIndexParams())\n        }\n    self.client.create_collection(\n        collection_name=collection_name,\n        vectors_config=vectors_config,\n        sparse_vectors_config=sparse_vectors_config,\n    )\n    collection_info = self.client.get_collection(collection_name=collection_name)\n    assert collection_info.status == CollectionStatus.GREEN\n    assert collection_info.vectors_count in [0, None]\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(collection_info)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/base/","title":"base","text":"<p>langroid/vector_store/base.py </p>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore","title":"<code>VectorStore(config)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for a vector store.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def __init__(self, config: VectorStoreConfig):\n    self.config = config\n    if config.embedding_model is None:\n        self.embedding_model = EmbeddingModel.create(config.embedding)\n    else:\n        self.embedding_model = config.embedding_model\n    if hasattr(self.config, \"embedding_model\"):\n        self.config.embedding_model = None\n    self.embedding_fn: EmbeddingFunction = self.embedding_model.embedding_fn()\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clone","title":"<code>clone()</code>","text":"<p>Return a vector-store clone suitable for agent cloning.</p> <p>The default implementation deep-copies the configuration, reuses any existing embedding model, and instantiates a fresh store of the same type. Subclasses can override when sharing the instance is required (e.g., embedded/local stores that rely on file locks).</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def clone(self) -&gt; \"VectorStore\":\n    \"\"\"Return a vector-store clone suitable for agent cloning.\n\n    The default implementation deep-copies the configuration, reuses any\n    existing embedding model, and instantiates a fresh store of the same\n    type. Subclasses can override when sharing the instance is required\n    (e.g., embedded/local stores that rely on file locks).\n    \"\"\"\n\n    config_class = self.config.__class__\n    config_data = self.config.model_dump(mode=\"python\")\n    config_data[\"embedding_model\"] = None\n    config_copy = config_class.model_validate(config_data)\n    logger.debug(\n        \"Cloning VectorStore %s: original collection=%s, copied collection=%s\",\n        type(self).__name__,\n        getattr(self.config, \"collection_name\", None),\n        getattr(config_copy, \"collection_name\", None),\n    )\n    # Preserve the calculated collection contents without forcing replaces\n    if hasattr(config_copy, \"replace_collection\"):\n        config_copy.replace_collection = False  # type: ignore[attr-defined]\n    cloned_embedding: Optional[EmbeddingModel] = None\n    if (\n        hasattr(self, \"embedding_model\")\n        and getattr(self, \"embedding_model\") is not None\n    ):\n        cloned_embedding = self.embedding_model.clone()  # type: ignore[attr-defined]\n        if hasattr(config_copy, \"embedding_model\"):\n            config_copy.embedding_model = cloned_embedding\n\n    cloned_store = type(self)(config_copy)  # type: ignore[call-arg]\n    if hasattr(cloned_store.config, \"embedding_model\"):\n        cloned_store.config.embedding_model = None\n    logger.debug(\n        \"Cloned VectorStore %s: cloned collection=%s\",\n        type(self).__name__,\n        getattr(cloned_store.config, \"collection_name\", None),\n    )\n    if hasattr(cloned_store.config, \"replace_collection\"):\n        cloned_store.config.replace_collection = False\n    # Some stores might not honour replace_collection; ensure same collection\n    if getattr(self.config, \"collection_name\", None) is not None:\n        setattr(\n            cloned_store.config,\n            \"collection_name\",\n            getattr(self.config, \"collection_name\", None),\n        )\n    return cloned_store\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clear_empty_collections","title":"<code>clear_empty_collections()</code>  <code>abstractmethod</code>","text":"<p>Clear all empty collections in the vector store. Returns the number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_empty_collections(self) -&gt; int:\n    \"\"\"Clear all empty collections in the vector store.\n    Returns the number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>  <code>abstractmethod</code>","text":"<p>Clear all collections in the vector store.</p> <p>Parameters:</p> Name Type Description Default <code>really</code> <code>bool</code> <p>Whether to really clear all collections. Defaults to False.</p> <code>False</code> <code>prefix</code> <code>str</code> <p>Prefix of collections to clear.</p> <code>''</code> <p>Returns:     int: Number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"\n    Clear all collections in the vector store.\n\n    Args:\n        really (bool, optional): Whether to really clear all collections.\n            Defaults to False.\n        prefix (str, optional): Prefix of collections to clear.\n    Returns:\n        int: Number of collections deleted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.list_collections","title":"<code>list_collections(empty=False)</code>  <code>abstractmethod</code>","text":"<p>List all collections in the vector store (only non empty collections if empty=False).</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"List all collections in the vector store\n    (only non empty collections if empty=False).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.set_collection","title":"<code>set_collection(collection_name, replace=False)</code>","text":"<p>Set the current collection to the given collection name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the collection if it         already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def set_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Set the current collection to the given collection name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the collection if it\n            already exists. Defaults to False.\n    \"\"\"\n\n    self.config.collection_name = collection_name\n    self.config.replace_collection = replace\n    if replace:\n        self.create_collection(collection_name, replace=True)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>  <code>abstractmethod</code>","text":"<p>Create a collection with the given name. Args:     collection_name (str): Name of the collection.     replace (bool, optional): Whether to replace the         collection if it already exists. Defaults to False.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"Create a collection with the given name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the\n            collection if it already exists. Defaults to False.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.compute_from_docs","title":"<code>compute_from_docs(docs, calc)</code>","text":"<p>Compute a result on a set of documents, using a dataframe calc string like <code>df.groupby('state')['income'].mean()</code>.</p> <p>If full_eval is False (default), the input expression is sanitized to prevent most common code injection attack vectors. If full_eval is True, sanitization is bypassed - use only with trusted input!</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def compute_from_docs(self, docs: List[Document], calc: str) -&gt; str:\n    \"\"\"Compute a result on a set of documents,\n    using a dataframe calc string like `df.groupby('state')['income'].mean()`.\n\n    If full_eval is False (default), the input expression is sanitized to prevent\n    most common code injection attack vectors.\n    If full_eval is True, sanitization is bypassed - use only with trusted input!\n    \"\"\"\n    # convert each doc to a dict, using dotted paths for nested fields\n    dicts = [flatten_dict(doc.model_dump(by_alias=True)) for doc in docs]\n    df = pd.DataFrame(dicts)\n\n    try:\n        # SECURITY MITIGATION: Eval input is sanitized to prevent most common\n        # code injection attack vectors when full_eval is False.\n        vars = {\"df\": df}\n        if not self.config.full_eval:\n            calc = sanitize_command(calc)\n        code = compile(calc, \"&lt;calc&gt;\", \"eval\")\n        result = eval(code, vars, {})\n    except Exception as e:\n        # return error message so LLM can fix the calc string if needed\n        err = f\"\"\"\n        Error encountered in pandas eval: {str(e)}\n        \"\"\"\n        if isinstance(e, KeyError) and \"not in index\" in str(e):\n            # Pd.eval sometimes fails on a perfectly valid exprn like\n            # df.loc[..., 'column'] with a KeyError.\n            err += \"\"\"\n            Maybe try a different way, e.g. \n            instead of df.loc[..., 'column'], try df.loc[...]['column']\n            \"\"\"\n        return err\n    return stringify(result)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.maybe_add_ids","title":"<code>maybe_add_ids(documents)</code>","text":"<p>Add ids to metadata if absent, since some vecdbs don't like having blank ids.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def maybe_add_ids(self, documents: Sequence[Document]) -&gt; None:\n    \"\"\"Add ids to metadata if absent, since some\n    vecdbs don't like having blank ids.\"\"\"\n    for d in documents:\n        if d.metadata.id in [None, \"\"]:\n            d.metadata.id = ObjectRegistry.new_id()\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.similar_texts_with_scores","title":"<code>similar_texts_with_scores(text, k=1, where=None)</code>  <code>abstractmethod</code>","text":"<p>Find k most similar texts to the given text, in terms of vector distance metric (e.g., cosine similarity).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to find similar texts for.</p> required <code>k</code> <code>int</code> <p>Number of similar texts to retrieve. Defaults to 1.</p> <code>1</code> <code>where</code> <code>Optional[str]</code> <p>Where clause to filter the search.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document,float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef similar_texts_with_scores(\n    self,\n    text: str,\n    k: int = 1,\n    where: Optional[str] = None,\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    Find k most similar texts to the given text, in terms of vector distance metric\n    (e.g., cosine similarity).\n\n    Args:\n        text (str): The text to find similar texts for.\n        k (int, optional): Number of similar texts to retrieve. Defaults to 1.\n        where (Optional[str], optional): Where clause to filter the search.\n\n    Returns:\n        List[Tuple[Document,float]]: List of (Document, score) tuples.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.add_context_window","title":"<code>add_context_window(docs_scores, neighbors=0)</code>","text":"<p>In each doc's metadata, there may be a window_ids field indicating the ids of the chunks around the current chunk. These window_ids may overlap, so we - coalesce each overlapping groups into a single window (maintaining ordering), - create a new document for each part, preserving metadata,</p> <p>We may have stored a longer set of window_ids than we need during chunking. Now, we just want <code>neighbors</code> on each side of the center of the window_ids list.</p> <p>Parameters:</p> Name Type Description Default <code>docs_scores</code> <code>List[Tuple[Document, float]]</code> <p>List of pairs of documents to add context windows to together with their match scores.</p> required <code>neighbors</code> <code>int</code> <p>Number of neighbors on \"each side\" of match to retrieve. Defaults to 0. \"Each side\" here means before and after the match, in the original text.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[Tuple[Document, float]]</code> <p>List[Tuple[Document, float]]: List of (Document, score) tuples.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def add_context_window(\n    self, docs_scores: List[Tuple[Document, float]], neighbors: int = 0\n) -&gt; List[Tuple[Document, float]]:\n    \"\"\"\n    In each doc's metadata, there may be a window_ids field indicating\n    the ids of the chunks around the current chunk.\n    These window_ids may overlap, so we\n    - coalesce each overlapping groups into a single window (maintaining ordering),\n    - create a new document for each part, preserving metadata,\n\n    We may have stored a longer set of window_ids than we need during chunking.\n    Now, we just want `neighbors` on each side of the center of the window_ids list.\n\n    Args:\n        docs_scores (List[Tuple[Document, float]]): List of pairs of documents\n            to add context windows to together with their match scores.\n        neighbors (int, optional): Number of neighbors on \"each side\" of match to\n            retrieve. Defaults to 0.\n            \"Each side\" here means before and after the match,\n            in the original text.\n\n    Returns:\n        List[Tuple[Document, float]]: List of (Document, score) tuples.\n    \"\"\"\n    # We return a larger context around each match, i.e.\n    # a window of `neighbors` on each side of the match.\n    docs = [d for d, s in docs_scores]\n    scores = [s for d, s in docs_scores]\n    if neighbors == 0:\n        return docs_scores\n    doc_chunks = [d for d in docs if d.metadata.is_chunk]\n    if len(doc_chunks) == 0:\n        return docs_scores\n    window_ids_list = []\n    id2metadata = {}\n    # id -&gt; highest score of a doc it appears in\n    id2max_score: Dict[int | str, float] = {}\n    for i, d in enumerate(docs):\n        window_ids = d.metadata.window_ids\n        if len(window_ids) == 0:\n            window_ids = [d.id()]\n        id2metadata.update({id: d.metadata for id in window_ids})\n\n        id2max_score.update(\n            {id: max(id2max_score.get(id, 0), scores[i]) for id in window_ids}\n        )\n        n = len(window_ids)\n        chunk_idx = window_ids.index(d.id())\n        neighbor_ids = window_ids[\n            max(0, chunk_idx - neighbors) : min(n, chunk_idx + neighbors + 1)\n        ]\n        window_ids_list += [neighbor_ids]\n\n    # window_ids could be from different docs,\n    # and they may overlap, so we coalesce overlapping groups into\n    # separate windows.\n    window_ids_list = self.remove_overlaps(window_ids_list)\n    final_docs = []\n    final_scores = []\n    for w in window_ids_list:\n        metadata = copy.deepcopy(id2metadata[w[0]])\n        metadata.window_ids = w\n        document = Document(\n            content=\"\".join([d.content for d in self.get_documents_by_ids(w)]),\n            metadata=metadata,\n        )\n        # make a fresh id since content is in general different\n        document.metadata.id = ObjectRegistry.new_id()\n        final_docs += [document]\n        final_scores += [max(id2max_score[id] for id in w)]\n    return list(zip(final_docs, final_scores))\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.remove_overlaps","title":"<code>remove_overlaps(windows)</code>  <code>staticmethod</code>","text":"<p>Given a collection of windows, where each window is a sequence of ids, identify groups of overlapping windows, and for each overlapping group, order the chunk-ids using topological sort so they appear in the original order in the text.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>List[int | str]</code> <p>List of windows, where each window is a sequence of ids.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List[int|str]: List of windows, where each window is a sequence of ids, and no two windows overlap.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@staticmethod\ndef remove_overlaps(windows: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"\n    Given a collection of windows, where each window is a sequence of ids,\n    identify groups of overlapping windows, and for each overlapping group,\n    order the chunk-ids using topological sort so they appear in the original\n    order in the text.\n\n    Args:\n        windows (List[int|str]): List of windows, where each window is a\n            sequence of ids.\n\n    Returns:\n        List[int|str]: List of windows, where each window is a sequence of ids,\n            and no two windows overlap.\n    \"\"\"\n    ids = set(id for w in windows for id in w)\n    # id -&gt; {win -&gt; # pos}\n    id2win2pos: Dict[str, Dict[int, int]] = {id: {} for id in ids}\n\n    for i, w in enumerate(windows):\n        for j, id in enumerate(w):\n            id2win2pos[id][i] = j\n\n    n = len(windows)\n    # relation between windows:\n    order = np.zeros((n, n), dtype=np.int8)\n    for i, w in enumerate(windows):\n        for j, x in enumerate(windows):\n            if i == j:\n                continue\n            if len(set(w).intersection(x)) == 0:\n                continue\n            id = list(set(w).intersection(x))[0]  # any common id\n            if id2win2pos[id][i] &gt; id2win2pos[id][j]:\n                order[i, j] = -1  # win i is before win j\n            else:\n                order[i, j] = 1  # win i is after win j\n\n    # find groups of windows that overlap, like connected components in a graph\n    groups = components(np.abs(order))\n\n    # order the chunk-ids in each group using topological sort\n    new_windows = []\n    for g in groups:\n        # find total ordering among windows in group based on order matrix\n        # (this is a topological sort)\n        _g = np.array(g)\n        order_matrix = order[_g][:, _g]\n        ordered_window_indices = topological_sort(order_matrix)\n        ordered_window_ids = [windows[i] for i in _g[ordered_window_indices]]\n        flattened = [id for w in ordered_window_ids for id in w]\n        flattened_deduped = list(dict.fromkeys(flattened))\n        # Note we are not going to split these, and instead we'll return\n        # larger windows from concatenating the connected groups.\n        # This ensures context is retained for LLM q/a\n        new_windows += [flattened_deduped]\n\n    return new_windows\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.get_all_documents","title":"<code>get_all_documents(where='')</code>  <code>abstractmethod</code>","text":"<p>Get all documents in the current collection, possibly filtered by <code>where</code>.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_all_documents(self, where: str = \"\") -&gt; List[Document]:\n    \"\"\"\n    Get all documents in the current collection, possibly filtered by `where`.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.get_documents_by_ids","title":"<code>get_documents_by_ids(ids)</code>  <code>abstractmethod</code>","text":"<p>Get documents by their ids. Args:     ids (List[str]): List of document ids.</p> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\n    \"\"\"\n    Get documents by their ids.\n    Args:\n        ids (List[str]): List of document ids.\n\n    Returns:\n        List[Document]: List of documents\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/vector_store/chromadb/","title":"chromadb","text":"<p>langroid/vector_store/chromadb.py </p>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB","title":"<code>ChromaDB(config=ChromaDBConfig())</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def __init__(self, config: ChromaDBConfig = ChromaDBConfig()):\n    super().__init__(config)\n    try:\n        import chromadb\n    except ImportError:\n        raise LangroidImportError(\"chromadb\", \"chromadb\")\n    self.config = config\n    self.client = chromadb.Client(\n        chromadb.config.Settings(\n            # chroma_db_impl=\"duckdb+parquet\",\n            # is_persistent=bool(config.storage_path),\n            persist_directory=config.storage_path,\n        )\n    )\n    if self.config.collection_name is not None:\n        self.create_collection(\n            self.config.collection_name,\n            replace=self.config.replace_collection,\n        )\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections in the vector store with the given prefix.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections in the vector store with the given prefix.\"\"\"\n\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll = [c for c in self.client.list_collections() if c.name.startswith(prefix)]\n    if len(coll) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for c in coll:\n        n_empty_deletes += c.count() == 0\n        n_non_empty_deletes += c.count() &gt; 0\n        self.client.delete_collection(name=c.name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and\n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>List non-empty collections in the vector store. Args:     empty (bool, optional): Whether to list empty collections. Returns:     List[str]: List of non-empty collection names.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    List non-empty collections in the vector store.\n    Args:\n        empty (bool, optional): Whether to list empty collections.\n    Returns:\n        List[str]: List of non-empty collection names.\n    \"\"\"\n    colls = self.client.list_collections()\n    if empty:\n        return [coll.name for coll in colls]\n    return [coll.name for coll in colls if coll.count() &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection in the vector store, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create or replace.     replace (bool, optional): Whether to replace an existing collection.         Defaults to False.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection in the vector store, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create or replace.\n        replace (bool, optional): Whether to replace an existing collection.\n            Defaults to False.\n\n    \"\"\"\n    self.config.collection_name = collection_name\n    if collection_name in self.list_collections(empty=True) and replace:\n        logger.warning(f\"Replacing existing collection {collection_name}\")\n        self.client.delete_collection(collection_name)\n    self.collection = self.client.create_collection(\n        name=self.config.collection_name,\n        embedding_function=self.embedding_fn,\n        get_or_create=not replace,\n        metadata={\n            \"hnsw:space\": self.config.distance,\n            \"hnsw:construction_ef\": self.config.construction_ef,\n            \"hnsw:search_ef\": self.config.search_ef,\n            # we could expose other configs, see:\n            # https://docs.trychroma.com/docs/collections/configure\n        },\n    )\n</code></pre>"},{"location":"reference/vector_store/lancedb/","title":"lancedb","text":"<p>langroid/vector_store/lancedb.py </p>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB","title":"<code>LanceDB(config=LanceDBConfig())</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def __init__(self, config: LanceDBConfig = LanceDBConfig()):\n    super().__init__(config)\n    if not has_lancedb:\n        raise LangroidImportError(\"lancedb\", \"lancedb\")\n\n    self.config: LanceDBConfig = config\n    self.host = config.host\n    self.port = config.port\n    self.is_from_dataframe = False  # were docs ingested from a dataframe?\n    self.df_metadata_columns: List[str] = []  # metadata columns from dataframe\n\n    load_dotenv()\n    if self.config.cloud:\n        logger.warning(\n            \"LanceDB Cloud is not available yet. Switching to local storage.\"\n        )\n        config.cloud = False\n    else:\n        try:\n            self.client = lancedb.connect(\n                uri=config.storage_path,\n            )\n        except Exception as e:\n            new_storage_path = config.storage_path + \".new\"\n            logger.warning(\n                f\"\"\"\n                Error connecting to local LanceDB at {config.storage_path}:\n                {e}\n                Switching to {new_storage_path}\n                \"\"\"\n            )\n            self.client = lancedb.connect(\n                uri=new_storage_path,\n            )\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [\n        c for c in self.list_collections(empty=True) if c.startswith(prefix)\n    ]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for name in coll_names:\n        nr = self.client.open_table(name).head(1).shape[0]\n        n_empty_deletes += nr == 0\n        n_non_empty_deletes += nr &gt; 0\n        self.client.drop_table(name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and\n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n    colls = self.client.table_names(limit=None)\n    if len(colls) == 0:\n        return []\n    if empty:  # include empty tbls\n        return colls  # type: ignore\n    counts = [self.client.open_table(coll).head(1).shape[0] for coll in colls]\n    return [coll for coll, count in zip(colls, counts) if count &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/lancedb/#langroid.vector_store.lancedb.LanceDB.add_dataframe","title":"<code>add_dataframe(df, content='content', metadata=[])</code>","text":"<p>Add a dataframe to the collection. Args:     df (pd.DataFrame): A dataframe     content (str): The name of the column in the dataframe that contains the         text content to be embedded using the embedding model.     metadata (List[str]): A list of column names in the dataframe that contain         metadata to be stored in the database. Defaults to [].</p> Source code in <code>langroid/vector_store/lancedb.py</code> <pre><code>def add_dataframe(\n    self,\n    df: pd.DataFrame,\n    content: str = \"content\",\n    metadata: List[str] = [],\n) -&gt; None:\n    \"\"\"\n    Add a dataframe to the collection.\n    Args:\n        df (pd.DataFrame): A dataframe\n        content (str): The name of the column in the dataframe that contains the\n            text content to be embedded using the embedding model.\n        metadata (List[str]): A list of column names in the dataframe that contain\n            metadata to be stored in the database. Defaults to [].\n    \"\"\"\n    self.is_from_dataframe = True\n    actual_metadata = metadata.copy()\n    self.df_metadata_columns = actual_metadata  # could be updated below\n    # get content column\n    content_values = df[content].values.tolist()\n    embedding_vecs = self.embedding_fn(content_values)\n\n    # add vector column\n    df[\"vector\"] = embedding_vecs\n    if content != \"content\":\n        # rename content column to \"content\", leave existing column intact\n        df = df.rename(columns={content: \"content\"}, inplace=False)\n\n    if \"id\" not in df.columns:\n        docs = dataframe_to_documents(df, content=\"content\", metadata=metadata)\n        ids = [str(d.id()) for d in docs]\n        df[\"id\"] = ids\n\n    if \"id\" not in actual_metadata:\n        actual_metadata += [\"id\"]\n\n    colls = self.list_collections(empty=True)\n    coll_name = self.config.collection_name\n    if (\n        coll_name not in colls\n        or self.client.open_table(coll_name).head(1).shape[0] == 0\n    ):\n        # collection either doesn't exist or is empty, so replace it\n        # and set new schema from df\n        self.client.create_table(\n            self.config.collection_name,\n            data=df,\n            mode=\"overwrite\",\n        )\n        doc_cls = dataframe_to_document_model(\n            df,\n            content=content,\n            metadata=actual_metadata,\n            exclude=[\"vector\"],\n        )\n        self.config.document_class = doc_cls  # type: ignore\n    else:\n        # collection exists and is not empty, so append to it\n        tbl = self.client.open_table(self.config.collection_name)\n        tbl.add(df)\n</code></pre>"},{"location":"reference/vector_store/meilisearch/","title":"meilisearch","text":"<p>langroid/vector_store/meilisearch.py </p> <p>MeiliSearch as a pure document store, without its (experimental) vector-store functionality. We aim to use MeiliSearch for fast lexical search. Note that what we call \"Collection\" in Langroid is referred to as \"Index\" in MeiliSearch. Each data-store has its own terminology, but for uniformity we use the Langroid terminology here.</p>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch","title":"<code>MeiliSearch(config=MeiliSearchConfig())</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def __init__(self, config: MeiliSearchConfig = MeiliSearchConfig()):\n    super().__init__(config)\n    try:\n        import meilisearch_python_sdk as meilisearch\n    except ImportError:\n        raise LangroidImportError(\"meilisearch\", \"meilisearch\")\n\n    self.config: MeiliSearchConfig = config\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    self.key = os.getenv(\"MEILISEARCH_API_KEY\") or \"masterKey\"\n    self.url = os.getenv(\"MEILISEARCH_API_URL\") or f\"http://{self.host}:{self.port}\"\n    if config.cloud and None in [self.key, self.url]:\n        logger.warning(\n            f\"\"\"MEILISEARCH_API_KEY, MEILISEARCH_API_URL env variable must be set \n            to use MeiliSearch in cloud mode. Please set these values \n            in your .env file. Switching to local MeiliSearch at \n            {self.url} \n            \"\"\"\n        )\n        config.cloud = False\n\n    self.client: Callable[[], meilisearch.AsyncClient] = lambda: (\n        meilisearch.AsyncClient(url=self.url, api_key=self.key)\n    )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of db until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.clear_empty_collections","title":"<code>clear_empty_collections()</code>","text":"<p>All collections are treated as non-empty in MeiliSearch, so this is a no-op</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def clear_empty_collections(self) -&gt; int:\n    \"\"\"All collections are treated as non-empty in MeiliSearch, so this is a\n    no-op\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Delete all indices whose names start with <code>prefix</code></p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Delete all indices whose names start with `prefix`\"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [c for c in self.list_collections() if c.startswith(prefix)]\n    deletes = asyncio.run(self._async_delete_indices(coll_names))\n    n_deletes = sum(deletes)\n    logger.warning(f\"Deleted {n_deletes} indices in MeiliSearch\")\n    return n_deletes\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of index names stored. We treat any existing index as non-empty.</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of index names stored. We treat any existing index as non-empty.\n    \"\"\"\n    indexes = asyncio.run(self._async_get_indexes())\n    if len(indexes) == 0:\n        return []\n    else:\n        return [ind.uid for ind in indexes]\n</code></pre>"},{"location":"reference/vector_store/meilisearch/#langroid.vector_store.meilisearch.MeiliSearch.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/meilisearch.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    self.config.collection_name = collection_name\n    collections = self.list_collections()\n    if collection_name in collections:\n        logger.warning(\n            f\"MeiliSearch Non-empty Index {collection_name} already exists\"\n        )\n        if not replace:\n            logger.warning(\"Not replacing collection\")\n            return\n        else:\n            logger.warning(\"Recreating fresh collection\")\n            asyncio.run(self._async_delete_index(collection_name))\n    asyncio.run(self._async_create_index(collection_name))\n    collection_info = asyncio.run(self._async_get_index(collection_name))\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(collection_info)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/pineconedb/","title":"pineconedb","text":"<p>langroid/vector_store/pineconedb.py </p>"},{"location":"reference/vector_store/pineconedb/#langroid.vector_store.pineconedb.PineconeDB","title":"<code>PineconeDB(config=PineconeDBConfig())</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/pineconedb.py</code> <pre><code>def __init__(self, config: PineconeDBConfig = PineconeDBConfig()):\n    super().__init__(config)\n    if not has_pinecone:\n        raise LangroidImportError(\"pinecone\", \"pinecone\")\n    self.config: PineconeDBConfig = config\n    load_dotenv()\n    key = os.getenv(\"PINECONE_API_KEY\")\n\n    if not key:\n        raise ValueError(\"PINECONE_API_KEY not set, could not instantiate client\")\n    self.client = Pinecone(api_key=key)\n\n    if config.collection_name:\n        self.create_collection(\n            collection_name=config.collection_name,\n            replace=config.replace_collection,\n        )\n</code></pre>"},{"location":"reference/vector_store/pineconedb/#langroid.vector_store.pineconedb.PineconeDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Returns:</p> Type Description <code>int</code> <p>Number of Pinecone indexes that were deleted</p> <p>Parameters:</p> Name Type Description Default <code>really</code> <code>bool</code> <p>Optional[bool] - whether to really delete all Pinecone collections</p> <code>False</code> <code>prefix</code> <code>str</code> <p>Optional[str] - string to match potential Pinecone indexes for deletion</p> <code>''</code> Source code in <code>langroid/vector_store/pineconedb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"\n    Returns:\n        Number of Pinecone indexes that were deleted\n\n    Args:\n        really: Optional[bool] - whether to really delete all Pinecone collections\n        prefix: Optional[str] - string to match potential Pinecone\n            indexes for deletion\n    \"\"\"\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    indexes = [\n        c for c in self._list_index_metas(empty=True) if c.name.startswith(prefix)\n    ]\n    if len(indexes) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes, n_non_empty_deletes = 0, 0\n    for index_desc in indexes:\n        self.delete_collection(collection_name=index_desc.name)\n        n_empty_deletes += index_desc.total_vector_count == 0\n        n_non_empty_deletes += index_desc.total_vector_count &gt; 0\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty indexes and\n        {n_non_empty_deletes} non-empty indexes\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/pineconedb/#langroid.vector_store.pineconedb.PineconeDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of Pinecone indices that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Optional[bool] - whether to include empty collections</p> <code>False</code> Source code in <code>langroid/vector_store/pineconedb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of Pinecone indices that have at least one vector.\n\n    Args:\n        empty: Optional[bool] - whether to include empty collections\n    \"\"\"\n    indexes = self.client.list_indexes()\n    res: List[str] = []\n    if empty:\n        res.extend(indexes.names())\n        return res\n\n    for index in indexes.names():\n        index_meta = self.client.Index(name=index)\n        if index_meta.describe_index_stats().get(\"total_vector_count\", 0) &gt; 0:\n            res.append(index)\n    return res\n</code></pre>"},{"location":"reference/vector_store/pineconedb/#langroid.vector_store.pineconedb.PineconeDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing collection if <code>replace</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>str - Configuration of the collection to create.</p> required <code>replace</code> <code>bool</code> <p>Optional[Bool] - Whether to replace an existing collection with the same name. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/pineconedb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n    collection if `replace` is True.\n\n    Args:\n        collection_name: str - Configuration of the collection to create.\n        replace: Optional[Bool] - Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    pattern = re.compile(r\"^[a-z0-9-]+$\")\n    if not pattern.match(collection_name):\n        raise ValueError(\n            \"Pinecone index names must be lowercase alphanumeric characters or '-'\"\n        )\n    self.config.collection_name = collection_name\n    if collection_name in self.list_collections(empty=True):\n        index = self.client.Index(name=collection_name)\n        stats = index.describe_index_stats()\n        status = self.client.describe_index(name=collection_name)\n        if status[\"status\"][\"ready\"] and stats[\"total_vector_count\"] &gt; 0:\n            logger.warning(f\"Non-empty collection {collection_name} already exists\")\n            if not replace:\n                logger.warning(\"Not replacing collection\")\n                return\n            else:\n                logger.warning(\"Recreating fresh collection\")\n        self.delete_collection(collection_name=collection_name)\n\n    payload = {\n        \"name\": collection_name,\n        \"dimension\": self.embedding_dim,\n        \"spec\": self.config.spec,\n        \"metric\": self.config.metric,\n        \"timeout\": self.config.timeout,\n    }\n\n    if self.config.deletion_protection:\n        payload[\"deletion_protection\"] = self.config.deletion_protection\n\n    try:\n        self.client.create_index(**payload)\n    except PineconeApiException as e:\n        logger.error(e)\n</code></pre>"},{"location":"reference/vector_store/pineconedb/#langroid.vector_store.pineconedb.PineconeDB.get_all_documents","title":"<code>get_all_documents(prefix='', namespace='')</code>","text":"<p>Returns:</p> Type Description <code>List[Document]</code> <p>All documents for the collection currently defined in</p> <code>List[Document]</code> <p>the configuration object</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>str - document id prefix to search for</p> <code>''</code> <code>namespace</code> <code>str</code> <p>str - partition of vectors to search within the index</p> <code>''</code> Source code in <code>langroid/vector_store/pineconedb.py</code> <pre><code>def get_all_documents(\n    self, prefix: str = \"\", namespace: str = \"\"\n) -&gt; List[Document]:\n    \"\"\"\n    Returns:\n        All documents for the collection currently defined in\n        the configuration object\n\n    Args:\n        prefix: str - document id prefix to search for\n        namespace: str - partition of vectors to search within the index\n    \"\"\"\n    if self.config.collection_name is None:\n        raise ValueError(\"No collection name set, cannot retrieve docs\")\n    docs = []\n\n    request_filters: Dict[str, Union[str, int]] = {\n        \"limit\": self.config.pagination_size\n    }\n    if prefix:\n        request_filters[\"prefix\"] = prefix\n    if namespace:\n        request_filters[\"namespace\"] = namespace\n\n    index = self.client.Index(name=self.config.collection_name)\n\n    while True:\n        response = index.list_paginated(**request_filters)\n        vectors = response.get(\"vectors\", [])\n\n        if not vectors:\n            logger.warning(\"Received empty list while requesting for vector ids\")\n            logger.warning(\"Halting fetch requests\")\n            if settings.debug:\n                logger.debug(f\"Request for failed fetch was: {request_filters}\")\n            break\n\n        docs.extend(\n            self.get_documents_by_ids(\n                ids=[vector.get(\"id\") for vector in vectors],\n                namespace=namespace if namespace else \"\",\n            )\n        )\n\n        pagination_token = response.get(\"pagination\", {}).get(\"next\", None)\n\n        if not pagination_token:\n            break\n\n        request_filters[\"pagination_token\"] = pagination_token\n\n    return docs\n</code></pre>"},{"location":"reference/vector_store/pineconedb/#langroid.vector_store.pineconedb.PineconeDB.get_documents_by_ids","title":"<code>get_documents_by_ids(ids, namespace='')</code>","text":"<p>Returns:</p> Type Description <code>List[Document]</code> <p>Fetches document text embedded in Pinecone index metadata</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List[str] - vector data object ids to retrieve</p> required <code>namespace</code> <code>str</code> <p>str - partition of vectors to search within the index</p> <code>''</code> Source code in <code>langroid/vector_store/pineconedb.py</code> <pre><code>def get_documents_by_ids(\n    self, ids: List[str], namespace: str = \"\"\n) -&gt; List[Document]:\n    \"\"\"\n    Returns:\n        Fetches document text embedded in Pinecone index metadata\n\n    Args:\n        ids: List[str] - vector data object ids to retrieve\n        namespace: str - partition of vectors to search within the index\n    \"\"\"\n    if self.config.collection_name is None:\n        raise ValueError(\"No collection name set, cannot retrieve docs\")\n    index = self.client.Index(name=self.config.collection_name)\n\n    if namespace:\n        records = index.fetch(ids=ids, namespace=namespace)\n    else:\n        records = index.fetch(ids=ids)\n\n    id_mapping = {key: value for key, value in records[\"vectors\"].items()}\n    ordered_payloads = [id_mapping[_id] for _id in ids if _id in id_mapping]\n    return [\n        self.transform_pinecone_vector(payload.get(\"metadata\", {}))\n        for payload in ordered_payloads\n    ]\n</code></pre>"},{"location":"reference/vector_store/pineconedb/#langroid.vector_store.pineconedb.PineconeDB.transform_pinecone_vector","title":"<code>transform_pinecone_vector(metadata_dict)</code>","text":"<p>Parses the metadata response from the Pinecone vector query and formats it into a dictionary that can be parsed by the Document class associated with the PineconeDBConfig class</p> <p>Returns:</p> Type Description <code>Document</code> <p>Well formed dictionary object to be transformed into a Document</p> <p>Parameters:</p> Name Type Description Default <code>metadata_dict</code> <code>Dict[str, Any]</code> <p>Dict - the metadata dictionary from the Pinecone vector query match</p> required Source code in <code>langroid/vector_store/pineconedb.py</code> <pre><code>def transform_pinecone_vector(self, metadata_dict: Dict[str, Any]) -&gt; Document:\n    \"\"\"\n    Parses the metadata response from the Pinecone vector query and\n    formats it into a dictionary that can be parsed by the Document class\n    associated with the PineconeDBConfig class\n\n    Returns:\n        Well formed dictionary object to be transformed into a Document\n\n    Args:\n        metadata_dict: Dict - the metadata dictionary from the Pinecone\n            vector query match\n    \"\"\"\n    return self.config.document_class(\n        **{**metadata_dict, \"metadata\": {**metadata_dict}}\n    )\n</code></pre>"},{"location":"reference/vector_store/postgres/","title":"postgres","text":"<p>langroid/vector_store/postgres.py </p>"},{"location":"reference/vector_store/postgres/#langroid.vector_store.postgres.PostgresDB","title":"<code>PostgresDB(config=PostgresDBConfig())</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/postgres.py</code> <pre><code>def __init__(self, config: PostgresDBConfig = PostgresDBConfig()):\n    super().__init__(config)\n    if not has_postgres:\n        raise LangroidImportError(\"pgvector\", \"postgres\")\n    try:\n        from sqlalchemy.orm import sessionmaker\n    except ImportError:\n        raise LangroidImportError(\"sqlalchemy\", \"postgres\")\n\n    self.config: PostgresDBConfig = config\n    self.engine = self._create_engine()\n    PostgresDB._create_vector_extension(self.engine)\n    self.SessionLocal = sessionmaker(\n        autocommit=False, autoflush=False, bind=self.engine\n    )\n    self.metadata = MetaData()\n    self._setup_table()\n</code></pre>"},{"location":"reference/vector_store/postgres/#langroid.vector_store.postgres.PostgresDB.index_exists","title":"<code>index_exists(connection, index_name)</code>","text":"<p>Check if an index exists.</p> Source code in <code>langroid/vector_store/postgres.py</code> <pre><code>def index_exists(self, connection: Connection, index_name: str) -&gt; bool:\n    \"\"\"Check if an index exists.\"\"\"\n    query = text(\n        \"SELECT 1 FROM pg_indexes WHERE indexname = :index_name\"\n    ).bindparams(index_name=index_name)\n    result = connection.execute(query).scalar()\n    return bool(result)\n</code></pre>"},{"location":"reference/vector_store/postgres/#langroid.vector_store.postgres.PostgresDB.delete_collection","title":"<code>delete_collection(collection_name)</code>","text":"<p>Deletes a collection and its associated HNSW index, handling metadata synchronization issues.</p> Source code in <code>langroid/vector_store/postgres.py</code> <pre><code>def delete_collection(self, collection_name: str) -&gt; None:\n    \"\"\"\n    Deletes a collection and its associated HNSW index, handling metadata\n    synchronization issues.\n    \"\"\"\n    with self.engine.connect() as connection:\n        connection.execute(text(\"COMMIT\"))\n        index_name = f\"hnsw_index_{collection_name}_embedding\"\n        drop_index_query = text(f\"DROP INDEX CONCURRENTLY IF EXISTS {index_name}\")\n        connection.execute(drop_index_query)\n\n        # 3. Now, drop the table using SQLAlchemy\n        table = Table(collection_name, self.metadata)\n        table.drop(self.engine, checkfirst=True)\n\n        # 4. Refresh metadata again after dropping the table\n        self.metadata.clear()\n        self.metadata.reflect(bind=self.engine)\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/","title":"qdrantdb","text":"<p>langroid/vector_store/qdrantdb.py </p>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB","title":"<code>QdrantDB(config=QdrantDBConfig())</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def __init__(self, config: QdrantDBConfig = QdrantDBConfig()):\n    super().__init__(config)\n    self.config: QdrantDBConfig = config\n    from qdrant_client import QdrantClient\n\n    if self.config.use_sparse_embeddings:\n        try:\n            from transformers import AutoModelForMaskedLM, AutoTokenizer\n        except ImportError:\n            raise ImportError(\n                \"\"\"\n                To use sparse embeddings,\n                you must install langroid with the [transformers] extra, e.g.:\n                pip install \"langroid[transformers]\"\n                \"\"\"\n            )\n\n        self.sparse_tokenizer = AutoTokenizer.from_pretrained(\n            self.config.sparse_embedding_model\n        )\n        self.sparse_model = AutoModelForMaskedLM.from_pretrained(\n            self.config.sparse_embedding_model\n        )\n    self.host = config.host\n    self.port = config.port\n    load_dotenv()\n    key = os.getenv(\"QDRANT_API_KEY\")\n    url = os.getenv(\"QDRANT_API_URL\")\n    if config.docker:\n        if url is None:\n            logger.warning(\n                f\"\"\"The QDRANT_API_URL env variable must be set to use\n                QdrantDB in local docker mode. Please set this\n                value in your .env file.\n                Switching to local storage at {config.storage_path}\n                \"\"\"\n            )\n            config.cloud = False\n        else:\n            config.cloud = True\n    elif config.cloud and None in [key, url]:\n        logger.warning(\n            f\"\"\"QDRANT_API_KEY, QDRANT_API_URL env variable must be set to use\n            QdrantDB in cloud mode. Please set these values\n            in your .env file.\n            Switching to local storage at {config.storage_path}\n            \"\"\"\n        )\n        config.cloud = False\n\n    if config.cloud:\n        self.client = QdrantClient(\n            url=url,\n            api_key=key,\n            timeout=config.timeout,\n        )\n    else:\n        try:\n            self.client = QdrantClient(\n                path=config.storage_path,\n            )\n        except Exception as e:\n            new_storage_path = config.storage_path + \".new\"\n            logger.warning(\n                f\"\"\"\n                Error connecting to local QdrantDB at {config.storage_path}:\n                {e}\n                Switching to {new_storage_path}\n                \"\"\"\n            )\n            self.client = QdrantClient(\n                path=new_storage_path,\n            )\n\n    # Note: Only create collection if a non-null collection name is provided.\n    # This is useful to delay creation of vecdb until we have a suitable\n    # collection name (e.g. we could get it from the url or folder path).\n    if config.collection_name is not None:\n        self.create_collection(\n            config.collection_name, replace=config.replace_collection\n        )\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.clone","title":"<code>clone()</code>","text":"<p>Create an independent Qdrant client when running against Qdrant Cloud.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def clone(self) -&gt; \"QdrantDB\":\n    \"\"\"Create an independent Qdrant client when running against Qdrant Cloud.\"\"\"\n    if not self.config.cloud:\n        return self\n    cloned = super().clone()\n    assert isinstance(cloned, QdrantDB)\n    return cloned\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.close","title":"<code>close()</code>","text":"<p>Close the QdrantDB client and release any resources (e.g., file locks). This is especially important for local storage to release the .lock file.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Close the QdrantDB client and release any resources (e.g., file locks).\n    This is especially important for local storage to release the .lock file.\n    \"\"\"\n    if hasattr(self.client, \"close\"):\n        # QdrantLocal has a close method that releases the lock\n        self.client.close()\n        logger.info(f\"Closed QdrantDB connection for {self.config.storage_path}\")\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.clear_all_collections","title":"<code>clear_all_collections(really=False, prefix='')</code>","text":"<p>Clear all collections with the given prefix.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def clear_all_collections(self, really: bool = False, prefix: str = \"\") -&gt; int:\n    \"\"\"Clear all collections with the given prefix.\"\"\"\n\n    if not really:\n        logger.warning(\"Not deleting all collections, set really=True to confirm\")\n        return 0\n    coll_names = [\n        c for c in self.list_collections(empty=True) if c.startswith(prefix)\n    ]\n    if len(coll_names) == 0:\n        logger.warning(f\"No collections found with prefix {prefix}\")\n        return 0\n    n_empty_deletes = 0\n    n_non_empty_deletes = 0\n    for name in coll_names:\n        info = self.client.get_collection(collection_name=name)\n        points_count = from_optional(info.points_count, 0)\n\n        n_empty_deletes += points_count == 0\n        n_non_empty_deletes += points_count &gt; 0\n        self.client.delete_collection(collection_name=name)\n    logger.warning(\n        f\"\"\"\n        Deleted {n_empty_deletes} empty collections and\n        {n_non_empty_deletes} non-empty collections.\n        \"\"\"\n    )\n    return n_empty_deletes + n_non_empty_deletes\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.list_collections","title":"<code>list_collections(empty=False)</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> <p>Parameters:</p> Name Type Description Default <code>empty</code> <code>bool</code> <p>Whether to include empty collections.</p> <code>False</code> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def list_collections(self, empty: bool = False) -&gt; List[str]:\n    \"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n\n    Args:\n        empty (bool, optional): Whether to include empty collections.\n    \"\"\"\n\n    colls = list(self.client.get_collections())[0][1]\n    if empty:\n        return [coll.name for coll in colls]\n    counts = []\n    for coll in colls:\n        try:\n            counts.append(\n                from_optional(\n                    self.client.get_collection(\n                        collection_name=coll.name\n                    ).points_count,\n                    0,\n                )\n            )\n        except Exception:\n            logger.warning(f\"Error getting collection {coll.name}\")\n            counts.append(0)\n    return [coll.name for coll, count in zip(colls, counts) if (count or 0) &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True. Args:     collection_name (str): Name of the collection to create.     replace (bool): Whether to replace an existing collection         with the same name. Defaults to False.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n    \"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\n    from qdrant_client.http.models import (\n        CollectionStatus,\n        Distance,\n        SparseIndexParams,\n        SparseVectorParams,\n        VectorParams,\n    )\n\n    self.config.collection_name = collection_name\n    if self.client.collection_exists(collection_name=collection_name):\n        coll = self.client.get_collection(collection_name=collection_name)\n        if (\n            coll.status == CollectionStatus.GREEN\n            and from_optional(coll.points_count, 0) &gt; 0\n        ):\n            logger.warning(f\"Non-empty Collection {collection_name} already exists\")\n            if not replace:\n                logger.warning(\"Not replacing collection\")\n                return\n            else:\n                logger.warning(\"Recreating fresh collection\")\n        self.client.delete_collection(collection_name=collection_name)\n\n    vectors_config = {\n        \"\": VectorParams(\n            size=self.embedding_dim,\n            distance=Distance.COSINE,\n        )\n    }\n    sparse_vectors_config = None\n    if self.config.use_sparse_embeddings:\n        sparse_vectors_config = {\n            \"text-sparse\": SparseVectorParams(index=SparseIndexParams())\n        }\n    self.client.create_collection(\n        collection_name=collection_name,\n        vectors_config=vectors_config,\n        sparse_vectors_config=sparse_vectors_config,\n    )\n    collection_info = self.client.get_collection(collection_name=collection_name)\n    assert collection_info.status == CollectionStatus.GREEN\n    assert collection_info.vectors_count in [0, None]\n    if settings.debug:\n        level = logger.getEffectiveLevel()\n        logger.setLevel(logging.INFO)\n        logger.info(collection_info)\n        logger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.is_valid_uuid","title":"<code>is_valid_uuid(uuid_to_test)</code>","text":"<p>Check if a given string is a valid UUID.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def is_valid_uuid(uuid_to_test: str) -&gt; bool:\n    \"\"\"\n    Check if a given string is a valid UUID.\n    \"\"\"\n    try:\n        uuid_obj = uuid.UUID(uuid_to_test)\n        return str(uuid_obj) == uuid_to_test\n    except Exception:\n        pass\n    # Check for valid unsigned 64-bit integer\n    try:\n        int_value = int(uuid_to_test)\n        return 0 &lt;= int_value &lt;= 18446744073709551615\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/vector_store/weaviatedb/","title":"weaviatedb","text":"<p>langroid/vector_store/weaviatedb.py </p>"},{"location":"reference/vector_store/weaviatedb/#langroid.vector_store.weaviatedb.VectorDistances","title":"<code>VectorDistances</code>","text":"<p>Fallback class when weaviate is not installed, to avoid import errors.</p>"},{"location":"reference/vector_store/weaviatedb/#langroid.vector_store.weaviatedb.WeaviateDB","title":"<code>WeaviateDB(config=WeaviateDBConfig())</code>","text":"<p>               Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/weaviatedb.py</code> <pre><code>def __init__(self, config: WeaviateDBConfig = WeaviateDBConfig()):\n    super().__init__(config)\n    try:\n        import weaviate\n        from weaviate.classes.init import Auth\n    except ImportError:\n        raise LangroidImportError(\"weaviate\", \"weaviate\")\n\n    self.config: WeaviateDBConfig = config\n    load_dotenv()\n    if self.config.docker:\n        self.client = weaviate.connect_to_local(\n            host=self.config.host,\n            port=self.config.port,\n        )\n        self.config.cloud = False\n    elif self.config.cloud:\n        key = os.getenv(\"WEAVIATE_API_KEY\")\n        url = os.getenv(\"WEAVIATE_API_URL\")\n        if url is None or key is None:\n            raise ValueError(\n                \"\"\"WEAVIATE_API_KEY, WEAVIATE_API_URL env variables must be set to \n                use WeaviateDB in cloud mode. Please set these values\n                in your .env file.\n                \"\"\"\n            )\n        self.client = weaviate.connect_to_weaviate_cloud(\n            cluster_url=url,\n            auth_credentials=Auth.api_key(key),\n        )\n    else:\n        self.client = weaviate.connect_to_embedded(\n            version=\"latest\", persistence_data_path=self.config.storage_path\n        )\n\n    if config.collection_name is not None:\n        WeaviateDB.validate_and_format_collection_name(config.collection_name)\n</code></pre>"},{"location":"reference/vector_store/weaviatedb/#langroid.vector_store.weaviatedb.WeaviateDB.validate_and_format_collection_name","title":"<code>validate_and_format_collection_name(name)</code>  <code>staticmethod</code>","text":"<p>Formats the collection name to comply with Weaviate's naming rules: - Name must start with a capital letter. - Name can only contain letters, numbers, and underscores. - Replaces invalid characters with underscores.</p> Source code in <code>langroid/vector_store/weaviatedb.py</code> <pre><code>@staticmethod\ndef validate_and_format_collection_name(name: str) -&gt; str:\n    \"\"\"\n    Formats the collection name to comply with Weaviate's naming rules:\n    - Name must start with a capital letter.\n    - Name can only contain letters, numbers, and underscores.\n    - Replaces invalid characters with underscores.\n    \"\"\"\n    if not name:\n        raise ValueError(\"Collection name cannot be empty.\")\n\n    formatted_name = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", name)\n\n    # Ensure the first letter is capitalized\n    if not formatted_name[0].isupper():\n        formatted_name = formatted_name.capitalize()\n\n    # Check if the name now meets the criteria\n    if not re.match(r\"^[A-Z][A-Za-z0-9_]*$\", formatted_name):\n        raise ValueError(\n            f\"Invalid collection name '{name}'.\"\n            \" Names must start with a capital letter \"\n            \"and contain only letters, numbers, and underscores.\"\n        )\n\n    if formatted_name != name:\n        logger.warning(\n            f\"Collection name '{name}' was reformatted to '{formatted_name}' \"\n            \"to comply with Weaviate's rules.\"\n        )\n\n    return formatted_name\n</code></pre>"},{"location":"tutorials/langroid-tour/","title":"A quick tour of Langroid","text":"<p>This is a quick tour of some Langroid features. For a more detailed guide, see the Getting Started guide. There are many more features besides the ones shown here. To explore langroid more, see the sections of the main docs, and a  Colab notebook  you can try yourself.  </p>"},{"location":"tutorials/langroid-tour/#chat-directly-with-llm","title":"Chat directly with LLM","text":"<p>Imports:</p> <pre><code>import langroid as lr\nimport langroid.language_models as lm\n</code></pre> <p>Set up the LLM; note how you can specify the chat model -- if omitted, defaults to OpenAI <code>GPT4o</code>. See the guide to using Langroid with  local/open LLMs, and with non-OpenAI LLMs.</p> <pre><code>llm_config = lm.OpenAIGPTConfig( \n   chat_model=\"gpt-5-mini\"\n)\nllm = lm.OpenAIGPT(llm_config)\n</code></pre> <p>Chat with bare LLM -- no chat accumulation, i.e. follow-up responses will not be aware of prior conversation history (you need an Agent for that, see below).</p> <pre><code>llm.chat(\"1 2 4 7 11 ?\")\n# ==&gt; answers 16, with some explanation\n</code></pre>"},{"location":"tutorials/langroid-tour/#agent","title":"Agent","text":"<p>Make a <code>ChatAgent</code>,  and chat with it; now accumulates conv history</p> <pre><code>agent = lr.ChatAgent(lr.ChatAgentConfig(llm=llm_config))\nagent.llm_response(\"Find the next number: 1 2 4 7 11 ?\")\n# =&gt; responds 16\nagent.llm_response(\"and then?)\n# =&gt; answers 22\n</code></pre>"},{"location":"tutorials/langroid-tour/#task","title":"Task","text":"<p>Make a <code>Task</code> and create a chat loop with the user:</p> <pre><code>task = lr.Task(agent, interactive=True)\ntask.run()\n</code></pre>"},{"location":"tutorials/langroid-tour/#toolsfunctionsstructured-outputs","title":"Tools/Functions/Structured outputs:","text":"<p>Define a <code>ToolMessage</code>  using Pydantic (v1) -- this gets transpiled into system-message instructions to the LLM, so you never have to deal with writing a JSON schema. (Besides JSON-based tools, Langroid also supports  XML-based tools, which  are far more reliable when having the LLM return code in a structured output.)</p> <pre><code>from pydantic import BaseModel\n\nclass CityTemperature(BaseModel):\n    city: str\n    temp: float\n\nclass WeatherTool(lr.ToolMessage):\n    request: str = \"weather_tool\" #(1)!\n    purpose: str = \"To extract &lt;city_temp&gt; info from text\" #(2)!\n\n    city_temp: CityTemperature\n\n    # tool handler\n    def handle(self) -&gt; CityTemperature:\n        return self.city_temp\n</code></pre> <ol> <li>When this tool is enabled for an agent, a method named <code>weather_tool</code> gets auto-inserted in the agent class,     with body being the <code>handle</code> method -- this method handles the LLM's generation     of this tool.</li> <li>The value of the <code>purpose</code> field is used to populate the system message to the LLM,    along with the Tool's schema derived from its Pydantic-based definition.</li> </ol> <p>Enable the Agent to use the <code>ToolMessage</code>, and set a system message describing the  agent's task:</p> <pre><code>agent.enable_message(WeatherTool)\nagent.config.system_message = \"\"\"\n Your job is to extract city and temperature info from user input\n and return it using the `weather_tool`.\n\"\"\"\n</code></pre> <p>Create specialized task that returns a <code>CityTemperature</code> object:</p> <pre><code># configure task to terminate after (a) LLM emits a tool, (b) tool is handled by Agent\ntask_config = lr.TaskConfig(done_sequences=[\"T,A\"])\n\n# create a task that returns a CityTemperature object\ntask = lr.Task(agent, interactive=False, config=task_config)[CityTemperature]\n\n# run task, with built-in tool-handling loop\ndata = task.run(\"It is 45 degrees F in Boston\")\n\nassert data.city == \"Boston\"\nassert int(data.temp) == 45\n</code></pre>"},{"location":"tutorials/langroid-tour/#chat-with-a-document-rag","title":"Chat with a document (RAG)","text":"<p>Create a <code>DocChatAgent</code>.</p> <pre><code>doc_agent_config = lr.agent.special.DocChatAgentConfig(llm=llm_config)\ndoc_agent = lr.agent.special.DocChatAgent(doc_agent_config)\n</code></pre> <p>Ingest the contents of a web page into the agent  (this involves chunking, indexing into a vector-database, etc.):</p> <pre><code>doc_agent.ingest_doc_paths(\"https://en.wikipedia.org/wiki/Ludwig_van_Beethoven\")\n</code></pre> <p>Ask a question:</p> <pre><code>result = doc_agent.llm_response(\"When did Beethoven move from Bonn to Vienna?\")\n</code></pre> <p>You should see the streamed response with citations like this:</p> <p></p>"},{"location":"tutorials/langroid-tour/#two-agent-interaction","title":"Two-agent interaction","text":"<p>Set up a teacher agent:</p> <pre><code>from langroid.agent.tools.orchestration import DoneTool\n\nteacher = lr.ChatAgent(\n    lr.ChatAgentConfig(\n        llm=llm_config,\n        system_message=f\"\"\"\n        Ask a numbers-based question, and your student will answer.\n        You can then provide feedback or hints to the student to help them\n        arrive at the right answer. Once you receive the right answer,\n        use the `{DoneTool.name()}` tool to end the session.\n        \"\"\"\n    )\n)\n\nteacher.enable_message(DoneTool)\nteacher_task = lr.Task(teacher, interactive=False)\n</code></pre> <p>Set up a student agent:</p> <pre><code>student = lr.ChatAgent(\n    lr.ChatAgentConfig(\n        llm=llm_config,\n        system_message=f\"\"\"\n        You will receive a numbers-related question. Answer to the best of\n        your ability. If your answer is wrong, you will receive feedback or hints,\n        and you can revise your answer, and repeat this process until you get \n        the right answer.\n        \"\"\"\n    )\n)\n\nstudent_task = lr.Task(student, interactive=False, single_round=True)\n</code></pre> <p>Make the <code>student_task</code> a subtask of the <code>teacher_task</code>:</p> <pre><code>teacher_task.add_sub_task(student_task)\n</code></pre> <p>Run the teacher task:</p> <pre><code>teacher_task.run()\n</code></pre> <p>You should then see this type of interaction:</p> <p></p>"},{"location":"tutorials/llm-usage-options/","title":"Options for accessing LLMs","text":"<p>This is a work-in-progress document. It will be updated frequently.</p> <p>The variety of ways to access the power of Large Language Models (LLMs) is growing  rapidly, and there are a bewildering array of options. This document is an attempt to  categorize and describe some of the most popular and useful ways to access LLMs, via these 2x2x2  combinations:</p> <ul> <li>Websites (non-programmatic) or APIs (programmatic)</li> <li>Open-source or Proprietary </li> <li>Chat-based interface or integrated assistive tools.</li> </ul> <p>We will go into some of these combinations below. More will be added over time.</p>"},{"location":"tutorials/llm-usage-options/#chat-based-web-non-api-access-to-proprietary-llms","title":"Chat-based Web (non-API) access to Proprietary LLMs","text":"<p>This is best for non-programmatic use of LLMs: you go to a website and  interact with the LLM via a chat interface --  you write prompts and/or upload documents, and the LLM responds with plain text or can create artifacts (e.g. reports, code, charts, podcasts, etc) that you can then copy into your files, workflow or codebase. They typically allow you to upload text-based documents of various types, and some let you upload images, screen-shots, etc and ask questions about them.</p> <p>Most of them are capable of doing internet search to inform their responses.</p> <p>Chat Interface vs Integrated Tools</p> <p>Note that when using a chat-based interaction, you have to copy various artifacts from the web-site into another place, like your code editor, document, etc. AI-integrated tools relieve you of this burden by bringing the LLM power into  your workflow directly. More on this in a later section.</p> <p>Pre-requisites: </p> <ul> <li>Computer: Besides having a modern web browser (Chrome, Firefox, etc) and internet access, there are no other special requirements, since the LLM is  running on a remote server.</li> <li>Coding knowledge: Where (typically Python) code is produced, you will get best results if you are conversant with Python so that you can understand and modify the code as needed. In this category you do not need to know how to interact with an LLM API via code.</li> </ul> <p>Here are some popular options in this category:</p>"},{"location":"tutorials/llm-usage-options/#openai-chatgpt","title":"OpenAI ChatGPT","text":"<p>Free access at https://chatgpt.com/</p> <p>With a ChatGPT-Plus monthly subscription ($20/month), you get additional features like:</p> <ul> <li>access to more powerful models</li> <li>access to OpenAI canvas - this offers a richer interface than just a chat window, e.g. it automatically creates windows for code snippets, and shows results of running code (e.g. output, charts etc).</li> </ul> <p>Typical use: Since there is fixed monthly subscription (i.e. not metered by amount of  usage), this is a cost-effective way to non-programmatically  access a top LLM such as <code>GPT-4o</code> or <code>o1</code>  (so-called \"reasoning/thinking\" models). Note however that there are limits on how many queries you can make within a certain time period, but usually the limit is fairly generous. </p> <p>What you can create, besides text-based artifacts:</p> <ul> <li>produce Python (or other language) code which you can copy/paste into notebooks or files</li> <li>SQL queries that you can copy/paste into a database tool</li> <li>Markdown-based tables</li> <li>You can't get diagrams, but you can get code for diagrams,  e.g. python code for plots, mermaid code for flowcharts.</li> <li>images in some cases.</li> </ul>"},{"location":"tutorials/llm-usage-options/#openai-custom-gpts-simply-known-as-gpts","title":"OpenAI Custom GPTs (simply known as \"GPTs\")","text":"<p>https://chatgpt.com/gpts/editor</p> <p>Here you can conversationally interact with a \"GPT Builder\" that will  create a version of ChatGPT that is customized to your needs, i.e. with necessary background instructions, context, and/or documents.  The end result is a specialized GPT that you can then use for your specific purpose and share with others (all of this is non-programmatic). </p> <p>E.g. here is a \"Knowledge Graph Builder\" GPT</p> <p>Private GPTs requires an OpenAI Team Account</p> <p>To share a custom GPT within a private group, you need an OpenAI Team account, see pricing here. Without a Team account, any shared GPT is public and can be accessed by anyone.</p>"},{"location":"tutorials/llm-usage-options/#anthropicclaude","title":"Anthropic/Claude","text":"<p>https://claude.ai</p> <p>The Claude basic web-based interface is similar to OpenAI ChatGPT, powered by  Anthropic's proprietary LLMs.  Anthropic's equivalent of ChatGPT-Plus is called \"Claude Pro\", which is also  a $20/month subscription, giving you access to advanced models  (e.g. <code>Claude-3.5-Sonnet</code>) and features.</p> <p>Anthropic's equivalent of Custom GPTs is called  Projects,  where you can create an  LLM-powered interface that is augmented with your custom context and data.</p> <p>Whichever product you are using, the interface auto-creates artifacts as needed -- these are stand-alone documents (code, text, images, web-pages, etc)  that you may want to copy and paste into your own codebase, documents, etc. For example you can prompt Claude to create full working interactive applications, and copy the code, polish it and deploy it for others to use. See examples here.</p>"},{"location":"tutorials/llm-usage-options/#microsoft-copilot-lab","title":"Microsoft Copilot Lab","text":"<p>Note</p> <p>Microsoft's \"Copilot\" is an overloaded term that can refer to many different  AI-powered tools. Here we are referring to the one that is a collaboration between Microsoft and OpenAI, and is based on OpenAI's GPT-4o LLM, and powered by  Bing's search engine.</p> <p>Accessible via https://copilot.cloud.microsoft.com/</p> <p>The basic capabilities are similar to OpenAI's and Anthropic's offerings, but come with so-called \"enterprise grade\" security and privacy features, which purportedly make it suitable for use in educational and corporate settings. Read more on what you can do with Copilot Lab here.</p> <p>Like the other proprietary offerings, Copilot can:</p> <ul> <li>perform internet search to inform its responses</li> <li>generate/run code and show results including charts</li> </ul>"},{"location":"tutorials/llm-usage-options/#google-gemini","title":"Google Gemini","text":"<p>Accessible at gemini.google.com.</p>"},{"location":"tutorials/llm-usage-options/#ai-powered-productivity-tools","title":"AI-powered productivity tools","text":"<p>These tools \"bring the AI to your workflow\", which is a massive productivity boost, compared to repeatedly context-switching, e.g. copying/pasting between a chat-based AI web-app and your workflow.</p> <ul> <li>Cursor: AI Editor/Integrated Dev Environment (IDE). This is a fork of VSCode.</li> <li>Zed: built in Rust; can be customized to use Jetbrains/PyCharm keyboard shortcuts.</li> <li>Google Colab Notebooks with Gemini.</li> <li>Google NotebookLM: allows you to upload a set of text-based documents,    and create artifacts such as study guide, FAQ, summary, podcasts, etc.</li> </ul>"},{"location":"tutorials/llm-usage-options/#apis-for-proprietary-llms","title":"APIs for Proprietary LLMs","text":"<p>Using an API key allows programmatic access to the LLMs, meaning you can make invocations to the LLM from within your own code, and receive back the results. This is useful for building applications involving more complex workflows where LLMs are used within a larger codebase, to access \"intelligence\" as needed.</p> <p>E.g. suppose you are writing code that handles queries from a user, and you want to  classify the user's intent into one of 3 types: Information, or Action or Done. Pre-LLMs, you would have had to write a bunch of rules or train a custom  \"intent classifier\" that maps, for example:</p> <ul> <li>\"What is the weather in Pittsburgh?\" -&gt; Information</li> <li>\"Set a timer for 10 minutes\" -&gt; Action</li> <li>\"Ok I have no more questions\u221e\" -&gt; Done</li> </ul> <p>But using an LLM API, this is almost trivially easy - you instruct the LLM it should classify the intent into one of these 3 types, and send the user query to the LLM, and receive back the intent.  (You can use Tools to make this robust, but that is outside the scope of this document.)</p> <p>The most popular proprietary LLMs available via API are from OpenAI (or via  its partner Microsoft), Anthropic, and Google:</p> <ul> <li>OpenAI, to interact with <code>GPT-4o</code> family of models, and the <code>o1</code> family of \"thinking/reasoning\" models.</li> <li>Anthropic to use the <code>Claude</code> series of models.</li> <li>Google to use the <code>Gemini</code> family of models.</li> </ul> <p>These LLM providers are home to some of the most powerful LLMs available today, specifically OpenAI's <code>GPT-4o</code> and Anthropic's <code>Claude-3.5-Sonnet</code>, and Google's <code>Gemini 1.5 Pro</code> (as of Oct 2024).</p> <p>Billing: Unlike the fixed monthly subscriptions of ChatGPT, Claude and others,  LLM usage via API is typically billed by token usage, i.e. you pay for the total number of input and output \"tokens\" (a slightly technical term, but think of it as a word for now).</p> <p>Using an LLM API involves these steps:</p> <ul> <li>create an account on the provider's website as a \"developer\" or organization,</li> <li>get an API key,</li> <li>use the API key in your code to make requests to the LLM. </li> </ul> <p>Prerequisites:</p> <ul> <li>Computer: again, since the API is served over the internet, there are no special   requirements for your computer.</li> <li> <p>Programming skills: Using an LLM API involves either:</p> <ul> <li>directly making REST API calls from your code, or </li> <li>use a scaffolding library (like Langroid) that abstracts away the details of the    API calls.</li> </ul> <p>In either case, you must be highly proficient in (Python) programming    to use this option.</p> </li> </ul>"},{"location":"tutorials/llm-usage-options/#web-interfaces-to-open-llms","title":"Web-interfaces to Open LLMs","text":"<p>Open LLMs</p> <p>These are LLMs that have been publicly released, i.e. their parameters (\"weights\")  are publicly available -- we refer to these as open-weight LLMs. If in addition, the training datasets, and data-preprocessing and training code are also available, we would call these open-source LLMs. But lately there is a looser usage of the term \"open-source\",referring to just the weights being available. For our purposes we will just refer all of these models as Open LLMs.</p> <p>There are many options here, but some popular ones are below. Note that some of these are front-ends that allow you to interact with not only Open LLMs but also  proprietary LLM APIs.</p> <ul> <li>LMStudio</li> <li>OpenWebUI</li> <li>Msty</li> <li>AnythingLLM</li> <li>LibreChat</li> </ul>"},{"location":"tutorials/llm-usage-options/#api-access-to-open-llms","title":"API Access to Open LLMs","text":"<p>This is a good option if you are fairly proficient in (Python) coding. There are in  fact two possibilities here:</p> <ul> <li>The LLM is hosted remotely, and you make REST API calls to the remote server. This   is a good option when you want to run large LLMs and you don't have the resources (GPU and memory) to run them locally.<ul> <li>groq amazingly it is free, and you can run <code>llama-3.1-70b</code></li> <li>cerebras</li> <li>open-router</li> </ul> </li> <li>The LLM is running on your computer. This is a good option if your machine has sufficient RAM to accommodate the LLM you are trying to run, and if you are  concerned about data privacy. The most user-friendly option is Ollama; see more below.</li> </ul> <p>Note that all of the above options provide an OpenAI-Compatible API to interact with the LLM, which is a huge convenience: you can write code to interact with OpenAI's LLMs (e.g. <code>GPT4o</code> etc) and then easily switch to one of the above options, typically by changing a simple config (see the respective websites for instructions).</p> <p>Of course, directly working with the raw LLM API quickly becomes tedious. This is where a scaffolding library like langroid comes in very handy - it abstracts away the details of the API calls, and provides a simple programmatic interface to the LLM, and higher-level abstractions like  Agents, Tasks, etc. Working with such a library is going to be far more productive than directly working with the raw API. Below are instructions on how to use langroid with some the above Open/Local LLM options.</p> <p>See here for  a guide to using Langroid with Open LLMs.</p>"},{"location":"tutorials/local-llm-setup/","title":"Setting up a Local/Open LLM to work with Langroid","text":"<p>Examples scripts in <code>examples/</code> directory.</p> <p>There are numerous examples of scripts that can be run with local LLMs,   in the <code>examples/</code>   directory of the main <code>langroid</code> repo. These examples are also in the    <code>langroid-examples</code>,   although the latter repo may contain some examples that are not in the <code>langroid</code> repo.   Most of these example scripts allow you to specify an LLM in the format <code>-m &lt;model&gt;</code>,   where the specification of <code>&lt;model&gt;</code> is described in the quide below for local/open LLMs,    or in the Non-OpenAI LLM guide. Scripts    that have the string <code>local</code> in their name have been especially designed to work with    certain local LLMs, as described in the respective scripts.   If you want a pointer to a specific script that illustrates a 2-agent chat, have a look    at <code>chat-search-assistant.py</code>.   This specific script, originally designed for GPT-4/GPT-4o, works well with <code>llama3-70b</code>    (tested via Groq, mentioned below).</p>"},{"location":"tutorials/local-llm-setup/#easiest-with-ollama","title":"Easiest: with Ollama","text":"<p>As of version 0.1.24, Ollama provides an OpenAI-compatible API server for the LLMs it supports, which massively simplifies running these LLMs with Langroid. Example below.</p> <p><pre><code>ollama pull mistral:7b-instruct-v0.2-q8_0\n</code></pre> This provides an OpenAI-compatible  server for the <code>mistral:7b-instruct-v0.2-q8_0</code> model.</p> <p>You can run any Langroid script using this model, by setting the <code>chat_model</code> in the <code>OpenAIGPTConfig</code> to <code>ollama/mistral:7b-instruct-v0.2-q8_0</code>, e.g.</p> <pre><code>import langroid.language_models as lm\nimport langroid as lr\n\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"ollama/mistral:7b-instruct-v0.2-q8_0\",\n    chat_context_length=16_000, # adjust based on model\n)\nagent_config = lr.ChatAgentConfig(\n    llm=llm_config,\n    system_message=\"You are helpful but concise\",\n)\nagent = lr.ChatAgent(agent_config)\n# directly invoke agent's llm_response method\n# response = agent.llm_response(\"What is the capital of Russia?\")\ntask = lr.Task(agent, interactive=True)\ntask.run() # for an interactive chat loop\n</code></pre>"},{"location":"tutorials/local-llm-setup/#setup-ollama-with-a-gguf-model-from-huggingface","title":"Setup Ollama with a GGUF model from HuggingFace","text":"<p>Some models are not directly supported by Ollama out of the box. To server a GGUF model with Ollama, you can download the model from HuggingFace and set up a custom Modelfile for it.</p> <p>E.g. download the GGUF version of <code>dolphin-mixtral</code> from here</p> <p>(specifically, download this file <code>dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf</code>)</p> <p>To set up a custom ollama model based on this:</p> <ul> <li>Save this model at a convenient place, e.g. <code>~/.ollama/models/</code></li> <li>Create a modelfile for this model. First see what an existing modelfile   for a similar model looks like, e.g. by running:</li> </ul> <p><pre><code>ollama show --modelfile dolphin-mixtral:latest\n</code></pre> You will notice this file has a FROM line followed by a prompt template and other settings. Create a new file with these contents. Only  change the  <code>FROM ...</code> line with the path to the model you downloaded, e.g. <pre><code>FROM /Users/blah/.ollama/models/dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf\n</code></pre></p> <ul> <li>Save this modelfile somewhere, e.g. <code>~/.ollama/modelfiles/dolphin-mixtral-gguf</code></li> <li> <p>Create a new ollama model based on this file: <pre><code>ollama create dolphin-mixtral-gguf -f ~/.ollama/modelfiles/dolphin-mixtral-gguf\n</code></pre></p> </li> <li> <p>Run this new model using <code>ollama run dolphin-mixtral-gguf</code></p> </li> </ul> <p>To use this model with Langroid you can then specify <code>ollama/dolphin-mixtral-gguf</code> as the <code>chat_model</code> param in the <code>OpenAIGPTConfig</code> as in the previous section. When a script supports it, you can also pass in the model name via <code>-m ollama/dolphin-mixtral-gguf</code></p>"},{"location":"tutorials/local-llm-setup/#local-llms-using-lmstudio","title":"Local LLMs using LMStudio","text":"<p>LMStudio is one of the simplest ways to download run open-weight LLMs locally. See their docs at lmstudio.ai for installation and usage  instructions. Once you download a model, you can use the \"server\" option to have it  served via an OpenAI-compatible API at a local IP like <code>https://127.0.0.1:1234/v1</code>. As with any other scenario of running a local LLM, you can use this with Langroid by setting <code>chat_model</code> as follows (note you should not include the <code>https://</code> part):</p> <pre><code>llm_config = lm.OpenAIGPTConfig(\n    chat_model=\"local/127.0.0.1234/v1\",\n    ...\n)\n</code></pre>"},{"location":"tutorials/local-llm-setup/#setup-llamacpp-with-a-gguf-model-from-huggingface","title":"Setup llama.cpp with a GGUF model from HuggingFace","text":"<p>See <code>llama.cpp</code>'s GitHub page for build and installation instructions.</p> <p>After installation, begin as above with downloading a GGUF model from HuggingFace; for example, the quantized <code>Qwen2.5-Coder-7B</code> from here; specifically, this file.</p> <p>Now, the server can be started with <code>llama-server -m qwen2.5-coder-7b-instruct-q2_k.gguf</code>.</p> <p>In addition, your <code>llama.cpp</code> may be built with support for simplified management of HuggingFace models (specifically, <code>libcurl</code> support is required); in this case, <code>llama.cpp</code> will download HuggingFace models to a cache directory, and the server may be run with: <pre><code>llama-server \\\n      --hf-repo Qwen/Qwen2.5-Coder-7B-Instruct-GGUF \\\n      --hf-file qwen2.5-coder-7b-instruct-q2_k.gguf\n</code></pre></p> <p>To use the model with Langroid, specify <code>llamacpp/localhost:{port}</code> as the <code>chat_model</code>; the default port is 8080.</p>"},{"location":"tutorials/local-llm-setup/#setup-vllm-with-a-model-from-huggingface","title":"Setup vLLM with a model from HuggingFace","text":"<p>See the vLLM docs for installation and configuration options. To run a HuggingFace model with vLLM, use <code>vllm serve</code>, which provides an OpenAI-compatible server. </p> <p>For example, to run <code>Qwen2.5-Coder-32B</code>, run <code>vllm serve Qwen/Qwen2.5-Coder-32B</code>.</p> <p>If the model is not publicly available, set the environment varaible <code>HF_TOKEN</code> to your HuggingFace token with read access to the model repo.</p> <p>To use the model with Langroid, specify <code>vllm/Qwen/Qwen2.5-Coder-32B</code> as the <code>chat_model</code> and, if a port other than the default 8000 was used, set <code>api_base</code> to <code>localhost:{port}</code>.</p>"},{"location":"tutorials/local-llm-setup/#setup-vllm-with-a-gguf-model-from-huggingface","title":"Setup vLLM with a GGUF model from HuggingFace","text":"<p><code>vLLM</code> supports running quantized models from GGUF files; however, this is currently an experimental feature. To run a quantized <code>Qwen2.5-Coder-32B</code>, download the model from the repo, specifically this file. </p> <p>The model can now be run with <code>vllm serve qwen2.5-coder-32b-instruct-q4_0.gguf --tokenizer Qwen/Qwen2.5-Coder-32B</code> (the tokenizer of the base model rather than the quantized model should be used).</p> <p>To use the model with Langroid, specify <code>vllm/qwen2.5-coder-32b-instruct-q4_0.gguf</code> as the <code>chat_model</code> and, if a port other than the default 8000 was used, set <code>api_base</code> to <code>localhost:{port}</code>.</p>"},{"location":"tutorials/local-llm-setup/#local-llms-hosted-on-groq","title":"\"Local\" LLMs hosted on Groq","text":"<p>In this scenario, an open-source LLM (e.g. <code>llama3.1-8b-instant</code>) is hosted on a Groq server which provides an OpenAI-compatible API. Using this with langroid is exactly analogous to the Ollama scenario above: you can set the <code>chat_model</code> in the <code>OpenAIGPTConfig</code> to <code>groq/&lt;model_name&gt;</code>, e.g. <code>groq/llama3.1-8b-instant</code>.  For this to work, ensure you have a <code>GROQ_API_KEY</code> environment variable set in your <code>.env</code> file. See groq docs.</p>"},{"location":"tutorials/local-llm-setup/#local-llms-hosted-on-cerebras","title":"\"Local\" LLMs hosted on Cerebras","text":"<p>This works exactly like with Groq, except you set up a <code>CEREBRAS_API_KEY</code> environment variable, and specify the <code>chat_model</code> as <code>cerebras/&lt;model_name&gt;</code>, e.g. <code>cerebras/llama3.1-8b</code>. See the Cerebras docs for details on which LLMs are supported.</p>"},{"location":"tutorials/local-llm-setup/#openproprietary-llms-via-openrouter","title":"Open/Proprietary LLMs via OpenRouter","text":"<p>OpenRouter is a paid service that provides an OpenAI-compatible API  for practically any LLM, open or proprietary. Using this with Langroid is similar to the <code>groq</code> scenario above:</p> <ul> <li>Ensure you have an <code>OPENROUTER_API_KEY</code> set up in your environment (or <code>.env</code> file), and </li> <li>Set the <code>chat_model</code> in the <code>OpenAIGPTConfig</code> to    <code>openrouter/&lt;model_name&gt;</code>, where <code>&lt;model_name&gt;</code> is the name of the model on the  OpenRouter website, e.g. <code>qwen/qwen-2.5-7b-instruct</code>.</li> </ul> <p>This is a good option if you want to use larger open LLMs without having to download them locally (especially if your local machine does not have the resources to run them). Besides using specific LLMs, OpenRouter also has smart routing/load-balancing. OpenRouter is also convenient for using proprietary LLMs (e.g. gemini, amazon) via  a single convenient API.</p>"},{"location":"tutorials/local-llm-setup/#local-llms-hosted-on-glhfchat","title":"\"Local\" LLMs hosted on GLHF.chat","text":"<p>See glhf.chat for a list of available models.</p> <p>To run with one of these models, set the <code>chat_model</code> in the <code>OpenAIGPTConfig</code> to <code>\"glhf/&lt;model_name&gt;\"</code>, where <code>model_name</code> is <code>hf:</code> followed by the HuggingFace repo  path, e.g. <code>Qwen/Qwen2.5-Coder-32B-Instruct</code>, so the full <code>chat_model</code> would be <code>\"glhf/hf:Qwen/Qwen2.5-Coder-32B-Instruct\"</code>. </p>"},{"location":"tutorials/local-llm-setup/#deepseek-llms","title":"DeepSeek LLMs","text":"<p>As of 26-Dec-2024, DeepSeek models are available via their api. To use it with Langroid:</p> <ul> <li>set up your <code>DEEPSEEK_API_KEY</code> environment variable in the <code>.env</code> file or as  an explicit export in your shell</li> <li>set the <code>chat_model</code> in the <code>OpenAIGPTConfig</code> to <code>deepseek/deepseek-chat</code> to use the  <code>DeepSeek-V3</code> model, or <code>deepseek/deepseek-reasoner</code> to use the full (i.e. non-distilled) <code>DeepSeek-R1</code> \"reasoning\" model.</li> </ul> <p>The DeepSeek models are also available via OpenRouter (see the corresponding  in the OpenRouter section here) or ollama (see those instructions). E.g. you can use the DeepSeek R1 or its distilled variants by setting <code>chat_model</code> to  <code>openrouter/deepseek/deepseek-r1</code> or <code>ollama/deepseek-r1:8b</code>.</p>"},{"location":"tutorials/local-llm-setup/#other-non-openai-llms-supported-by-litellm","title":"Other non-OpenAI LLMs supported by LiteLLM","text":"<p>For other scenarios of running local/remote LLMs, it is possible that the <code>LiteLLM</code> library supports an \"OpenAI adaptor\" for these models (see their docs).</p> <p>Depending on the specific model, the <code>litellm</code> docs may say you need to  specify a model in the form <code>&lt;provider&gt;/&lt;model&gt;</code>, e.g. <code>palm/chat-bison</code>.  To use the model with Langroid, simply prepend <code>litellm/</code> to this string, e.g. <code>litellm/palm/chat-bison</code>, when you specify the <code>chat_model</code> in the <code>OpenAIGPTConfig</code>.</p> <p>To use <code>litellm</code>, ensure you have the <code>litellm</code> extra installed,  via <code>pip install langroid[litellm]</code> or equivalent.</p>"},{"location":"tutorials/local-llm-setup/#harder-with-oobabooga","title":"Harder: with oobabooga","text":"<p>Like Ollama, oobabooga/text-generation-webui provides an OpenAI-API-compatible API server, but the setup  is significantly more involved. See their github page for installation and model-download instructions.</p> <p>Once you have finished the installation, you can spin up the server for an LLM using something like this:</p> <p><pre><code>python server.py --api --model mistral-7b-instruct-v0.2.Q8_0.gguf --verbose --extensions openai --nowebui\n</code></pre> This will show a message saying that the OpenAI-compatible API is running at <code>http://127.0.0.1:5000</code></p> <p>Then in your Langroid code you can specify the LLM config using <code>chat_model=\"local/127.0.0.1:5000/v1</code> (the <code>v1</code> is the API version, which is required). As with Ollama, you can use the <code>-m</code> arg in many of the example scripts, e.g. <pre><code>python examples/docqa/rag-local-simple.py -m local/127.0.0.1:5000/v1\n</code></pre></p> <p>Recommended: to ensure accurate chat formatting (and not use the defaults from ooba),   append the appropriate HuggingFace model name to the   -m arg, separated by //, e.g.  <pre><code>python examples/docqa/rag-local-simple.py -m local/127.0.0.1:5000/v1//mistral-instruct-v0.2\n</code></pre>   (no need to include the full model name, as long as you include enough to    uniquely identify the model's chat formatting template)</p>"},{"location":"tutorials/local-llm-setup/#other-local-llm-scenarios","title":"Other local LLM scenarios","text":"<p>There may be scenarios where the above <code>local/...</code> or <code>ollama/...</code> syntactic shorthand does not work.(e.g. when using vLLM to spin up a local LLM at an OpenAI-compatible endpoint). For these scenarios, you will have to explicitly create an instance of  <code>lm.OpenAIGPTConfig</code> and set both the <code>chat_model</code> and <code>api_base</code> parameters. For example, suppose you are able to get responses from this endpoint using something like: <pre><code>curl http://192.168.0.5:5078/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"Mistral-7B-Instruct-v0.2\",\n        \"messages\": [\n             {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n        ]\n    }'\n</code></pre> To use this endpoint with Langroid, you would create an <code>OpenAIGPTConfig</code> like this: <pre><code>import langroid.language_models as lm\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"Mistral-7B-Instruct-v0.2\",\n    api_base=\"http://192.168.0.5:5078/v1\",\n)\n</code></pre></p>"},{"location":"tutorials/local-llm-setup/#quick-testing-with-local-llms","title":"Quick testing with local LLMs","text":"<p>As mentioned here,  you can run many of the tests in the main langroid repo against a local LLM (which by default run against an OpenAI model),  by specifying the model as <code>--m &lt;model&gt;</code>,  where <code>&lt;model&gt;</code> follows the syntax described in the previous sections. Here's an example:</p> <p><pre><code>pytest tests/main/test_chat_agent.py --m ollama/mixtral\n</code></pre> Of course, bear in mind that the tests may not pass due to weaknesses of the local LLM.</p>"},{"location":"tutorials/non-openai-llms/","title":"Using Langroid with Non-OpenAI LLMs","text":"<p>Langroid was initially written to work with OpenAI models via their API. This may sound limiting, but fortunately:</p> <ul> <li>Many open-source LLMs can be served via  OpenAI-compatible endpoints. See the Local LLM Setup guide for details.</li> <li>There are tools like LiteLLM    that provide an OpenAI-like API for hundreds of non-OpenAI LLM providers  (e.g. Anthropic's Claude, Google's Gemini).</li> <li>AI gateways like LangDB, Portkey, and OpenRouter provide unified access to multiple LLM providers with additional features like cost control, observability, caching, and fallback strategies.</li> </ul> <p>Below we show how you can use these various options with Langroid.</p>"},{"location":"tutorials/non-openai-llms/#create-an-openaigptconfig-object-with-chat_model-litellm","title":"Create an <code>OpenAIGPTConfig</code> object with <code>chat_model = \"litellm/...\"</code>","text":"<p>Install <code>litellm</code> extra</p> <p>To use <code>litellm</code> you need to install Langroid with the <code>litellm</code> extra, e.g.: <code>pip install \"langroid[litellm]\"</code></p> <p>Next, look up the instructions in LiteLLM docs for the specific model you are  interested. Here we take the example of Anthropic's <code>claude-instant-1</code> model. Set up the necessary environment variables as specified in the LiteLLM docs, e.g. for the <code>claude-instant-1</code> model, you will need to set the <code>ANTHROPIC_API_KEY</code> <pre><code>export ANTHROPIC_API_KEY=my-api-key\n</code></pre></p> <p>Now you are ready to create an instance of <code>OpenAIGPTConfig</code> with the  <code>chat_model</code> set to <code>litellm/&lt;model_spec&gt;</code>, where you should set <code>model_spec</code> based on LiteLLM  docs. For example, for the <code>claude-instant-1</code> model, you would set <code>chat_model</code> to <code>litellm/claude-instant-1</code>. But if you are using the model via a 3rd party provider, (e.g. those via Amazon Bedrock), you may also need to have a <code>provider</code> part in the <code>model_spec</code>, e.g.  <code>litellm/bedrock/anthropic.claude-instant-v1</code>. In general you can see which of these to use, from the LiteLLM docs.</p> <pre><code>import langroid.language_models as lm\n\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"litellm/claude-instant-v1\",\n    chat_context_length=8000, # adjust according to model\n)\n</code></pre> <p>A similar process works for the <code>Gemini 1.5 Pro</code> LLM:</p> <ul> <li>get the API key here</li> <li>set the <code>GEMINI_API_KEY</code> environment variable in your <code>.env</code> file or shell</li> <li>set <code>chat_model=\"litellm/gemini/gemini-1.5-pro-latest\"</code> in the <code>OpenAIGPTConfig</code> object</li> </ul> <p>For other gemini models supported by litellm, see their docs</p>"},{"location":"tutorials/non-openai-llms/#gemini-llms-via-openai-client-without-litellm","title":"Gemini LLMs via OpenAI client, without LiteLLM","text":"<p>This is now the recommended way to use Gemini LLMs with Langroid, where you don't need to use LiteLLM. As of 11/20/2024, these models are available via the OpenAI client.</p> <p>To use langroid with Gemini LLMs, all you have to do is:</p> <ul> <li>set the <code>GEMINI_API_KEY</code> environment variable in your <code>.env</code> file or shell</li> <li>set <code>chat_model=\"gemini/&lt;model_name&gt;\"</code> in the <code>OpenAIGPTConfig</code> object,   where  is one of \"gemini-1.5-flash\", \"gemini-1.5-flash-8b\", or \"gemini-1.5-pro\" <p>See here for details on Gemini models.</p> <p>For example, you can use this <code>llm_config</code>:</p> <pre><code>llm_config = lm.OpenAIGPTConfig(\n    chat_model=\"gemini/\" + lm.OpenAIChatModel.GEMINI_1_5_FLASH,\n)\n</code></pre> <p>In most tests you can switch to a gemini model, e.g. <code>--m gemini/gemini-1.5-flash</code>,  e.g.:</p> <pre><code>pytest -xvs tests/main/test_llm.py --m gemini/gemini-1.5-flash\n</code></pre> <p>Many of the example scripts allow switching the model using <code>-m</code> or <code>--model</code>, e.g.</p> <pre><code>python3 examples/basic/chat.py -m gemini/gemini-1.5-flash\n</code></pre>"},{"location":"tutorials/non-openai-llms/#ai-gateways-for-multiple-llm-providers","title":"AI Gateways for Multiple LLM Providers","text":"<p>In addition to LiteLLM, Langroid integrates with AI gateways that provide unified access to multiple LLM providers with additional enterprise features:</p>"},{"location":"tutorials/non-openai-llms/#langdb","title":"LangDB","text":"<p>LangDB is an AI gateway offering OpenAI-compatible APIs to access 250+ LLMs with cost control, observability, and performance benchmarking. LangDB enables seamless model switching while providing detailed analytics and usage tracking.</p> <p>To use LangDB with Langroid: - Set up your <code>LANGDB_API_KEY</code> and <code>LANGDB_PROJECT_ID</code> environment variables - Set <code>chat_model=\"langdb/&lt;provider&gt;/&lt;model_name&gt;\"</code> in the <code>OpenAIGPTConfig</code> (e.g., <code>\"langdb/anthropic/claude-3.7-sonnet\"</code>)</p> <p>For detailed setup and usage instructions, see the LangDB integration guide.</p>"},{"location":"tutorials/non-openai-llms/#portkey","title":"Portkey","text":"<p>Portkey is a comprehensive AI gateway that provides access to 200+ models from various providers through a unified API. It offers advanced features like intelligent caching, automatic retries, fallback strategies, and comprehensive observability tools for production deployments.</p> <p>To use Portkey with Langroid: - Set up your <code>PORTKEY_API_KEY</code> environment variable (plus provider API keys like <code>OPENAI_API_KEY</code>) - Set <code>chat_model=\"portkey/&lt;provider&gt;/&lt;model_name&gt;\"</code> in the <code>OpenAIGPTConfig</code> (e.g., <code>\"portkey/openai/gpt-4o-mini\"</code>)</p> <p>For detailed setup and usage instructions, see the Portkey integration guide.</p>"},{"location":"tutorials/non-openai-llms/#openrouter","title":"OpenRouter","text":"<p>OpenRouter provides access to a wide variety of both open and proprietary LLMs through a unified API. It features automatic routing and load balancing, making it particularly useful for accessing larger open LLMs without local resources and for using multiple providers through a single interface.</p> <p>To use OpenRouter with Langroid: - Set up your <code>OPENROUTER_API_KEY</code> environment variable - Set <code>chat_model=\"openrouter/&lt;model_name&gt;\"</code> in the <code>OpenAIGPTConfig</code></p> <p>For more details, see the Local LLM Setup guide.</p>"},{"location":"tutorials/non-openai-llms/#working-with-the-created-openaigptconfig-object","title":"Working with the created <code>OpenAIGPTConfig</code> object","text":"<p>From here you can proceed as usual, creating instances of <code>OpenAIGPT</code>, <code>ChatAgentConfig</code>, <code>ChatAgent</code> and <code>Task</code> object as usual.</p> <p>E.g. you can create an object of class <code>OpenAIGPT</code> (which represents any LLM with an OpenAI-compatible API) and interact with it directly: <pre><code>llm = lm.OpenAIGPT(llm_config)\nmessages = [\n    LLMMessage(content=\"You are a helpful assistant\",  role=Role.SYSTEM),\n    LLMMessage(content=\"What is the capital of Ontario?\",  role=Role.USER),\n],\nresponse = mdl.chat(messages, max_tokens=50)\n</code></pre></p> <p>When you interact directly with the LLM, you are responsible for keeping dialog history. Also you would often want an LLM to have access to tools/functions and external data/documents (e.g. vector DB or traditional DB). An Agent class simplifies managing all of these. For example, you can create an Agent powered by the above LLM, wrap it in a Task and have it run as an interactive chat app:</p> <pre><code>agent_config = lr.ChatAgentConfig(llm=llm_config, name=\"my-llm-agent\")\nagent = lr.ChatAgent(agent_config)\n\ntask = lr.Task(agent, name=\"my-llm-task\")\ntask.run()\n</code></pre>"},{"location":"tutorials/non-openai-llms/#example-simple-chat-script-with-a-non-openai-proprietary-model","title":"Example: Simple Chat script with a non-OpenAI proprietary model","text":"<p>Many of the Langroid example scripts have a convenient <code>-m</code>  flag that lets you easily switch to a different model. For example, you can run  the <code>chat.py</code> script in the <code>examples/basic</code> folder with the  <code>litellm/claude-instant-v1</code> model: <pre><code>python3 examples/basic/chat.py -m litellm/claude-instant-1\n</code></pre></p>"},{"location":"tutorials/non-openai-llms/#quick-testing-with-non-openai-models","title":"Quick testing with non-OpenAI models","text":"<p>There are numerous tests in the main Langroid repo that involve LLMs, and once you setup the dev environment as described in the README of the repo,  you can run any of those tests (which run against the default GPT4 model) against local/remote models that are proxied by <code>liteLLM</code> (or served locally via the options mentioned above, such as <code>oobabooga</code>, <code>ollama</code> or <code>llama-cpp-python</code>), using the <code>--m &lt;model-name&gt;</code> option, where <code>model-name</code> takes one of the forms above. Some examples of tests are:</p> <p><pre><code>pytest -s tests/test_llm.py --m local/localhost:8000\npytest -s tests/test_llm.py --m litellm/claude-instant-1\n</code></pre> When the <code>--m</code> option is omitted, the default OpenAI GPT4 model is used.</p> <p><code>chat_context_length</code> is not affected by <code>--m</code></p> <p>Be aware that the <code>--m</code> only switches the model, but does not affect the <code>chat_context_length</code>    parameter in the <code>OpenAIGPTConfig</code> object. which you may need to adjust for different models.   So this option is only meant for quickly testing against different models, and not meant as   a way to switch between models in a production environment.</p>"},{"location":"tutorials/postgresql-agent/","title":"Chat with a PostgreSQL DB using SQLChatAgent","text":"<p>The <code>SQLChatAgent</code> is designed to facilitate interactions with an SQL database using natural language. A ready-to-use script based on the <code>SQLChatAgent</code> is available in the <code>langroid-examples</code>  repo at <code>examples/data-qa/sql-chat/sql_chat.py</code> (and also in a similar location in the main <code>langroid</code> repo). This tutorial walks you through how you might use the <code>SQLChatAgent</code> if you were to write your own script from scratch. We also show some of the internal workings of this Agent.</p> <p>The agent uses the schema context to generate SQL queries based on a user's input. Here is a tutorial on how to set up an agent with your PostgreSQL database. The steps for other databases are similar. Since the agent implementation relies  on SqlAlchemy, it should work with any SQL DB that supports SqlAlchemy. It offers enhanced functionality for MySQL and PostgreSQL by  automatically extracting schemas from the database. </p>"},{"location":"tutorials/postgresql-agent/#before-you-begin","title":"Before you begin","text":"<p>Data Privacy Considerations</p> <p>Since the SQLChatAgent uses the OpenAI GPT-4 as the underlying language model, users should be aware that database information processed by the agent may be sent to OpenAI's API and should therefore be comfortable with this.</p> <ol> <li> <p>Install PostgreSQL dev libraries for your platform, e.g.</p> <ul> <li><code>sudo apt-get install libpq-dev</code> on Ubuntu,</li> <li><code>brew install postgresql</code> on Mac, etc.</li> </ul> </li> <li> <p>Follow the general setup guide to get started with Langroid (mainly, install <code>langroid</code> into your virtual env, and set up suitable values in  the <code>.env</code> file). Note that to use the SQLChatAgent with a PostgreSQL database, you need to install the <code>langroid[postgres]</code> extra, e.g.:</p> <ul> <li><code>pip install \"langroid[postgres]\"</code> or </li> <li><code>poetry add \"langroid[postgres]\"</code> or <code>uv add \"langroid[postgres]\"</code></li> <li><code>poetry install -E postgres</code> or <code>uv pip install --extra postgres -r pyproject.toml</code></li> </ul> </li> </ol> <p>If this gives you an error, try <code>pip install psycopg2-binary</code> in your virtualenv.</p>"},{"location":"tutorials/postgresql-agent/#initialize-the-agent","title":"Initialize the agent","text":"<pre><code>from langroid.agent.special.sql.sql_chat_agent import (\n    SQLChatAgent,\n    SQLChatAgentConfig,\n)\n\nagent = SQLChatAgent(\n    config=SQLChatAgentConfig(\n        database_uri=\"postgresql://example.db\",\n    )\n)\n</code></pre>"},{"location":"tutorials/postgresql-agent/#configuration","title":"Configuration","text":"<p>The following components of <code>SQLChatAgentConfig</code> are optional but strongly recommended for improved results:</p> <ul> <li><code>context_descriptions</code>: A nested dictionary that specifies the schema context for   the agent to use when generating queries, for example:</li> </ul> <pre><code>{\n  \"table1\": {\n    \"description\": \"description of table1\",\n    \"columns\": {\n      \"column1\": \"description of column1 in table1\",\n      \"column2\": \"description of column2 in table1\"\n    }\n  },\n  \"employees\": {\n    \"description\": \"The 'employees' table contains information about the employees. It relates to the 'departments' and 'sales' tables via foreign keys.\",\n    \"columns\": {\n      \"id\": \"A unique identifier for an employee. This ID is used as a foreign key in the 'sales' table.\",\n      \"name\": \"The name of the employee.\",\n      \"department_id\": \"The ID of the department the employee belongs to. This is a foreign key referencing the 'id' in the 'departments' table.\"\n    }\n  }\n}\n</code></pre> <p>By default, if no context description json file is provided in the config, the  agent will automatically generate the file using the built-in Postgres table/column comments.</p> <ul> <li> <p><code>schema_tools</code>: When set to <code>True</code>, activates a retrieval mode where the agent   systematically requests only the parts of the schemas relevant to the current query.    When this option is enabled, the agent performs the following steps:</p> <ol> <li>Asks for table names.</li> <li>Asks for table descriptions and column names from possibly relevant table    names.</li> <li>Asks for column descriptions from possibly relevant columns.</li> <li>Writes the SQL query.</li> </ol> </li> </ul> <p>Setting <code>schema_tools=True</code> is especially useful for large schemas where it is costly or impossible    to include the entire schema in a query context.    By selectively using only the relevant parts of the context descriptions, this mode   reduces token usage, though it may result in 1-3 additional OpenAI API calls before   the final SQL query is generated.</p>"},{"location":"tutorials/postgresql-agent/#putting-it-all-together","title":"Putting it all together","text":"<p>In the code below, we will allow the agent to generate the context descriptions from table comments by excluding the <code>context_descriptions</code> config option. We set <code>schema_tools</code> to <code>True</code> to enable the retrieval mode.</p> <pre><code>from langroid.agent.special.sql.sql_chat_agent import (\n    SQLChatAgent,\n    SQLChatAgentConfig,\n)\n\n# Initialize SQLChatAgent with a PostgreSQL database URI and enable schema_tools\nagent = SQLChatAgent(gi\nconfig = SQLChatAgentConfig(\n    database_uri=\"postgresql://example.db\",\n    schema_tools=True,\n)\n)\n\n# Run the task to interact with the SQLChatAgent\ntask = Task(agent)\ntask.run()\n</code></pre> <p>By following these steps, you should now be able to set up an <code>SQLChatAgent</code> that interacts with a PostgreSQL database, making querying a seamless experience.</p> <p>In the <code>langroid</code> repo we have provided a ready-to-use script <code>sql_chat.py</code> based on the above, that you can use right away to interact with your PostgreSQL database:</p> <pre><code>python3 examples/data-qa/sql-chat/sql_chat.py\n</code></pre> <p>This script will prompt you for the database URI, and then start the agent.</p>"},{"location":"tutorials/supported-models/","title":"Langroid Supported LLMs and Providers","text":"<p>Langroid supports a wide range of Language Model providers through its  <code>OpenAIGPTConfig</code> class. </p> <p>OpenAIGPTConfig is not just for OpenAI models!</p> <p>The <code>OpenAIGPTConfig</code> class is a generic configuration class that can be used to configure any LLM provider that is OpenAI API-compatible. This includes both local and remote models.</p> <p>You would typically set up the <code>OpenAIGPTConfig</code> class with the <code>chat_model</code> parameter, which specifies the model you want to use, and other  parameters such as <code>max_output_tokens</code>, <code>temperature</code>, etc (see the  <code>OpenAIGPTConfig</code> class and its parent class  <code>LLModelConfig</code> for full parameter details):</p> <pre><code>import langroid.language_models as lm\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"&lt;model-name&gt;\", # possibly includes a &lt;provider-name&gt; prefix\n    api_key=\"api-key\", # optional, prefer setting in environment variables\n    # ... other params such as max_tokens, temperature, etc.\n)\n</code></pre> <p>Below are <code>chat_model</code> examples for each supported provider. For more details see the guides on setting up Langroid with  local  and non-OpenAI LLMs. Once you set up the <code>OpenAIGPTConfig</code>, you can then directly interact with the LLM, or set up an Agent with this LLM, and use it by itself, or in a multi-agent setup, as shown in the Langroid quick tour</p> <p>Although we support specifying the <code>api_key</code> directly in the config (not recommended for security reasons), more typically you would set the <code>api_key</code> in your environment variables. Below is a table showing for each provider, an example <code>chat_model</code> setting,  and which environment variable to set for the API key.</p> Provider <code>chat_model</code> Example API Key Environment Variable OpenAI <code>gpt-4o</code> <code>OPENAI_API_KEY</code> Groq <code>groq/llama3.3-70b-versatile</code> <code>GROQ_API_KEY</code> Cerebras <code>cerebras/llama-3.3-70b</code> <code>CEREBRAS_API_KEY</code> Gemini <code>gemini/gemini-2.0-flash</code> <code>GEMINI_API_KEY</code> DeepSeek <code>deepseek/deepseek-reasoner</code> <code>DEEPSEEK_API_KEY</code> GLHF <code>glhf/hf:Qwen/Qwen2.5-Coder-32B-Instruct</code> <code>GLHF_API_KEY</code> OpenRouter <code>openrouter/deepseek/deepseek-r1-distill-llama-70b:free</code> <code>OPENROUTER_API_KEY</code> Ollama <code>ollama/qwen2.5</code> <code>OLLAMA_API_KEY</code> (usually <code>ollama</code>) VLLM <code>vllm/mistral-7b-instruct</code> <code>VLLM_API_KEY</code> LlamaCPP <code>llamacpp/localhost:8080</code> <code>LLAMA_API_KEY</code> Generic Local <code>local/localhost:8000/v1</code> No specific env var required LiteLLM <code>litellm/anthropic/claude-3-7-sonnet</code> Depends on provider <code>litellm/mistral-small</code> Depends on provider HF Template <code>local/localhost:8000/v1//mistral-instruct-v0.2</code> Depends on provider <code>litellm/ollama/mistral//hf</code>"},{"location":"tutorials/supported-models/#huggingface-chat-template-formatting","title":"HuggingFace Chat Template Formatting","text":"<p>For models requiring specific prompt formatting:</p> <pre><code>import langroid.language_models as lm\n\n# Specify formatter directly\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"local/localhost:8000/v1//mistral-instruct-v0.2\",\n    formatter=\"mistral-instruct-v0.2\"\n)\n\n# Using HF formatter auto-detection\nllm_config = lm.OpenAIGPTConfig(\n    chat_model=\"litellm/ollama/mistral//hf\",\n)\n</code></pre>"}]}