,id_,embedding,metadata,excluded_embed_metadata_keys,excluded_llm_metadata_keys,relationships,hash,text,start_char_idx,end_char_idx,text_template,metadata_template,metadata_seperator,class_name
0,67f8fd9a-fd4a-43eb-8959-fa25db6f7328,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 26, 'assignee': '', 'size': '', 'index_id': '9703'}",[],[],{},c04e142831e127d66e5053030cb447a5ba5d61e7f406c64814f093530ed62762,"[Bug]: AI Search showing warning for index migration to vector profile
### Bug Description

When I created and ingested data using llamaindex, AI search showing following warning and asking for migration.
`This index has vector fields created using the 2023-07-01-preview definition. It can't be edited using new API versions. We recommend that you migrate it to use vector profiles to restore full portal functionality. `

### Version

0.9.16

### Steps to Reproduce

Create index and ingest data using llamaindex.

### Relevant Logs/Tracbacks

```shell
This index has vector fields created using the 2023-07-01-preview definition. It can't be edited using new API versions. We recommend that you migrate it to use vector profiles to restore full portal functionality.
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
1,be90613a-2b24-459c-bb70-4fc8290aae44,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 26, 'assignee': '', 'size': '', 'index_id': '9702'}",[],[],{},afd9e6fd8ecd93a3cc2ab045b955fd3058afc7e8fa5cf5104a5388e712324c39,"[Bug]: collection not found in `QdrantVectorStore` in `MultiModalVectorStoreIndex`
### Bug Description

please see the issue here: https://github.com/qdrant/qdrant/issues/3289 

### Version

0.9.21

### Steps to Reproduce

- install python 3.11.7 and qdrant_client 1.7.0 and CLIP from their github
- run this notebook https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_pdf_tables.ipynb 
specifically this part
```python
# Read the images
documents_images = SimpleDirectoryReader(""./llama2/"").load_data()

# Create a local Qdrant vector store
client = qdrant_client.QdrantClient(path=""qdrant_index"")

text_store = QdrantVectorStore(
    client=client, collection_name=""text_collection""
)
image_store = QdrantVectorStore(
    client=client, collection_name=""image_collection""
)
storage_context = StorageContext.from_defaults(
    vector_store=text_store, image_store=image_store
)

# Create the MultiModal index
index = MultiModalVectorStoreIndex.from_documents(
    documents_images,
    storage_context=storage_context,
)
```

### Relevant Logs/Tracbacks

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[15], line 2
      1 # Create the MultiModal index
----> 2 index = MultiModalVectorStoreIndex.from_documents(
      3     documents_images,
      4     storage_context=storage_context,
      5 )
      6 retriever_engine = index.as_retriever(image_similarity_top_k=2)

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/llama_index/indices/base.py:106, in BaseIndex.from_documents(cls, documents, storage_context, service_context, show_progress, **kwargs)
     97     docstore.set_document_hash(doc.get_doc_id(), doc.hash)
     99 nodes = run_transformations(
    100     documents,  # type: ignore
    101     service_context.transformations,
    102     show_progress=show_progress,
    103     **kwargs,
    104 )
--> 106 return cls(
    107     nodes=nodes,
    108     storage_context=storage_context,
    109     service_context=service_context,
    110     show_progress=show_progress,
    111     **kwargs,
    112 )

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/llama_index/indices/multi_modal/base.py:80, in MultiModalVectorStoreIndex.__init__(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, image_vector_store, image_embed_model, is_image_to_text, **kwargs)
     76     storage_context.add_vector_store(SimpleVectorStore(), self.image_namespace)
     78 self._image_vector_store = storage_context.vector_stores[self.image_namespace]
---> 80 super().__init__(
     81     nodes=nodes,
     82     index_struct=index_struct,
     83     service_context=service_context,
     84     storage_context=storage_context,
     85     show_progress=show_progress,
     86     use_async=use_async,
     87     store_nodes_override=store_nodes_override,
     88     **kwargs,
     89 )

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py:52, in VectorStoreIndex.__init__(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, insert_batch_size, show_progress, **kwargs)
     50 self._store_nodes_override = store_nodes_override
     51 self._insert_batch_size = insert_batch_size
---> 52 super().__init__(
     53     nodes=nodes,
     54     index_struct=index_struct,
     55     service_context=service_context,
     56     storage_context=storage_context,
     57     show_progress=show_progress,
     58     **kwargs,
     59 )

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/llama_index/indices/base.py:71, in BaseIndex.__init__(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)
     69 if index_struct is None:
     70     assert nodes is not None
---> 71     index_struct = self.build_index_from_nodes(nodes)
     72 self._index_struct = index_struct
     73 self._storage_context.index_store.add_index_struct(self._index_struct)

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py:262, in VectorStoreIndex.build_index_from_nodes(self, nodes, **insert_kwargs)
    251 def build_index_from_nodes(
    252     self,
    253     nodes: Sequence[BaseNode],
    254     **insert_kwargs: Any,
    255 ) -> IndexDict:
    256     """"""Build the index from nodes.
    257 
    258     NOTE: Overrides BaseIndex.build_index_from_nodes.
    259         VectorStoreIndex only stores nodes in document store
    260         if vector store does not store text
    261     """"""
--> 262     return self._build_index_from_nodes(nodes, **insert_kwargs)

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/llama_index/indices/vector_store/base.py:243, in VectorStoreIndex._build_index_from_nodes(self, nodes, **insert_kwargs)
    241     run_async_tasks(tasks)
    242 else:
--> 243     self._add_nodes_to_index(
    244         index_struct,
    245         nodes,
    246         show_progress=self._show_progress,
    247         **insert_kwargs,
    248     )
    249 return index_struct

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/llama_index/indices/multi_modal/base.py:330, in MultiModalVectorStoreIndex._add_nodes_to_index(self, index_struct, nodes, show_progress, **insert_kwargs)
    326 # embed all nodes as text - incclude image nodes that have text attached
    327 text_nodes = self._get_node_with_embedding(
    328     text_nodes, show_progress, is_image=False
    329 )
--> 330 new_text_ids = self.storage_context.vector_stores[DEFAULT_VECTOR_STORE].add(
    331     text_nodes, **insert_kwargs
    332 )
    334 # embed image nodes as images directly
    335 # check if we should use text embedding for images instead of default
    336 image_nodes = self._get_node_with_embedding(
    337     image_nodes,
    338     show_progress,
    339     is_image=True,
    340 )

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/llama_index/vector_stores/qdrant.py:216, in QdrantVectorStore.add(self, nodes, **add_kwargs)
    209     self._create_collection(
    210         collection_name=self.collection_name,
    211         vector_size=len(nodes[0].get_embedding()),
    212     )
    214 points, ids = self._build_points(nodes)
--> 216 self._client.upsert(
    217     collection_name=self.collection_name,
    218     points=points,
    219 )
    221 return ids

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/qdrant_client/qdrant_client.py:987, in QdrantClient.upsert(self, collection_name, points, wait, ordering, shard_key_selector, **kwargs)
    959 """"""
    960 Update or insert a new point into the collection.
    961 
   (...)
    983     Operation Result(UpdateResult)
    984 """"""
    985 assert len(kwargs) == 0, f""Unknown arguments: {list(kwargs.keys())}""
--> 987 return self._client.upsert(
    988     collection_name=collection_name,
    989     points=points,
    990     wait=wait,
    991     ordering=ordering,
    992     shard_key_selector=shard_key_selector,
    993     **kwargs,
    994 )

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/qdrant_client/local/qdrant_local.py:420, in QdrantLocal.upsert(self, collection_name, points, **kwargs)
    417 def upsert(
    418     self, collection_name: str, points: types.Points, **kwargs: Any
    419 ) -> types.UpdateResult:
--> 420     collection = self._get_collection(collection_name)
    421     collection.upsert(points)
    422     return self._default_update_result()

File ~/repos/cc-assistant/backend/.venv/lib/python3.11/site-packages/qdrant_client/local/qdrant_local.py:121, in QdrantLocal._get_collection(self, collection_name)
    119 if collection_name in self.aliases:
    120     return self.collections[self.aliases[collection_name]]
--> 121 raise ValueError(f""Collection {collection_name} not found"")

ValueError: Collection text_collection not found
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
2,2973a956-ddc9-4003-9f42-8f1e67b822b8,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 26, 'assignee': '', 'size': 'M', 'index_id': '9699'}",[],[],{},80da2bac11448da02f8ed317781a3af018369f03fc9666582d6fc202f5e06327,"Fixing Paths and other documentation issues
# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [x] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [ ] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
3,389ef3fa-0845-4d4a-8dcc-9d8876659375,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 26, 'assignee': '', 'size': 'XS', 'index_id': '9700'}",[],[],{},d5230a5c1e5c603855af5fea1cd63592ee46629b3b5e68d77617a63629321e45,"Fixed typo in ChromaIndexDemo.ipynb
Fixed small typo",,,"{metadata_str}

{content}",{key}: {value},"
",Document
4,39b1f1cc-f322-41cb-b709-444a9139441c,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 26, 'assignee': '', 'size': 'M', 'index_id': '9698'}",[],[],{},4f2ed660e7aab9a427cd3dec2660f78a4f52c63ea6ae912a629eae9c7f0851ac,"Update zcp docs
Update documents & example for ZillizCloudPipelineIndex:
- change the example doc url (milvus 2.x -> milvus 2.2)
- add cell outputs in notebook",,,"{metadata_str}

{content}",{key}: {value},"
",Document
5,85d520bb-0fdb-46f4-83dc-ce5a3357a828,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 26, 'assignee': '', 'size': '', 'index_id': '9696'}",[],[],{},a336562629251dcbf57fe1dfe4d8bd9cff7d01a853bb59ddc05c1c498638163b,"[Bug]: pandas query engine triggers safe_eval
### Bug Description

Pandas query results in 
df['LOCATION'].value_counts().head()

It triggers File ""/usr/local/lib/python3.10/dist-packages/llama_index/exec_utils.py"", line 107, in _verify_source_safety
because it matches

pattern = r""_{1,2}\w+_{0,2}""


### Version

0.9.21

### Steps to Reproduce

Create PandasQueryEngine and query to trigger value_counts()

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
6,195df57a-1c0b-452e-83b2-3a6b19b60538,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 20, 'assignee': '', 'size': '', 'index_id': '9638'}",[],[],{},23c61f51db07fb721191adb3ab53b80fc33bb16c24354b8906f6eb750ad57a73,"[Bug]: Error with CorrectnessEvaluator Using HuggingFace API in RAG Evaluation Metrics
### Bug Description

Hello,

I am encountering an issue while attempting to run the RAG evaluation metrics, specifically the CorrectnessEvaluator.

- I created an index for the PaulGrahamEssayDataset available on the llamaindex hub, employing the HuggingFace embeddings model ""BAAI/bge-small-en"" and the HuggingFace API ""mistralai/Mistral-7B-v0.1"".
- Then, I computed predictions using the rag_dataset correlated with the PaulGrahamEssayDataset.
- In the final step, while trying to compute the correctness score, I encountered this error: ""ValueError: could not convert string to float: ''. But, when I modified the ServiceContext to a new one, using VertexAI's ""text-bison@001"", the process worked as expected.

Could you please provide any insights or guidance on why this error might be occurring with the HuggingFace API, and how it might be resolved?

Thanks.

### Version

0.9.17

### Steps to Reproduce

I reproduced this notebook but without using the OpenAI llm: https://github.com/run-llama/llama_index/blob/main/docs/examples/llama_dataset/downloading_llama_datasets.ipynb

I tried to use mistralai/Mistral-7B-v0.1 from HuggingFace API and it didn't work:
```
import os
from llama_index.llms import HuggingFaceInferenceAPI

HF_TOKEN = os.getenv(""HF_API_KEY"")
model_name = ""mistralai/Mistral-7B-v0.1""

llm = HuggingFaceInferenceAPI(
    model_name=model_name, token=HF_TOKEN
)
```

I also tried with text-bison@001 from VertexAI and this time it worked as expected:
```
from langchain.llms import VertexAI
from llama_index.llms import LangChainLLM

bison = VertexAI(model_name=""text-bison@001"",
               max_output_tokens=300,
               temperature=0.1,
               top_p=0.8,
               top_k=40,
               verbose=False
              )

llm_vertex = LangChainLLM(llm=bison)
```

### Relevant Logs/Tracbacks

```shell
ValueError                                Traceback (most recent call last)
Cell In[14], line 14
      5 evals = {
      6     ""correctness"": [],
      7     # ""relevancy"": [],
      8     # ""faithfulness"": [],
      9     # ""context_similarity"": [],
     10 }
     11 for example, prediction in tqdm.tqdm(
     12     zip(rag_dataset.examples, prediction_dataset.predictions)
     13 ):
---> 14     correctness_result = judges[""correctness""].evaluate(
     15         query=example.query,
     16         response=prediction.response,
     17         reference=example.reference_answer,
     18     )
     20     # relevancy_result = judges[""relevancy""].evaluate(
     21     #     query=example.query,
     22     #     response=prediction.response,
   (...)
     35     #     reference=""\n"".join(example.reference_contexts),
     36     # )
     38     evals[""correctness""].append(correctness_result)

File ~/miniconda/envs/genai/lib/python3.11/site-packages/llama_index/evaluation/base.py:56, in BaseEvaluator.evaluate(self, query, response, contexts, **kwargs)
     43 def evaluate(
     44     self,
     45     query: Optional[str] = None,
   (...)
     48     **kwargs: Any,
     49 ) -> EvaluationResult:
     50     """"""Run evaluation with query string, retrieved contexts,
     51     and generated response string.
     52 
     53     Subclasses can override this method to provide custom evaluation logic and
     54     take in additional arguments.
     55     """"""
---> 56     return asyncio.run(
     57         self.aevaluate(
     58             query=query,
     59             response=response,
     60             contexts=contexts,
     61             **kwargs,
     62         )
     63     )

File ~/miniconda/envs/genai/lib/python3.11/site-packages/nest_asyncio.py:31, in _patch_asyncio.<locals>.run(main, debug)
     29 task = asyncio.ensure_future(main)
     30 try:
---> 31     return loop.run_until_complete(task)
     32 finally:
     33     if not task.done():

File ~/miniconda/envs/genai/lib/python3.11/site-packages/nest_asyncio.py:99, in _patch_loop.<locals>.run_until_complete(self, future)
     96 if not f.done():
     97     raise RuntimeError(
     98         'Event loop stopped before Future completed.')
---> 99 return f.result()

File ~/miniconda/envs/genai/lib/python3.11/asyncio/futures.py:203, in Future.result(self)
    201 self.__log_traceback = False
    202 if self._exception is not None:
--> 203     raise self._exception.with_traceback(self._exception_tb)
    204 return self._result

File ~/miniconda/envs/genai/lib/python3.11/asyncio/tasks.py:267, in Task.__step(***failed resolving arguments***)
    263 try:
    264     if exc is None:
    265         # We use the `send` method directly, because coroutines
    266         # don't have `__iter__` and `__next__` methods.
--> 267         result = coro.send(None)
    268     else:
    269         result = coro.throw(exc)

File ~/miniconda/envs/genai/lib/python3.11/site-packages/llama_index/evaluation/correctness.py:140, in CorrectnessEvaluator.aevaluate(***failed resolving arguments***)
    132 eval_response = await self._service_context.llm.apredict(
    133     prompt=self._eval_template,
    134     query=query,
    135     generated_answer=response,
    136     reference_answer=reference,
    137 )
    139 # Use the parser function
--> 140 score, reasoning = self.parser_function(eval_response)
    142 return EvaluationResult(
    143     query=query,
    144     response=response,
   (...)
    147     feedback=reasoning,
    148 )

File ~/miniconda/envs/genai/lib/python3.11/site-packages/llama_index/evaluation/eval_utils.py:76, in default_parser(eval_response)
     66 """"""
     67 Default parser function for evaluation response.
     68 
   (...)
     73     Tuple[float, str]: A tuple containing the score as a float and the reasoning as a string.
     74 """"""
     75 score_str, reasoning_str = eval_response.split(""\n"", 1)
---> 76 score = float(score_str)
     77 reasoning = reasoning_str.lstrip(""\n"")
     78 return score, reasoning

ValueError: could not convert string to float: ''
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
7,e8fc4e69-f441-429a-ba1b-884ea8275b93,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 25, 'assignee': '', 'size': 'XS', 'index_id': '9694'}",[],[],{},1343e2d7d1ffa6f9b549054b5bd1ffdfb79809f22b0b65287de45da0a3e1dd26,"Clarify that one needs to explicitly pass custom `persist_dir` to `storage_context.persist`
# Description

Just a documentation update. I was following the getting-started guide and was confused why data persistence was ignoring the custom path I gave. It took me some time to figure out that the guide was missing a parameter.

## Type of Change

Please delete options that are not relevant.

# How Has This Been Tested?

I also tested it locally end-to-end, but I didn't ""add"" a new notebook.

- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [x] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
8,6fda1c89-f4c8-44b2-ba25-805761e83abc,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 9, 'assignee': '', 'size': '', 'index_id': '9416'}",[],[],{},6c3589320a50355681230c0c319b5d8fe217140d3b7fd4bdba417ef7e0dfd513,"[Question]: Why OpenAI key is always needed, even each nodes have embedding from local model?
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

I embeded every text by IngestionPipeline. But when I use VectorStoreIndex, It produces error about openai key.

```python
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.embeddings import HuggingFaceEmbedding

embed_model = HuggingFaceEmbedding(
    model_name=""intfloat/multilingual-e5-large"",
    max_length=512,
    embed_batch_size=10
)

pipeline = IngestionPipeline(
    transformations=[
        SentenceWindowNodeParser(
            window_size=4,
            sentence_splitter=splitter.split,
            include_prev_next_rel=False,
            include_metadata=True
        ),
        embed_model
    ],
)

nodes = pipeline.run(documents=documents)

d = 1024
faiss_index = faiss.IndexFlatL2(d)
vector_store = FaissVectorStore(faiss_index=faiss_index)
storage_context = StorageContext.from_defaults(vector_store=vector_store,)
index = VectorStoreIndex(
    nodes, 
    storage_context=storage_context, 
    show_progress=True
)
```

```python
LLMPredictor.__init__(self, llm, callback_manager, system_prompt, query_wrapper_prompt, pydantic_program_mode)
     95 def __init__(
     96     self,
     97     llm: Optional[LLMType] = ""default"",
   (...)
    101     pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,
    102 ) -> None:
    103     """"""Initialize params.""""""
--> 104     self._llm = resolve_llm(llm)
    106     if callback_manager:
    107         self._llm.callback_manager = callback_manager

File ~/.local/lib/python3.10/site-packages/llama_index/llms/utils.py:31, in resolve_llm(llm)
     29         validate_openai_api_key(llm.api_key)
     30     except ValueError as e:
---> 31         raise ValueError(
     32             ""\n******\n""
     33             ""Could not load OpenAI model. ""
     34             ""If you intended to use OpenAI, please check your OPENAI_API_KEY.\n""
     35             ""Original error:\n""
     36             f""{e!s}""
     37             ""\nTo disable the LLM entirely, set llm=None.""
     38             ""\n******""
     39         )
     41 if isinstance(llm, str):
     42     splits = llm.split("":"", 1)

ValueError: 
******
Could not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.
Original error:
No API key found for OpenAI.
Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.
API keys can be found or created at https://platform.openai.com/account/api-keys

To disable the LLM entirely, set llm=None.
******
```",,,"{metadata_str}

{content}",{key}: {value},"
",Document
9,cddd680d-995e-4e01-bffe-9ea7c96de62f,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 25, 'assignee': '', 'size': '', 'index_id': '9692'}",[],[],{},a3decede33e9ced62ebaaacddde704a63f0243d2f94366cd5996c9fcfc616f76,"About embeddings
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

I'm using llamaindex to develop RAG mechanism with chromadb. My relevant codes are below:

`  index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, service_context=service_context)
`
or 
```
             query_engine = index.as_query_engine()

                 #prompt = input(""ask:"")
                 prompt = ""some_searched_text""
                 response = query_engine.query(prompt)
```

I think we need to debug one of these parts. 

I need to find exactly where the embeddings are made and saved with the embedding function of the read files. I need to debug and access it, but I couldn't find it in llamaindex's project files. Please help me.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
10,5c4deb2d-1cdc-45b2-bfab-e306f3d11281,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 25, 'assignee': '', 'size': 'XS', 'index_id': '9691'}",[],[],{},b54d484142f01fd925c80a710d680832a9d37f2da0f44bb4f1b8a76f300d8e72,"update poetry lock and pyproject
# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [ ] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
11,4f0b88a1-54eb-4f62-8d5d-6e8e07c43ee4,,"{'state': 'open', 'year': 2023, 'month': 9, 'day': 25, 'assignee': '', 'size': '', 'index_id': '7812'}",[],[],{},9058d465a416ff564a27481c5ec5bf4e25e97d0f3bf51fa9fb373944202c3822,"[Bug]: Excessive Program Memory Usage Leading to Frequent Server Termination
### Bug Description

After deploying my code to the server, I've been encountering frequent terminations during queries. Upon investigating the logs, I discovered that it's primarily due to high memory usage, occasionally reaching 100%, resulting in the termination of the process. The server is already at its maximum capacity with 16GB of RAM. I'm puzzled as to why a simple document retrieval and question-answer functionality would consume such a significant amount of memory. How should I go about optimizing the code?

The following is the program code I have deployed to the serverï¼š
`llm = OpenAI(temperature=3, model=""text-davinci-003"", max_tokens=4096)
service_context = ServiceContext.from_defaults(llm=llm)

app = Flask(__name__)
qa_history = []

@app.route(""/"", methods=[""GET"", ""POST""])
def index():
    client = Translate()

    storage_context = StorageContext.from_defaults(persist_dir=""./storage/knowledgebase"")
    vindex = load_index_from_storage(storage_context)

    if request.method == ""POST"":
        prompt = request.form[""question""]+'Answer in Chinese'

        query_engine = vindex.as_query_engine()
        response = query_engine.query(prompt)

        return redirect(url_for(""index"", result=response))
    result = request.args.get(""result"")
    return render_template(""index.html"", result=result)

if __name__ == ""__main__"":
    app.run()`

### Version

0.8.9

### Steps to Reproduce

to deploy these code on a server.

### Relevant Logs/Tracbacks

```shell
Thu Sep 21 12:51:01 2023z Worker with pid 2702 was terminated due to signal 9
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
12,a3bac33c-ed5b-409c-aefd-c0c45b706478,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 25, 'assignee': '', 'size': '', 'index_id': '9690'}",[],[],{},e85c43124d4b8a05f3b39f681441fc513fc299ec7196f487cb39717586824d47,"[Bug]: The IngestionPipeline does not take vector_store parameters constructed from TimescaleVectorStore and ElasticsearchVectorStore
### Bug Description

All other vector_stores(Qdrant, weaviate, pinecone, chroma) work well with the ingestion pipeline except for the TimescaleVectorStore and the ElasticsearchVectorStore

### Version

0.9.21

### Steps to Reproduce

# For Timescale
vector_store = TimescaleVectorStore(
                        service_url=timescale_config[""service_url""],
                        table_name=""default"",
                        num_dimensions=timescale_config[""dimension""],
                    )

documents: List[Document] = SimpleDirectoryReader(input_dir=directory_path).load_data()

pipeline = IngestionPipeline(
            transformations=[
                SentenceSplitter(),
                OpenAIEmbedding()
            ],
            vector_store=vector_store,
        )
        pipeline.run(documents=documents, show_progress=True)

#For Elastic search
es_client = Elasticsearch(
                    cloud_id=elasticsearch_config[""cloud_id""],
                    api_key=elasticsearch_config[""api_key""]
                )
                vector_store = ElasticsearchStore(
                    index_name=elasticsearch_config['index_name'],
                    es_client=es_client,
                    distance_strategy=elasticsearch_config[""distance_strategy""],
                )

documents: List[Document] = SimpleDirectoryReader(input_dir=directory_path).load_data()

pipeline = IngestionPipeline(
            transformations=[
                SentenceSplitter(),
                OpenAIEmbedding()
            ],
            vector_store=vector_store,
        )
        pipeline.run(documents=documents, show_progress=True)

### Relevant Logs/Tracbacks

```shell
pipeline = IngestionPipeline(
  File ""/home/gich2009/Work/BAYESNET/venv-new/lib/python3.10/site-packages/llama_index/ingestion/pipeline.py"", line 167, in __init__
    super().__init__(
  File ""/home/gich2009/Work/BAYESNET/venv-new/lib/python3.10/site-packages/pydantic/v1/main.py"", line 341, in __init__
    raise validation_error
pydantic.v1.error_wrappers.ValidationError: 1 validation error for IngestionPipeline
vector_store
  value is not a valid dict (type=type_error.dict)
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
13,f4e00091-1ad4-45cb-b368-f6752b9b96c2,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 25, 'assignee': '', 'size': '', 'index_id': '9688'}",[],[],{},08f676ca95eb683c3c60169c27591323711d6c42f814cb3f73f2c72bee7acf5a,"[Bug]: openai agent uses invalid tool name to lookup function
### Bug Description

The `OpenAIAgent` with `QueryEngineTool`s created with `SQLTableRetrieverQueryEngine`s appears to look up functions using invalid tool names as in if the available tools are named `Tool_A` and `Tool_B` it sometimes uses tool name as `Tool_A.Tool_A` or `Tool_A.Tool_B` when the intended tool name to use is just `Tool_A` to lookup the function.

This happens with the `gpt-4-1106-preview` model but not if I use `gpt-3.5-turbo`.

### Version

0.9.21

### Steps to Reproduce

Create an `OpenAIAgent` with a few tools and run a `query()` or `chat()` call to observe the problem when using the `gpt-4-1106-preview` model.

### Relevant Logs/Tracbacks

```shell
File ""test.py"", line 23, in query
    return str(self.agent.chat(query_str))
  File "".venv/lib/python3.10/site-packages/llama_index/callbacks/utils.py"", line 39, in wrapper
    return func(self, *args, **kwargs)
  File "".venv/lib/python3.10/site-packages/llama_index/agent/runner/base.py"", line 473, in chat
    chat_response = self._chat(
  File "".venv/lib/python3.10/site-packages/llama_index/agent/runner/base.py"", line 431, in _chat
    cur_step_output = self._run_step(task.task_id, mode=mode)
  File "".venv/lib/python3.10/site-packages/llama_index/agent/runner/base.py"", line 293, in _run_step
    cur_step_output = self.agent_worker.run_step(step, task, **kwargs)
  File "".venv/lib/python3.10/site-packages/llama_index/callbacks/utils.py"", line 39, in wrapper
    return func(self, *args, **kwargs)
  File "".venv/lib/python3.10/site-packages/llama_index/agent/openai/step.py"", line 573, in run_step
    return self._run_step(
  File "".venv/lib/python3.10/site-packages/llama_index/agent/openai/step.py"", line 470, in _run_step
    self._call_function(
  File "".venv/lib/python3.10/site-packages/llama_index/agent/openai/step.py"", line 357, in _call_function
    EventPayload.TOOL: get_function_by_name(
  File "".venv/lib/python3.10/site-packages/llama_index/agent/openai/step.py"", line 51, in get_function_by_name
    raise ValueError(f""Tool with name {name} not found"")
ValueError: Tool with name MyToolName.MyToolName not found
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
14,6d5dc907-a292-4a38-ae2c-2e7f69adc37d,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 25, 'assignee': '', 'size': '', 'index_id': '9687'}",[],[],{},9609ae8fbc096811bdc59cf95a0a6f092e83bbe54e009508e971f083fab117a0,"[Bug]: cannot import name HnswVectorSearchAlgorithmConfiguration
### Bug Description

when I used latest version of azure-search-documents. It is throwing below error:
`An error occurred: cannot import name 'HnswVectorSearchAlgorithmConfiguration' from 'azure.search.documents.indexes.models'`

### Version

0.9.16

### Steps to Reproduce

Use azure-search-documents==11.4.0 version

### Relevant Logs/Tracbacks

```shell
An error occurred: cannot import name 'HnswVectorSearchAlgorithmConfiguration' from 'azure.search.documents.indexes.models'
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
15,a9f345b6-1b61-46d6-a57e-cddc14064184,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 22, 'assignee': '', 'size': '', 'index_id': '9670'}",[],[],{},822c1ef3a09eece2775d624503a8d8b4c3f4a39f51a519419fa6e56ca2e9e07b,"[Question]: saving and loading to query on faissdb
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

```
from llama_index import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    ServiceContext,)
from llama_index.vector_stores import ChromaVectorStore
from llama_index.storage.storage_context import StorageContext
from llama_index.embeddings import HuggingFaceEmbedding
from llama_index.llms import HuggingFaceLLM
import torch
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.query_engine import CitationQueryEngine
import faiss

def run_ingestion():
    d = 1536
    faiss_index = faiss.IndexFlatL2(d)
    documents = SimpleDirectoryReader(""./data_ten_docs"",filename_as_id=True).load_data()

    vector_store = FaissVectorStore(faiss_index=faiss_index)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)


    llm = HuggingFaceLLM(
        model_name=""/home/user/my_llm""
    )

    embed_model = HuggingFaceEmbedding(model_name=""my_embedding"")

    service_context = ServiceContext.from_defaults(
        llm=llm,
        embed_model=embed_model
    )

    index = VectorStoreIndex.from_documents(
        documents, storage_context=storage_context, service_context=service_context
    )
    index.storage_context.persist()




    query_engine = index.as_query_engine()
    response = query_engine.query(""when is halloween?"")
    print(response)



if __name__ == ""__main__"":
        run_ingestion()

```

i want to save vectordb. then load the vectordb from path. query the saved vectordb with given query. Please fix my code with explainations.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
16,3dcaadc0-95ff-42ef-b464-c16157407cb8,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 24, 'assignee': '', 'size': '', 'index_id': '9686'}",[],[],{},fe8383bd28cf21e513ea236ed021bd7776439e4106c74e54190cdfce8fc11a2f,"[Feature Request]: Add support for cloud Epsilla Vector Store: pyepsilla.cloud.client.Client
### Feature Description

Llama index already supports the local version of epsilla vector store but it would be good to extend the support to include the cloud client for epsilla vector store.

 raise TypeError(
TypeError: client should be an instance of pyepsilla.vectordb.Client, got <class 'pyepsilla.cloud.client.Client'>

### Reason

Most developers will use the cloud version of epsilla since it frees up memory and compute for other application tasks.

### Value of Feature

It adds utility to the EpsillaVectorStore abstraction.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
17,b0359c4f-2ebb-4324-ae85-0fa295ac5c6a,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 22, 'assignee': '', 'size': '', 'index_id': '9664'}",[],[],{},9045cdd5b6bd226498aaaac78d4a6da7979e8518eb28e0bb0747b94599c3bc4c,"[Question]: Difference between DocumentSummaryIndex, TreeIndex and RecursiveRetriever?
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

It seems that both of `DocumentSummaryIndex`, `TreeIndex` and `RecursiveRetriever` are building tree or graph structured hierarchies of node, so I seem to met troubles understanding their differences. What are the practical considerations to decide which one to choose? and, can these Indexes/Retrievers be unified under a single abstract class?",,,"{metadata_str}

{content}",{key}: {value},"
",Document
18,0763e761-23d9-4afc-b20a-3cc22b97e03d,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 23, 'assignee': '', 'size': '', 'index_id': '9684'}",[],[],{},fa8ecfded69f6f7501714b84914f98600f61644d9f5a8aeb2ef948aad05a0780,"[Documentation]: Links is not working in multiple docs page
### Documentation Issue Description

There are several links that are not working on the documentation pages:

`Agent page` [see here](https://docs.llamaindex.ai/en/stable/use_cases/agents.html):
>Learn more
Our Putting It All Together section has [more on agents](https://docs.llamaindex.ai/en/latest/use_cases/agents.html#/docs/understanding/putting_it_all_together/agents.md)
---
`Chatbots page` [see here](https://docs.llamaindex.ai/en/stable/use_cases/chatbots.html):
>[Building a chatbot](https://docs.llamaindex.ai/en/stable/use_cases/chatbots.html#/docs/understanding/putting_it_all_together/chatbots/building_a_chatbot.md) tutorial
---
`Q&A` [see here](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a.html):
>[Searching Pandas tables](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a.html#/examples/query_engine/pandas_query_engine.md)

>[Text to SQL](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a.html#/examples/index_structs/struct_indices/SQLIndexDemo.md)

>For further examples of Q&A use cases, see our [Q&A section in Putting it All Together](https://docs.llamaindex.ai/en/stable/use_cases/q_and_a.html#/understanding/putting_it_all_together/q_and_a.html).

### Documentation Link

https://docs.llamaindex.ai/en/stable/use_cases/agents.html
https://docs.llamaindex.ai/en/stable/use_cases/chatbots.html
https://docs.llamaindex.ai/en/stable/use_cases/q_and_a.html",,,"{metadata_str}

{content}",{key}: {value},"
",Document
19,caec1c23-6080-4445-9e03-1240a34dba63,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': 'XL', 'index_id': '9488'}",[],[],{},b9e3f13ecf306f6fb6df709768f2922f5c6bc630ae3ca939ccc5015470223388,"Support for Nvidia Triton LLM
# Description

This PR includes support for the Nvidia Triton Inference Server LLM. To keep the PR as small as possible it only includes the GRPC client for now and not HTTP (follow up PR to come) and also only supports sync `chat` and `complete` methods while all others are marked as `NotImplemented` for now.

Fixes #9185 

## Type of Change

Please delete options that are not relevant.

- [ ] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] I stared at the code and made sure it makes sense
- [x] I tested using the Nvidia GenerativeAIExamples repo which has docker scripts for the complete environment setup. Happy to include a notebook but doing so with the Triton server setup seemed like overkill.

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
20,9763e111-4196-46b1-b642-9c9718e5fdf2,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 23, 'assignee': '', 'size': '', 'index_id': '9679'}",[],[],{},31ab9bddffc9574703cdf3b73486b50105b3ca3d87204777827a1453932b69e2,"retrieverFn doesn't work on GPT 3.5 1106
### Bug Description

I tried vectorized Fn to store an arbitrary number of function / tool for gpt to use. It works well for gpt 3.5 turbo but doesn't work for gpt 3.5 turbo 1106

### Version

Stable as of 20231223

### Steps to Reproduce

Call openai agent using retriever of vectorized fn

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
21,852f54d9-d8e7-4ef4-aca4-ce128d9c6933,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 21, 'assignee': '', 'size': 'L', 'index_id': '9658'}",[],[],{},a36841f5d03596f2bbf5b9353ec8c9a0c730deb797c8c4f6c4aa3ab4b248ff6c,"add `iter_data()` method to SimpleDirectoryReader
# Description

Adds functionality to iterate over documents as the load with `SimpleDirectoryReader`

## Type of Change

- [x] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

- [x] Added new notebook (that tests end-to-end)

",,,"{metadata_str}

{content}",{key}: {value},"
",Document
22,93160deb-078d-4e88-a337-cd77e283756c,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 23, 'assignee': '', 'size': '', 'index_id': '9678'}",[],[],{},3a9caa8334b79335f0a51b5e3d6ce472eff4f56d9ef5763c049a683ea877022e,"[Bug]: Getting incomplete responses in async mode with anthropic
### Bug Description

Hello I am using llama-index (v0.9.16) and using async streaming apis (`achat_stream`) with the [simple chat engine](https://github.com/run-llama/llama_index/blob/ce75aa5d026524d9b36709769ea3f2e203ac5269/llama_index/chat_engine/simple.py#L141).

However it seems like when I use openai sdk (for anyscale and open models), I get full responses while if I use anthropic sdk (for claude) I get the error

```
<asyncio.locks.Event object at 0x13de7fe50 [unset]> is bound to a different event loop
```
I am confused as to if this is because of llama index or anthropic (given the same code works with openai call)

Any pointers?

### Version

0.9.16

### Steps to Reproduce

```
from llama_index.llms import Anthropic
from llama_index import ServiceContext
from llama_index.chat_engine import SimpleChatEngine


llm = Anthropic(
                model=model,
                temperature=temperature,
                api_key=api_key,
                max_tokens=max_tokens,
                base_url=base_url,
                timeout=timeout,
            )
            
service_context = ServiceContext.from_defaults(
        llm=llm,
    )
ce = SimpleChatEngine.from_defaults(service_context=service_context)
response = await ce.astream_chat(user_message, chat_history)
```

### Relevant Logs/Tracbacks

```shell
WARNING::llama_index.chat_engine.types::Encountered exception writing response to history: <asyncio.locks.Event object at 0x13de7fe50 [unset]> is bound to a different event loop
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
23,147d15e3-9c92-4382-8dbe-7340b21d577a,,"{'state': 'open', 'year': 2023, 'month': 4, 'day': 18, 'assignee': '', 'size': '', 'index_id': '1239'}",[],[],{},06f1998593ce095950e3c31880d9f28903468b6a50616670bdaa4654af82bb9a,"Help with graph structure, any advice is appreciated!
Hi all!

I start by saying sorry for this long thread, Im not good at getting to the point as you can see... I'm knee-deep into this and have so many questions, and so little people I can ask (nobody really lol), I really want to make this happen since I honestly believe in the potential and value it can bring.

### Intro & Context
I'm working on a hackathon project for work: A Slack bot that answers questions about our product, so far created in Google Colab as a POC. I'm a Product Operations person and have no real programming background (I know SQL and thought myself qbasic when I was a kid, but thats it), I really have a hard time to understand the ""reference"" part of the docs and to translate the info into actual functions. So bare with me, I'm really proud that I got so far (thanks GPT4 and for all the example notebooks!)

I did read the WHOLE documentation, multiple times, believe me that it's not so clear for someone like me (not an engineer, but a technically inclined person, I do have a proxmox server and play around with docker but thats it lol). Meaning I'm good with finding answers online and to integrate answers to my cases.

Our product documentation is dispersed, we find information across 4 different platforms (3 KBs & Slack channels) and have many knowledge gaps. Info can be on spot and relevant, but some other articles can be outdated, wrong, and other topics can just be missing.  Slack in our org is super-active and many questions get only answered in specific slack channels, but it's hard to navigate and to find those. I general I'd say that any employee searching for answers has a hard time to find them, and not all employees have access to all knowledge bases...

My idea was to have centralized knowledge access via Slack, no matter the source.
By logging (an analyzing) all questions, answers & emoji reactions to snowflake, we would be able to identify areas that need updated or new content (no answer or negative reaction to answer). Thus replacing a knowledge related project I really didn't want to do (or believed in) with a smart and AI based tool, bringing demand driven knowledge improvement.

I thought of ""simply"" indexing each source and building a graph of those indices that can be queried.

I do have a working POC, but 
- getting an answer is very slow (~20 sec) & 
- I'm sure it could be done much better (probably the reason for the slow response)

Any hint or advice is appreciated !

Let me share my questions first followed by code and some info:

### Questions: 
1) **High level structure** 
I created an Index for each source, added a broad explanation of each as summary and combined those into a graph. I wonder if this is the right approach. A graph needs summaries and it makes not much sense to summarize each source as a whole (Summarize Confluence....?). Wouldn't it make more sense to first index each article separately, then creating one graph per source, and then creating one graph thats made out of those 4 graphs? The docs talk about being able to indefinitely stack indices over indices.

2) **Vector VS Tree VS ...**
I created SimpleVector indices for each source and combined them as a TreeIndex-Graph. The example notebooks here I saw used for the graph either List or Knowledge-graph indeces. I thought list would force the query to go over all the content for every query which i wanted to avoid & Knowledge-graph wouldn't be a good fit since my data is not structured like a knowledge-graph should be. Is my gut feeling wrong? 
Or... if I want to make sure to get combined answers from multiple sources , do I then I need to use ListIndex for the Graph??
Also, if I stack multiple indices over each other (as suggested in Question 1), what index type would make sense to use in such a case? Tree over Tree over Vector? Tree over Keyword over Vector? 
I saw [that page](https://gpt-index.readthedocs.io/en/latest/use_cases/queries.html) that talks about Routing VS Synthesis over Heterogenous Data, and I get the impression it would make sense to index each article with either vectorindex or keywordindex, have each article summarized and create a TreeGraph each for all 3 knowledgebases and then 1 listIndex for the final graph. Slack is different since its one big txt file that contains question, answer, summary. Mabye this could be used differently im not sure...
I have a hard time understanding the differences between indices since from my understanding all indices will turn text into a vector full of numbers, not only the vector one, so its a bit confusing...

3) **query_configs**
I did not define num_children, child_branch_factor, and am not sure if I defined index_struct_type, query_mode, similarity_top_k or response_mode correctly. Did I miss a parameter, did I chose some parameters wrongly? Does something in my code scream ""this will make the query slow!""?

4) **Text Chunking**
I did not specify chunking, I assume it defaults to some chunk size? Articles on all the platforms can vary from very short to very long, there is no clear pattern. 

5) **Playground**
I think I could answer some of my questions would I understand how to use the playground with such a complex structure, would be nice if someone could explain how to test not only different indices for the docs, but also testing different models, different indices for the graph, different query_configs etc... 

5) **Summaries**
Like already mentioned, I understand those summaries I did make no sense and the LLM won't know from those summaries based on a query to what node to reach... Are those summaries really necessary? Can't I just take the whole content of all 4 sources, index the everything? Im sure I can but would it be more efficient? 

7) **Making it future-proof**
The short-term goal of this bot will be to identify areas of improvement to give feedback to product owners and have content created. This means articles can be updated, removed, and new ones can be created. I of course want the most updated version of the content in my index. What needs to change to pull/load/index everything again but to skip what didn't change, only indexing the delta and removing whatever was removed?

### Info & Code

What I did:
I'm dealing with 4 sources: 3 Knowledge Bases and Slack.
KBs: client facing helpcenter, Confluence, bloomfire

I could not make the Confluence connector nor the official ""Confluence by atlassian-python-api"" work (i need to basic auth with a base64 encoded ""username:apitoken"" combo, but both other solutions mentioned ask for username and pwd separately, couldnt figure out a solution), so I grabbed the articles with a simple get_data loop from the relevant spaces with a script and pulling all relevant articles. Each article is one CSV file containing columns: title, content, URL. 
All CSVs are in the same folder.
The other 2 KBs I got 1 JSON file per KB containing all articles.

I'm not copying the whole thing, only the most relevant parts I have questions about or a lack of confidence in:

**Loading content & Indexing**
```
reader = JSONReader()

# LLMPredictor (gpt-4)
llm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=""gpt-4""))

service_context_gpt4 = ServiceContext.from_defaults(llm_predictor=llm_predictor_gpt4)

# Contentful
docs1 = reader.load_data(""contentful_articles.json"") 
index1 = GPTSimpleVectorIndex.from_documents(docs2, service_context=service_context_gpt4)
index1.save_to_disk('contentful_index.json')
index1 = GPTSimpleVectorIndex.load_from_disk('contentful_index.json')

# Confluence
docs2 = SimpleDirectoryReader('docs').load_data()
index2 = GPTSimpleVectorIndex.from_documents(docs2, service_context=service_context_gpt4)
index2.save_to_disk('confluence_index.json')
index2 = GPTSimpleVectorIndex.load_from_disk('confluence_index.json')
```
Contentful, Bloomfire were handled the same way, loading with JSONReader and indexing with GPTSimpleVectorIndex.

Slack channels were different, due to PII issues I did not get yet access to production slack, so I created a new slack workspace, invited 2 colleagues and we reproduced some discussions from the real slack in where questions were answered. I then created a script that with openAIs help only takes those threads where the LLM decided a question was answered:

```
def process_messages(messages):
    chatgpt_prompt = (
        ""If the following thread contains a question that you deem answered by the responses, ""
        ""please summarize it as Question: question | Answer: answer | Summary: summary. ""
        ""If the question is not answered, skip it completely. ""
        ""The Summary should be <=20 words. I will take your response and create an Index from it."")
```
Like that I created a .txt file with only those answered questions from the channels of interest and a summary. Loaded this one file with SimpleDirectoryReader and used the same GPTSimpleVectorIndex.

### Graph

**Summaries**
```
index1_summary = ""A client-facing knowledgebase ""
index2_summary = ""Those are articles from our Confluence Knowledgebase for teams A, B, C, D, E, F and G. Here you can learn about the teams, the product and internal information""
index3_summary = This is a collection of articles with very detailed information about the technical side of X. Very relevant for {employee type Y}""
index4_summary = ""A collection of answered questions from multiple company slack chat channels""
```

(replaced some too specific info with generic stuff)

**Building Graph**


```
all_indices = [index1, index2, index3, index4]
index_summaries=[index1_summary, index2_summary, index3_summary, index4_summary]

graph = ComposableGraph.from_indices(GPTTreeIndex, all_indices, index_summaries=index_summaries)    
graph.save_to_disk(""graph.json"")
```


** Querying **

```
# set query config
query_configs = [
    {
        ""index_struct_type"": ""simple_dict"",
        ""query_mode"": ""default"",
        ""query_kwargs"": {
            ""similarity_top_k"": 3,
            ""response_mode"": ""tree_summarize""
        }
    },
]


response = index.query(text, query_configs=query_configs)
    
print(f""Your questions was {text}: \nThe answer is: {str(response)}"")
```

And this is sent to the user to Slack

",,,"{metadata_str}

{content}",{key}: {value},"
",Document
24,2cd7e086-603f-4253-9c87-00b0da47c324,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 6, 'assignee': '', 'size': '', 'index_id': '9343'}",[],[],{},45baa90b6d1198552c499223740e2dcc3fda60409fdbc7758738ce06fec891dc,"[Feature Request]: Enable completely offline operation
### Feature Description

When using huggingface models and embeddings that have been downloaded in advance, it should be possible to run llama_index on a computer that is not connected to the internet at all, since the usage doesn't depend on openai or any other online services then.

### Reason

Right now, even just this import line leads to a network timeout if the computer is not connected to the internet:

```python
from llama_index.embeddings import LangchainEmbedding
```

<details>
<summary>
(full error message here)
</summary>

```
File ""/app/server.py"", line 8, in <module>
  from llama_index.embeddings import LangchainEmbedding
File ""/usr/local/lib/python3.11/site-packages/llama_index/__init__.py"", line 21, in <module>
  from llama_index.indices import (
File ""/usr/local/lib/python3.11/site-packages/llama_index/indices/__init__.py"", line 4, in <module>
  from llama_index.indices.composability.graph import ComposableGraph
File ""/usr/local/lib/python3.11/site-packages/llama_index/indices/composability/__init__.py"", line 4, in <module>
  from llama_index.indices.composability.graph import ComposableGraph
File ""/usr/local/lib/python3.11/site-packages/llama_index/indices/composability/graph.py"", line 7, in <module>
  from llama_index.indices.base import BaseIndex
File ""/usr/local/lib/python3.11/site-packages/llama_index/indices/base.py"", line 6, in <module>
  from llama_index.chat_engine.types import BaseChatEngine, ChatMode
File ""/usr/local/lib/python3.11/site-packages/llama_index/chat_engine/__init__.py"", line 1, in <module>
  from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine
File ""/usr/local/lib/python3.11/site-packages/llama_index/chat_engine/condense_question.py"", line 6, in <module>
  from llama_index.chat_engine.types import (
File ""/usr/local/lib/python3.11/site-packages/llama_index/chat_engine/types.py"", line 11, in <module>
  from llama_index.memory import BaseMemory
File ""/usr/local/lib/python3.11/site-packages/llama_index/memory/__init__.py"", line 1, in <module>
  from llama_index.memory.chat_memory_buffer import ChatMemoryBuffer
File ""/usr/local/lib/python3.11/site-packages/llama_index/memory/chat_memory_buffer.py"", line 12, in <module>
  class ChatMemoryBuffer(BaseMemory):
File ""/usr/local/lib/python3.11/site-packages/llama_index/memory/chat_memory_buffer.py"", line 18, in ChatMemoryBuffer
  default_factory=cast(Callable[[], Any], GlobalsHelper().tokenizer),
                                          ^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/llama_index/utils.py"", line 55, in tokenizer
  enc = tiktoken.get_encoding(""gpt2"")
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/tiktoken/registry.py"", line 73, in get_encoding
  enc = Encoding(**constructor())
                   ^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/tiktoken_ext/openai_public.py"", line 11, in gpt2
  mergeable_ranks = data_gym_to_mergeable_bpe_ranks(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/tiktoken/load.py"", line 82, in data_gym_to_mergeable_bpe_ranks
  vocab_bpe_contents = read_file_cached(vocab_bpe_file).decode()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/tiktoken/load.py"", line 50, in read_file_cached
  contents = read_file(blobpath)
             ^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/tiktoken/load.py"", line 24, in read_file
  resp = requests.get(blobpath)
         ^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/requests/api.py"", line 73, in get
  return request(""get"", url, params=params, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/requests/api.py"", line 59, in request
  return session.request(method=method, url=url, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/requests/sessions.py"", line 589, in request
  resp = self.send(prep, **send_kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/requests/sessions.py"", line 703, in send
  r = adapter.send(request, **kwargs)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/site-packages/requests/adapters.py"", line 507, in send
  raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /gpt-2/encodings/main/vocab.bpe (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f0c8803b350>, 'Connection to openaipublic.blob.core.windows.net timed out. (connect timeout=None)'))
```

</details>

Since for this application I'm only using the embeddings and no chat, therefore I managed to ""fix"" this by rewriting the `GlobalsHelper.tokenizer` method to just return `None`, so that `tiktoken.get_encoding(""gpt2"")` is no longer called. I also had to use `set_global_tokenizer(..)` to prevent another network timeout later on.

It would be nice if this use case worked out of the box without me having to modify the library.

### Value of Feature

Our use case is that we want to search across documents that shouldn't leave our internal network. While I don't believe that the llama_index library will leak any information on purpose, we believe it's best to add additional safety measures, just in case any of the libraries in the dependency tree suddenly hit a bug or receive a malicious update.

Being able to run completely offline is also a feature that would enhance user trust in the library, and as such I believe that this is a feature that you could then prominently advertise in the readme and on the website of the library, once the necessary changes have been made. From what I've seen, it also seems like the required changes shouldn't be too massive.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
25,405cbd4b-90cd-4c49-b28c-0fac1f84b581,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 10, 'assignee': '', 'size': 'S', 'index_id': '9421'}",[],[],{},068777a8dd3196d0f952c96091899842047a30a5d4cac48225da981b92ea3dda,"Fix cleanup process in  _delete_node of document_summary
# Description

Cannot remove doc from DocumentSummaryIndex by delete_ref_doc(...)

As designed, the user can remove the nodes from the document summary index according to ""doc_id"" by delete_ref_doc(...) which will call delete_nodes(...) from BaseIndex to do the work. 
<img width=""600"" alt=""æˆªå±2023-12-10 16 12 57"" src=""https://github.com/run-llama/llama_index/assets/114048/d1b804fd-9830-4dce-985f-ac39acffcb4d"">

However, it passes the related node_ids instead of doc_id itself. 
<img width=""692"" alt=""æˆªå±2023-12-10 16 10 15"" src=""https://github.com/run-llama/llama_index/assets/114048/68e89cb2-d63d-40cc-adee-1dcec1ee443a"">

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [ ] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
26,c20fb5fd-c4be-4dc5-a289-32c34158ba8a,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 21, 'assignee': '', 'size': '', 'index_id': '9653'}",[],[],{},0e94fde95203279c4fc6ab47443f5865bfdba39f95f4a0dc41be699a172bbf60,"[Feature Request]: Add multi-filter single key solution
### Feature Description

This is following up on what the bot suggested in this ticket: [https://github.com/run-llama/llama_index/issues/9627](https://github.com/run-llama/llama_index/issues/9627).

I need functionality for multi-filter, single-key with chroma in particular. Something like this, for example:
key=""Month"", value=[""September"", ""October""] with an OR filter condition and an IN operator
As far as I understand, this is not currently supported.

I have the following working solution and am hoping this or something similar could be merged into the repo:

1) vector_stores/types.py

```
class MetadataFilter(BaseModel):
    key: str
    value: Union[StrictInt, StrictFloat, StrictStr, List[Union[StrictInt, StrictFloat, StrictStr]]]
    operator: FilterOperator = FilterOperator.EQ
```

2) vector_stores/chroma.py
```
def _transform_chroma_filter_operator(operator: str) -> str:
    """"""Translate standard metadata filter operator to Chroma specific spec.""""""
    if operator == ""!="":
        return ""$ne""
    elif operator == ""=="":
        return ""$eq""
    elif operator == "">"":
        return ""$gt""
    elif operator == ""<"":
        return ""$lt""
    elif operator == "">="":
        return ""$gte""
    elif operator == ""<="":
        return ""$lte""
    elif operator == ""in"":
        return ""$in""
    else:
        raise ValueError(f""Filter operator {operator} not supported"")
```


3) vector_stores/chroma.py
```

def _to_chroma_filter(
    standard_filters: MetadataFilters,
) -> dict:
    """"""Translate standard metadata filters to Chroma specific spec.""""""
    filters = {}
    filters_list = []
    condition = standard_filters.condition or ""and""
    condition = _transform_chroma_filter_condition(condition)

    if standard_filters.filters:
        for filter in standard_filters.filters:
            if filter.operator:
                operator = _transform_chroma_filter_operator(filter.operator)
                # Handle list values for 'in' operator
                if filter.operator == FilterOperator.IN and isinstance(filter.value, list):
                    filters_list.append({filter.key: {""$in"": filter.value}})
                else:
                    filters_list.append({filter.key: {operator: filter.value}})
            else:
                # Assuming default behavior for filters without an explicit operator
                filters_list.append({filter.key: filter.value})

    if len(filters_list) == 1:
        # If there is only one filter, return it directly
        return filters_list[0]
    elif len(filters_list) > 1:
        # Combine multiple filters based on the specified condition
        filters[condition] = filters_list

    return filters
```


And then this gets initialized like this:

```

metadata_filters = MetadataFilters(
    filters=[
        MetadataFilter(key=""Month"", value=[""September"", ""October""], operator=FilterOperator.IN)
    ],
    condition=FilterCondition.OR
)
```

The functionality this doesn't support yet is something like this:
key=""Day"", value=[7, 14] with an AND filter and operators [GTE, LTE].

Would love to have that functionality as well and then need to extend it to the other vector stores.

Thanks again

### Reason

_No response_

### Value of Feature

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
27,6db0e1e3-f645-407c-ad3a-cd2169101fe3,,"{'state': 'open', 'year': 2023, 'month': 9, 'day': 19, 'assignee': '', 'size': '', 'index_id': '7726'}",[],[],{},cc0f4426b470d5f718e9ea78878829ecba61288e2801f79614c30f0415f614fc,"[Bug]: TypeError: 'NoneType' object is not callable
### Bug Description

sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1064, 'You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near \'This query will count the number of rows in the ""pictures"" table.\n\nNow, let m...\' at line 3')
[SQL: SELECT COUNT(*) FROM pictures;

This query will count the number of rows in the ""pictures"" table.

Now, let me run the query and get the result...]
(Background on this error at: https://sqlalche.me/e/20/f405)
Exception ignored in: <function Llama.__del__ at 0x7fc890166700>
Traceback (most recent call last):
  File ""/root/anaconda3/lib/python3.9/site-packages/llama_cpp/llama.py"", line 1611, in __del__
TypeError: 'NoneType' object is not callable


### Version

latest

### Steps to Reproduce

Here is the complete code

from sqlalchemy import select, create_engine, MetaData, Table
engine = create_engine('mariadb+pymysql://xxxxxx')
metadata = MetaData()
table = Table(""pictures"", metadata, autoload_with=engine)
stmt = select(table.columns)
with engine.connect() as connection:
    results = connection.execute(stmt).fetchone()
    print(results)
    #print(results.keys())

from llama_index import SQLDatabase
sql_database = SQLDatabase(engine)

from llama_index.indices.struct_store.sql_query import NLSQLTableQueryEngine
from IPython.display import Markdown, display

query_engine = NLSQLTableQueryEngine(sql_database=sql_database,tables=[""pictures""],)
query_str = (""how many pictures are there in the database?"")

response = query_engine.query(query_str)
sql_query = response.metadata[""sql_query""]
display(Markdown(f""<b>{sql_query}</b>""))
display(Markdown(f""<b>{response}</b>""))

It connects well and shows a line from the database, and even creates a correct SQL, but LLaMA is unable to show results



### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
28,176ce639-b4fe-4871-82d0-5956a6d0eac6,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 21, 'assignee': '', 'size': '', 'index_id': '9652'}",[],[],{},85ce062caf4e2c0264599b9a78f0060e96ad76812c715e852225cb658fb5ce5e,"[Question]: Maximum number of subquestions with SubQuestionQueryEngine?
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

How can we limit the number of subquestions generated with SubQuestionQueryEngine? I am facing an use case where in some scenarios it's generating up to 30 subquestions and it leads to rate limit errors.
Can we suggest it in the prompt or force it somehow?",,,"{metadata_str}

{content}",{key}: {value},"
",Document
29,563f31f3-b6ae-4e10-8303-3cf249de3787,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 21, 'assignee': '', 'size': '', 'index_id': '9651'}",[],[],{},b9710aaf5c589aa66387771893580ecb3d550f8cc8d995a5546e3d61643ea21d,"[Bug]: using `output_cls` with french content and `gpt-4-1106-preview` model produces encoding error in the json response
### Bug Description

I'm working on an AI assisted PowerPoint presentation generator.  
And while playing with some french content, I noticed that combining these 3 parameters produces an encoding error:
- using french content (I guess it can be of any other language that have accentuated characters)
- using `gpt-4-1106-preview` model (for some reason, it doesn't happen with other models)
- using the `output_cls` parameter to produce formatted output (I guess that the bug happens when formatting)

Using this text as input:
```
voilÃ  un contenu qui parle dâ€™Ã©lectricitÃ©""
```

And this query:
```
What is this about ?
```

Here's an example of a valid output (using `gpt-3.5-turbo-1106` model):
```json
{""title"":""voilÃ  un contenu qui parle dâ€™Ã©lectricitÃ©""}
```

And here's an example of an invalid output (using `gpt-4-1106-preview`):
```json
{""title"":""voil\\u00e0 un contenu qui parle d\\u2019\\u00e9lectricit\\u00e9""}
```

### Version

0.9.19

### Steps to Reproduce

Run this python script to showcase the error, it will run the same query on the same content with 3 different models, 2 succeeding and 1 failing.

```python
import os
from dotenv import load_dotenv
from llama_index import (
    ServiceContext,
)
from llama_index.llms import OpenAI
from pydantic import BaseModel
from llama_index import Document, VectorStoreIndex


class PowerpointPresentation(BaseModel):
    """"""Data model for a powerpoint presentation.""""""
    title: str


TEXT = ""voilÃ  un contenu qui parle dâ€™Ã©lectricitÃ©""

OK_MODEL_1 = ""gpt-3.5-turbo-1106""
OK_MODEL_2 = ""gpt-4""
KO_MODEL = ""gpt-4-1106-preview""
models = [OK_MODEL_1, OK_MODEL_2, KO_MODEL]

load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')

documents = [Document(text=TEXT)]
for model in models:
    print('-' * 20)
    print('model:', model)
    service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0, model=model))
    index = VectorStoreIndex.from_documents(documents, service_context=service_context)
    query_engine = index.as_query_engine(output_cls=PowerpointPresentation)
    response = query_engine.query(""What is this about ?"")
    print('response:', response)

```

---

Expected output:

```
--------------------
model: gpt-3.5-turbo-1106
response: {""title"":""voilÃ  un contenu qui parle dâ€™Ã©lectricitÃ©""}
--------------------
model: gpt-4
response: {""title"":""Ã©lectricitÃ©""}
--------------------
model: gpt-4-1106-preview
response: {""title"":""voil\\u00e0 un contenu qui parle d\\u2019\\u00e9lectricit\\u00e9""}
```

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
30,ce7a176d-584b-403d-95cc-7848e17f065b,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 21, 'assignee': '', 'size': '', 'index_id': '9650'}",[],[],{},aa029ef523a57435a9e995d272fc438667ec763c77deccf66763b31a3bcf3245,"[Feature Request]: Import triplets to KnowledgeGraphIndex and VectorStoreIndex directly
### Feature Description

Add something like `KnowledgeGraphIndex.from_triplets(""example.csv"", header=false)` and `VectorStoreIndex.from_triplets(""example.csv"", header=false)` to create index.

The CSV file contains s, p, o format data:
```
s0,p0,o0
s1,p1,o1
```

### Reason

Now when we want to import any `RDF` like file such as `.rdf`, `.ttl`,we need to convert it to documents using `llamahub` then use KnowledgeGraphIndex.from_documents`. However the original file is already a knowledge graph file, convert it to paragraph and use AI model to extract triplets will waste lots of time and computer resources. Also the AI generated triplets may not as accurate as the original file.

### Value of Feature

This route is way more directly than using documents, saving lots of time and resources when importing RDF like file to knowledge graph.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
31,04fb1bea-ce3a-4d2c-8a0d-24827e37e247,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 21, 'assignee': '', 'size': '', 'index_id': '9649'}",[],[],{},83c98b2fa4678d2c7381741c5b743b8cb1ae4a18bfd05eb6c8b8077df09fc755,"[Question]: 
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

```
from silly import no_ssl_verification
with no_ssl_verification():
    from silly import no_ssl_verification
    from llama_index import SimpleDirectoryReader
    from llama_index.vector_stores import ChromaVectorStore
    from llama_index import VectorStoreIndex, ServiceContext
    from llama_index.storage.storage_context import StorageContext
    from llama_index.embeddings import HuggingFaceEmbedding
    from llama_index.llms import HuggingFaceLLM
    import chromadb
    import torch

    def run_ingestion():
                # load documents
                documents = SimpleDirectoryReader(""/home/user/Desktop/rags/data_ten_docs"", filename_as_id=True).load_data()

                # create db
                db = chromadb.PersistentClient(path=""./storagedummy/chromadbdummy"")

                # creating collection with another method. get_or_create_collection method will create a collection only if it does not exist yet and collection name is the same as before
                chroma_collection = db.get_or_create_collection(""chroma_db_example"")
                vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
                storage_context = StorageContext.from_defaults(vector_store=vector_store)

                system_prompt = ""true_system_prompt""
                query_wrapper_prompt = ""true_query_wrapper_prompt""

                llm = HuggingFaceLLM(
                    context_window=4096,
                    max_new_tokens=256,
                    generate_kwargs={""temperature"": 0.1, ""do_sample"": True},
                    system_prompt=system_prompt,
                    query_wrapper_prompt=query_wrapper_prompt,
                    tokenizer_name=""/home/user/Desktop/models/my_model"",
                    model_name=""/home/user/Desktop/models/my_model"",
                    device_map=""auto"",
                    tokenizer_kwargs={""max_length"": 4096},
                    model_kwargs={""torch_dtype"": torch.float16}

                )

                embed_model = HuggingFaceEmbedding(model_name=""dbmdz/bert-base-turkish-cased"")

                service_context = ServiceContext.from_defaults(
                    llm=llm,
                    embed_model=embed_model
                )

                index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, service_context=service_context)
                print(""vectordb saved at db_chroma"")



                # ---------------------------
                # -----EMBEDDING SECTION-----
                # ---------------------------
                # we need the use the same embedding as we're used to embed the data in vector store
                # define embedding function
                search_text = ""search_text""
                embedding = embed_model.get_text_embedding(search_text)
                results = index.as_query_engine().query(search_text)
                # since we have embedding, we can query chroma collection. results will be chroma collection query.
                # results_orig = chroma_collection.query(
                #     query_embeddings=embedding,
                #     n_results=2)  # we need two similar nodes

                # we can see the query's returned similar nodes to the ""search_text""
                # in ""results"", chromaDB saves all vector embeddings and all the corresponding nodes
                # so llm can efficiently query the data
                print(results)


                # -----------------------
                # -----QUERY SECTION-----
                # -----------------------
                # let's see what's inside the Chroma Vector Store Collection to query it
                # first, run a sample query
                query_engine = index.as_query_engine()
                while True:
                    prompt = input(""please ask:"")
                    response = (query_engine.query(prompt))
                    print(response)


    if __name__ == ""__main__"":
            run_ingestion()

```
i use my local llm model. get embedding function from huggingface.
the ""search_text, system_prompt, query_wrapper_prompt"" are not wrong.

the ""print(results)"" part returns wrong txt files. 
""response = (query_engine.query(prompt))"" part returns wrong outputs sometimes.

What can i change in my architecture? Please help.
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
32,dea96fdd-cdab-4504-a5b7-0bb0a2ac6037,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9603'}",[],[],{},30805f5ddee6702bc9a4c1a8b4fcde1c23e78a4471129c46e6ae6065a0d3627b,"[Bug]:  Unterminated string starting at: line 1 column 2407517 (char 2407516)
### Bug Description

I am trying to load a simple `VectorStoreIndex`  from a remote file storage i.e. AWS S3 bucket. I never got any error in the past 6 months until now from yesterday i.e. 19th December 2023 I get the following error: 
```
[Bug]:  Unterminated string starting at: line 1 column 2407517 (char 2407516)
```

As a result of the following line of code _(full code given below)_

```
storage_context = StorageContext.from_defaults(persist_dir=s3_index_path, fs=s3)
```

### Version

0.9.10

### Steps to Reproduce

Using the script:
```
import s3fs
from llama_index import StorageContext, load_index_from_storage

s3 = s3fs.S3FileSystem()
s3_index_path = f""s3://{bucket_name}/indexes/{project_id}/{bot_id}/storage_context""
storage_context = StorageContext.from_defaults(persist_dir=s3_index_path, fs=s3)
index = load_index_from_storage(storage_context=storage_context)
```

### Relevant Logs/Tracbacks

```shell
ERROR:root:Unterminated string starting at: line 1 column 2407517 (char 2407516)	

Unterminated string starting at: line 1 column 2407517 (char 2407516)	


ERROR:root:Traceback: Traceback (most recent call last):	


File ""/home/app.py"", line 495, in query_apigateway	

storage_context = StorageContext.from_defaults(persist_dir=index_path, fs=s3)	


File ""/usr/local/lib/python3.10/site-packages/llama_index/storage/storage_context.py"", line 88, in from_defaults	


docstore = docstore or SimpleDocumentStore.from_persist_dir(	


File ""/usr/local/lib/python3.10/site-packages/llama_index/storage/docstore/simple_docstore.py"", line 56, in from_persist_dir	


return cls.from_persist_path(persist_path, namespace=namespace, fs=fs)	


File ""/usr/local/lib/python3.10/site-packages/llama_index/storage/docstore/simple_docstore.py"", line 73, in from_persist_path	


simple_kvstore = SimpleKVStore.from_persist_path(persist_path, fs=fs)	


File ""/usr/local/lib/python3.10/site-packages/llama_index/storage/kvstore/simple_kvstore.py"", line 76, in from_persist_path	


data = json.load(f)	


File ""/usr/local/lib/python3.10/json/__init__.py"", line 293, in load	

return loads(fp.read(),	

File ""/usr/local/lib/python3.10/json/__init__.py"", line 346, in loads	


return _default_decoder.decode(s)	

File ""/usr/local/lib/python3.10/json/decoder.py"", line 337, in decode	

obj, end = self.raw_decode(s, idx=_w(s, 0).end())	


File ""/usr/local/lib/python3.10/json/decoder.py"", line 353, in raw_decode	

obj, end = self.scan_once(s, idx)

json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 2407517 (char 2407516)
```
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
33,5b734f65-9ff7-4680-b6ad-7790f53b63b4,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': 'L', 'index_id': '9625'}",[],[],{},02e601e62d08826538ae8b65c8542577436f4b3765433716d2870db6b2db5009,"Update PDFReader
# Description

Added the ability to use PDFPlumber as the underlying library extracting text, because from my own testing it performed better than PyPDF, albeit a little slower. A user will need to install the PDFPlumber library to use this. The reason behind including this functionality is because it was able to extract more/much higher quality text from PDFs than the PyPDF implementation.

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [x] New feature (non-breaking change which adds functionality)
- [x] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [x] I stared at the code and made sure it makes sense
- [x] Tested locally

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] New and existing unit tests pass locally with my changes
- [x] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
34,509da4b2-35ab-4b92-8beb-794a505cd8d3,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': 'M', 'index_id': '9626'}",[],[],{},c4db7b7a3e9937d10449620185c633b195b33b5a70cbe8ced0cc48bfd7429387,"OpenRouter: Support multiple models
# Description

Some people on [twitter](https://twitter.com/pquiggles/status/1737213028720849340) are asking for fallback model support. **NOTE**: I haven't tested that new params (like ""models"" in this case) are passed as POST parameters to the underlying API call - this was a very quick PR. Apologies - figured that someone might know that it'll just work?

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [x] Added new notebook (that tests end-to-end)
- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [x] I have added Google Colab support for the newly added notebooks.
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
35,5518b6c1-c10d-4480-a6b2-df2cd61b6f20,,"{'state': 'open', 'year': 2023, 'month': 9, 'day': 20, 'assignee': '', 'size': '', 'index_id': '7744'}",[],[],{},450dde57c503bf593c0d9c59cd9fee4ffd8179160e0ed2fb648a921d9eefe6bd,"[Bug]: Adding nodes to a keyword index generates quota exceeded error
### Bug Description

Different from vector index, when I try to add several nodes to a keyword index, it returns me a quota exceeded error. I wonder if it is possible to add a exception treatment which will wait some time and try again to add the remaining nodes.

### Version

llama-index-0.8.29.post1

### Steps to Reproduce

Use index.insert_nodes(node) several times.

### Relevant Logs/Tracbacks

```shell
/opt/conda/envs/whirlpalm/lib/python3.9/site-packages/llama_index/indices/base.py:181 in         â”‚
â”‚ insert_nodes                                                                                     â”‚
â”‚                                                                                                  â”‚
â”‚   178 â”‚   â”‚   """"""Insert nodes.""""""                                                                â”‚
â”‚   179 â”‚   â”‚   with self._service_context.callback_manager.as_trace(""insert_nodes""):              â”‚
â”‚   180 â”‚   â”‚   â”‚   self.docstore.add_documents(nodes, allow_update=True)                          â”‚
â”‚ â± 181 â”‚   â”‚   â”‚   self._insert(nodes, **insert_kwargs)                                           â”‚
â”‚   182 â”‚   â”‚   â”‚   self._storage_context.index_store.add_index_struct(self._index_struct)         â”‚
â”‚   183 â”‚                                                                                          â”‚
â”‚   184 â”‚   def insert(self, document: Document, **insert_kwargs: Any) -> None:                    â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/llama_index/indices/keyword_table/base.py: â”‚
â”‚ 173 in _insert                                                                                   â”‚
â”‚                                                                                                  â”‚
â”‚   170 â”‚   def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:            â”‚
â”‚   171 â”‚   â”‚   """"""Insert nodes.""""""                                                                â”‚
â”‚   172 â”‚   â”‚   for n in nodes:                                                                    â”‚
â”‚ â± 173 â”‚   â”‚   â”‚   keywords = self._extract_keywords(                                             â”‚
â”‚   174 â”‚   â”‚   â”‚   â”‚   n.get_content(metadata_mode=MetadataMode.LLM)                              â”‚
â”‚   175 â”‚   â”‚   â”‚   )                                                                              â”‚
â”‚   176 â”‚   â”‚   â”‚   self._index_struct.add_node(list(keywords), n)                                 â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/llama_index/indices/keyword_table/base.py: â”‚
â”‚ 222 in _extract_keywords                                                                         â”‚
â”‚                                                                                                  â”‚
â”‚   219 â”‚                                                                                          â”‚
â”‚   220 â”‚   def _extract_keywords(self, text: str) -> Set[str]:                                    â”‚
â”‚   221 â”‚   â”‚   """"""Extract keywords from text.""""""                                                  â”‚
â”‚ â± 222 â”‚   â”‚   response = self._service_context.llm_predictor.predict(                            â”‚
â”‚   223 â”‚   â”‚   â”‚   self.keyword_extract_template,                                                 â”‚
â”‚   224 â”‚   â”‚   â”‚   text=text,                                                                     â”‚
â”‚   225 â”‚   â”‚   )                                                                                  â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/llama_index/llm_predictor/base.py:142 in   â”‚
â”‚ predict                                                                                          â”‚
â”‚                                                                                                  â”‚
â”‚   139 â”‚   â”‚   if self._llm.metadata.is_chat_model:                                               â”‚
â”‚   140 â”‚   â”‚   â”‚   messages = prompt.format_messages(llm=self._llm, **prompt_args)                â”‚
â”‚   141 â”‚   â”‚   â”‚   messages = self._extend_messages(messages)                                     â”‚
â”‚ â± 142 â”‚   â”‚   â”‚   chat_response = self._llm.chat(messages)                                       â”‚
â”‚   143 â”‚   â”‚   â”‚   output = chat_response.message.content or """"                                   â”‚
â”‚   144 â”‚   â”‚   â”‚   # NOTE: this is an approximation, only for token counting                      â”‚
â”‚   145 â”‚   â”‚   â”‚   formatted_prompt = messages_to_prompt(messages)                                â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/llama_index/llms/base.py:151 in            â”‚
â”‚ wrapped_llm_chat                                                                                 â”‚
â”‚                                                                                                  â”‚
â”‚   148 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   EventPayload.SERIALIZED: _self.to_dict(),                          â”‚
â”‚   149 â”‚   â”‚   â”‚   â”‚   â”‚   },                                                                     â”‚
â”‚   150 â”‚   â”‚   â”‚   â”‚   )                                                                          â”‚
â”‚ â± 151 â”‚   â”‚   â”‚   â”‚   f_return_val = f(_self, messages, **kwargs)                                â”‚
â”‚   152 â”‚   â”‚   â”‚   â”‚                                                                              â”‚
â”‚   153 â”‚   â”‚   â”‚   â”‚   if isinstance(f_return_val, Generator):                                    â”‚
â”‚   154 â”‚   â”‚   â”‚   â”‚   â”‚   # intercept the generator and add a callback to the end                â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/llama_index/llms/langchain.py:57 in chat   â”‚
â”‚                                                                                                  â”‚
â”‚    54 â”‚   @llm_chat_callback()                                                                   â”‚
â”‚    55 â”‚   def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:        â”‚
â”‚    56 â”‚   â”‚   lc_messages = to_lc_messages(messages)                                             â”‚
â”‚ â±  57 â”‚   â”‚   lc_message = self._llm.predict_messages(messages=lc_messages, **kwargs)            â”‚
â”‚    58 â”‚   â”‚   message = from_lc_messages([lc_message])[0]                                        â”‚
â”‚    59 â”‚   â”‚   return ChatResponse(message=message)                                               â”‚
â”‚    60                                                                                            â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/langchain/chat_models/base.py:601 in       â”‚
â”‚ predict_messages                                                                                 â”‚
â”‚                                                                                                  â”‚
â”‚   598 â”‚   â”‚   â”‚   _stop = None                                                                   â”‚
â”‚   599 â”‚   â”‚   else:                                                                              â”‚
â”‚   600 â”‚   â”‚   â”‚   _stop = list(stop)                                                             â”‚
â”‚ â± 601 â”‚   â”‚   return self(messages, stop=_stop, **kwargs)                                        â”‚
â”‚   602 â”‚                                                                                          â”‚
â”‚   603 â”‚   async def apredict(                                                                    â”‚
â”‚   604 â”‚   â”‚   self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any            â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/langchain/chat_models/base.py:551 in       â”‚
â”‚ __call__                                                                                         â”‚
â”‚                                                                                                  â”‚
â”‚   548 â”‚   â”‚   callbacks: Callbacks = None,                                                       â”‚
â”‚   549 â”‚   â”‚   **kwargs: Any,                                                                     â”‚
â”‚   550 â”‚   ) -> BaseMessage:                                                                      â”‚
â”‚ â± 551 â”‚   â”‚   generation = self.generate(                                                        â”‚
â”‚   552 â”‚   â”‚   â”‚   [messages], stop=stop, callbacks=callbacks, **kwargs                           â”‚
â”‚   553 â”‚   â”‚   ).generations[0][0]                                                                â”‚
â”‚   554 â”‚   â”‚   if isinstance(generation, ChatGeneration):                                         â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/langchain/chat_models/base.py:309 in       â”‚
â”‚ generate                                                                                         â”‚
â”‚                                                                                                  â”‚
â”‚   306 â”‚   â”‚   â”‚   except BaseException as e:                                                     â”‚
â”‚   307 â”‚   â”‚   â”‚   â”‚   if run_managers:                                                           â”‚
â”‚   308 â”‚   â”‚   â”‚   â”‚   â”‚   run_managers[i].on_llm_error(e)                                        â”‚
â”‚ â± 309 â”‚   â”‚   â”‚   â”‚   raise e                                                                    â”‚
â”‚   310 â”‚   â”‚   flattened_outputs = [                                                              â”‚
â”‚   311 â”‚   â”‚   â”‚   LLMResult(generations=[res.generations], llm_output=res.llm_output)            â”‚
â”‚   312 â”‚   â”‚   â”‚   for res in results                                                             â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/langchain/chat_models/base.py:299 in       â”‚
â”‚ generate                                                                                         â”‚
â”‚                                                                                                  â”‚
â”‚   296 â”‚   â”‚   for i, m in enumerate(messages):                                                   â”‚
â”‚   297 â”‚   â”‚   â”‚   try:                                                                           â”‚
â”‚   298 â”‚   â”‚   â”‚   â”‚   results.append(                                                            â”‚
â”‚ â± 299 â”‚   â”‚   â”‚   â”‚   â”‚   self._generate_with_cache(                                             â”‚
â”‚   300 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   m,                                                                 â”‚
â”‚   301 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   stop=stop,                                                         â”‚
â”‚   302 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   run_manager=run_managers[i] if run_managers else None,             â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/langchain/chat_models/base.py:446 in       â”‚
â”‚ _generate_with_cache                                                                             â”‚
â”‚                                                                                                  â”‚
â”‚   443 â”‚   â”‚   â”‚   â”‚   â”‚   ""Asked to cache, but no cache found at `langchain.cache`.""             â”‚
â”‚   444 â”‚   â”‚   â”‚   â”‚   )                                                                          â”‚
â”‚   445 â”‚   â”‚   â”‚   if new_arg_supported:                                                          â”‚
â”‚ â± 446 â”‚   â”‚   â”‚   â”‚   return self._generate(                                                     â”‚
â”‚   447 â”‚   â”‚   â”‚   â”‚   â”‚   messages, stop=stop, run_manager=run_manager, **kwargs                 â”‚
â”‚   448 â”‚   â”‚   â”‚   â”‚   )                                                                          â”‚
â”‚   449 â”‚   â”‚   â”‚   else:                                                                          â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/langchain/chat_models/vertexai.py:158 in   â”‚
â”‚ _generate                                                                                        â”‚
â”‚                                                                                                  â”‚
â”‚   155 â”‚   â”‚   â”‚   )                                                                              â”‚
â”‚   156 â”‚   â”‚   else:                                                                              â”‚
â”‚   157 â”‚   â”‚   â”‚   chat = self.client.start_chat(message_history=history.history, **params)       â”‚
â”‚ â± 158 â”‚   â”‚   response = chat.send_message(question.content)                                     â”‚
â”‚   159 â”‚   â”‚   text = self._enforce_stop_words(response.text, stop)                               â”‚
â”‚   160 â”‚   â”‚   return ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])   â”‚
â”‚   161                                                                                            â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/vertexai/language_models/_language_models. â”‚
â”‚ py:855 in send_message                                                                           â”‚
â”‚                                                                                                  â”‚
â”‚    852 â”‚   â”‚   â”‚   â”‚   for example in self._examples                                             â”‚
â”‚    853 â”‚   â”‚   â”‚   ]                                                                             â”‚
â”‚    854 â”‚   â”‚                                                                                     â”‚
â”‚ â±  855 â”‚   â”‚   prediction_response = self._model._endpoint.predict(                              â”‚
â”‚    856 â”‚   â”‚   â”‚   instances=[prediction_instance],                                              â”‚
â”‚    857 â”‚   â”‚   â”‚   parameters=prediction_parameters,                                             â”‚
â”‚    858 â”‚   â”‚   )                                                                                 â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/google/cloud/aiplatform/models.py:1564 in  â”‚
â”‚ predict                                                                                          â”‚
â”‚                                                                                                  â”‚
â”‚   1561 â”‚   â”‚   â”‚   â”‚   ),                                                                        â”‚
â”‚   1562 â”‚   â”‚   â”‚   )                                                                             â”‚
â”‚   1563 â”‚   â”‚   else:                                                                             â”‚
â”‚ â± 1564 â”‚   â”‚   â”‚   prediction_response = self._prediction_client.predict(                        â”‚
â”‚   1565 â”‚   â”‚   â”‚   â”‚   endpoint=self._gca_resource.name,                                         â”‚
â”‚   1566 â”‚   â”‚   â”‚   â”‚   instances=instances,                                                      â”‚
â”‚   1567 â”‚   â”‚   â”‚   â”‚   parameters=parameters,                                                    â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/google/cloud/aiplatform_v1/services/predic â”‚
â”‚ tion_service/client.py:606 in predict                                                            â”‚
â”‚                                                                                                  â”‚
â”‚    603 â”‚   â”‚   )                                                                                 â”‚
â”‚    604 â”‚   â”‚                                                                                     â”‚
â”‚    605 â”‚   â”‚   # Send the request.                                                               â”‚
â”‚ â±  606 â”‚   â”‚   response = rpc(                                                                   â”‚
â”‚    607 â”‚   â”‚   â”‚   request,                                                                      â”‚
â”‚    608 â”‚   â”‚   â”‚   retry=retry,                                                                  â”‚
â”‚    609 â”‚   â”‚   â”‚   timeout=timeout,                                                              â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:113 in  â”‚
â”‚ __call__                                                                                         â”‚
â”‚                                                                                                  â”‚
â”‚   110 â”‚   â”‚   â”‚   metadata.extend(self._metadata)                                                â”‚
â”‚   111 â”‚   â”‚   â”‚   kwargs[""metadata""] = metadata                                                  â”‚
â”‚   112 â”‚   â”‚                                                                                      â”‚
â”‚ â± 113 â”‚   â”‚   return wrapped_func(*args, **kwargs)                                               â”‚
â”‚   114                                                                                            â”‚
â”‚   115                                                                                            â”‚
â”‚   116 def wrap_method(                                                                           â”‚
â”‚                                                                                                  â”‚
â”‚ /opt/conda/envs/whirlpalm/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:74 in      â”‚
â”‚ error_remapped_callable                                                                          â”‚
â”‚                                                                                                  â”‚
â”‚    71 â”‚   â”‚   try:                                                                               â”‚
â”‚    72 â”‚   â”‚   â”‚   return callable_(*args, **kwargs)                                              â”‚
â”‚    73 â”‚   â”‚   except grpc.RpcError as exc:                                                       â”‚
â”‚ â±  74 â”‚   â”‚   â”‚   raise exceptions.from_grpc_error(exc) from exc                                 â”‚
â”‚    75 â”‚                                                                                          â”‚
â”‚    76 â”‚   return error_remapped_callable                                                         â”‚
â”‚    77
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
36,5e5c7d86-f564-43f2-a3f9-3cb90b0b0321,,"{'state': 'open', 'year': 2023, 'month': 10, 'day': 25, 'assignee': '', 'size': '', 'index_id': '8475'}",[],[],{},5ed6aa49028fa99580723031f6da45f2352f38ceae497f271a84437f3bd28ee1,"[Bug]: PandasQueryEngine with llama ccp
### Bug Description

Hello I am trying to use the PandasQueryEngine as in this example : https://docs.llamaindex.ai/en/stable/examples/query_engine/pandas_query_engine.html but with llama ccp as ServiceContext.

I am on a Mac with M2 and macOS 13.6.

I have two issues : 
- first I installed llama ccp with codellama-7b.Q4_0.gguf as it is presented here : https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html. The install worked with llama_cpp_python==0.2.11 and text completion works. But when I try to use the model wrapped in ServiceContext with 
```from llama_cpp import Llama
from llama_index.embeddings import HuggingFaceEmbedding

llm = Llama(model_path=""/Users/yoannrussello/Downloads/codellama-7b.Q4_0.gguf"")
embed_model = HuggingFaceEmbedding(model_name=""BAAI/bge-small-en-v1.5"")
# create a service context
service_context = ServiceContext.from_defaults(
    llm=llm,
    embed_model=embed_model,
)
```
I got the error : 
```AttributeError                            Traceback (most recent call last)
[/Users/yoannrussello/Documents/pdf-parsing/notebooks/test_llama.ipynb](https://file+.vscode-resource.vscode-cdn.net/Users/yoannrussello/Documents/pdf-parsing/notebooks/test_llama.ipynb) Cell 13 line 1
----> [1](vscode-notebook-cell:/Users/yoannrussello/Documents/pdf-parsing/notebooks/test_llama.ipynb#X15sZmlsZQ%3D%3D?line=0) service_context = ServiceContext.from_defaults(llm=llm,    embed_model=embed_model)

File [~/.pyenv/versions/3.10.4/envs/pdf-parsing_3.10.4/lib/python3.10/site-packages/llama_index/indices/service_context.py:167](https://file+.vscode-resource.vscode-cdn.net/Users/yoannrussello/Documents/pdf-parsing/notebooks/~/.pyenv/versions/3.10.4/envs/pdf-parsing_3.10.4/lib/python3.10/site-packages/llama_index/indices/service_context.py:167), in ServiceContext.from_defaults(cls, llm_predictor, llm, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, system_prompt, query_wrapper_prompt, chunk_size, chunk_overlap, context_window, num_output, chunk_size_limit)
    163 embed_model = resolve_embed_model(embed_model)
    164 embed_model.callback_manager = callback_manager
    166 prompt_helper = prompt_helper or _get_default_prompt_helper(
--> 167     llm_metadata=llm_predictor.metadata,
    168     context_window=context_window,
    169     num_output=num_output,
    170 )
    172 node_parser = node_parser or _get_default_node_parser(
    173     chunk_size=chunk_size,
    174     chunk_overlap=chunk_overlap,
    175     callback_manager=callback_manager,
    176 )
    178 llama_logger = llama_logger or LlamaLogger()

File [~/.pyenv/versions/3.10.4/envs/pdf-parsing_3.10.4/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:115](https://file+.vscode-resource.vscode-cdn.net/Users/yoannrussello/Documents/pdf-parsing/notebooks/~/.pyenv/versions/3.10.4/envs/pdf-parsing_3.10.4/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:115), in LLMPredictor.metadata(self)
    112 @property
    113 def metadata(self) -> LLMMetadata:
    114     """"""Get LLM metadata.""""""
--> 115     return self._llm.metadata

AttributeError: 'Llama' object has no attribute 'metadata' 
```
I don't know if this error needs to be fixed here or in llama ccp or if it needs a new features.

- I tried to use HuggingFaceModel like here : https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html. The model is loaded but when I run an easy query I got the error : 
```
Llama.generate: prefix-match hit
There was an error running the output as Python code. Error message: invalid syntax (<unknown>, line 3)

llama_print_timings:        load time =  1575.28 ms
llama_print_timings:      sample time =   168.43 ms [/](https://file+.vscode-resource.vscode-cdn.net/)   256 runs   (    0.66 ms per token,  1519.89 tokens per second)
llama_print_timings: prompt eval time =   386.26 ms [/](https://file+.vscode-resource.vscode-cdn.net/)    30 tokens (   12.88 ms per token,    77.67 tokens per second)
llama_print_timings:        eval time =  7143.62 ms [/](https://file+.vscode-resource.vscode-cdn.net/)   255 runs   (   28.01 ms per token,    35.70 tokens per second)
llama_print_timings:       total time =  7977.79 ms
Traceback (most recent call last):
  File ""/Users/yoannrussello/.pyenv/versions/3.10.4/envs/pdf-parsing_3.10.4/lib/python3.10/site-packages/llama_index/query_engine/pandas_query_engine.py"", line 59, in default_output_processor
    tree = ast.parse(output)
  File ""/Users/yoannrussello/.pyenv/versions/3.10.4/lib/python3.10/ast.py"", line 50, in parse
    return compile(source, filename, mode, flags,
  File ""<unknown>"", line 3
    [SYS] <<INS>> 
                  ^
SyntaxError: invalid syntax
```
I've found this closed topic that has a similar issue : https://github.com/run-llama/llama_index/issues/7269.
So I tried to add instructions : 
```
instruction_str = (
    ""We wish to convert this query to executable Python code using Pandas.\n""
    ""The final line of code should be a Python expression that can be called ""
    ""in a notebook. This expression should represent a solution ""
    ""to the query. Your response should ONLY contain python code.""
)
query_engine = PandasQueryEngine(df, service_context=service_context,   instruction_str = instruction_str)
response = query_engine.query(""What is the biggest sepal length?"")
print(response)
```
But I got the error : 
```
Llama.generate: prefix-match hit
There was an error running the output as Python code. Error message: invalid syntax (<unknown>, line 3)

llama_print_timings:        load time =  1575.28 ms
llama_print_timings:      sample time =   168.43 ms [/](https://file+.vscode-resource.vscode-cdn.net/)   256 runs   (    0.66 ms per token,  1519.89 tokens per second)
llama_print_timings: prompt eval time =   386.26 ms [/](https://file+.vscode-resource.vscode-cdn.net/)    30 tokens (   12.88 ms per token,    77.67 tokens per second)
llama_print_timings:        eval time =  7143.62 ms [/](https://file+.vscode-resource.vscode-cdn.net/)   255 runs   (   28.01 ms per token,    35.70 tokens per second)
llama_print_timings:       total time =  7977.79 ms
Traceback (most recent call last):
  File ""/Users/yoannrussello/.pyenv/versions/3.10.4/envs/pdf-parsing_3.10.4/lib/python3.10/site-packages/llama_index/query_engine/pandas_query_engine.py"", line 59, in default_output_processor
    tree = ast.parse(output)
  File ""/Users/yoannrussello/.pyenv/versions/3.10.4/lib/python3.10/ast.py"", line 50, in parse
    return compile(source, filename, mode, flags,
  File ""<unknown>"", line 3
    [SYS] <<INS>> 
                  ^
SyntaxError: invalid syntax
```
More details in the steps to reproduce.

My primary goal was to use llama offline to extract data from pdf tables converted to pandas dataframe like here : https://gpt-index.readthedocs.io/en/latest/examples/query_engine/pdf_tables/recursive_retriever.html.

### Version

0.8.45.post1

### Steps to Reproduce

Install METAL as in here : https://gpt-index.readthedocs.io/en/v0.8.25/examples/llm/llama_2_llama_cpp.html
Install the libraries
```
huggingface-hub==0.17.3
llama-hub==0.0.38
llama-index==0.8.45.post1
llama_cpp_python==0.2.11
```
Then run the following code : 
```
from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt
from llama_cpp import Llama
from llama_index.embeddings import HuggingFaceEmbedding
from llama_index.llms import LlamaCPP

import pandas as pd

model_url = ""https://huggingface.co/TheBloke/CodeLlama-13B-GGUF/resolve/main/codellama-13b.Q4_0.gguf""
llm = LlamaCPP(
    # You can pass in the URL to a GGML model to download it automatically
    model_url=model_url,
    # optionally, you can set the path to a pre-downloaded model instead of model_url
    model_path=None,
    temperature=0.1,
    max_new_tokens=256,
    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room
    context_window=3900,
    # kwargs to pass to __call__()
    generate_kwargs={},
    # kwargs to pass to __init__()
    # set to at least 1 to use GPU
    model_kwargs={""n_gpu_layers"": 1},
    # transform inputs into Llama2 format
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
    verbose=True,
)
embed_model = HuggingFaceEmbedding(model_name=""BAAI/bge-small-en-v1.5"")
instruction_str = (
    ""We wish to convert this query to executable Python code using Pandas.\n""
    ""The final line of code should be a Python expression that can be called ""
    ""with the `eval()` function. This expression should represent a solution ""
    ""to the query. Your response should ONLY contain python code.""
)

data = {
    ""sepal_length"": [5.1, 4.9, 4.7, 4.6],
    ""sepal_width"": [3.5, 3.0, 3.2, 3.1],
    ""petal_length"": [1.4, 1.4, 1.3, 1.5]
}
df = pd.DataFrame(data)

# create a service context
service_context = ServiceContext.from_defaults(
    llm=llm,
    embed_model=embed_model,
)

query_engine = PandasQueryEngine(df, service_context=service_context)
response = query_engine.query(""What is the biggest sepal length?"")
```



### Relevant Logs/Tracbacks

```shell
llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from /Users/yoannrussello/Library/Caches/llama_index/models/codellama-13b.Q4_0.gguf (version GGUF V2 (latest))
llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32016,     1,     1 ]
llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor    9:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   18:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   27:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   36:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   63:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   64:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   65:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   66:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   67:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   68:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   69:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   70:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   71:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   72:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   73:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   74:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   75:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   76:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   77:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   78:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   79:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   80:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   81:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   82:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   83:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   84:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   85:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   86:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   87:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   88:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   89:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   90:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   91:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   92:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   93:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   94:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor   95:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   96:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor   97:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor   98:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor   99:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  100:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  101:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  102:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  103:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  104:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  105:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  106:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  107:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  108:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  109:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  110:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  111:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  112:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  113:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  114:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  115:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  116:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  117:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  118:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  119:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  120:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  121:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  122:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  123:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  124:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  125:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  126:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  127:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  128:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  129:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  130:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  131:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  132:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  133:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  134:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  135:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  136:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  137:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  138:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  139:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  142:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  143:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  144:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  150:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  151:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  152:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  153:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  154:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  155:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  156:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  157:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  158:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  159:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  160:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  161:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  162:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  168:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  169:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  170:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  171:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  177:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  178:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  179:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  180:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  186:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  187:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  188:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  189:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  191:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  193:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  195:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  196:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  197:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  198:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  200:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  202:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  204:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  205:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  206:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  207:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  213:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  214:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  215:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  216:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  222:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  223:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  224:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  225:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  231:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  232:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  233:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  234:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  235:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  236:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  237:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  238:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  239:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  240:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  241:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  242:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  243:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  249:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  250:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  251:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  252:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  254:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  258:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  259:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  260:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  261:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  267:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  268:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  269:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  270:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  272:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  273:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  275:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  276:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  277:                    output.weight q6_K     [  5120, 32016,     1,     1 ]
llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  289:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  290:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  291:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  292:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  293:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  294:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  295:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  296:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  297:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  298:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  299:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  300:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  301:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  302:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  303:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  304:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  305:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  306:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  307:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  308:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  309:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  310:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  311:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  312:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  313:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  314:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  315:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  316:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  317:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  318:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  319:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  320:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  321:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  322:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  323:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  324:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  325:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  326:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  327:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  328:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  329:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  330:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  331:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  332:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  333:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  334:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  335:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  336:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  337:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  338:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  339:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  340:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  341:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  342:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  343:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  344:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  345:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  346:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  347:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  348:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  349:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  350:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  351:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  352:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  353:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  354:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]
llama_model_loader: - tensor  355:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  356:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]
llama_model_loader: - tensor  357:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - tensor  358:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  359:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  360:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  361:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]
llama_model_loader: - tensor  362:               output_norm.weight f32      [  5120,     1,     1,     1 ]
llama_model_loader: - kv   0:                       general.architecture str     
llama_model_loader: - kv   1:                               general.name str     
llama_model_loader: - kv   2:                       llama.context_length u32     
llama_model_loader: - kv   3:                     llama.embedding_length u32     
llama_model_loader: - kv   4:                          llama.block_count u32     
llama_model_loader: - kv   5:                  llama.feed_forward_length u32     
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     
llama_model_loader: - kv   7:                 llama.attention.head_count u32     
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     
llama_model_loader: - kv  10:                       llama.rope.freq_base f32     
llama_model_loader: - kv  11:                          general.file_type u32     
llama_model_loader: - kv  12:                       tokenizer.ggml.model str     
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     
llama_model_loader: - kv  19:               general.quantization_version u32     
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_0:  281 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_print_meta: format           = GGUF V2 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32016
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 16384
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 40
llm_load_print_meta: n_layer          = 40
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: n_ff             = 13824
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: model type       = 13B
llm_load_print_meta: model ftype      = mostly Q4_0
llm_load_print_meta: model params     = 13.02 B
llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) 
llm_load_print_meta: general.name   = codellama_codellama-13b-hf
llm_load_print_meta: BOS token = 1 '<s>'
llm_load_print_meta: EOS token = 2 '</s>'
llm_load_print_meta: UNK token = 0 '<unk>'
llm_load_print_meta: LF token  = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.12 MB
llm_load_tensors: mem required  = 7024.12 MB
...................................................................................................
llama_new_context_with_model: n_ctx      = 3900
llama_new_context_with_model: freq_base  = 1000000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: kv self size  = 3046.88 MB
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M2 Max
ggml_metal_init: picking default device: Apple M2 Max
ggml_metal_init: loading '/Users/yoannrussello/.pyenv/versions/3.10.4/envs/pdf-parsing_3.10.4/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'
ggml_metal_init: loaded kernel_add                            0x3ac66fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                        0x3ac66ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                            0x3ac6701b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                        0x3ac670410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                          0x3ac670670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                           0x3ac6708d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                           0x3ac670b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                           0x3ac670d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max                       0x3ac670ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_4                     0x3ac671250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                  0x3ac6714b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                0x3ac671710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                   0x3ac672fb0 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                   0x3ac673210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                  0x3ac673470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                  0x3ac6736d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                  0x3ac673930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                  0x3ac673b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                  0x3ac674270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                  0x3ac6744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                  0x3ac668090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                  0x3ac6682f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                       0x3ac6751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                           0x3ac675430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_f32_f32                0x3ac675690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x3ac6758f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_f16_f32_1row           0x3ac675b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_f16_f32_l4             0x3ac675db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x3ac676010 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x3ac6befe0 | th_max =  896 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x3ac6bf240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x3ac6bf4a0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x3ac6bf700 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x3ac6bf960 | th_max =  576 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x3ac6bfbc0 | th_max =  640 | th_width =   32
ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x3ac6bfe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                 0x3ac6c0080 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x3ac6c02e0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x3ac6c0540 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x3ac6c07a0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x3ac6c0a00 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x3ac6c0c60 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x3ac6c0ec0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x3ac6c1120 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x3ac6c1380 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x3ac6c15e0 | th_max =  768 | th_width =   32
ggml_metal_init: loaded kernel_rope_f32                       0x3ac6c1840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_f16                       0x3ac6c1aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_alibi_f32                      0x3ac6c1d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                    0x3ac6c1f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                    0x3ac6c21c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                    0x3ac6c2420 | th_max = 1024 | th_width =   32
ggml_metal_init: hasUnifiedMemory              = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 21845.34 MB
ggml_metal_init: maxTransferRate               = built-in GPU
llama_new_context_with_model: compute buffer total size = 348.18 MB
llama_new_context_with_model: max tensor size =   128.24 MB
ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.72 MB, (59107.19 / 21845.34), warning: current allocated size is greater than the recommended max working set size
ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3048.88 MB, (62156.06 / 21845.34), warning: current allocated size is greater than the recommended max working set size
ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   342.31 MB, (62498.38 / 21845.34), warning: current allocated size is greater than the recommended max working set size
AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
37,2266dc84-a940-45af-9be7-df6d70357671,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 1, 'assignee': '', 'size': '', 'index_id': '9244'}",[],[],{},803871f8926355dc37d3e320f59e1c72fca5f1750a569539a70d2ed674f93281,"[Bug]: KÃ¹zu Graph Store, RuntimeError: Parameters must be a dict
### Bug Description

While following the [KuzuGraphDemo](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/KuzuGraphDemo.html), I got an RuntimeError.

Please view the full log in my colab notebook.

### Version

llama_index: 0.9.10

kuzu: 0.1.0

### Steps to Reproduce

here's the colab notebook: https://colab.research.google.com/drive/10cRzBdRZxmUbBPnWw_dJJWZLQVuptNZs?usp=sharing

```py
index = KnowledgeGraphIndex.from_documents(
    documents,
    max_triplets_per_chunk=2,
    storage_context=storage_context,
    service_context=service_context,
)
```

Error occurs at this step.

### Relevant Logs/Tracbacks

```shell
â”‚ /usr/local/lib/python3.10/dist-packages/llama_index/indices/knowledge_graph/base.py:221 in       â”‚
â”‚ upsert_triplet                                                                                   â”‚
â”‚                                                                                                  â”‚
â”‚   218 â”‚   â”‚   â”‚   triplet (str): Knowledge triplet                                               â”‚
â”‚   219 â”‚   â”‚                                                                                      â”‚
â”‚   220 â”‚   â”‚   """"""                                                                                â”‚
â”‚ â± 221 â”‚   â”‚   self._graph_store.upsert_triplet(*triplet)                                         â”‚
â”‚   222 â”‚                                                                                          â”‚
â”‚   223 â”‚   def add_node(self, keywords: List[str], node: BaseNode) -> None:                       â”‚
â”‚   224 â”‚   â”‚   """"""Add node.                                                                       â”‚
â”‚                                                                                                  â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚    self = <llama_index.indices.knowledge_graph.base.KnowledgeGraphIndex object at            â”‚ â”‚
â”‚ â”‚           0x77184e9b2020>                                                                    â”‚ â”‚
â”‚ â”‚ triplet = ('I', 'worked on', 'writing')                                                      â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                                                  â”‚
â”‚ /usr/local/lib/python3.10/dist-packages/llama_index/graph_stores/kuzu.py:153 in upsert_triplet   â”‚
â”‚                                                                                                  â”‚
â”‚   150 â”‚   â”‚   â”‚   â”‚   [(""subj"", subj), (""obj"", obj), (""pred"", rel)],                             â”‚
â”‚   151 â”‚   â”‚   â”‚   )                                                                              â”‚
â”‚   152 â”‚   â”‚                                                                                      â”‚
â”‚ â± 153 â”‚   â”‚   is_subj_exists = check_entity_exists(self.connection, subj)                        â”‚
â”‚   154 â”‚   â”‚   is_obj_exists = check_entity_exists(self.connection, obj)                          â”‚
â”‚   155 â”‚   â”‚                                                                                      â”‚
â”‚   156 â”‚   â”‚   if not is_subj_exists:                                                             â”‚
â”‚                                                                                                  â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚ check_entity_exists = <function KuzuGraphStore.upsert_triplet.<locals>.check_entity_exists   â”‚ â”‚
â”‚ â”‚                       at 0x77184e8b2d40>                                                     â”‚ â”‚
â”‚ â”‚    check_rel_exists = <function KuzuGraphStore.upsert_triplet.<locals>.check_rel_exists at   â”‚ â”‚
â”‚ â”‚                       0x77184e8b28c0>                                                        â”‚ â”‚
â”‚ â”‚       create_entity = <function KuzuGraphStore.upsert_triplet.<locals>.create_entity at      â”‚ â”‚
â”‚ â”‚                       0x77184e8b2830>                                                        â”‚ â”‚
â”‚ â”‚          create_rel = <function KuzuGraphStore.upsert_triplet.<locals>.create_rel at         â”‚ â”‚
â”‚ â”‚                       0x77184e8b15a0>                                                        â”‚ â”‚
â”‚ â”‚                 obj = 'writing'                                                              â”‚ â”‚
â”‚ â”‚                 rel = 'worked on'                                                            â”‚ â”‚
â”‚ â”‚                self = <llama_index.graph_stores.kuzu.KuzuGraphStore object at                â”‚ â”‚
â”‚ â”‚                       0x7f1aeb961960>                                                        â”‚ â”‚
â”‚ â”‚                subj = 'I'                                                                    â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                                                  â”‚
â”‚ /usr/local/lib/python3.10/dist-packages/llama_index/graph_stores/kuzu.py:118 in                  â”‚
â”‚ check_entity_exists                                                                              â”‚
â”‚                                                                                                  â”‚
â”‚   115 â”‚   â”‚   """"""Add triplet.""""""                                                                 â”‚
â”‚   116 â”‚   â”‚                                                                                      â”‚
â”‚   117 â”‚   â”‚   def check_entity_exists(connection: Any, entity: str) -> bool:                     â”‚
â”‚ â± 118 â”‚   â”‚   â”‚   is_exists_result = connection.execute(                                         â”‚
â”‚   119 â”‚   â”‚   â”‚   â”‚   ""MATCH (n:%s) WHERE n.ID = $entity RETURN n.ID"" % self.node_table_name,    â”‚
â”‚   120 â”‚   â”‚   â”‚   â”‚   [(""entity"", entity)],                                                      â”‚
â”‚   121 â”‚   â”‚   â”‚   )                                                                              â”‚
â”‚                                                                                                  â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®         â”‚
â”‚ â”‚ connection = <kuzu.connection.Connection object at 0x7f1ae8e0ebf0>                   â”‚         â”‚
â”‚ â”‚     entity = 'I'                                                                     â”‚         â”‚
â”‚ â”‚       self = <llama_index.graph_stores.kuzu.KuzuGraphStore object at 0x7f1aeb961960> â”‚         â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯         â”‚
â”‚                                                                                                  â”‚
â”‚ /usr/local/lib/python3.10/dist-packages/kuzu/connection.py:77 in execute                         â”‚
â”‚                                                                                                  â”‚
â”‚    74 â”‚   â”‚   """"""                                                                                â”‚
â”‚    75 â”‚   â”‚   self.init_connection()                                                             â”‚
â”‚    76 â”‚   â”‚   if type(parameters) != dict:                                                       â”‚
â”‚ â±  77 â”‚   â”‚   â”‚   raise RuntimeError(""Parameters must be a dict"")                                â”‚
â”‚    78 â”‚   â”‚   prepared_statement = self.prepare(                                                 â”‚
â”‚    79 â”‚   â”‚   â”‚   query) if type(query) == str else query                                        â”‚
â”‚    80 â”‚   â”‚   _query_result = self._connection.execute(                                          â”‚
â”‚                                                                                                  â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®                           â”‚
â”‚ â”‚ parameters = [('entity', 'I')]                                     â”‚                           â”‚
â”‚ â”‚      query = 'MATCH (n:entity) WHERE n.ID = $entity RETURN n.ID'   â”‚                           â”‚
â”‚ â”‚       self = <kuzu.connection.Connection object at 0x7f1ae8e0ebf0> â”‚                           â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯                           â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
RuntimeError: Parameters must be a dict
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
38,aae9efdf-92ae-4372-839f-1c18558d7519,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9618'}",[],[],{},1fc96e7a0fc4af22b55cdcb43f15cee3080b6e78e168a9bb1d6ac1ccfd46f425,"[Question]: Unable to run agents using azure openai 
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

```
from llama_index.embeddings import resolve_embed_model
from llama_index.llms import AzureOpenAI
from llama_index import SimpleDirectoryReader, ServiceContext
from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool, LlamaToolkit
from llama_index.langchain_helpers.agents import create_llama_agent


llm = AzureOpenAI(
    model=""gpt-35-turbo"",
    deployment_name=""custom"",
    api_key=""custom"",
    azure_endpoint=""custom"",
    api_version=""2023-07-01-preview"")

embed_model = resolve_embed_model(""local:BAAI/bge-small-en-v1.5"")

from llama_index import set_global_service_context
service_context = ServiceContext.from_defaults(llm=llm,  embed_model=embed_model)
set_global_service_context(service_context)


march_2022 = SimpleDirectoryReader(input_files=[""uber_10q_march_2022.pdf""]).load_data()
june_2022 = SimpleDirectoryReader(input_files=[""uber_10q_june_2022.pdf""]).load_data()
sept_2022 = SimpleDirectoryReader(input_files=[""uber_10q_sept_2022.pdf""]).load_data()

march_index =  VectorStoreIndex.from_documents(march_2022)
june_index =  VectorStoreIndex.from_documents(june_2022)
sept_index =  VectorStoreIndex.from_documents(sept_2022)

march_engine = march_index.as_query_engine(similarity_top_k=3)
june_engine = june_index.as_query_engine(similarity_top_k=3)
sept_engine = sept_index.as_query_engine(similarity_top_k=3)



# initializing zero-shot ReAct agent

uber_config_sept = IndexToolConfig(
    query_engine=sept_engine, 
    name=f""Uber 10Q September 2022"",
    description=f""Provides information about Uber quarterly financials ending September 2022"",
    tool_kwargs={""return_direct"": False}
)
uber_config_june = IndexToolConfig(
    query_engine=june_engine, 
    name=f""Uber 10Q June 2022"",
    description=f""Provides information about Uber quarterly financials ending June 2022"",
    tool_kwargs={""return_direct"": False}
)
uber_config_march = IndexToolConfig(
    query_engine=march_engine, 
    name=f""Uber 10Q March 2022"",
    description=f""Provides information about Uber quarterly financials ending March 2022"",
    tool_kwargs={""return_direct"": False}
)

toolkit = LlamaToolkit(
    index_configs=[uber_config_sept, uber_config_june, uber_config_march],
)

# this is a light wrapper around `initialize_agent` in langchain (which defaults to zero-shot)
agent_chain = create_llama_agent(
    toolkit,
    llm = llm
)

agent_chain.run(input=""Analyze Uber revenue growth over the last few quarters"")
```



The error that I am getting is 
_ValidationError: 2 validation errors for LLMChain
llm
  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)
llm
  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)_



Not in this in multiple agents based work AZURE OPENAI is showing erro ",,,"{metadata_str}

{content}",{key}: {value},"
",Document
39,0a2a9d16-adc6-45da-b83c-57f101128c21,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': 'L', 'index_id': '9491'}",[],[],{},835c09b0101de1fa53cff6429dd65051207e8278ddea57651923502765534cb4,"Centralize Logger
# Description
This PR seeks to create a simplified method for managing logging for the Llama Index package. The implementation should embrace Python best practices to ensure integration with the greater Python ecosystem.

This PR is focused only on implementations of the Python `logging` library. Notably, it does not address:
- Tracing, which requires unique integration with other tools.
- Existing `print()` statements. These are often used with the `verbose` flag and thus have a different lifecycle than application logging. This behavior could be unified with the logging pipeline, but should addressed in an isolated project.
- The `llama_index/logger/base.LlamaLogger` looks to be dead code? It's probably better to not touch it in this PR.

The core logic changes are found in `llama_index/logger/__init__.py`. I added some comments to call out other important changes. Most of the files touched are simply removing `logger = logger.getLogger(__name__)`.

The most likely negative result from this PR is removing a file's `logger` reference, which could result in a run time error. The linting and test suites provide a good check here.

## Developer Interaction

A developer sets the logging level for the package with the following line:
```python
logging.getLogger(""llama_index"").setLevel(logging.DEBUG))
```

The logger can also be configured using [configuration files](https://docs.python.org/3/howto/logging.html#configuring-logging). 

## Problem
There are currently >100 distinct loggers within the package.
```python
loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
for l in loggers:
    if ""llama_index"" in l.name:
        print(l.name)
```

The following line is repeated in many files. Each invocation creates a new logger, named after the file where it lives. The most likely explanation for this repeated code is ""copy pasta""; simply replicating an existing pattern in a new file.
```python
logger = logging.getLogger(__name__)
```

To ensure reliable logging configuration, a developer must set configuration for every logger.

## Solution
Llama Index should have a single logger that is used across the entire project.

- Define `logger` in `llama_index/logger/__init__.py`. Placing in this directory seeks to avoid circular imports. All package scoped logging configuration should be done in this file. 
- This includes the `.addHandler(NullHandler())` configuration; which is identified as best practice by Python documentation. The `NullHandler()` simply sets the package to send logs to ""nowhere"" (e.g. `/dev/null`). The developer can then choose to route the logs.
- The logger does not need to be exported, so it does not need to live in `llama_index/__init__.py`.
- Use with a simple import statement. `from llama_index.logging import logger`

Fixes #9450 

Note: Python [documentation](https://docs.python.org/3/library/logging.html#logger-objects) states that loggers are hierarchical; e.g. the `llama_index.agent.openai_agent` logger is a child of the module's `llama_index` logger. Thus, we should be able to push down configuration from the parent logger to all children. In practice, I was not able to get this to work.

## Type of Change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?
- Manually verified that configurations work as expected using project that imports `llama_index`
- `make test` and `make lint` to verify that all files updated are safe.

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [ ] I stared at the code and made sure it makes sense
- [x] Verify that a project using Llama Index can configure logger correctly. [Add](https://python-poetry.org/docs/cli/#add) the target branch to your Poetry environment. Follow the instructions in `llama_index/logger/__init__.py`.

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [x] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
40,684857dc-7dd6-40d6-b8bf-8684edf27e91,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9611'}",[],[],{},a689983d7dad9790f856aa782fabd92b4b9efbcc356ec8b5048b7a7a80bac810,"[Bug]: ã€Urgentã€‘ã€Azure OpenAI Embeddingsã€‘TypeError: 'NoneType' object is not iterable
### Bug Description

There was an error when I followed this tutorial to implement Azure OpenAI Embedding

https://docs.llamaindex.ai/en/latest/examples/customization/llms/AzureOpenAI.html#



The Codeï¼š
`from llama_index.llms import AzureOpenAI
from llama_index.embeddings import AzureOpenAIEmbedding
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
import logging
import sys

logging.basicConfig(
    stream=sys.stdout, level=logging.INFO
)  # logging.DEBUG for more verbose output
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

api_key = ""xxx""
azure_endpoint = ""https://xxxxxxx/azure""
api_version = ""2023-06-01-preview""

llm = AzureOpenAI(
    model=""gpt-35-turbo-16k"",
    deployment_name=""my-custom-llm"",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)


embed_model = AzureOpenAIEmbedding(
    model=""text-embedding-ada-002"",
    deployment_name=""my-custom-embedding"",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)


from llama_index import set_global_service_context

service_context = ServiceContext.from_defaults(
    llm=llm,
    embed_model=embed_model,
)

set_global_service_context(service_context)



documents = SimpleDirectoryReader(
    input_files=[""/mnt/workspace/workgroup/lengmou/Data/NL2SQL/artificial_query.txt""]
).load_data()
index = VectorStoreIndex.from_documents(documents)
`





---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
[/mnt/workspace/workgroup/lengmou/TARS-Code/LlamaIndex/test.ipynb](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/TARS-Code/LlamaIndex/test.ipynb) Cell 4 line 4
      [1](vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/workgroup/lengmou/TARS-Code/LlamaIndex/test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0) documents = SimpleDirectoryReader(
      [2](vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/workgroup/lengmou/TARS-Code/LlamaIndex/test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1)     input_files=[""[/mnt/workspace/workgroup/lengmou/Data/NL2SQL/artificial_query.txt](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/Data/NL2SQL/artificial_query.txt)""]
      [3](vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/workgroup/lengmou/TARS-Code/LlamaIndex/test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2) ).load_data()
----> [4](vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/workgroup/lengmou/TARS-Code/LlamaIndex/test.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3) index = VectorStoreIndex.from_documents(documents)

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/base.py:102](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/base.py:102), in BaseIndex.from_documents(cls, documents, storage_context, service_context, show_progress, **kwargs)
     97     docstore.set_document_hash(doc.get_doc_id(), doc.hash)
     98 nodes = service_context.node_parser.get_nodes_from_documents(
     99     documents, show_progress=show_progress
    100 )
--> 102 return cls(
    103     nodes=nodes,
    104     storage_context=storage_context,
    105     service_context=service_context,
    106     show_progress=show_progress,
    107     **kwargs,
    108 )

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:49](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:49), in VectorStoreIndex.__init__(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, **kwargs)
     47 self._use_async = use_async
     48 self._store_nodes_override = store_nodes_override
---> 49 super().__init__(
     50     nodes=nodes,
     51     index_struct=index_struct,
     52     service_context=service_context,
     53     storage_context=storage_context,
     54     show_progress=show_progress,
     55     **kwargs,
     56 )

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/base.py:71](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/base.py:71), in BaseIndex.__init__(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)
     69 if index_struct is None:
     70     assert nodes is not None
---> 71     index_struct = self.build_index_from_nodes(nodes)
     72 self._index_struct = index_struct
     73 self._storage_context.index_store.add_index_struct(self._index_struct)

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:254](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:254), in VectorStoreIndex.build_index_from_nodes(self, nodes, **insert_kwargs)
    243 def build_index_from_nodes(
    244     self,
    245     nodes: Sequence[BaseNode],
    246     **insert_kwargs: Any,
    247 ) -> IndexDict:
    248     """"""Build the index from nodes.
    249 
    250     NOTE: Overrides BaseIndex.build_index_from_nodes.
    251         VectorStoreIndex only stores nodes in document store
    252         if vector store does not store text
    253     """"""
--> 254     return self._build_index_from_nodes(nodes, **insert_kwargs)

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:235](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:235), in VectorStoreIndex._build_index_from_nodes(self, nodes, **insert_kwargs)
    233     run_async_tasks(tasks)
    234 else:
--> 235     self._add_nodes_to_index(
    236         index_struct,
    237         nodes,
    238         show_progress=self._show_progress,
    239         **insert_kwargs,
    240     )
    241 return index_struct

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:188](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:188), in VectorStoreIndex._add_nodes_to_index(self, index_struct, nodes, show_progress, **insert_kwargs)
    185 if not nodes:
    186     return
--> 188 nodes = self._get_node_with_embedding(nodes, show_progress)
    189 new_ids = self._vector_store.add(nodes, **insert_kwargs)
    191 if not self._vector_store.stores_text or self._store_nodes_override:
    192     # NOTE: if the vector store doesn't store text,
    193     # we need to add the nodes to the index struct and document store

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:100](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/vector_store/base.py:100), in VectorStoreIndex._get_node_with_embedding(self, nodes, show_progress)
     89 def _get_node_with_embedding(
     90     self,
     91     nodes: Sequence[BaseNode],
     92     show_progress: bool = False,
     93 ) -> List[BaseNode]:
     94     """"""Get tuples of id, node, and embedding.
     95 
     96     Allows us to store these nodes in a vector store.
     97     Embeddings are called in batches.
     98 
     99     """"""
--> 100     id_to_embed_map = embed_nodes(
    101         nodes, self._service_context.embed_model, show_progress=show_progress
    102     )
    104     results = []
    105     for node in nodes:

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/utils.py:137](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/indices/utils.py:137), in embed_nodes(nodes, embed_model, show_progress)
    134     else:
    135         id_to_embed_map[node.node_id] = node.embedding
--> 137 new_embeddings = embed_model.get_text_embedding_batch(
    138     texts_to_embed, show_progress=show_progress
    139 )
    141 for new_id, text_embedding in zip(ids_to_embed, new_embeddings):
    142     id_to_embed_map[new_id] = text_embedding

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/embeddings/base.py:250](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/embeddings/base.py:250), in BaseEmbedding.get_text_embedding_batch(self, texts, show_progress)
    244 if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:
    245     # flush
    246     with self.callback_manager.event(
    247         CBEventType.EMBEDDING,
    248         payload={EventPayload.SERIALIZED: self.to_dict()},
    249     ) as event:
--> 250         embeddings = self._get_text_embeddings(cur_batch)
    251         result_embeddings.extend(embeddings)
    252         event.on_end(
    253             payload={
    254                 EventPayload.CHUNKS: cur_batch,
    255                 EventPayload.EMBEDDINGS: embeddings,
    256             },
    257         )

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/embeddings/openai.py:344](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/embeddings/openai.py:344), in OpenAIEmbedding._get_text_embeddings(self, texts)
    337 def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
    338     """"""Get text embeddings.
    339 
    340     By default, this is a wrapper around _get_text_embedding.
    341     Can be overridden for batch queries.
    342 
    343     """"""
--> 344     return get_embeddings(
    345         self._client,
    346         texts,
    347         engine=self._text_engine,
    348         **self.additional_kwargs,
    349     )

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/tenacity/__init__.py:289](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/tenacity/__init__.py:289), in BaseRetrying.wraps.<locals>.wrapped_f(*args, **kw)
    287 @functools.wraps(f)
    288 def wrapped_f(*args: t.Any, **kw: t.Any) -> t.Any:
--> 289     return self(f, *args, **kw)

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/tenacity/__init__.py:379](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/tenacity/__init__.py:379), in Retrying.__call__(self, fn, *args, **kwargs)
    377 retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
    378 while True:
--> 379     do = self.iter(retry_state=retry_state)
    380     if isinstance(do, DoAttempt):
    381         try:

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/tenacity/__init__.py:314](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/tenacity/__init__.py:314), in BaseRetrying.iter(self, retry_state)
    312 is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)
    313 if not (is_explicit_retry or self.retry(retry_state)):
--> 314     return fut.result()
    316 if self.after is not None:
    317     self.after(retry_state)

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/concurrent/futures/_base.py:439](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/concurrent/futures/_base.py:439), in Future.result(self, timeout)
    437     raise CancelledError()
    438 elif self._state == FINISHED:
--> 439     return self.__get_result()
    441 self._condition.wait(timeout)
    443 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/concurrent/futures/_base.py:391](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/concurrent/futures/_base.py:391), in Future.__get_result(self)
    389 if self._exception:
    390     try:
--> 391         raise self._exception
    392     finally:
    393         # Break a reference cycle with the exception in self._exception
    394         self = None

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/tenacity/__init__.py:382](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/tenacity/__init__.py:382), in Retrying.__call__(self, fn, *args, **kwargs)
    380 if isinstance(do, DoAttempt):
    381     try:
--> 382         result = fn(*args, **kwargs)
    383     except BaseException:  # noqa: B902
    384         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/embeddings/openai.py:161](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/llama_index/embeddings/openai.py:161), in get_embeddings(client, list_of_text, engine, **kwargs)
    157 assert len(list_of_text) <= 2048, ""The batch size should not be larger than 2048.""
    159 list_of_text = [text.replace(""\n"", "" "") for text in list_of_text]
--> 161 data = client.embeddings.create(input=list_of_text, model=engine, **kwargs).data
    162 return [d.embedding for d in data]

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/resources/embeddings.py:106](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/resources/embeddings.py:106), in Embeddings.create(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)
    100         embedding.embedding = np.frombuffer(  # type: ignore[no-untyped-call]
    101             base64.b64decode(data), dtype=""float32""
    102         ).tolist()
    104     return obj
--> 106 return self._post(
    107     ""[/embeddings](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/embeddings)"",
    108     body=maybe_transform(params, embedding_create_params.EmbeddingCreateParams),
    109     options=make_request_options(
    110         extra_headers=extra_headers,
    111         extra_query=extra_query,
    112         extra_body=extra_body,
    113         timeout=timeout,
    114         post_parser=parser,
    115     ),
    116     cast_to=CreateEmbeddingResponse,
    117 )

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_base_client.py:1088](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_base_client.py:1088), in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)
   1074 def post(
   1075     self,
   1076     path: str,
   (...)
   1083     stream_cls: type[_StreamT] | None = None,
   1084 ) -> ResponseT | _StreamT:
   1085     opts = FinalRequestOptions.construct(
   1086         method=""post"", url=path, json_data=body, files=to_httpx_files(files), **options
   1087     )
-> 1088     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_base_client.py:853](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_base_client.py:853), in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)
    844 def request(
    845     self,
    846     cast_to: Type[ResponseT],
   (...)
    851     stream_cls: type[_StreamT] | None = None,
    852 ) -> ResponseT | _StreamT:
--> 853     return self._request(
    854         cast_to=cast_to,
    855         options=options,
    856         stream=stream,
    857         stream_cls=stream_cls,
    858         remaining_retries=remaining_retries,
    859     )

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_base_client.py:932](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_base_client.py:932), in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)
    928         err.response.read()
    930     raise self._make_status_error_from_response(err.response) from None
--> 932 return self._process_response(
    933     cast_to=cast_to,
    934     options=options,
    935     response=response,
    936     stream=stream,
    937     stream_cls=stream_cls,
    938 )

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_base_client.py:518](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_base_client.py:518), in BaseClient._process_response(self, cast_to, options, response, stream, stream_cls)
    515 if response.request.headers.get(RAW_RESPONSE_HEADER) == ""true"":
    516     return cast(ResponseT, api_response)
--> 518 return api_response.parse()

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_response.py:63](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/_response.py:63), in APIResponse.parse(self)
     61 parsed = self._parse()
     62 if is_given(self._options.post_parser):
---> 63     parsed = self._options.post_parser(parsed)
     65 self._parsed = parsed
     66 return parsed

File [/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/resources/embeddings.py:94](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/mnt/workspace/workgroup/lengmou/miniconda3/envs/llamaindex/lib/python3.9/site-packages/openai/resources/embeddings.py:94), in Embeddings.create.<locals>.parser(obj)
     90 if is_given(encoding_format):
     91     # don't modify the response object if a user explicitly asked for a format
     92     return obj
---> 94 for embedding in obj.data:
     95     data = cast(object, embedding.embedding)
     96     if not isinstance(data, str):
     97         # numpy is not installed [/](https://vscode-remote+dsw-002dgateway-002ealibaba-002dinc-002ecom.vscode-resource.vscode-cdn.net/) base64 optimisation isn't enabled for this model yet

TypeError: 'NoneType' object is not iterable



Could you please help me take a look at this issue? Thank youï¼

### Version

0.9.16.post1

### Steps to Reproduce

`from llama_index.llms import AzureOpenAI
from llama_index.embeddings import AzureOpenAIEmbedding
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
import logging
import sys

logging.basicConfig(
    stream=sys.stdout, level=logging.INFO
)  # logging.DEBUG for more verbose output
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

api_key = ""xxx""
azure_endpoint = ""https://xxxxxxx/azure""
api_version = ""2023-06-01-preview""

llm = AzureOpenAI(
    model=""gpt-35-turbo-16k"",
    deployment_name=""my-custom-llm"",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)


embed_model = AzureOpenAIEmbedding(
    model=""text-embedding-ada-002"",
    deployment_name=""my-custom-embedding"",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)


from llama_index import set_global_service_context

service_context = ServiceContext.from_defaults(
    llm=llm,
    embed_model=embed_model,
)

set_global_service_context(service_context)



documents = SimpleDirectoryReader(
    input_files=[""/mnt/workspace/workgroup/lengmou/Data/NL2SQL/artificial_query.txt""]
).load_data()
index = VectorStoreIndex.from_documents(documents)
`

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
41,e2ca247c-b6d7-41fb-a292-234a40fe7bb8,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9627'}",[],[],{},1d1382f0d1574df07750333374728f5106a83e617bf400e8e303e9bad2684fff,"[Question]: Multi-filter, single-key operations using chroma
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question
(using version 0.9.17)

This builds on a previous ticket opened last month: [issues/9182](https://github.com/run-llama/llama_index/issues/9182)

Picking up from where that left off, it looks like we're still limited in the multi-filter, single key operations (Unless I'm missing something). Things like this
eg: key=""Month"", value=[""September"", ""October""] with an OR filter condition and an IN operator
or another case: 
eg: key=""Day"", value=[7, 14] with an AND filter and operators [GTE, LTE]

I don't believe the updated solution allows for something like this.

Using Chroma, I have a solution for the first case working by: 

1) vector_stores/types.py

```
class MetadataFilter(BaseModel):
    key: str
    value: Union[StrictInt, StrictFloat, StrictStr, List[Union[StrictInt, StrictFloat, StrictStr]]]
    operator: FilterOperator = FilterOperator.EQ
```

2) vector_stores/chroma.py
```
def _transform_chroma_filter_operator(operator: str) -> str:
    """"""Translate standard metadata filter operator to Chroma specific spec.""""""
    if operator == ""!="":
        return ""$ne""
    elif operator == ""=="":
        return ""$eq""
    elif operator == "">"":
        return ""$gt""
    elif operator == ""<"":
        return ""$lt""
    elif operator == "">="":
        return ""$gte""
    elif operator == ""<="":
        return ""$lte""
    elif operator == ""in"":
        return ""$in""
    else:
        raise ValueError(f""Filter operator {operator} not supported"")
```


3) vector_stores/chroma.py
```

def _to_chroma_filter(
    standard_filters: MetadataFilters,
) -> dict:
    """"""Translate standard metadata filters to Chroma specific spec.""""""
    filters = {}
    filters_list = []
    condition = standard_filters.condition or ""and""
    condition = _transform_chroma_filter_condition(condition)

    if standard_filters.filters:
        for filter in standard_filters.filters:
            if filter.operator:
                operator = _transform_chroma_filter_operator(filter.operator)
                # Handle list values for 'in' operator
                if filter.operator == FilterOperator.IN and isinstance(filter.value, list):
                    filters_list.append({filter.key: {""$in"": filter.value}})
                else:
                    filters_list.append({filter.key: {operator: filter.value}})
            else:
                # Assuming default behavior for filters without an explicit operator
                filters_list.append({filter.key: filter.value})

    if len(filters_list) == 1:
        # If there is only one filter, return it directly
        return filters_list[0]
    elif len(filters_list) > 1:
        # Combine multiple filters based on the specified condition
        filters[condition] = filters_list

    return filters
```


And then this gets initialized like this:

```

metadata_filters = MetadataFilters(
    filters=[
        MetadataFilter(key=""Month"", value=[""September"", ""October""], operator=FilterOperator.IN)
    ],
    condition=FilterCondition.OR
)
```


Can we implement support for something like this? This case looks easy to add. I'd really like a way to handle something like this as well but may take more effort:

eg: key=""Day"", value=[7, 14] with an AND filter and operators [GTE, LTE]
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
42,8042677a-a01e-4022-a255-02fc93d20e77,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9623'}",[],[],{},dc7ff75f68cdd6a70c2d59dfdfc51b633fe831d29f9ff72f74786bee4140f32b,"[Feature Request]: Building object index from multiple athena database
### Feature Description

I wonder if we already have this feature to build an object index from multiple athena database or how we could write the code to combine multiple `SimpleToolNodeMapping` from different dbs to a single node mapping?Athena

### Reason

Sometimes we need to join multiple tables acorss differeent dbs to query the data, it would be great to be able to create an object index with all the tables in all dbs. 

### Value of Feature

We would be able to get the insight of data across different dbs. ",,,"{metadata_str}

{content}",{key}: {value},"
",Document
43,1da5098e-9987-44c8-8062-8d1203a9f0bc,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9620'}",[],[],{},0e8b0a032140e6aa8626440068658cc6dffed6283780293c4e09d25aba54ee2e,"[Bug]: change colab button link to openrouter instead of anyscale
### Bug Description

Change the ""open in colab"" button to `https://github.com/run-llama/llama_index/blob/main/docs/examples/llm/openrouter.ipynb`
now it is `https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/anyscale.ipynb`

### Version

na

### Steps to Reproduce

na

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
44,e0e3ef98-3efd-435e-9e20-f12b48e58bd6,,"{'state': 'open', 'year': 2023, 'month': 11, 'day': 27, 'assignee': '', 'size': 'L', 'index_id': '9164'}",[],[],{},d445964afa63a1b3fe70cfdfcd34561b8e143a3a87fd0e83cef34ca54bce2f9f,"AsyncCallback Iterator To Stream Internal Events
# Description

Llama Index should have an out-of-the-box callback that allows the user to stream all internal events through an async iterator. 

The current streaming misses all the internal events like tool usage and agent actions.

This async iterator should also stream token events as well.

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [ X] New feature (non-breaking change which adds functionality)

# How Has This Been Tested?

This is still in draft and needs input and fixes.
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
45,0954670d-2ffe-42d6-80ee-823fddb0d262,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 8, 'assignee': '', 'size': '', 'index_id': '9398'}",[],[],{},29d0b3fd2ba186d68823bf198dc6aab3425848dc7599ab60ed4be49382f4433e,"[Bug]: 
### Bug Description

When I use the correct api-key, the requested URL is incorrect. When my api-key is incorrect, such as XX, the requested URL is correct but I get a 401 error, indicating that my api-key is incorrect.

`embed_model = AzureOpenAIEmbedding(api_key=""XXX"",
                                   azure_endpoint=""XXX"",
                                   azure_deployment=""XXX"",
                                   model=""text-embedding-ada-002"",
                                   api_type='azure',
                                   api_base=""https://XXX.openai.azure.com"",
                                   api_version=""2023-07-01-preview""
                                   )
service_context = ServiceContext.from_defaults(embed_model=embed_model)

documents = SimpleDirectoryReader(""./data"").load_data()

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

llm = AzureOpenAI(
    engine=""aios-gpt35-16k"", model=""gpt-35-turbo-16k"", temperature=0.0
)

engine = index.as_query_engine()
response = engine.query(""What is azure openai service?"")`

the correct api-key, the requested URL is incorrectï¼š
![image](https://github.com/run-llama/llama_index/assets/20795442/256ce28e-764a-46e1-ad4f-732ff785b1a4)

my api-key is incorrect, such as XX, the requested URL is correct
![image](https://github.com/run-llama/llama_index/assets/20795442/a868e25d-85c3-40f6-8243-043b54bcc021)




### Version

0.9.18

### Steps to Reproduce

![image](https://github.com/run-llama/llama_index/assets/20795442/f718e7e1-4753-4fb0-aa18-3ef25c532e0f)


### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
46,18b0f454-c4ac-49df-86f5-ec95961fd0d9,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9613'}",[],[],{},69f1181c9d32eceac046dfae247017d61ebd68eeb5f817ba4bd2b13a6dcb548d,"Fix #9612: Set node content for astra db
# Description

Adds a check for the _node_content field for documents that may not have been originally indexed by Llama

Fixes #9612 

## Type of Change

- [x] Bug fix (non-breaking change which fixes an issue)

# How Has This Been Tested?

- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [x] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [x] New and existing unit tests pass locally with my changes
- [x] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
47,eb77e0ca-0f1d-482b-b092-d949a1429ae4,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9612'}",[],[],{},999a973e0bc5e1e21737f539e520500a0636c0499c8a8d729c2924d8d129993c,"[Bug]: AstraDB query fails if _node_content field not set in node metadata
### Bug Description

If the `_node_content` field is not set in the node metadata, upon creation of a Node object, the `metadata_dict_to_node` function will produce an error. Set that value if not already available based on the retrieved contents of the node. This is mostly for supporting retrieval of documents indexed by a non-llama framework

### Version

0.9.16post1

### Steps to Reproduce

1. Index documents using a non-llama framework into Astra DB
2. Attempt to retrieve Astra DB documents using llama-index
3. Observe error if `_node_content` is not set in the metadata

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
48,035c5216-ea32-4671-be9f-b7591d0179cf,,"{'state': 'open', 'year': 2023, 'month': 11, 'day': 10, 'assignee': '', 'size': '', 'index_id': '8832'}",[],[],{},0d03a33fe2a7c345cbb6c386bf4a561b737a701fb8e17bbe457fed1ba9936a2f,"[Bug]: VectorStoreIndex.refresh_ref_docs() duplicate docs with PGVectorStore
### Bug Description

`VectorStoreIndex.refresh_ref_docs()` inserts every time instead of the expected upsert behavior when using PGVectorStore as vector store

### Version

0.8.66

### Steps to Reproduce

Here are snippets of my code:
```py
from llama_index.vector_stores import PGVectorStore
from llama_index.indices.vector_store import VectorStoreIndex

confluence_vector_store = PGVectorStore.from_params(
    database=os.environ.get(""DB_DATABASE""),
    host=os.environ.get(""DB_HOST""),
    password=os.environ.get(""DB_PASSWORD""),
    port=os.environ.get(""DB_PORT""),
    user=os.environ.get(""DB_USER""),
    table_name=""confluence"",
    embed_dim=1536
)

confluence_index = VectorStoreIndex.from_vector_store(
    vector_store=confluence_vector_store, delete_from_docstore=True)

docs = Document(
        text=""This isn't in the index yet, but it will be soon!"",
        id_=""doc_id_3"",
    )

confluence_index.refresh_ref_docs(docs, update_kwargs={""delete_kwargs"": {""delete_from_docstore"": True}})


# Additional Debugging code I tried to run to locate the problem. It seems like the data isn't being added to the datastore at all, which is why the get_document_hash() function in refresh_ref_docs() is returning null. 
# print(confluence_index._storage_context.docstore._kvstore.get_all())
# ^ this ends up returning an empty kvstore even when the vector storage is populated.
```

Running the same code twice SHOULD have it only insert the document once into the document, with the second one doing nothing since the document is already there. Hoewver, it ends up being inserted twice.

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
49,75c1c178-66e8-4433-ba07-be4a8cfc69d9,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9609'}",[],[],{},99e3e8e543b0553dc8a1e701f55dcc1deee1d378e48a7c5d680f884c479cb53b,"[Bug]: broken when initializing a graph store with an existing falkordb table 
### Bug Description

https://github.com/run-llama/llama_index/blob/2ef251303c3554afcb3560a695ea028bac301ea9/llama_index/graph_stores/falkordb.py#L38C7-L38C7

This line will cause a problem when I call `load_indices_from_storage` to load a existing graph store due to creating an index on id twice.

I can quickly prevent this issue just add a try-expect, I'm not familiar with falkordb, not sure what's elegant fix for this.

### Version

latest version (0.9.16)

### Steps to Reproduce

1. create a graph store using falkordb
2. load that store via `load_indices_from_storage`

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
50,5907c7b1-b1e4-409c-8596-5d73a5d5495c,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 19, 'assignee': '', 'size': '', 'index_id': '9604'}",[],[],{},f570ca8600e6960e0b721be057e6c20d2d4ff91ac52af950c849c6cf6a1e8a71,"[Question]: How do the different indices query work underneath
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

I'm using multiple indices(vector index, document index, summary index, keywords index and graph index) for result generation with a custom retriever combining all index query results. For the vector index, I know it did a vector search to retrieve the related nodes, but how about other four indices? text search or others?",,,"{metadata_str}

{content}",{key}: {value},"
",Document
51,5f38ef3d-20a2-4ceb-be3c-def10f1f3090,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 18, 'assignee': '', 'size': '', 'index_id': '9583'}",[],[],{},7c8aeda5e7f0747a042ee253a9b1f6e87311f99faf03b773c88b31c6255d6d35,"[Bug]: ElasticSearchStore in docker container
### Bug Description

Cannot connect to elasticsearch when using docker compose

### Version

0.9.15.post2

### Steps to Reproduce

1. Setup a docker-compose.yml file that has both elasticsearch and code that uses llama-index with ElasticsearchStore from llama_index.vector_stores.
2. In the docker-compose.yml specify a network and bind both in that network
3. Run code and try to create an ElasticsearchStore using: 
`ElasticsearchStore(
                index_name=index_id,
                es_url = ""http://elasticsearch:9200""
            )`
After doing step 3, the code will get stuck and will not even throw a connection error

docker-compose.yml example:
version: '3'

services:
  llamachatbot:
    build: ./llama
    depends_on: 
      - elasticsearch
    container_name: llamachatbot
    ports:
      - ""5500:5500""
    networks:
      - elastic-network
    volumes:
      - documents-uploads:/app/uploads

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.license.self_generated.type=trial
    ports:
      - ""9200:9200""
    networks:
      - elastic-network
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.9.0
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_NAME=kibana.example.com
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=your_elasticsearch_username
      - ELASTICSEARCH_PASSWORD=your_elasticsearch_password
    ports:
      - ""5601:5601""
    networks:
      - elastic-network
    depends_on:
      - elasticsearch
    volumes:
      - kibana-data:/usr/share/kibana/data
networks:
  elastic-network:
    driver: bridge
volumes:
  elasticsearch-data:
  kibana-data:
  documents-uploads:

When i run the code locally and not in a docker container and change the es_url to ""http://localhost:9200"" it works.

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
52,3b0f67a6-3140-4183-9d5a-06b8bbbb6267,,"{'state': 'open', 'year': 2023, 'month': 9, 'day': 19, 'assignee': '', 'size': '', 'index_id': '7720'}",[],[],{},939924ca45b95e628d9b3f7b636145d2f348f0b33216195f5d28d7aa3e4fd7ee,"[Bug]: Error when running MetadataExtractionSEC.ipynb
### Bug Description

llama_index\docs\examples\metadata_extraction\MetadataExtractionSEC.ipynb -- this is a demo program, my llama-index    version is    0.8.29.post1

hitting error --- 

Traceback (most recent call last):
  File ""c:\Users\MetaData\With_Metadata.py"", line 123, in <module>
    response = final_engine.query(
               ^^^^^^^^^^^^^^^^^^^
  File ""d:\Venv\llamaindex_venv\Lib\site-packages\llama_index\indices\query\base.py"", line 23, in query
    response = self._query(str_or_query_bundle)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""d:\Venv\llamaindex_venv\Lib\site-packages\llama_index\query_engine\sub_question_query_engine.py"", line 126, in _query
    sub_questions = self._question_gen.generate(self._metadatas, query_bundle)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""d:\Venv\llamaindex_venv\Lib\site-packages\llama_index\question_gen\llm_generators.py"", line 56, in generate
    prediction = self._llm_predictor.predict(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""d:\Venv\llamaindex_venv\Lib\site-packages\llama_index\llm_predictor\base.py"", line 140, in predict
    messages = prompt.format_messages(llm=self._llm, **prompt_args)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""d:\Venv\llamaindex_venv\Lib\site-packages\llama_index\prompts\base.py"", line 105, in format_messages
    prompt = self.format(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File ""d:\Venv\llamaindex_venv\Lib\site-packages\llama_index\prompts\base.py"", line 97, in format
    prompt = self.output_parser.format(prompt)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""d:\Venv\llamaindex_venv\Lib\site-packages\llama_index\question_gen\output_parser.py"", line 17, in format
    raise NotImplementedError()
NotImplementedError

### Version

0.8.29.post1

### Steps to Reproduce

Just run

### Relevant Logs/Tracbacks

```shell
refer to the main track
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
53,4d1c55e4-f1c7-44eb-9623-86657841b58b,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 11, 'assignee': '', 'size': 'XL', 'index_id': '9431'}",[],[],{},36f016f0a3cde1758b19137c9feb9ff692bbf717092f3d4198a528763f843774,"Dev awiss
# Description

Try to use clickhouse as vectorDB.
Try to chunk docs with independent parser service.
Special designed schema and tricks for better query and retriever. 

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [ ] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
54,08598344-60db-4b11-ab18-7ba1de7d48a9,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 11, 'assignee': '', 'size': '', 'index_id': '9435'}",[],[],{},f68b173067886de67496564e100eadf25cadfdcbbb226b16515514599eae48f1,"[Bug]: [nltk_data] Error loading punkt: <urlopen error [WinError 10060] A
### Bug Description

I am using a vector Index which connects to a chromaDB client as my database. I have initialized the index as a chat engine. When the query the chat engine, two things happen:

1. The response time is nearly 2-3mins.
2. It throws the below warning

```
[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A
[nltk_data]     connection attempt failed because the connected party
[nltk_data]     did not properly respond after a period of time, or
[nltk_data]     established connection failed because connected host
[nltk_data]     has failed to respond>
```
 

### Version

0.9.8.post1

### Steps to Reproduce

Clone, setup and run the below repository: (Follow readme for instructions)
https://github.com/umang299/document-gpt

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
55,2c5267e0-df3f-4f33-b05c-e527861519e0,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 17, 'assignee': '', 'size': '', 'index_id': '9571'}",[],[],{},a911bfd30dfa7e20ccf905364ffa84cf56d0ff0a2c3d030399f90652a27715de,"[Bug]: PGVectorStore ""A string literal cannot contain NUL (0x00) characters""
### Bug Description

sqlalchemy reports:  ""ValueError: A string literal cannot contain NUL (0x00) characters""

when writing PGVectorStore to postgress

### Version

0.9.15.post2

### Steps to Reproduce

Running this code on local embedding: embed_model=""local:BAAI/bge-small-en-v1.5""

service_context = ServiceContext.from_defaults(llm=None, embed_model=""local:BAAI/bge-small-en-v1.5"")
set_global_service_context(service_context)

import psycopg2
from sqlalchemy import make_url 
from llama_index.indices.vector_store import VectorStoreIndex
from llama_index.vector_stores import PGVectorStore

connection_string = ""postgresql://postgres:xxxxxxx@192.168.1.186:5432""
db_name = ""vector_db""
conn = psycopg2.connect(connection_string)
conn.autocommit = True

with conn.cursor() as c:
    #c.execute(f""CREATE EXTENSION vector"")
    c.execute(f""DROP DATABASE IF EXISTS {db_name}"")
    c.execute(f""CREATE DATABASE {db_name}"")

base_path = ""./data/""
directories = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]
for directory in directories:
        url = make_url(connection_string)
        vector_store = PGVectorStore.from_params(
            database=db_name,
            host=url.host,
            password=url.password,
            port=url.port,
            user=url.username,
            table_name=directory,
            embed_dim=384, 
            )
        dir_path = os.path.join(base_path, directory)
        fullDirectory = base_path + directory
        print (""Creating vector store for: "" + fullDirectory)
        documents = SimpleDirectoryReader(fullDirectory, recursive=True).load_data()
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, show_progress=True)



### Relevant Logs/Tracbacks

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_290/4288094974.py in <module>
     15         documents = SimpleDirectoryReader(fullDirectory, recursive=True).load_data()
     16         storage_context = StorageContext.from_defaults(vector_store=vector_store)
---> 17         index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, show_progress=True)

/usr/local/lib/python3.10/dist-packages/llama_index/indices/base.py in from_documents(cls, documents, storage_context, service_context, show_progress, **kwargs)
    104             )
    105 
--> 106             return cls(
    107                 nodes=nodes,
    108                 storage_context=storage_context,

/usr/local/lib/python3.10/dist-packages/llama_index/indices/vector_store/base.py in __init__(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, show_progress, **kwargs)
     47         self._use_async = use_async
     48         self._store_nodes_override = store_nodes_override
---> 49         super().__init__(
     50             nodes=nodes,
     51             index_struct=index_struct,

/usr/local/lib/python3.10/dist-packages/llama_index/indices/base.py in __init__(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)
     69             if index_struct is None:
     70                 assert nodes is not None
---> 71                 index_struct = self.build_index_from_nodes(nodes)
     72             self._index_struct = index_struct
     73             self._storage_context.index_store.add_index_struct(self._index_struct)

/usr/local/lib/python3.10/dist-packages/llama_index/indices/vector_store/base.py in build_index_from_nodes(self, nodes, **insert_kwargs)
    253             if vector store does not store text
    254         """"""
--> 255         return self._build_index_from_nodes(nodes, **insert_kwargs)
    256 
    257     def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:

/usr/local/lib/python3.10/dist-packages/llama_index/indices/vector_store/base.py in _build_index_from_nodes(self, nodes, **insert_kwargs)
    234             run_async_tasks(tasks)
    235         else:
--> 236             self._add_nodes_to_index(
    237                 index_struct,
    238                 nodes,

/usr/local/lib/python3.10/dist-packages/llama_index/indices/vector_store/base.py in _add_nodes_to_index(self, index_struct, nodes, show_progress, **insert_kwargs)
    188 
    189         nodes = self._get_node_with_embedding(nodes, show_progress)
--> 190         new_ids = self._vector_store.add(nodes, **insert_kwargs)
    191 
    192         if not self._vector_store.stores_text or self._store_nodes_override:

/usr/local/lib/python3.10/dist-packages/llama_index/vector_stores/postgres.py in add(self, nodes, **add_kwargs)
    310                 item = self._node_to_table_row(node)
    311                 session.add(item)
--> 312             session.commit()
    313         return ids
    314 

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py in commit(self)
   1967             trans = self._autobegin_t()
   1968 
-> 1969         trans.commit(_to_root=True)
   1970 
   1971     def prepare(self) -> None:

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py in commit(self, _to_root)

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/state_changes.py in _go(fn, self, *arg, **kw)
    137             self._next_state = _StateChangeStates.CHANGE_IN_PROGRESS
    138             try:
--> 139                 ret_value = fn(self, *arg, **kw)
    140             except:
    141                 raise

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py in commit(self, _to_root)
   1254         if self._state is not SessionTransactionState.PREPARED:
   1255             with self._expect_state(SessionTransactionState.PREPARED):
-> 1256                 self._prepare_impl()
   1257 
   1258         if self._parent is None or self.nested:

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py in _prepare_impl(self)

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/state_changes.py in _go(fn, self, *arg, **kw)
    137             self._next_state = _StateChangeStates.CHANGE_IN_PROGRESS
    138             try:
--> 139                 ret_value = fn(self, *arg, **kw)
    140             except:
    141                 raise

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py in _prepare_impl(self)
   1229                 if self.session._is_clean():
   1230                     break
-> 1231                 self.session.flush()
   1232             else:
   1233                 raise exc.FlushError(

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py in flush(self, objects)
   4310         try:
   4311             self._flushing = True
-> 4312             self._flush(objects)
   4313         finally:
   4314             self._flushing = False

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py in _flush(self, objects)
   4445 
   4446         except:
-> 4447             with util.safe_reraise():
   4448                 transaction.rollback(_capture_exception=True)
   4449 

/usr/local/lib/python3.10/dist-packages/sqlalchemy/util/langhelpers.py in __exit__(self, type_, value, traceback)
    144             assert exc_value is not None
    145             self._exc_info = None  # remove potential circular references
--> 146             raise exc_value.with_traceback(exc_tb)
    147         else:
    148             self._exc_info = None  # remove potential circular references

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/session.py in _flush(self, objects)
   4406             self._warn_on_events = True
   4407             try:
-> 4408                 flush_context.execute()
   4409             finally:
   4410                 self._warn_on_events = False

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/unitofwork.py in execute(self)
    464         else:
    465             for rec in topological.sort(self.dependencies, postsort_actions):
--> 466                 rec.execute(self)
    467 
    468     def finalize_flush_changes(self) -> None:

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/unitofwork.py in execute(self, uow)
    640     @util.preload_module(""sqlalchemy.orm.persistence"")
    641     def execute(self, uow):
--> 642         util.preloaded.orm_persistence.save_obj(
    643             self.mapper,
    644             uow.states_for_mapper_hierarchy(self.mapper, False, False),

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/persistence.py in save_obj(base_mapper, states, uowtransaction, single)
     91         )
     92 
---> 93         _emit_insert_statements(
     94             base_mapper,
     95             uowtransaction,

/usr/local/lib/python3.10/dist-packages/sqlalchemy/orm/persistence.py in _emit_insert_statements(base_mapper, uowtransaction, mapper, table, insert, bookkeeping, use_orm_insert_stmt, execution_options)
   1134                 multiparams = [rec[2] for rec in records]
   1135 
-> 1136                 result = connection.execute(
   1137                     statement, multiparams, execution_options=execution_options
   1138                 )

/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py in execute(self, statement, parameters, execution_options)
   1414             raise exc.ObjectNotExecutableError(statement) from err
   1415         else:
-> 1416             return meth(
   1417                 self,
   1418                 distilled_parameters,

/usr/local/lib/python3.10/dist-packages/sqlalchemy/sql/elements.py in _execute_on_connection(self, connection, distilled_params, execution_options)
    514             if TYPE_CHECKING:
    515                 assert isinstance(self, Executable)
--> 516             return connection._execute_clauseelement(
    517                 self, distilled_params, execution_options
    518             )

/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py in _execute_clauseelement(self, elem, distilled_parameters, execution_options)
   1637             linting=self.dialect.compiler_linting | compiler.WARN_LINTING,
   1638         )
-> 1639         ret = self._execute_context(
   1640             dialect,
   1641             dialect.execution_ctx_cls._init_compiled,

/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)
   1841 
   1842         if context.execute_style is ExecuteStyle.INSERTMANYVALUES:
-> 1843             return self._exec_insertmany_context(
   1844                 dialect,
   1845                 context,

/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py in _exec_insertmany_context(self, dialect, context)
   2118 
   2119             except BaseException as e:
-> 2120                 self._handle_dbapi_exception(
   2121                     e,
   2122                     sql_util._long_statement(sub_stmt),

/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context, is_sub_exec)
   2344             else:
   2345                 assert exc_info[1] is not None
-> 2346                 raise exc_info[1].with_traceback(exc_info[2])
   2347         finally:
   2348             del self._reentrant_error

/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py in _exec_insertmany_context(self, dialect, context)
   2110                         break
   2111                 else:
-> 2112                     dialect.do_execute(
   2113                         cursor,
   2114                         sub_stmt,

/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py in do_execute(self, cursor, statement, parameters, context)
    920 
    921     def do_execute(self, cursor, statement, parameters, context=None):
--> 922         cursor.execute(statement, parameters)
    923 
    924     def do_execute_no_params(self, cursor, statement, context=None):

ValueError: A string literal cannot contain NUL (0x00) characters.
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
56,42b698ff-1906-48b2-a39d-99499c4d7188,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 7, 'assignee': '', 'size': '', 'index_id': '9383'}",[],[],{},9d0f517e45978b24b423f5f92cc013509c55765d33a1894859ffd9c1df536faa,"[Bug]: MetadataFilters does not allow condition = FilterCondition.OR
### Bug Description

I added MetadataFilters to my query engine. The query engine seems fine when I'm using the default FilterCondition.AND. When I try to use FilterCondition.OR, the query engine returns 'Empty response'. It also disregards any format requirement in the query. 

### Version

0.9.13

### Steps to Reproduce

from llama_index.vector_stores.types import MetadataFilters, ExactMatchFilter
from llama_index import Document, VectorStoreIndex

raw = [{'yob': '1983', 'name': Elena}, {'yob': 'No info', 'name': 'Elena'}, {'yob': '1980', 'name': 'henz'}]

list_of_documents = [Document(metadata = {'yob' : r['yob']}, text = r['name']) for r in raw]

index = VectorStoreIndex.from_documents(list_of_documents, service_context=service_context)
filters = MetadataFilters(filters=[ExactMatchFilter(key=""yob"", value='1983'), ExactMatchFilter(key=""yob"", value='No info')], condition=FilterCondition.OR)
query_engine = index.as_query_engine(filters=filters, similarity_top_k=10)
query=""Find records with name Elena.""
response=query_engine.query(query)

---- Expect it to return record 1 and 2, instead return 'empty response'


### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
57,0ec185e8-d1b3-4acf-b666-31c71b048c22,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 8, 'assignee': '', 'size': '', 'index_id': '9405'}",[],[],{},050522935b23838c9682b28359096d10a4e0e1d11670196150f1c41b2ac6eb70,"[Bug]: 
### Bug Description

Until 4th December, 2023 my app used to work perfectly fine.
However, after I restarted my caprover server from AWS on 5th December, 2023 to solve an issue with the deployment. The service which was using Llama broke down with an **ERROR:  Exception in ASGI application** , The code was reaching till before the highlighted part and then failing here for some reason.

When running locally this was working but whenever I am trying to run it through my app, I am getting this ASGI exception in the logs. I would love to hear about any possible solutions as my app has been down for 4 days now due to this issue.




### Version

0.7.4

### Steps to Reproduce

![Capture](https://github.com/run-llama/llama_index/assets/53633591/3b1b4fd0-80e1-4ee8-8c3f-bab499229a66)


### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
58,5b5ecdc6-5fca-441d-b8ed-8c40060b3da8,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 15, 'assignee': '', 'size': '', 'index_id': '9546'}",[],[],{},c724faee9720cd3b1d01991793fb05d41761d8a341d280158cbd9d783cde642d,"[Bug]: Error when storing to mongo DB index and document store for Knowledge Graph
### Bug Description

I am trying to create a KnowledgeGraphIndex using Nebula, the default document store and index store is very big and takes a long time to load, it is working though. 

I am experimenting on using MongoDB as a alternative to see if it resolve the long loading issue. However I hit error after the LLM has generated the triplets.

It is showing error that the Command Document too large from pymongo module.

Here are the docs I am following
https://docs.llamaindex.ai/en/stable/examples/docstore/MongoDocstoreDemo.html
https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_query_engine.html

### Version

0.9.15

### Steps to Reproduce

Here's the code snippet

documents = SimpleDirectoryReader(""kg"").load_data()
graph_store = NebulaGraphStore(
    space_name=space_name,
    edge_types=edge_types,
    rel_prop_names=rel_prop_names,
    tags=tags,
)
docstore = MongoDocumentStore.from_uri(uri=MONGO_URI)
index_store = MongoIndexStore.from_uri(uri=MONGO_URI)

storage_context = StorageContext.from_defaults(
        graph_store=graph_store,
        docstore=docstore,
        index_store=index_store
)
index = KnowledgeGraphIndex.from_documents(
    documents,
    max_triplets_per_chunk=10,
    storage_context=storage_context,
    service_context=service_context,
    include_embeddings=True,
)

### Relevant Logs/Tracbacks

```shell
Traceback (most recent call last):
  File ""/home/ssm-user/llama/testnebula.py"", line 84, in <module>
    index = KnowledgeGraphIndex.from_documents(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/llama_index/indices/base.py"", line 106, in from_documents
    return cls(
           ^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/llama_index/indices/knowledge_graph/base.py"", line 81, in __init__
    super().__init__(
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/llama_index/indices/base.py"", line 73, in __init__
    self._storage_context.index_store.add_index_struct(self._index_struct)
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/llama_index/storage/index_store/keyval_index_store.py"", line 38, in add_index_struct
    self._kvstore.put(key, data, collection=self._collection)
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/llama_index/storage/kvstore/mongodb_kvstore.py"",line 112, in put
    self._db[collection].replace_one(
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/collection.py"", line 973, in replace_one
    self._update_retryable(
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/collection.py"", line 881, in _update_retryable
    return self.__database.client._retryable_write(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/mongo_client.py"", line 1523, in _retryable_write
    return self._retry_with_session(retryable, func, s, bulk)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/mongo_client.py"", line 1421, in _retry_with_session
    return self._retry_internal(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/_csot.py"", line 107, in csot_wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/mongo_client.py"", line 1462, in _retry_internal
    ).run()
      ^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/mongo_client.py"", line 2315, in run
    return self._read() if self._is_read else self._write()
                                              ^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/mongo_client.py"", line 2423, in _write
    return self._func(self._session, conn, self._retryable)  # type: ignore
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/collection.py"", line 862, in _update
    return self._update(
           ^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/collection.py"", line 816, in _update
    result = conn.command(
             ^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/helpers.py"", line 322, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/pool.py"", line 996, in command
    self._raise_connection_failure(error)
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/pool.py"", line 968, in command
    return command(
           ^^^^^^^^
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/network.py"", line 164, in command
    message._raise_document_too_large(name, size, max_bson_size + message._COMMAND_OVERHEAD)
  File ""/home/ssm-user/llama/lib/python3.11/site-packages/pymongo/message.py"", line 1182, in _raise_document_too_large
    raise DocumentTooLarge(f""{operation!r} command document too large"")
pymongo.errors.DocumentTooLarge: 'update' command document too large
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
59,23677e89-8ee0-443f-a5d8-c614aa06e43f,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 16, 'assignee': '', 'size': '', 'index_id': '9565'}",[],[],{},e0779fd41bc1e93ef9486eb871bdc151f70ecaa889ced539f0ed14e277b429b1,"[Question]: Connecting to Mixtral 8x7b Chat on together.ai
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

I am trying to connect to mistral chat model on together.ai

model is defined as OpenAILike
llm = OpenAILike(
    model=""DiscoResearch/DiscoLM-mixtral-8x7b-v2"",
    api_base=""https://api.together.xyz/v1"",
    api_key=""<secret key>"",
    temperatue=0.1
)

but I am not getting any responses as I suspect that model is expecting specific prompt template.
Anyone managed to make it work, quick sample would be appreciated ?",,,"{metadata_str}

{content}",{key}: {value},"
",Document
60,e25d6e4d-8028-4e78-80ba-5c1a2645d4a2,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 16, 'assignee': '', 'size': '', 'index_id': '9560'}",[],[],{},608412a4683d93c97eed3d2ee14b08262b4dcdf406f86ffcbc04950d6faa05c6,"(blocking) KnowledgeGraphRAGRetriever : when will hybrid search or keyword_embedding be supported ?
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

As I've been looking into the docs, I realized that the [KnowledgeGraphRAGRetriever ](https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html#perform-graph-rag-query) doesn't support yet the keyword_embedding option, which is a blocker for my project. I'd like to know how to leverage my NebulaGraph DB and embeddings to have the best possible results in my RAG.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
61,618bbb9e-ddcd-402d-a5f6-79514941ae54,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 2, 'assignee': '', 'size': '', 'index_id': '9269'}",[],[],{},584316f81cc92e4ae94dd818e4619c02447fbcb32fb763f6bdc40be9724ac95c,"[Feature Request]: Adding Table(s) to Knowledge graph building using LLM's
### Feature Description

Building the table(s) to knowledge graph with the help of LLM's. Happy to take lead and start working for the PR :)

### Reason

Querying and indexing the existing table is always a pain point with the size of table grows. As KnowledgeGraphIndex in llama-index is adapted for converting documents into knowledge graph, a similar advantages build when trying to convert a table into knowledge graph. 

### Value of Feature

With Table entities converted to knowledge graphs the implicit relations between the entities can be explicitly modelled, even the relations between different tables can be brought to a single graph which is easy to query and index compared to the tables.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
62,1d371035-da4b-434a-bf6f-71f9830ba066,,"{'state': 'open', 'year': 2023, 'month': 11, 'day': 9, 'assignee': 'nerdai', 'size': '', 'index_id': '8802'}",[],[],{},efd5220fed9e42d1566fb108c1291c1c030137137678a09b7d9f36d8876c183b,"[Question]:  Conversational chat agent with history and previous responses as context.
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

Here is the code where I implemented a RAG agent,

def chat_app(query_str):    
    query_str = query_str 
    chat_agent = CondenseQuestionChatEngine.from_defaults(query_engine=query_engine,service_context=service_context)
    chat_resp = chat_agent.chat(query_str)
    return chat_resp
chat_resp = chat_app(""query_str"")
while True:
    query_str = input(""User: "")
    if query_str == ""exit"":
        break
    response = chat_app(query_str)
    
    print(f""Agent: {response}"")
    print()
    
    how do I pass previous responses and context to the CondenseQuestionChatEngine.from_defaults so that the user need not be specific about the details or need to supply the context every time",,,"{metadata_str}

{content}",{key}: {value},"
",Document
63,428c6aeb-d05e-4bf8-9742-487496e55315,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 14, 'assignee': '', 'size': '', 'index_id': '9525'}",[],[],{},97786fae1b076ba265e24c12f8a2ae48950c0da65548045568e236961c2d9949,"[Bug]: Indexing a large number of Documents resulted in ValueError with ChromaDB
### Bug Description

Indexing a large number of Documents resulted in ValueError with ChromaDB.
In my case, I had 34k files, loaded into 191k+ Documents, and parsed into 229k Nodes.

I found #7648, and https://github.com/chroma-core/chroma/issues/1049. 
From the log, it seems that LlamaIndex correctly run in batch of 41665 as the fixed for #7648 intended. However, chromadb complains that the maximum batch size is 5461.
Log available below.

Using LlamaIndex 0.9.15 and ChromaDB 0.4.19

### Version

0.9.15

### Steps to Reproduce

Run this code against a large set of document. I suspect >5500 will do.

```python
# initialize client, setting path to save data
db = chromadb.PersistentClient(path=""./chroma_db"")

# create collection
chroma_collection = db.get_or_create_collection(""collection_name"")

# assign chroma as the vector_store to the context
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

index = VectorStoreIndex.from_documents(
    documents,
    show_progress=True,
    storage_context=storage_context,
)
```

### Relevant Logs/Tracbacks

```text
[REDACTED project folder]\venv\Scripts\python.exe [REDACTED project folder]\main.py
2023-12-14 13:46:57,044 I main.py L62: <module>(): Loading starts
2023-12-14 13:46:58,055 I [REDACTED custom loader module].py L74: _add_files(): Total files added: 34155
Loading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34155/34155 [02:20<00:00, 243.86file/s]
2023-12-14 13:49:18,132 I main.py L71: <module>(): Loading ends
2023-12-14 13:49:18,132 I main.py L72: <module>(): Indexing starts
Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191025/191025 [02:54<00:00, 1094.34it/s]
Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229210/229210 [1:09:27<00:00, 54.99it/s]
Traceback (most recent call last):
  File ""[REDACTED project folder]\main.py"", line 84, in <module>
    index = VectorStoreIndex.from_documents(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""[REDACTED project folder]\venv\Lib\site-packages\llama_index\indices\base.py"", line 106, in from_documents
    return cls(
           ^^^^
  File ""[REDACTED project folder]\venv\Lib\site-packages\llama_index\indices\vector_store\base.py"", line 49, in __init__
    super().__init__(
  File ""[REDACTED project folder]\venv\Lib\site-packages\llama_index\indices\base.py"", line 71, in __init__
    index_struct = self.build_index_from_nodes(nodes)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""[REDACTED project folder]\venv\Lib\site-packages\llama_index\indices\vector_store\base.py"", line 255, in build_index_from_nodes
    return self._build_index_from_nodes(nodes, **insert_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""[REDACTED project folder]\venv\Lib\site-packages\llama_index\indices\vector_store\base.py"", line 236, in _build_index_from_nodes
    self._add_nodes_to_index(
  File ""[REDACTED project folder]\venv\Lib\site-packages\llama_index\indices\vector_store\base.py"", line 190, in _add_nodes_to_index
    new_ids = self._vector_store.add(nodes, **insert_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""[REDACTED project folder]\venv\Lib\site-packages\llama_index\vector_stores\chroma.py"", line 243, in add
    self._collection.add(
  File ""[REDACTED project folder]\venv\Lib\site-packages\chromadb\api\models\Collection.py"", line 168, in add
    self._client._add(ids, self.id, embeddings, metadatas, documents, uris)
  File ""[REDACTED project folder]\venv\Lib\site-packages\chromadb\telemetry\opentelemetry\__init__.py"", line 127, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""[REDACTED project folder]\venv\Lib\site-packages\chromadb\api\segment.py"", line 344, in _add
    validate_batch(
  File ""[REDACTED project folder]\venv\Lib\site-packages\chromadb\api\types.py"", line 505, in validate_batch
    raise ValueError(
ValueError: Batch size 41665 exceeds maximum batch size 5461

Process finished with exit code 1
```",,,"{metadata_str}

{content}",{key}: {value},"
",Document
64,f8295eb2-bb26-43c9-b704-5d8ba1ffd128,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 15, 'assignee': '', 'size': '', 'index_id': '9543'}",[],[],{},efdec8e509a72e8f2a45496ac25543fb9a927b18c614809777912d35bcaa55cd,"[Question]: get response with document page number in SubQuestionQueryEngine ?
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

How can I get reference page numbers while querying with SubQuestionQueryEngine, I was able to get it using other methods like graph as a query engine?",,,"{metadata_str}

{content}",{key}: {value},"
",Document
65,4732d13a-6def-4a7e-bcf4-af44c250ab0f,,"{'state': 'open', 'year': 2023, 'month': 10, 'day': 28, 'assignee': '', 'size': '', 'index_id': '8551'}",[],[],{},d425ee672c52d311118f646513421476bb6afc5e8e30cc41793d8acf25130545,"[Bug]: UnstructuredElementNodeParser getting ValueError: Could not extract json string from output: 
### Bug Description



```

llm = Bedrock(
    model=""anthropic.claude-v2"", profile_name=profile
)
# create UnstructuredElementNodeParser from LLamaIndex
from llama_index.node_parser import (
    UnstructuredElementNodeParser,
)

node_parser = UnstructuredElementNodeParser(llm=llm)
```

### Version

0.8.50

### Steps to Reproduce

https://docs.llamaindex.ai/en/latest/examples/query_engine/sec_tables/tesla_10q_table.html#try-table-comparisons
was trying the above example ,

the only change i did is passing the bedrock model in the code ,and getting the error

67 match = re.search(r""\{.*\}"", text.strip(), re.MULTILINE | re.IGNORECASE | re.DOTALL)
     68 if not match:
---> 69     raise ValueError(f""Could not extract json string from output: {text}"")
     71 return match.group()

ValueError: Could not extract json string from output:  Here is a summary of the table without directly referencing the context:

The table outlines the contents and structure of a business financial report. It contains sections on business details, risk factors, financial statements, controls and procedures, and more. The table provides a useful overview of the key components of the report and should be kept.

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
66,ea42c8b9-0c48-4fa3-9d28-62bc05617ea7,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 12, 'assignee': '', 'size': 'L', 'index_id': '9470'}",[],[],{},530ef7d163d777bc9b9c1ff0b49b8da6fdb077949bf810149480f5069afc2d67,"Add Anyscale Embedding model support
# Description

Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.

Fixes # (issue)

## Type of Change

Please delete options that are not relevant.

- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [ ] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
67,0794254b-9f53-413b-ae66-0d1d914769b6,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 14, 'assignee': '', 'size': '', 'index_id': '9518'}",[],[],{},141fb7169ce3d66f01be814d37f391c59b445bb85140a394a9baa1e8b2be442b,"[Bug]: Unable to connect milvus with latest llama-index package 0.9.15
### Bug Description

Hello Everyone,

We are unable to connect Milvus with the latest version of llama-index, resulting in the error message: ""Node content not found in metadata dict.""
Current Environment:

openai==0.28.1
llama-index==0.8.17

Attempt to connect Milvus with the latest version of llama-index using the provided code snippet.
Observe the ""Node content not found in metadata dict"" error.

The connection to Milvus works as expected with the current versions of openai (0.28.1) and llama-index (0.8.17).
After upgrading to the latest version of llama-index, we face the mentioned issue.

MILVUS_DB_URL = f""http://{MILVUS_DB_HOST}:{MILVUS_DB_PORT}""

vector_store = MilvusVectorStore(
    collection_name=milvus_collection_name,
    uri=MILVUS_DB_URL
    # host=MILVUS_DB_HOST,
    # port=MILVUS_DB_PORT,
)

storage_context = StorageContext.from_defaults(vector_store=vector_store)

index = VectorStoreIndex.from_documents(
    documents=[],
    storage_context=storage_context,
    service_context=service_context,
)

**Error:** <MilvusException: (code=2, message=Fail connecting to server on 1xx.1xx.2xx.3xx:19530. Timeout)>


Alternative Code Attempt:

vector_store = MilvusVectorStore(
    collection_name=milvus_collection_name,
    host=MILVUS_DB_HOST,
    port=MILVUS_DB_PORT,
)

**Error:** Node content not found in metadata dict"" error.

Please investigate this issue at priority and help us to proceed further.

Thanks,
Sridhar

### Version

llama-index 0.9.15

### Steps to Reproduce

Just to execute the program

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
68,f9f5a650-0ae3-4861-9c75-62d8a57f4b7c,,"{'state': 'open', 'year': 2023, 'month': 10, 'day': 27, 'assignee': '', 'size': '', 'index_id': '8536'}",[],[],{},032838c2d1bf5625fd94cda9509ea97d567b95534323f27b476167fb13a614a9,"[Bug]:  UnstructuredElementNodeParser doesn't pass llm/embed model to internally generated service_context
### Bug Description

I'm creating a `llm` and `embed_model` **_without_** setting the OpenAI environment variables for security reasons. 
I've passed in the `llm` constructor parameter to `UnstructuredElementNodeParser`. I get the message: `Could not load OpenAIEmbedding.` Looking deeper into the code I found that `UnstructuredElementNodeParser` creates an ad-hoc service context without the `embed_model` parameter, which I've also thought about passing in to its constructor but that isn't possible.

I think it's good practice to allow the parser to use given llm and embed models rather than force it to recreate these objects. WDYT?

### Version

0.8.5.post2

### Steps to Reproduce

```python
llm = AzureOpenAI(engine=""query-engine-model"",
                  mode=""gpt-35-turbo"",
                  temperature=0.0,
                  embedding_llm=embedding_llm,
                  api_key=get_openai_api_key(),
                  api_base=openai.api_base,
                  api_type=openai.api_type,
                  api_version=""2023-03-15-preview"")

node_parser = UnstructuredElementNodeParser(llm=llm)
raw_nodes = node_parser.get_nodes_from_documents([Document(text=str(pf.getvalue()), metadata={
    ""filename"": pf.name,
    ""extension"": pf.type
}) for pf in pdf_files])
base_nodes, node_mappings = node_parser.get_base_nodes_and_mappings(raw_nodes)
```

### Relevant Logs/Tracbacks

```shell
File ""/Users/user/my_project/my_script.py"", line 54, in with_unstructured
    raw_nodes = node_parser.get_nodes_from_documents([Document(text=str(pf.getvalue()), metadata={
File ""/Users/user/Library/Caches/pypoetry/virtualenvs/my-venv/lib/python3.11/site-packages/llama_index/node_parser/unstructured_element.py"", line 290, in get_nodes_from_documents
    nodes = self.get_nodes_from_node(document)
File ""/Users/user/Library/Caches/pypoetry/virtualenvs/my-venv/lib/python3.11/site-packages/llama_index/node_parser/unstructured_element.py"", line 264, in get_nodes_from_node
    extract_table_summaries(table_elements, self.llm, self.summary_query_str)
File ""/Users/user/Library/Caches/pypoetry/virtualenvs/my-venv/lib/python3.11/site-packages/llama_index/node_parser/unstructured_element.py"", line 113, in extract_table_summaries
    service_context = ServiceContext.from_defaults(llm=llm)
File ""/Users/user/Library/Caches/pypoetry/virtualenvs/my-venv/lib/python3.11/site-packages/llama_index/indices/service_context.py"", line 163, in from_defaults
    embed_model = resolve_embed_model(embed_model)
File ""/Users/user/Library/Caches/pypoetry/virtualenvs/my-venv/lib/python3.11/site-packages/llama_index/embeddings/utils.py"", line 43, in resolve_embed_model
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
69,bfabdbce-6f3f-44e5-9da2-2b70805d0211,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 14, 'assignee': '', 'size': '', 'index_id': '9510'}",[],[],{},ecea713a273b27a29e4508071525bbeb982ebec9c4a84a8a51ef5500dfdf3e8e,"[Question]: SQLAutoVectorQueryEngine ValueError: Invalid result.ind: -2
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

For some SQLAutoVectorQueryEngine queries I get the following error when calling `query_engine.query()`.  For example ""Tell me about the author"".  I'm using service_context to select gpt-4.  I've noticed if don't specify the service_context the error seems to go away.

`chunk_size = 1024`
`llm = OpenAI(temperature=1, model=""gpt-4"", streaming=True)`
`service_context = ServiceContext.from_defaults(chunk_size=chunk_size, llm=llm)`

`query_engine = SQLAutoVectorQueryEngine(`
`    sql_tool, vector_tool, service_context=service_context`
`)`

**ERROR BELOW**
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[101], [line 1](vscode-notebook-cell:?execution_count=101&line=1)
----> [1](vscode-notebook-cell:?execution_count=101&line=1) response = query_engine.query(
      [2](vscode-notebook-cell:?execution_count=101&line=2)     ""Tell me about the author.""
      [3](vscode-notebook-cell:?execution_count=101&line=3) )

File [c:\Python312\Lib\site-packages\llama_index\core\base_query_engine.py:30](file:///C:/Python312/Lib/site-packages/llama_index/core/base_query_engine.py:30), in BaseQueryEngine.query(self, str_or_query_bundle)
     [28](file:///C:/Python312/Lib/site-packages/llama_index/core/base_query_engine.py:28) if isinstance(str_or_query_bundle, str):
     [29](file:///C:/Python312/Lib/site-packages/llama_index/core/base_query_engine.py:29)     str_or_query_bundle = QueryBundle(str_or_query_bundle)
---> [30](file:///C:/Python312/Lib/site-packages/llama_index/core/base_query_engine.py:30) return self._query(str_or_query_bundle)

File [c:\Python312\Lib\site-packages\llama_index\query_engine\sql_join_query_engine.py:328](file:///C:/Python312/Lib/site-packages/llama_index/query_engine/sql_join_query_engine.py:328), in SQLJoinQueryEngine._query(self, query_bundle)
    [326](file:///C:/Python312/Lib/site-packages/llama_index/query_engine/sql_join_query_engine.py:326)     return response
    [327](file:///C:/Python312/Lib/site-packages/llama_index/query_engine/sql_join_query_engine.py:327) else:
--> [328](file:///C:/Python312/Lib/site-packages/llama_index/query_engine/sql_join_query_engine.py:328)     raise ValueError(f""Invalid result.ind: {result.ind}"")

ValueError: Invalid result.ind: -2

**DEBUG CAPTURE BELOW**

DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': <MessageRole.USER: 'user'>, 'content': ""Some choices are given below. It is provided in a numbered list (1 to 2), where each item in the list corresponds to a summary.\n---------------------\n(1) Useful for translating a natural language query into a SQL query over a table containing the schedule of surgical procedures.\n\n(2) Useful for answering semantic questions about how to perform common surgical procedures.\n---------------------\nUsing only the choices above and not prior knowledge, generate the selection object and reason that is most relevant to the question: 'Tell me about the author.'\n""}], 'model': 'gpt-4', 'stream': False, 'temperature': 0.9, 'tool_choice': {'type': 'function', 'function': {'name': 'SingleSelection'}}, 'tools': [{'type': 'function', 'function': {'name': 'SingleSelection', 'description': 'A single selection of a choice.', 'parameters': {'title': 'SingleSelection', 'description': 'A single selection of a choice.', 'type': 'object', 'properties': {'index': {'title': 'Index', 'type': 'integer'}, 'reason': {'title': 'Reason', 'type': 'string'}}, 'required': ['index', 'reason']}}}]}}
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': <MessageRole.USER: 'user'>, 'content': ""Some choices are given below. It is provided in a numbered list (1 to 2), where each item in the list corresponds to a summary.\n---------------------\n(1) Useful for translating a natural language query into a SQL query over a table containing the schedule of surgical procedures.\n\n(2) Useful for answering semantic questions about how to perform common surgical procedures.\n---------------------\nUsing only the choices above and not prior knowledge, generate the selection object and reason that is most relevant to the question: 'Tell me about the author.'\n""}], 'model': 'gpt-4', 'stream': False, 'temperature': 0.9, 'tool_choice': {'type': 'function', 'function': {'name': 'SingleSelection'}}, 'tools': [{'type': 'function', 'function': {'name': 'SingleSelection', 'description': 'A single selection of a choice.', 'parameters': {'title': 'SingleSelection', 'description': 'A single selection of a choice.', 'type': 'object', 'properties': {'index': {'title': 'Index', 'type': 'integer'}, 'reason': {'title': 'Reason', 'type': 'string'}}, 'required': ['index', 'reason']}}}]}}
DEBUG:httpcore.connection:close.started
close.started
DEBUG:httpcore.connection:close.complete
close.complete
DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None
connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000027891E40530>
connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000027891E40530>
DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000027891ED6ED0> server_hostname='api.openai.com' timeout=60.0
start_tls.started ssl_context=<ssl.SSLContext object at 0x0000027891ED6ED0> server_hostname='api.openai.com' timeout=60.0
DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000027893B7F440>
start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000027893B7F440>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 14 Dec 2023 00:23:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-4-0613'), (b'openai-organization', b'user-vnoy76paarbktt519cuphchn'), (b'openai-processing-ms', b'1136'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000'), (b'x-ratelimit-limit-tokens_usage_based', b'10000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9843'), (b'x-ratelimit-remaining-tokens_usage_based', b'9843'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'942ms'), (b'x-ratelimit-reset-tokens_usage_based', b'942ms'), (b'x-request-id', b'b22b369ab64ca295b0930a9fea8edc2a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83524c456f723508-SMF'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3="":443""; ma=86400')])
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 14 Dec 2023 00:23:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-4-0613'), (b'openai-organization', b'user-vnoy76paarbktt519cuphchn'), (b'openai-processing-ms', b'1136'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000'), (b'x-ratelimit-limit-tokens_usage_based', b'10000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9843'), (b'x-ratelimit-remaining-tokens_usage_based', b'9843'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'942ms'), (b'x-ratelimit-reset-tokens_usage_based', b'942ms'), (b'x-request-id', b'b22b369ab64ca295b0930a9fea8edc2a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83524c456f723508-SMF'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3="":443""; ma=86400')])
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions ""HTTP/1.1 200 OK""
HTTP Request: POST https://api.openai.com/v1/chat/completions ""HTTP/1.1 200 OK""
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
response_closed.started
DEBUG:httpcore.http11:response_closed.complete
response_closed.complete
DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions ""200 OK""
HTTP Request: POST https://api.openai.com/v1/chat/completions ""200 OK""",,,"{metadata_str}

{content}",{key}: {value},"
",Document
70,b17571fe-57c6-4495-beb3-94bbfb99040c,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 14, 'assignee': '', 'size': '', 'index_id': '9523'}",[],[],{},9f5a11b5f8d93edf1b4397802f27c5e2edd403aa59e8ecbfbef503d790f55d1e,"[Bug]: Text2SQL generates right SQL but provides wrong answer.
### Bug Description

When using an LLM I obtain correct SQL query for my question.


```
query_str = ""What is the total amount for segment called : 'Pharma' for the first, second and third quarter of 2022? ""
response = query_engine.query(query_str)
```
The response I get is:
```
2023-12-14 12:08:02,393 INFO sqlalchemy.engine.Engine BEGIN (implicit)
2023-12-14 12:08:02,394 INFO sqlalchemy.engine.Engine SELECT SUM(Amount) FROM finance WHERE Segment = 'Pharma' AND Year = 2022 AND Quarter IN ('Q1', 'Q2', 'Q3'); 
### Answer: 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
```

If I take this query and run it using `sql_database.run_sql(sql_query)` I do obtained a correct answer.
```
2023-12-14 12:08:54,797 INFO sqlalchemy.engine.Engine BEGIN (implicit)
2023-12-14 12:08:54,798 INFO sqlalchemy.engine.Engine SELECT SUM(Amount) FROM finance WHERE Segment = 'Pharma' AND Year = 2022 AND Quarter IN ('Q1', 'Q2', 'Q3');
2023-12-14 12:08:54,799 INFO sqlalchemy.engine.Engine [generated in 0.00039s] ()
2023-12-14 12:08:54,900 INFO sqlalchemy.engine.Engine COMMIT
('[(1980417814.2799988,)]',
 {'result': [(1980417814.2799988,)], 'col_keys': ['SUM(Amount)']})
```

### Version

0.9.13

### Steps to Reproduce

None

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
71,180b98dd-1aab-47a8-924d-c47326fae477,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 14, 'assignee': '', 'size': '', 'index_id': '9522'}",[],[],{},44df1bceded13e559b9690a7a54d62b311aaeccfee98817975e662c7b7d331be,"[Bug]: sqalchemy warning in postgres vectorstore hybrid search
### Bug Description

Following warning pops up when using hybrid search with postgres vector
```
.../llama_index/vector_stores/postgres.py:510: SAWarning: UserDefinedType REGCONFIG() will not produce a cache key becaus
e the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLA
lchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this warning at
: https://sqlalche.me/e/20/cprf)                                                                                                                                        
  res = session.execute(stmt)                     
```

This also is present in lantern vector_store (it's in the notebook) and has similar code as postgres vectorstore

### Version

latest

### Steps to Reproduce

Setup a hybrid postgres vecotr store and run a query

### Relevant Logs/Tracbacks

```shell
See https://docs.sqlalchemy.org/en/20/errors.html#assertion-attributes-for-caching for an explanation (that i don't fully understand, so i can't propose a fix)

the `_build_sparse_query` in `vector_stores/postgres.py` has a subclassed `UserDefinedType` that is mentioned in the 2nd item in the url above. 

the line referenced in the warning is the execution of the statement.
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
72,9f183513-56ce-4ae5-9f97-a708e3528c85,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 14, 'assignee': '', 'size': '', 'index_id': '9520'}",[],[],{},24661ced9c04ca5941dd5c03cc7cc56ce9e320165c9cbcd6d7ec49cabec8cdc3,"[Bug]: *** RuntimeError: Timeout context manager should be used inside a task
### Bug Description

*** RuntimeError: Timeout context manager should be used inside a task

### Version

llama_index==0.8.63.post2

### Steps to Reproduce

step1. use elasticsearch vector store
step2. start a server
step3. client request, engine generate reply

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
73,8cb26357-76c4-4487-ae22-36eb7e02375c,,"{'state': 'open', 'year': 2023, 'month': 8, 'day': 12, 'assignee': '', 'size': '', 'index_id': '7244'}",[],[],{},4556a72b7509581e3cc10b010c18516ace09b23d9dd761f66dd1e14c1fbfdabf,"[Bug]: Streamlit and Llama_index - asyncio.Queue for StreamingAgentChatResponse - RuntimeError: There is no current event loop in thread 'ScriptRunner.scriptThread'
### Bug Description

error from StreamingAgentChatResponse for line 60:
_aqueue: asyncio.Queue = asyncio.Queue()



### Version

0.8.0

### Steps to Reproduce

When running llama_index with a simple streamlit example:
App.py
import streamlit as st
from llama_index import LLMPredictor

run: 
streamlit run App.py 

### Relevant Logs/Tracbacks

```shell
error occurs in StreamingAgentChatResponse for _aqueue: asyncio.Queue = asyncio.Queue()
2023-08-12 08:38:47.123 Uncaught app exception
Traceback (most recent call last):
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py"", line 552, in _run_script
    exec(code, module.__dict__)
  File ""/home/wjwong/work/test_app/App.py"", line 2, in <module>
    from llama_index import LLMPredictor
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/__init__.py"", line 20, in <module>
    from llama_index.indices.keyword_table import (
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/indices/__init__.py"", line 4, in <module>
    from llama_index.indices.keyword_table.base import (
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/indices/keyword_table/__init__.py"", line 4, in <module>
    from llama_index.indices.keyword_table.base import (
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/indices/keyword_table/base.py"", line 18, in <module>
    from llama_index.indices.base import BaseIndex
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/indices/base.py"", line 6, in <module>
    from llama_index.chat_engine.types import BaseChatEngine, ChatMode
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/chat_engine/__init__.py"", line 1, in <module>
    from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/chat_engine/condense_question.py"", line 5, in <module>
    from llama_index.chat_engine.types import (
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/chat_engine/types.py"", line 51, in <module>
    class StreamingAgentChatResponse:
  File ""/root/miniconda3/envs/py39/lib/python3.9/site-packages/llama_index/chat_engine/types.py"", line 60, in StreamingAgentChatResponse
    _aqueue: asyncio.Queue = asyncio.Queue()
  File ""/root/miniconda3/envs/py39/lib/python3.9/asyncio/queues.py"", line 36, in __init__
    self._loop = events.get_event_loop()
  File ""/root/miniconda3/envs/py39/lib/python3.9/asyncio/events.py"", line 642, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'ScriptRunner.scriptThread'.
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
74,ffd22b7a-eea2-4cf2-b252-031e7e942de2,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 14, 'assignee': '', 'size': '', 'index_id': '9519'}",[],[],{},5b6eef5aa6a3e21eef4c239280d4732e50708b301c178f14a3acb30d70831d25,"[Bug]: Metadata Filter won't work properly on HNSW PgVectorStore due to PgVector limitation
### Bug Description

There is an open issue on PgVector Repo which causes indexed embedding (HNSW) won't work properly when combined with a filter.
* https://github.com/pgvector/pgvector/issues/259
 
The query in question is similar to the one used by the PgVectorStore retriever.
```
SELECT id, text
FROM document
WHERE category = $1
ORDER BY embedding <-> $2
LIMIT 10;
```

PS: This problem only occurs when you index your PgVector table (with HNSW). The standard table or IFFlat indexed table won't face this issue, but you will hit a performance issue once you have a lot of data.

### Version

latest

### Steps to Reproduce

Query your PgVectorStore with and without metadata filtering, and you will notice something weird happening.


### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
75,b3f4c78a-8ce9-4324-8636-fd25ee37affc,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': '', 'index_id': '9492'}",[],[],{},fbcd7fb9e5dc9e95f6f10558725b3eacf8a1910aabfae23b86cc6af58f882761,"Connection Timeout with Elastic Search
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

Hey,
I am trying to use Query Fusion retriever on top of hybrid retriever.I am getting connection timeout error because for now multiple sub queries are generated for a single query.On troubleshooting I found that request_timeout value should be increased.If this is correct how can we achieve this via Llamaindex? Tried adding vector_store_kwargs while creating retriever but no help.

**Code**:
def load_index_query_engine(llama_doc_list):
 hybrid_retriever_list=[]
for pdf_doc in doc_list:
index = load_index_from_storage(storage_context)
vector_retriever = index.as_retriever(
similarity_top_k=10,
vector_store_query_mode=""hybrid"",
**vector_store_kwargs={""request_timeout"":30}**
)
nodes = service_context.node_parser.get_nodes_from_documents(llama_doc_list[pdf_doc])
bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)
hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)
hybrid_retriever_list.append(hybrid_retriever)
return hybrid_retriever_list

retriever = QueryFusionRetriever(   
    hybrid_retriever_list,
    similarity_top_k=2,
    num_queries=2,  
    use_async=True,
    verbose=True,
    llm=llm
    # query_gen_prompt=""..."",  # we could override the query generation prompt here
)

query_engine = RetrieverQueryEngine.from_args(
    retriever=retriever,
    # node_postprocessors=[reranker],
     service_context=service_context,
    # streaming=True
)
with torch.no_grad() and torch.inference_mode():
    response = query_engine.query(question)
    print(response)

**Error**:
ConnectionError                           Traceback (most recent call last)
File <timed exec>:4

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/core/base_query_engine.py:30, in BaseQueryEngine.query(self, str_or_query_bundle)
     28 if isinstance(str_or_query_bundle, str):
     29     str_or_query_bundle = QueryBundle(str_or_query_bundle)
---> 30 return self._query(str_or_query_bundle)

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:170, in RetrieverQueryEngine._query(self, query_bundle)
    166 """"""Answer a query.""""""
    167 with self.callback_manager.event(
    168     CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}
    169 ) as query_event:
--> 170     nodes = self.retrieve(query_bundle)
    171     response = self._response_synthesizer.synthesize(
    172         query=query_bundle,
    173         nodes=nodes,
    174     )
    176     query_event.on_end(payload={EventPayload.RESPONSE: response})

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:126, in RetrieverQueryEngine.retrieve(self, query_bundle)
    125 def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
--> 126     nodes = self._retriever.retrieve(query_bundle)
    127     return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/core/base_retriever.py:54, in BaseRetriever.retrieve(self, str_or_query_bundle)
     49 with self.callback_manager.as_trace(""query""):
     50     with self.callback_manager.event(
     51         CBEventType.RETRIEVE,
     52         payload={EventPayload.QUERY_STR: query_bundle.query_str},
     53     ) as retrieve_event:
---> 54         nodes = self._retrieve(query_bundle)
     55         retrieve_event.on_end(
     56             payload={EventPayload.NODES: nodes},
     57         )
     58 return nodes

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/retrievers/fusion_retriever.py:166, in QueryFusionRetriever._retrieve(self, query_bundle)
    163     queries = [query_bundle.query_str]
    165 if self.use_async:
--> 166     results = self._run_nested_async_queries(queries)
    167 else:
    168     results = self._run_sync_queries(queries)

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/retrievers/fusion_retriever.py:124, in QueryFusionRetriever._run_nested_async_queries(self, queries)
    121         tasks.append(retriever.aretrieve(query))
    122         task_queries.append(query)
--> 124 task_results = run_async_tasks(tasks)
    126 results = {}
    127 for i, (query, query_result) in enumerate(zip(task_queries, task_results)):

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/async_utils.py:49, in run_async_tasks(tasks, show_progress, progress_bar_desc)
     46 async def _gather() -> List[Any]:
     47     return await asyncio.gather(*tasks_to_execute)
---> 49 outputs: List[Any] = asyncio.run(_gather())
     50 return outputs

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/nest_asyncio.py:31, in _patch_asyncio.<locals>.run(main, debug)
     29 task = asyncio.ensure_future(main)
     30 try:
---> 31     return loop.run_until_complete(task)
     32 finally:
     33     if not task.done():

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/nest_asyncio.py:99, in _patch_loop.<locals>.run_until_complete(self, future)
     96 if not f.done():
     97     raise RuntimeError(
     98         'Event loop stopped before Future completed.')
---> 99 return f.result()

File ~/anaconda3/envs/python3/lib/python3.10/asyncio/futures.py:201, in Future.result(self)
    199 self.__log_traceback = False
    200 if self._exception is not None:
--> 201     raise self._exception.with_traceback(self._exception_tb)
    202 return self._result

File ~/anaconda3/envs/python3/lib/python3.10/asyncio/tasks.py:234, in Task.__step(***failed resolving arguments***)
    232         result = coro.send(None)
    233     else:
--> 234         result = coro.throw(exc)
    235 except StopIteration as exc:
    236     if self._must_cancel:
    237         # Task is cancelled right before coro stops.

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/async_utils.py:47, in run_async_tasks.<locals>._gather()
     46 async def _gather() -> List[Any]:
---> 47     return await asyncio.gather(*tasks_to_execute)

File ~/anaconda3/envs/python3/lib/python3.10/asyncio/tasks.py:304, in Task.__wakeup(self, future)
    302 def __wakeup(self, future):
    303     try:
--> 304         future.result()
    305     except BaseException as exc:
    306         # This may also be a cancellation.
    307         self.__step(exc)

File ~/anaconda3/envs/python3/lib/python3.10/asyncio/tasks.py:232, in Task.__step(***failed resolving arguments***)
    228 try:
    229     if exc is None:
    230         # We use the `send` method directly, because coroutines
    231         # don't have `__iter__` and `__next__` methods.
--> 232         result = coro.send(None)
    233     else:
    234         result = coro.throw(exc)

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/core/base_retriever.py:72, in BaseRetriever.aretrieve(self, str_or_query_bundle)
     67 with self.callback_manager.as_trace(""query""):
     68     with self.callback_manager.event(
     69         CBEventType.RETRIEVE,
     70         payload={EventPayload.QUERY_STR: query_bundle.query_str},
     71     ) as retrieve_event:
---> 72         nodes = await self._aretrieve(query_bundle)
     73         retrieve_event.on_end(
     74             payload={EventPayload.NODES: nodes},
     75         )
     76 return nodes

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/core/base_retriever.py:94, in BaseRetriever._aretrieve(self, query_bundle)
     88 async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
     89     """"""Asynchronously retrieve nodes given query.
     90 
     91     Implemented by the user.
     92 
     93     """"""
---> 94     return self._retrieve(query_bundle)

Cell In[19], line 17, in HybridRetriever._retrieve(self, query, **kwargs)
     15 def _retrieve(self, query, **kwargs):
     16     bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)
---> 17     vector_nodes = self.vector_retriever.retrieve(query, **kwargs)
     19     # combine the two lists of nodes
     20     all_nodes = []

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/core/base_retriever.py:54, in BaseRetriever.retrieve(self, str_or_query_bundle)
     49 with self.callback_manager.as_trace(""query""):
     50     with self.callback_manager.event(
     51         CBEventType.RETRIEVE,
     52         payload={EventPayload.QUERY_STR: query_bundle.query_str},
     53     ) as retrieve_event:
---> 54         nodes = self._retrieve(query_bundle)
     55         retrieve_event.on_end(
     56             payload={EventPayload.NODES: nodes},
     57         )
     58 return nodes

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:88, in VectorIndexRetriever._retrieve(self, query_bundle)
     82     if query_bundle.embedding is None and len(query_bundle.embedding_strs) > 0:
     83         query_bundle.embedding = (
     84             self._service_context.embed_model.get_agg_embedding_from_queries(
     85                 query_bundle.embedding_strs
     86             )
     87         )
---> 88 return self._get_nodes_with_embeddings(query_bundle)

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:164, in VectorIndexRetriever._get_nodes_with_embeddings(self, query_bundle_with_embeddings)
    160 def _get_nodes_with_embeddings(
    161     self, query_bundle_with_embeddings: QueryBundle
    162 ) -> List[NodeWithScore]:
    163     query = self._build_vector_store_query(query_bundle_with_embeddings)
--> 164     query_result = self._vector_store.query(query, **self._kwargs)
    165     return self._build_node_list_from_query_result(query_result)

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/vector_stores/elasticsearch.py:452, in ElasticsearchStore.query(self, query, custom_query, es_filter, **kwargs)
    424 def query(
    425     self,
    426     query: VectorStoreQuery,
   (...)
    431     **kwargs: Any,
    432 ) -> VectorStoreQueryResult:
    433     """"""Query index for top k most similar nodes.
    434 
    435     Args:
   (...)
    450 
    451     """"""
--> 452     return asyncio.get_event_loop().run_until_complete(
    453         self.aquery(query, custom_query, es_filter, **kwargs)
    454     )

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/nest_asyncio.py:99, in _patch_loop.<locals>.run_until_complete(self, future)
     96 if not f.done():
     97     raise RuntimeError(
     98         'Event loop stopped before Future completed.')
---> 99 return f.result()

File ~/anaconda3/envs/python3/lib/python3.10/asyncio/futures.py:201, in Future.result(self)
    199 self.__log_traceback = False
    200 if self._exception is not None:
--> 201     raise self._exception.with_traceback(self._exception_tb)
    202 return self._result

File ~/anaconda3/envs/python3/lib/python3.10/asyncio/tasks.py:234, in Task.__step(***failed resolving arguments***)
    232         result = coro.send(None)
    233     else:
--> 234         result = coro.throw(exc)
    235 except StopIteration as exc:
    236     if self._must_cancel:
    237         # Task is cancelled right before coro stops.

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/llama_index/vector_stores/elasticsearch.py:524, in ElasticsearchStore.aquery(self, query, custom_query, es_filter, **kwargs)
    521     logger.debug(f""Calling custom_query, Query body now: {es_query}"")
    523 async with self.client as client:
--> 524     response = await client.search(
    525         index=self.index_name,
    526         **es_query,
    527         size=query.similarity_top_k,
    528         _source={""excludes"": [self.vector_field]},
    529     )
    531 top_k_nodes = []
    532 top_k_ids = []

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/elasticsearch/_async/client/__init__.py:3735, in AsyncElasticsearch.search(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, from_, highlight, human, ignore_throttled, ignore_unavailable, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, rank, request_cache, rescore, rest_total_hits_as_int, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version)
   3733 if __body is not None:
   3734     __headers[""content-type""] = ""application/json""
-> 3735 return await self.perform_request(  # type: ignore[return-value]
   3736     ""POST"", __path, params=__query, headers=__headers, body=__body
   3737 )

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/elasticsearch/_async/client/_base.py:285, in BaseClient.perform_request(self, method, path, params, headers, body)
    282 else:
    283     target = path
--> 285 meta, resp_body = await self.transport.perform_request(
    286     method,
    287     target,
    288     headers=request_headers,
    289     body=body,
    290     request_timeout=self._request_timeout,
    291     max_retries=self._max_retries,
    292     retry_on_status=self._retry_on_status,
    293     retry_on_timeout=self._retry_on_timeout,
    294     client_meta=self._client_meta,
    295 )
    297 # HEAD with a 404 is returned as a normal response
    298 # since this is used as an 'exists' functionality.
    299 if not (method == ""HEAD"" and meta.status == 404) and (
    300     not 200 <= meta.status < 299
    301     and (
   (...)
    305     )
    306 ):

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/elastic_transport/_async_transport.py:258, in AsyncTransport.perform_request(self, method, target, body, headers, max_retries, retry_on_status, retry_on_timeout, request_timeout, client_meta)
    256 start_time = self._loop.time()
    257 try:
--> 258     resp = await node.perform_request(
    259         method,
    260         target,
    261         body=request_body,
    262         headers=request_headers,
    263         request_timeout=request_timeout,
    264     )
    265     _logger.info(
    266         ""%s %s%s [status:%s duration:%.3fs]""
    267         % (
   (...)
    273         )
    274     )
    276     if method != ""HEAD"":

File ~/anaconda3/envs/python3/lib/python3.10/site-packages/elastic_transport/_node/_http_aiohttp.py:218, in AiohttpHttpNode.perform_request(self, method, target, body, headers, request_timeout)
    210         err = ConnectionError(str(e), errors=(e,))
    211     self._log_request(
    212         method=""HEAD"" if is_head else method,
    213         target=target,
   (...)
    216         exception=err,
    217     )
--> 218     raise err from None
    220 meta = ApiResponseMeta(
    221     node=self.config,
    222     duration=duration,
   (...)
    225     headers=HttpHeaders(response.headers),
    226 )
    227 self._log_request(
    228     method=""HEAD"" if is_head else method,
    229     target=target,
   (...)
    233     response=raw_data,
    234 )
ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: ClientOSError([Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2702)))",,,"{metadata_str}

{content}",{key}: {value},"
",Document
76,c6eb1ea1-d7a3-43a3-8621-5772d6290fd2,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 14, 'assignee': '', 'size': '', 'index_id': '9509'}",[],[],{},749d5a07a1532865ed266c29665b436002db270d112d6a1153855152a07bfdd9,"[Bug]: RedisVectorStore delete_ref_doc doesn't delete all nodes
### Bug Description

`index.delete_ref_doc(ref_doc_id=doc_id)`

Won't delete all of the nodes of a document if there are more than 10.

Because [the code in delete](https://github.com/run-llama/llama_index/blob/main/llama_index/vector_stores/redis.py#L199) does a search for nodes then deletes what it finds node by node.  However the query default is to limit to 10 items returned.  Documents created using SentenceSplitter often have more than 10 nodes.

I believe setting setting the limit explicitly in query_params passed to the search in that code, and putting that code in a loop that repeats until the number of nodes returned < limit would do it, but I don't know a lot about redis so there is probably a better way.


### Version

0.9.13

### Steps to Reproduce

1.  Index a doc with more than 10 nodes.
2. Delete the doc using index.delete_ref_doc(doc_id)
3. Verify nodes are still in the redis data store from that document.

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
77,eabb83ff-79d2-488b-b009-754e55e171a7,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': '', 'index_id': '9507'}",[],[],{},ca9757f0ade0acffcfe414884500cc0b388b2f43db4aee3bbad20557c7056ee2,"[Bug]: UnstructuredElementNodeParser failed on table with <a> tag in cells
### Bug Description

An HTML table like 

 ```
 <body>
        <h2>European Countries and Their Telephone Country Codes</h2>
        <table>
            <tr>
                <th>Country</th>
                <th>Telephone Country Code</th>
            </tr>
            <tr>
                <td>Some text<a href=""http://google.com"">Germany</a></td>
                <td>+49</td>
            </tr>
            <tr>
                <td>France</td>
                <td>+33</td>
            </tr>
...
```
isn't properly processed 

### Version

0.9.15

### Steps to Reproduce

Given file `eutel.html`:
```
<!DOCTYPE html>
<html>
    <head>
        <title>European Countries and Their Telephone Country Codes</title>
    </head>
    <body>
        <h2>European Countries and Their Telephone Country Codes</h2>
        <table>
            <tr>
                <th>Country</th>
                <th>Telephone Country Code</th>
            </tr>
            <tr>
                <td>Some text<a href=""http://google.com"">Germany</a></td>
                <td>+49</td>
            </tr>
            <tr>
                <td>France</td>
                <td>+33</td>
            </tr>
            <!-- Add more rows as needed -->
        </table>
  
</body>
</html>
```
the code :

```
from llama_index.readers.file.flat_reader import FlatReader
from llama_index.node_parser import (UnstructuredElementNodeParser,)
from pathlib import Path

file = ""eutel.html""
reader = FlatReader()
eutel_doc = reader.load_data(Path(file))
node_parser = UnstructuredElementNodeParser()
raw_nodes = node_parser.get_nodes_from_documents(eutel_doc, show_progress=True)
base_nodes, node_mappings = node_parser.get_base_nodes_and_mappings(raw_nodes)
print(node_mappings)
```
outputs nothing : {}

if in the HTML file the text ""Some text"" before the anchor on Germany is removed then it properly outputs

`{'id_1_table': TextNode(id_='id_1_table', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='id_1_table_ref', node_type=<ObjectType.INDEX: '3'>, metadata={'col_schema': 'Column: Country\nType: string\nSummary: Name of the country\n\nColumn: Telephone Country Code\nType: string\nSummary: International telephone country code for the country'}, hash='c37f994368d3660fee3961ab514b693099af730f4a1d66fdd79f630efad7f816')}, hash='5fd23eded1daf245a9066d0dbae43b27e2cf74913c1f33a8cf25ebda0fcd7b79', text='            Country   Telephone Country Code  \n0           Germany                      +49  \n1            France                      +33  \n2             Italy                      +39  \n3             Spain                      +34  \n4    United Kingdom                      +44  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n')}`


Analysis shows that the root cause is in https://github.com/run-llama/llama_index/blob/main/llama_index/node_parser/relational/unstructured_element.py line 71, the text before and after the anchors are counted as separated columns and then the table is considered as malformed

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
78,e1963732-2b31-4a06-9a2a-5a0a8caac898,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': '', 'index_id': '9490'}",[],[],{},9d8fbd35178744ef911e6a75e46162a76d2352e1cd99988cae2710b419dca0c7,"[Bug]: Error when trying to implement LiteLLM model
### Bug Description

When I try to use the example code for use LiteLLM:

llm = LiteLLM(""mistralai/Mixtral-8x7B-Instruct-v0.1"")

I get the following error:

ModuleNotFoundError: No module named 'litellm'

### Version

v0.9.14.post3

### Steps to Reproduce

Running this line of code appears to produce this error:

llm = LiteLLM(""mistralai/Mixtral-8x7B-Instruct-v0.1"")

### Relevant Logs/Tracbacks

```shell
File ~\miniconda3\lib\site-packages\llama_index\llms\litellm_utils.py:205, in validate_litellm_api_key(api_key, api_type)
    202 def validate_litellm_api_key(
    203     api_key: Optional[str] = None, api_type: Optional[str] = None
    204 ) -> None:
--> 205     import litellm
    207     api_key = litellm.validate_environment()
    208     if api_key is None:

ModuleNotFoundError: No module named 'litellm'
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
79,aa012f63-ce2f-4d0a-9e18-3ee1809654df,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': '', 'index_id': '9483'}",[],[],{},dcaa39ae9e8ddfcea728a4c6762017980b0b6c0ad3cc4833ec7425934954c085,"[Question]: How to use multi modal large model in local env base llama_index?
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

How to use multi modal large model in local env base llama_index?",,,"{metadata_str}

{content}",{key}: {value},"
",Document
80,00ea9232-1214-497d-8669-f5531dfd3a91,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': '', 'index_id': '9481'}",[],[],{},4843b662e58efd7e33aa9b5704f9ad7109f064ce1bc2c3a1e0821079fca033c2,"[Feature Request]: Support additional entities and anonymization approaches for PII scrubbing
### Feature Description

The existing `NERPIINodePostprocessor` calls a NER model for PII detection. The types of detected entities depend on the data the NER model was trained on. A framework like [Presidio](https://github.com/microsoft/presidio) allows the user to extract many more entities using a combination of NER and rule based approaches (e.g. for credit card detection). 
In addition, there are different types of de-identification operations, each serving a different use-case. In some cases we'd like to generate fake data, in some to replace with a placeholder, and in others to do instance anonymization in order to reverse the process later. Allowing the user to use something like Presidio would greatly increase the flexibility in PII scanning and handling.

p.s. I'm one of the maintainers of Presidio, and would be happy to help / work on this feature integration. I love llama-Index, see a lot of customer demand for PII scrubbing, and think that a tighter integration would benefit many users!

### Reason

- Adding an integration with Presidio instead of the existing NER approach would allow many more use cases and PII entity types to be handled, while still being able to use the same NER model as part of the Presidio process.
- Existing NER approaches lack many entity types and are sometimes not accurate enough to completely remove the PII.
- Each use case requires a different de-identification strategy (e.g. replace with fake), while the existing NER approach only does instance anonymization.

Presidio is already integrated into [LangChain](https://python.langchain.com/docs/guides/privacy/presidio_data_anonymization/) and [Rasa](https://rasa.com/docs/rasa/pii-management/).

### Value of Feature

Better PII handling, compliance, remove the need to pass PII to the LLM.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
81,c82d16e9-b760-41e6-82b7-d2268390f5b0,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 12, 'assignee': '', 'size': '', 'index_id': '9469'}",[],[],{},96923de7e192a97fad1c5692fb71d0de817183adc10f67095d72a9a14872298e,"[Bug]: cant retrieve image which are indexed using redis..retrieved_image is an empty list...the same code works without issue for qdrant database which is in llaamaindex documentation
### Bug Description

i migrated from qdrant database documentation to redis ..image retrieval not taking place properly..

from llama_index import VectorStoreIndex, StorageContext
from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex
from llama_index.vector_stores.redis import RedisVectorStore
import redis
import urllib.parse
from llama_index import (
    ServiceContext,
    SimpleDirectoryReader,
)

password = ""purposefullyhid""
encoded_password = urllib.parse.quote(password)
REDIS_HOST = ""13.72.116.240""
REDIS_PORT = 6380
redis_url = f""redis://:{encoded_password}@{REDIS_HOST}:{REDIS_PORT}""

redis_client = redis.Redis.from_url(redis_url)

text_vector_store = RedisVectorStore(
    index_name=""text_index11"",
    index_prefix=""llama_text3"",
    redis_url=redis_url,
    overwrite=True,
)

image_vector_store = RedisVectorStore(
    index_name=""image_index11"",
    index_prefix=""llama_image3"",
    redis_url=redis_url,
    overwrite=True,
)

storage_context = StorageContext.from_defaults(
    vector_store=text_vector_store, image_store=image_vector_store
)


documents = SimpleDirectoryReader(""./data_wiki/"").load_data()
index = MultiModalVectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
)

from PIL import Image
import matplotlib.pyplot as plt
import os

def plot_images(image_metadata_dict):
    original_images_urls = []
    images_shown = 0
    for image_id in image_metadata_dict:
        img_path = image_metadata_dict[image_id][""img_path""]
        if os.path.isfile(img_path) and os.path.getsize(img_path) > 0:
            try:
                image = Image.open(img_path).convert(""RGB"")

                plt.subplot(8, 8, len(original_images_urls) + 1)
                plt.imshow(image)
                plt.xticks([])
                plt.yticks([])

                original_images_urls.append(image_metadata_dict[image_id][""filename""])
                images_shown += 1
                if images_shown >= 64:
                    break
            except Exception as e:
                print(f""Error opening {img_path}: {e}"")

    plt.tight_layout()

plot_images(image_metadata_dict)                                                                                                                                                                      

def plot_images(image_paths):
    images_shown = 0
    plt.figure(figsize=(16, 9))
    for img_path in image_paths:
        if os.path.isfile(img_path):
            image = Image.open(img_path)

            plt.subplot(2, 3, images_shown + 1)
            plt.imshow(image)
            plt.xticks([])
            plt.yticks([])

            images_shown += 1
            if images_shown >= 9:
                break

              test_query = ""who are BTS team members""
# generate  retrieval results
retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=2)
retrieval_results = retriever.retrieve(test_query)

from llama_index.response.notebook_utils import display_source_node
from llama_index.schema import ImageNode

retrieved_image = []
for res_node in retrieval_results:
    if isinstance(res_node.node, ImageNode):
        retrieved_image.append(res_node.node.metadata[""file_path""])
    else:
        display_source_node(res_node, source_length=200)

plot_images(retrieved_image)

**output is** 
Node ID: 38351381-ac1a-4b8a-9107-cf2ddd8a539a
Similarity: 0.816846907139
Text: == Members == Jin (ì§„) â€“ vocalist Suga (ìŠˆê°€) â€“ rapper J-Hope (ì œì´í™‰) â€“ rapper RM (ì•Œì•°) â€“ leader, rapper Jimin (ì§€ë¯¼) â€“ vocalist V (ë·”) â€“ vocalist Jungkook (ì •êµ­) â€“ vocalist

== Discography ==

== Fi...

Node ID: baf75f27-1929-42a4-bac9-1c52609a7fac
Similarity: 0.816846907139
Text: == Members == Jin (ì§„) â€“ vocalist Suga (ìŠˆê°€) â€“ rapper J-Hope (ì œì´í™‰) â€“ rapper RM (ì•Œì•°) â€“ leader, rapper Jimin (ì§€ë¯¼) â€“ vocalist V (ë·”) â€“ vocalist Jungkook (ì •êµ­) â€“ vocalist

== Discography ==

== Fi...

Node ID: bc322ea3-a8f2-4e5a-a0fd-32adef23ed09
Similarity: 0.808160722256
Text: BTS (Korean: ë°©íƒ„ì†Œë…„ë‹¨; RR: Bangtan Sonyeondan; lit. Bulletproof Boy Scouts), also known as the Bangtan Boys, is a South Korean boy band formed in 2010. The band consists of Jin, Suga, J-Hope, RM, Jimi...

Node ID: 0f9194de-6cdc-489b-8051-50f1cad77af9
Similarity: 0.28735196590400003
Text:

Node ID: debfe5b4-ffc7-43f3-9a40-40e3efcf2ff7
Similarity: 0.28735196590400003
Text:

<Figure size 1600x900 with 0 Axes>
the image output is empty
<img width=""619"" alt=""llamaindex issue"" src=""https://github.com/run-llama/llama_index/assets/114779060/05a53e12-1a4d-4326-8875-0c04d2d80536"">







### Version

0.9.14.post3

### Steps to Reproduce

from llama_index import VectorStoreIndex, StorageContext
from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex
from llama_index.vector_stores.redis import RedisVectorStore
import redis
import urllib.parse
from llama_index import (
    ServiceContext,
    SimpleDirectoryReader,
)

password = ""purposefullyhid""
encoded_password = urllib.parse.quote(password)
REDIS_HOST = ""13.72.116.240""
REDIS_PORT = 6380
redis_url = f""redis://:{encoded_password}@{REDIS_HOST}:{REDIS_PORT}""

redis_client = redis.Redis.from_url(redis_url)

text_vector_store = RedisVectorStore(
    index_name=""text_index11"",
    index_prefix=""llama_text3"",
    redis_url=redis_url,
    overwrite=True,
)

image_vector_store = RedisVectorStore(
    index_name=""image_index11"",
    index_prefix=""llama_image3"",
    redis_url=redis_url,
    overwrite=True,
)

storage_context = StorageContext.from_defaults(
    vector_store=text_vector_store, image_store=image_vector_store
)


documents = SimpleDirectoryReader(""./data_wiki/"").load_data()
index = MultiModalVectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
)


### Relevant Logs/Tracbacks

```shell
Node ID: 1f790011-b6a5-4d8a-82a5-ebe6585f3c53
Similarity: 0.871842324734
Text: Van Gogh turned to well-known Hague School artists like Weissenbruch and Blommers, and he received technical advice from them as well as from painters like De Bock and Van der Weele, both of the Ha...

Node ID: 5ca18ec4-77e4-4d71-9018-ec8f50cea1ca
Similarity: 0.871842324734
Text: Van Gogh turned to well-known Hague School artists like Weissenbruch and Blommers, and he received technical advice from them as well as from painters like De Bock and Van der Weele, both of the Ha...

Node ID: c94421e1-c152-4625-908b-b82338e90c59
Similarity: 0.867530286312
Text: Vincent Willem van Gogh (Dutch: [ËˆvÉªnsÉ›nt ËˆÊ‹ÉªlÉ™É± vÉ‘Å‹ ËˆÉ£É”x] ; 30 March 1853 â€“ 29 July 1890) was a Dutch Post-Impressionist painter who is among the most famous and influential figures in the history...

Node ID: 70affaa1-e308-4cdc-83f7-9adf36efa20f
Similarity: 0.315268814564
Text:

Node ID: 46fc8e9f-5658-44b2-b8c6-7906d5d84101
Similarity: 0.315268814564
Text:

Node ID: 3c765355-b23d-43f3-9581-249051987e5b
Similarity: 0.313244283199
Text:

Node ID: c75047ee-99dc-4a56-871d-e0849667a5c9
Similarity: 0.313244283199
Text:

Node ID: 91d61107-0a17-40ba-b27b-a55f0218979e
Similarity: 0.30464488267900003
Text:

<Figure size 1600x900 with 0 Axes>
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
82,30e03570-2a81-484d-abdb-ca7010e681ec,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': '', 'index_id': '9477'}",[],[],{},bd4c8f2c89608a6a32e080bfc858f2e0a332085161cf11b12dfb1187b261c240,"[Bug]: VLLM Streaming Not Working
### Bug Description

Hi Team, 
I am trying to use llama-index with vllm, I want to stream the response first on notebook than to an api response.
But its not working. Vllm - 0.2.4
### Version

0.9.14.post1

### Steps to Reproduce

from llama_index.llms.vllm import Vllm

llm = Vllm(
    model=""meta-llama/Llama-2-7b-chat-hf"",
    tensor_parallel_size=2,
    max_new_tokens=500,
    vllm_kwargs={""gpu_memory_utilization"": 0.5},
)

message = [ChatMessage(content=""what is a black hole"", author=""user"")]
[x for x in llm.stream_chat(message)][-1]

also with - 
from llama_index.llms import ChatMessage
await llm.acomplete(""What is a black hole"")

### Relevant Logs/Tracbacks

![image](https://github.com/run-llama/llama_index/assets/35100919/cc4cb4cc-750d-40bd-9be0-d7e3c4d77b09)

![image](https://github.com/run-llama/llama_index/assets/35100919/acebaf2d-006e-41f8-91ef-46506f077364)


_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
83,9a1eeeb7-22bc-4519-a2e3-c1fcaad0401a,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 13, 'assignee': '', 'size': '', 'index_id': '9475'}",[],[],{},00d6489ce6aa1778b67b60a42dd8461ebc32958c240b6e892a8fc4073ae6abfd,"[Bug]: SimpleDirectoryReader() Fails on Corrupted PPTX File with ""File is not a zip file"" Error  
### Bug Description

The loader is failing on a powerpoint file in my azure blob directory. I am unsure why, but I suspect the file could be corrupted. Instead of logging an exception, the load for the entire directory fails. I don't want the full load to fail because of a single file. Should this line be modified to use try-except?

```extracted_documents = reader.load_data(file=input_file, extra_info=metadata)```

### Version

Current

### Steps to Reproduce

Try loading a corrupted powerpoint file using the SimpleDirectoryReader().

### Relevant Logs/Tracbacks

```shell
---------------------------------------------------------------------------
BadZipFile                                Traceback (most recent call last)
Cell In[10], line 14
      6 container_client = ContainerClient(account_url=storage_url, 
      7     credential=storage_key, 
      8     container_name=""powerpoints3"")
     10 loader = AzStorageBlobReader(account_url=storage_url, 
     11     credential=storage_key, 
     12     container_name=""powerpoints3"")
---> 14 documents = loader.load_data()

Cell In[6], line 130
    127     SimpleDirectoryReader = download_loader(""SimpleDirectoryReader"")
    128 loader = SimpleDirectoryReader(temp_dir, file_extractor=self.file_extractor, errors=""ignore"")
--> 130 return loader.load_data()

File /anaconda/envs/azureml_py38/lib/python3.8/site-packages/llama_index/download/llamahub_modules/file/base.py:143, in SimpleDirectoryReader.load_data(self)
    141         reader = download_loader(reader)()
    142 # try:
--> 143 extracted_documents = reader.load_data(
    144     file=input_file, extra_info=metadata
    145 )
    146 # except:
    147 #     pass
    148 documents.extend(extracted_documents)
...
-> 1336     raise BadZipFile(""File is not a zip file"")
   1337 if self.debug > 1:
   1338     print(endrec)

BadZipFile: File is not a zip file
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
84,c8dd7864-41cc-45bd-8d65-20e715862d33,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 12, 'assignee': '', 'size': '', 'index_id': '9471'}",[],[],{},c8987a8f9df81ffe2d2622178a86f254c87cf61685bf8b1258f6530034e72d5f,"[Feature Request]: Support AzureOpenAIMultiModal
### Feature Description

Azure OpenAI now has gpt-4-vision in their preview.  We should support AzureOpenAIMultiModal class.  

### Reason

My customers are using Azure OpenAI and would like to evaluate gpt-4-vision models.  

### Value of Feature

As llama-index have been supporting Azure models, it is self-evident that customers would be interested in evaluating the innovative capabilities that gpt-4-vision has to offer.  ",,,"{metadata_str}

{content}",{key}: {value},"
",Document
85,647f813c-0ec5-4725-a558-54b29fff5000,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 12, 'assignee': '', 'size': '', 'index_id': '9472'}",[],[],{},c13684e3ed3e9950a4e683b2919616a0d12d5c4ccc498263c17764d29f05b499,"[Feature Request]: Add stop words to ReAct agent
### Feature Description

The ReAct agent does not use any stop words and the current API does not allow these to be passed to the LLM API.
When using the ReAct agent chat abstraction the LLM often will generate an entire conversation before this output is collected by llama-index and then trimmed to the first `Thought:`, `Action:` set.

This is very, very slow for some models.

A better approach would be to use any available stop word setting in the APIs llama-index calls, or to instead use a streaming approach and implement stop words when possible this way.

Additionally stop words should be plumbed up to the chat, query, etc API. This could probably be its own issue.

### Reason

`ReActOutputParser` selects the first `Thought:`, `Action:` set to act on. This hides that the LLM is doing a lot of useless work.

`ReActAgent` should probably inject a stop word. If you build a chat or query from this the LLM will do a lot of work before its output is truncated to the first `Thought:`, `Action:` block.

### Value of Feature

LLM usage is expensive and especially slow when working locally. Currently with a variety of models, the ReAct agent is very inefficient because it generates large outputs containing many `Thought:`, `Action:` blocks and truncates to the first one. It should just avoid generating these large blocks with a stop word or by using streaming if available and stopping after the first block.

This would reduce cost and significantly increase speed for local inference.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
86,f4fe2aa3-cecd-469c-87c1-693bf281e824,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 12, 'assignee': '', 'size': '', 'index_id': '9450'}",[],[],{},c5e841dfb83d86a4aa08cb3aad701c40de0692f3c849ff9ff8cb49350f4981cd,"[Bug]: Logging level overrides
### Bug Description

## Overview
Logging configs in files over ride developer configs, leading to unexpected behavior. Repro included below.


## Possible Solutions
1. Have only one `logger.setLevel()` invocations in package.
2. Allow developer to configure log level with env var or singleton.
```LLAMA_INDEX_LOG_LEVEL=DEBUG```
```llama_index.logger.level = ""DEBUG""```

Happy to help out here if there is preferred implementation.

### Version

0.9.13

### Steps to Reproduce

As a developer, I want to see debug level log lines to understand how Llama Index is working. I implement the following lines from the [documentation](https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging.html#basic-logging):

```python
logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))
```

I expect to see the [this log line](https://github.com/run-llama/llama_index/blob/771219236021c9cf906c598bb6f750539c791e65/llama_index/agent/openai_agent.py#L366) in the console, but it does not appear.

If I remove the [logging config in the file](https://github.com/run-llama/llama_index/blob/771219236021c9cf906c598bb6f750539c791e65/llama_index/agent/openai_agent.py#L30), the debug configuration works as expected.

### Relevant Logs/Tracbacks

```shell
Logging excerpt AFTER removing the logging config override:

DEBUG:httpcore.http11:response_closed.complete
response_closed.complete
DEBUG:openai._base_client:HTTP Request: POST https://api.openai.com/v1/chat/completions ""200 OK""
HTTP Request: POST https://api.openai.com/v1/chat/completions ""200 OK""
DEBUG:llama_index.agent.openai_agent:Break: should continue False
Break: should continue False
```

The last 2 lines do not appear in released library.
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
87,ca9f0d42-d241-49b9-8486-5355add5a4f6,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 8, 'assignee': '', 'size': '', 'index_id': '9394'}",[],[],{},805e1ca51d79206b4fe549c95c3d7a9fd6df5c2f583e71c7e725e598c82b9e38,"[Question]: stream output is empty
### Question Validation

- [ ] I have searched both the documentation and discord for an answer.

### Question

Verison : 
llama-index         0.9.13
vllm                     0.1.7

My code is as follows:
```python
from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, load_index_from_storage
from llama_index.llms.vllm import VllmServer
from llama_index.embeddings import HuggingFaceEmbedding
from llama_index.prompts import PromptTemplate
import torch


logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

embed_model = HuggingFaceEmbedding(model_name=""./gte-large-zh"")
llm = VllmServer(
    api_url=""http://localhost:36000/generate"", max_new_tokens=512,
)
service_context = ServiceContext.from_defaults(
    chunk_size=512, llm=llm, embed_model=embed_model
)

documents = SimpleDirectoryReader(""./data/paul_graham/"").load_data()
index = VectorStoreIndex.from_documents(documents, service_context=service_context)

query_engine = index.as_query_engine(streaming=True)
streaming_response = query_engine.query(""who are you?"")
for text in streaming_response.response_gen:
    print(""text: "", text)
```
Running the above code, the output is as follows:
```
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmpqobfqyyh
Created a temporary directory at /tmp/tmpqobfqyyh
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmpqobfqyyh/_remote_module_non_scriptable.py
Writing /tmp/tmpqobfqyyh/_remote_module_non_scriptable.py
INFO:llama_index.indices.loading:Loading all indices.
Loading all indices.
text:  
text:  
text:  
text:  
text:  
text:  
text:  
text:  
text:  
text:  
text: 
text:  
text:  
text:  
text:  
text:  
...
```
But I print the source code llama_index/llms/vllm.py:373  `data = json.loads(chunk.decode(""utf-8""))` , there is output:
```
data:  
In the essay,
text:  
data:  
In the essay, the
text:  
data:  
In the essay, the author
text:  
data:  
In the essay, the author,
text:  
data:  
In the essay, the author, Paul
text:  
data:  
In the essay, the author, Paul Graham
text:  
data:  
In the essay, the author, Paul Graham,
text:  
data:  
In the essay, the author, Paul Graham, discusses
```
Please tell me how to solve the above problems. Is there something wrong with my usage? Looking forward to your answer, thanksï¼",,,"{metadata_str}

{content}",{key}: {value},"
",Document
88,fc59954e-d610-45c9-a6af-6a7875846c7a,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 11, 'assignee': '', 'size': '', 'index_id': '9439'}",[],[],{},4840906663289595a74a72a95c8fc351364f73b070e7624c51bfbaab141640c1,"[Bug]: Metadata filter not working with Elastic search indexing 
### Bug Description

While retrieving from ES with multiple metadatafilter condition(OR/AND) its not taking it into account. It always performs an AND operation even if its explicitly mentioned OR.
Example below code should filter and retrieve only 'mafia' or ""Stephen King"" bit its not doing as expected.

filters = MetadataFilters(
    filters=[
        MetadataFilter(key=""theme"", value=""Mafia""),
        MetadataFilter(key=""author"", value=""Stephen King""),
    ],
    condition=FilterCondition.OR,
)

retriever = index.as_retriever(filters=filters)

### Version

0.9.13

### Steps to Reproduce

nodes = [
TextNode(
text=""The Shawshank Redemption"",
metadata={
""author"": ""Stephen King"",
""theme"": ""Friendship"",
},
),
TextNode(
text=""The Godfather"",
metadata={
""director"": ""Francis Ford Coppola"",
""theme"": ""Mafia"",
},
),
TextNode(
text=""Inception"",
metadata={
""director"": ""Christopher Nolan"",
},
),
]

filters = MetadataFilters(
    filters=[
        MetadataFilter(key=""theme"", value=""Mafia""),
        MetadataFilter(key=""author"", value=""Stephen King""),
    ],
    condition=FilterCondition.OR,
)

retriever = index.as_retriever(filters=filters)

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
89,097ed1b3-7ac1-4b22-824e-7ca79124b89d,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 9, 'assignee': '', 'size': '', 'index_id': '9413'}",[],[],{},e8064637a018a4b62d17734eb598de49cc72e5c0a313dc3a93e251c77c184a9d,"[Question]: Different numbers of embeddings with the same local embedding model
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

Iâ€™m following up on this question which was closed before I had the time to add my code: https://github.com/run-llama/llama_index/issues/9272

I get very different numbers of embeddings depending on whether I use an explicit text_splitter or not.

Using the attached graham file and the basic BAAI/bge-small-en embedding model
[graham.txt](https://github.com/run-llama/llama_index/files/13612397/graham.txt), 

Running this 

```
from llama_index.embeddings import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(model_name=""BAAI/bge-small-en"", max_length=512)
service_context = ServiceContext.from_defaults(
    llm=llm, 
    embed_model=embed_model, 
)
```

gives me 18â€¯embeddings

```
Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.09it/s]
Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:06<00:00,  2.86it/s]
```

Running this

```
from llama_index.text_splitter import TokenTextSplitter
from transformers import AutoTokenizer
text_splitter = TokenTextSplitter(
    chunk_size=512,
    tokenizer=AutoTokenizer.from_pretrained(""BAAI/bge-small-en"").encode,
)

service_context = ServiceContext.from_defaults(
    llm=llm, 
    embed_model=""local:BAAI/bge-small-en"", 
    text_splitter=text_splitter,
)
```
gives me 93 embeddings (and notice the error message)
```
Parsing nodes:   0%|                                                                             | 0/1 [00:00<?, ?it/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (16918 > 512). Running this sequence through the model will result in indexing errors
Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.95s/it]
Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:07<00:00, 11.94it/s]
```

I am basically just trying to use a local embedding model other than OpenAi, Iâ€¯was originally experimenting with e5-multilingual-large, but I get the same problem with bge-small-en. So what is the correct way to setup a local embedding?",,,"{metadata_str}

{content}",{key}: {value},"
",Document
90,6d99b07e-fa6d-4b3b-9226-1ada1c80b003,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 6, 'assignee': '', 'size': '', 'index_id': '9339'}",[],[],{},6afd659d3193b5fc756df5b126686613a2c5dd92ad2dc300dad3ee04484a37b1,"[Bug]: Llama index Guidance for Sub-Question Query Engine
### Bug Description

Guidence package with the newest version throwing an error with openai library.
Version of guidance 0.0.64.
Version of llamaindex 0.9.12


### Version

0.9.12

### Steps to Reproduce

Just try to run our google colab example:
https://docs.llamaindex.ai/en/latest/examples/output_parsing/guidance_sub_question.html#

### Relevant Logs/Tracbacks

```shell
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/guidance/llms/_openai.py"", line 665, in __call__
    out = await self.llm.caller(**call_args)
  File ""/usr/local/lib/python3.10/dist-packages/guidance/llms/_openai.py"", line 348, in _library_call
    prev_base = openai.api_base
AttributeError: module 'openai' has no attribute 'api_base'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/guidance/_program_executor.py"", line 109, in run
    await self.visit(self.parse_tree, VariableStack([self.program._variables], self))
  File ""/usr/local/lib/python3.10/dist-packages/guidance/_program_executor.py"", line 559, in visit
    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))
  File ""/usr/local/lib/python3.10/dist-packages/guidance/_program_executor.py"", line 524, in visit
    command_output = await command_function(*positional_args, **named_args)
  File ""/usr/local/lib/python3.10/dist-packages/guidance/library/_geneach.py"", line 119, in geneach
    new_content += await parser.visit(
  File ""/usr/local/lib/python3.10/dist-packages/guidance/_program_executor.py"", line 559, in visit
    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))
  File ""/usr/local/lib/python3.10/dist-packages/guidance/_program_executor.py"", line 559, in visit
    visited_children.append(await self.visit(child, variable_stack, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))
  File ""/usr/local/lib/python3.10/dist-packages/guidance/_program_executor.py"", line 266, in visit
    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]
  File ""/usr/local/lib/python3.10/dist-packages/guidance/_program_executor.py"", line 266, in <listcomp>
    visited_children = [await self.visit(child, variable_stack, next_node, next_next_node, prev_node, node, parent_node) for child in node]
  File ""/usr/local/lib/python3.10/dist-packages/guidance/_program_executor.py"", line 379, in visit
    command_output = await command_function(*positional_args, **named_args)
  File ""/usr/local/lib/python3.10/dist-packages/guidance/library/_gen.py"", line 140, in gen
    gen_obj = await parser.llm_session(
  File ""/usr/local/lib/python3.10/dist-packages/guidance/llms/_openai.py"", line 667, in __call__
    except openai.error.RateLimitError:
AttributeError: module 'openai' has no attribute 'error'

Error in program:  module 'openai' has no attribute 'error'
---------------------------------------------------------------------------
JSONDecodeError                           Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/llama_index/output_parsers/utils.py in parse_json_markdown(text)
     44     try:
---> 45         json_obj = json.loads(json_string)
     46     except json.JSONDecodeError as e_json:

22 frames
JSONDecodeError: Expecting property name enclosed in double quotes: line 2 column 14 (char 15)

During handling of the above exception, another exception occurred:

ParserError                               Traceback (most recent call last)
ParserError: while parsing a flow mapping
  in ""<unicode string>"", line 2, column 13:
      ""items"": [{#geneach 'items' stop=']'}{#unl ... 
                ^
expected ',' or '}', but got '<scalar>'
  in ""<unicode string>"", line 3, column 47:
     ... n"": ""{gen 'sub_question' stop='""'}"",
                                         ^

During handling of the above exception, another exception occurred:

OutputParserException                     Traceback (most recent call last)
OutputParserException: Got invalid JSON object. Error: Expecting property name enclosed in double quotes: line 2 column 14 (char 15) while parsing a flow mapping
  in ""<unicode string>"", line 2, column 13:
      ""items"": [{#geneach 'items' stop=']'}{#unl ... 
                ^
expected ',' or '}', but got '<scalar>'
  in ""<unicode string>"", line 3, column 47:
     ... n"": ""{gen 'sub_question' stop='""'}"",
                                         ^. Got JSON string: {
  ""items"": [{#geneach 'items' stop=']'}{#unless @first}, {/unless}{
  ""sub_question"": ""{gen 'sub_question' stop='""'}"",
  ""tool_name"": ""{gen 'tool_name' stop='""'}"",
}{/geneach}],
}

The above exception was the direct cause of the following exception:

OutputParserException                     Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/llama_index/prompts/guidance_utils.py in parse_pydantic_from_guidance_program(program, cls, verbose)
    150         sub_questions = cls.parse_obj(json_dict)
    151     except Exception as e:
--> 152         raise OutputParserException(
    153             ""Failed to parse pydantic object from guidance program""
    154         ) from e

OutputParserException: Failed to parse pydantic object from guidance program
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
91,1af2cb8b-9cda-4ea2-8926-e8a022f191b7,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 11, 'assignee': '', 'size': '', 'index_id': '9427'}",[],[],{},9b4556bf42bdcdec0d4a717e98ee1d903c802da72b9f177ffe0d274559a7345a,"[Feature Request]: Postgres BM25 support
### Feature Description

Feature: add a variation of PGVectorStore which uses ParadeDB's BM25 extension.

BM25 is now possible in Postgres with a Rust extension [pg_bm25): https://github.com/paradedb/paradedb/tree/dev/pg_bm25

Unsure if it might be better to use [pg_search](https://github.com/paradedb/paradedb/tree/dev/pg_search) and get HNSW at the same time..

I'm interested in contributing on this myself, but am just starting to look into it. Interested to hear others' thoughts.

### Reason

Although the code comments for the PGVectorStore class currently suggest BM25 search is present in Postgres - it is not.

### Value of Feature

BM25 retrieval hit rate and MRR is measurable better than Postgres full text search with tsvector and tsquery. Indexing is also supposed to be faster with pg_bm25.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
92,c124d1d9-2926-4b7d-936d-a884e3803a68,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 9, 'assignee': '', 'size': '', 'index_id': '9417'}",[],[],{},a1dc1beec0ef3f8af732e5945b495ef9ec5b21d6bfbbef6efd230c15ba2aaa37,"i get these error when i search 
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

i get error when i type for searching An error occured while performing search: [Errno 2] No such file or directory: '/tmp/llama_index'",,,"{metadata_str}

{content}",{key}: {value},"
",Document
93,e71bf042-db9d-44a7-8ce3-64e212bb4ee8,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 11, 'assignee': '', 'size': '', 'index_id': '9426'}",[],[],{},d8dd516e78536c16d267935b21c6d982206dd2aac2c476eb1dd62e31655cfb0e,"Slack Loader with large lack channels
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

Hi team,

I am using the [Slack Loader ](https://llamahub.ai/l/slack)from Llama Hub. For smaller Slack channels it works fine. However, for larger channels with lots of messages created over months, I keep seeing this message:

`Rate limit error reached, sleeping for: 10 seconds`

Is there a recommended / idiomatic way to load larger Slack channels to avoid this issue?",,,"{metadata_str}

{content}",{key}: {value},"
",Document
94,8449b19f-1958-4ca4-91a4-9140cc22f9f7,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 11, 'assignee': '', 'size': '', 'index_id': '9425'}",[],[],{},d2595e4bf0bb63df45985f7850f943a0420bb754873c741c571250feaab8a901,"[Feature Request]: Make llama-index compartible with models finetuned and hosted on modal.com
### Feature Description

Modal.com is a cloud computing service that allows you to finetune and host models on their workers. They provide inference points for any models finetuned on their platform.

### Reason

I have not tried implementing the feature. I just read about the capabilities on modal.com and thought it would be a good integration feature for llama-index to allow for more configuration.

### Value of Feature

An integration feature to allow users who host their models on modal to use llama-index for their RAG and prompt engineering pipelines.",,,"{metadata_str}

{content}",{key}: {value},"
",Document
95,aae86393-8a1d-48bc-9e5a-14df1f63b214,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 10, 'assignee': '', 'size': '', 'index_id': '9419'}",[],[],{},70fc216076658dac654b6ebb8611e9d39e41f940ce58fe1d2f3cf798539aee9c,"[Bug]: Unable to retrieve more than 40 nodes when using PgVectorStore with HNSW
### Bug Description

When using the PgVectorStore index that was created using PgVector and indexed with [HNSW](https://github.com/pgvector/pgvector?tab=readme-ov-file#hnsw), we can't retrieve more than 40 nodes.

This is an expected behavior of PgVector
>  Specify the size of the dynamic candidate list for search (40 by default)

> To search more than 40 nearest neighbors, increase this SET hnsw.ef_search = x; value. Where x is the value of nearest neighbors you want to return.

To overcome this issue, we need to set the `hnsw.ef_search` value to be higher than the number of nodes we want to retrieve.

```
SET hnsw.ef_search = 100;
```

### Version

latest

### Steps to Reproduce

Create a vector index with Postgres and set the table index with HNSW.

```
CREATE INDEX ON items USING hnsw (embedding vector_cosine_ops);
```

### Relevant Logs/Tracbacks

_No response_",,,"{metadata_str}

{content}",{key}: {value},"
",Document
96,400bc690-b67d-4696-9e5d-be73bd020243,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 7, 'assignee': 'ravi03071991', 'size': '', 'index_id': '9373'}",[],[],{},46a5673acbea45bf44b05ba9f437ebd2e532ba88a5825b6abd29d0fdc80932db,"[Bug]: agent.chat - Error : [] is too short - 'messages'
### Bug Description

Trying to use a custom function in the tool using OpenAIAgent

The first run is successful and the function output is visible. For the second run, I'm getting an error.

Attached relevant file for you guys. Any help would be appreciated!

Also, great work building this library! 

### Version

0.9.10

### Steps to Reproduce

python agent.py

### Relevant Logs/Tracbacks

```shell
STARTING TURN 1
---------------

=== Calling Function ===
Calling function: GetStockHistoryWithIndicators with args: {
  ""symbol"": ""SBIN"",
  ""startDate"": ""2023-11-01"",
  ""endDate"": ""2023-11-30""
}
Got output: ""[{\""DATE\"":1701302400000,\""SERIES\"":\""EQ\"",\""OPEN\"":569.75,\""HIGH\"":570.2,\""LOW\"":563.65,\""PREV. CLOSE\"":568.6,\""LTP\"":565.15,\""CLOSE\"":564.75,\""VWAP\"":566.22,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":19758998,\""VALUE\"":11187929536.5,\""NO OF TRADES\"":314822,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-13122387.9847327564,\""volume_obv\"":19758998,\""volume_cmf\"":-0.6641221374,\""volume_fi\"":0.0,\""volume_em\"":0.0,\""volume_sma_em\"":0.0,\""volume_vpt\"":0.0,\""volume_vwap\"":566.2,\""volume_mfi\"":50.0,\""volume_nvi\"":1000.0,\""volatility_bbm\"":564.75,\""volatility_bbh\"":564.75,\""volatility_bbl\"":564.75,\""volatility_bbw\"":0.0,\""volatility_bbp\"":0.0,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":566.2,\""volatility_kch\"":572.75,\""volatility_kcl\"":559.65,\""volatility_kcw\"":2.3136700812,\""volatility_kcp\"":0.3893129771,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":563.65,\""volatility_dch\"":570.2,\""volatility_dcm\"":566.925,\""volatility_dcw\"":1.1598052236,\""volatility_dcp\"":0.1679389313,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":100.0,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":0.0,\""momentum_uo\"":0.0,\""momentum_stoch\"":16.7938931298,\""momentum_stoch_signal\"":16.7938931298,\""momentum_wr\"":-83.2061068702,\""momentum_ao\"":0.0,\""momentum_roc\"":0.0,\""momentum_ppo\"":0.0,\""momentum_ppo_signal\"":0.0,\""momentum_ppo_hist\"":0.0,\""momentum_pvo\"":0.0,\""momentum_pvo_signal\"":0.0,\""momentum_pvo_hist\"":0.0,\""momentum_kama\"":564.75,\""others_dr\"":0.0,\""others_dlr\"":0.0,\""others_cr\"":0.0},{\""DATE\"":1701216000000,\""SERIES\"":\""EQ\"",\""OPEN\"":568.0,\""HIGH\"":569.0,\""LOW\"":565.1,\""PREV. CLOSE\"":564.45,\""LTP\"":568.35,\""CLOSE\"":568.6,\""VWAP\"":567.58,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":10299034,\""VALUE\"":5845506569.1499996185,\""NO OF TRADES\"":206929,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-4935976.3437070074,\""volume_obv\"":30058032,\""volume_cmf\"":-0.1642148875,\""volume_fi\"":39651280.9000002369,\""volume_em\"":4.7334536424,\""volume_sma_em\"":4.7334536424,\""volume_vpt\"":70210.3247454628,\""volume_vwap\"":566.6682723894,\""volume_mfi\"":100.0,\""volume_nvi\"":1006.8171757415,\""volatility_bbm\"":566.675,\""volatility_bbh\"":570.525,\""volatility_bbl\"":562.825,\""volatility_bbw\"":1.358803547,\""volatility_bbp\"":0.75,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":566.8833333333,\""volatility_kch\"":572.1083333333,\""volatility_kcl\"":561.6583333333,\""volatility_kcw\"":1.8434128127,\""volatility_kcp\"":0.6642743222,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":563.65,\""volatility_dch\"":570.2,\""volatility_dcm\"":566.925,\""volatility_dcw\"":1.1558653549,\""volatility_dcp\"":0.7557251908,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":100.0,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":100.0,\""momentum_uo\"":35.6481481481,\""momentum_stoch\"":75.572519084,\""momentum_stoch_signal\"":46.1832061069,\""momentum_wr\"":-24.427480916,\""momentum_ao\"":0.0,\""momentum_roc\"":0.0,\""momentum_ppo\"":0.0543545809,\""momentum_ppo_signal\"":0.0108709162,\""momentum_ppo_hist\"":0.0434836648,\""momentum_pvo\"":-3.9596532201,\""momentum_pvo_signal\"":-0.791930644,\""momentum_pvo_hist\"":-3.1677225761,\""momentum_kama\"":572.8998344107,\""others_dr\"":0.6817175741,\""others_dlr\"":0.6794043869,\""others_cr\"":0.6817175741},{\""DATE\"":1701129600000,\""SERIES\"":\""EQ\"",\""OPEN\"":563.6,\""HIGH\"":565.2,\""LOW\"":561.1,\""PREV. CLOSE\"":560.35,\""LTP\"":564.8,\""CLOSE\"":564.45,\""VWAP\"":563.42,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":8153314,\""VALUE\"":4593727819.6999998093,\""NO OF TRADES\"":186090,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":234417.9001954477,\""volume_obv\"":21904718,\""volume_cmf\"":0.0061347721,\""volume_fi\"":29153061.7571430877,\""volume_em\"":-196.1165729665,\""volume_sma_em\"":-95.6915596621,\""volume_vpt\"":10702.3171830291,\""volume_vwap\"":566.010026098,\""volume_mfi\"":55.9878419246,\""volume_nvi\"":999.4687915007,\""volatility_bbm\"":565.9333333333,\""volatility_bbh\"":569.7125160786,\""volatility_bbl\"":562.1541505881,\""volatility_bbw\"":1.3355575728,\""volatility_bbp\"":0.3037494568,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":565.7833333333,\""volatility_kch\"":570.6333333333,\""volatility_kcl\"":560.9333333333,\""volatility_kcw\"":1.7144372109,\""volatility_kcp\"":0.3625429553,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":561.1,\""volatility_dch\"":570.2,\""volatility_dcm\"":565.65,\""volatility_dcw\"":1.6079632466,\""volatility_dcp\"":0.3681318681,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":46.2783171521,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":97.6329673464,\""momentum_uo\"":39.3442622951,\""momentum_stoch\"":36.8131868132,\""momentum_stoch_signal\"":43.0598663423,\""momentum_wr\"":-63.1868131868,\""momentum_ao\"":0.0,\""momentum_roc\"":0.0,\""momentum_ppo\"":0.0377335453,\""momentum_ppo_signal\"":0.016243442,\""momentum_ppo_hist\"":0.0214901033,\""momentum_pvo\"":-8.265273542,\""momentum_pvo_signal\"":-2.2865992236,\""momentum_pvo_hist\"":-5.9786743184,\""momentum_kama\"":564.3886892043,\""others_dr\"":-0.729862821,\""others_dlr\"":-0.7325393509,\""others_cr\"":-0.0531208499},{\""DATE\"":1700784000000,\""SERIES\"":\""EQ\"",\""OPEN\"":561.95,\""HIGH\"":562.45,\""LOW\"":559.25,\""PREV. CLOSE\"":559.95,\""LTP\"":560.7,\""CLOSE\"":560.35,\""VWAP\"":560.63,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":6529851,\""VALUE\"":3660804948.5500001907,\""NO OF TRADES\"":155259,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-1806160.5373045232,\""volume_obv\"":15374867,\""volume_cmf\"":-0.0403690705,\""volume_fi\"":21163711.6346940547,\""volume_em\"":-112.7131384774,\""volume_sma_em\"":-101.3654192672,\""volume_vpt\"":-36728.6139889087,\""volume_vwap\"":565.2326103736,\""volume_mfi\"":41.4518371735,\""volume_nvi\"":992.2089420097,\""volatility_bbm\"":564.5375,\""volatility_bbh\"":570.3763247961,\""volatility_bbl\"":558.6986752039,\""volatility_bbw\"":2.0685339047,\""volatility_bbp\"":0.1414090038,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":564.5083333333,\""volatility_kch\"":568.9458333333,\""volatility_kcl\"":560.0708333333,\""volatility_kcw\"":1.572164568,\""volatility_kcp\"":0.0314553991,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":559.25,\""volatility_dch\"":570.2,\""volatility_dcm\"":564.725,\""volatility_dcw\"":1.9396408565,\""volatility_dcp\"":0.100456621,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":29.4471725012,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":93.4566803951,\""momentum_uo\"":35.3191489362,\""momentum_stoch\"":10.0456621005,\""momentum_stoch_signal\"":40.8104559992,\""momentum_wr\"":-89.9543378995,\""momentum_ao\"":0.0,\""momentum_roc\"":0.0,\""momentum_ppo\"":-0.0336309198,\""momentum_ppo_signal\"":0.0062685697,\""momentum_ppo_hist\"":-0.0398994894,\""momentum_pvo\"":-12.7219272763,\""momentum_pvo_signal\"":-4.3736648341,\""momentum_pvo_hist\"":-8.3482624421,\""momentum_kama\"":561.4515773921,\""others_dr\"":-0.7263708034,\""others_dlr\"":-0.729021721,\""others_cr\"":-0.779105799},{\""DATE\"":1700697600000,\""SERIES\"":\""EQ\"",\""OPEN\"":561.75,\""HIGH\"":563.5,\""LOW\"":558.3,\""PREV. CLOSE\"":558.95,\""LTP\"":560.5,\""CLOSE\"":559.95,\""VWAP\"":560.72,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":6376210,\""VALUE\"":3575282774.3499999046,\""NO OF TRADES\"":198344,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-4135929.5757658742,\""volume_obv\"":8998657,\""volume_cmf\"":-0.0809103947,\""volume_fi\"":17775969.4011663571,\""volume_em\"":4.0776574172,\""volume_sma_em\"":-75.0046500961,\""volume_vpt\"":-41280.204958838,\""volume_vwap\"":564.6526755041,\""volume_mfi\"":33.0695661095,\""volume_nvi\"":991.5006640106,\""volatility_bbm\"":563.62,\""volatility_bbh\"":570.0029773617,\""volatility_bbl\"":557.2370226383,\""volatility_bbw\"":2.2649932088,\""volatility_bbp\"":0.2125166053,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":563.7233333333,\""volatility_kch\"":568.3133333333,\""volatility_kcl\"":559.1333333333,\""volatility_kcw\"":1.6284584045,\""volatility_kcp\"":0.0889615105,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":558.3,\""volatility_dch\"":570.2,\""volatility_dcm\"":564.25,\""volatility_dcw\"":2.1113516199,\""volatility_dcp\"":0.1386554622,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":28.3633589578,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":89.9054357118,\""momentum_uo\"":34.668989547,\""momentum_stoch\"":13.8655462185,\""momentum_stoch_signal\"":20.241465044,\""momentum_wr\"":-86.1344537815,\""momentum_ao\"":0.0,\""momentum_roc\"":0.0,\""momentum_ppo\"":-0.0948875761,\""momentum_ppo_signal\"":-0.0139626595,\""momentum_ppo_hist\"":-0.0809249166,\""momentum_pvo\"":-16.5939911604,\""momentum_pvo_signal\"":-6.8177300994,\""momentum_pvo_hist\"":-9.776261061,\""momentum_kama\"":560.1514481603,\""others_dr\"":-0.0713839565,\""others_dlr\"":-0.0714094469,\""others_cr\"":-0.8499335989},{\""DATE\"":1700611200000,\""SERIES\"":\""EQ\"",\""OPEN\"":562.95,\""HIGH\"":564.5,\""LOW\"":555.15,\""PREV. CLOSE\"":561.5,\""LTP\"":559.5,\""CLOSE\"":558.95,\""VWAP\"":559.0,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":14909071,\""VALUE\"":8334245035.5,\""NO OF TRADES\"":247815,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-6926397.4099902846,\""volume_obv\"":-5910414,\""volume_cmf\"":-0.10490333,\""volume_fi\"":13106677.9152854495,\""volume_em\"":-67.4170107581,\""volume_sma_em\"":-73.4871222285,\""volume_vpt\"":-67905.9233265488,\""volume_vwap\"":563.4967053811,\""volume_mfi\"":22.4665726252,\""volume_nvi\"":991.5006640106,\""volatility_bbm\"":562.8416666667,\""volatility_bbh\"":569.6290119176,\""volatility_bbl\"":556.0543214157,\""volatility_bbw\"":2.4118133581,\""volatility_bbp\"":0.2133145197,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":563.025,\""volatility_kch\"":568.4083333333,\""volatility_kcl\"":557.6416666667,\""volatility_kcw\"":1.9122892708,\""volatility_kcp\"":0.1215170279,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":570.2,\""volatility_dcm\"":562.675,\""volatility_dcw\"":2.6739313898,\""volatility_dcp\"":0.2524916944,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":25.806189293,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":86.4733676827,\""momentum_uo\"":36.1366622865,\""momentum_stoch\"":25.2491694352,\""momentum_stoch_signal\"":16.3867925847,\""momentum_wr\"":-74.7508305648,\""momentum_ao\"":-0.7616666667,\""momentum_roc\"":0.0,\""momentum_ppo\"":-0.1560291363,\""momentum_ppo_signal\"":-0.0423759549,\""momentum_ppo_hist\"":-0.1136531814,\""momentum_pvo\"":-14.9502192195,\""momentum_pvo_signal\"":-8.4442279234,\""momentum_pvo_hist\"":-6.5059912961,\""momentum_kama\"":559.1971465372,\""others_dr\"":-0.1785873739,\""others_dlr\"":-0.1787470312,\""others_cr\"":-1.0270030987},{\""DATE\"":1700524800000,\""SERIES\"":\""EQ\"",\""OPEN\"":566.0,\""HIGH\"":566.65,\""LOW\"":561.0,\""PREV. CLOSE\"":563.75,\""LTP\"":561.4,\""CLOSE\"":561.5,\""VWAP\"":563.11,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":14280013,\""VALUE\"":8041232058.75,\""NO OF TRADES\"":223495,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-18678974.4807867333,\""volume_obv\"":8369599,\""volume_cmf\"":-0.2325960735,\""volume_fi\"":16436300.0916731507,\""volume_em\"":158.263161245,\""volume_sma_em\"":-34.8620749829,\""volume_vpt\"":-2758.7131109678,\""volume_vwap\"":563.4172727154,\""volume_mfi\"":40.7702681496,\""volume_nvi\"":996.0240143876,\""volatility_bbm\"":562.65,\""volatility_bbh\"":569.0036266359,\""volatility_bbl\"":556.2963733641,\""volatility_bbw\"":2.258464991,\""volatility_bbp\"":0.4095005053,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":563.0285714286,\""volatility_kch\"":568.45,\""volatility_kcl\"":557.6071428571,\""volatility_kcw\"":1.9258093982,\""volatility_kcp\"":0.3590250329,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":570.2,\""volatility_dcm\"":562.675,\""volatility_dcw\"":2.6748422643,\""volatility_dcp\"":0.4219269103,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":40.5301211849,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":83.7745167919,\""momentum_uo\"":35.6284153005,\""momentum_stoch\"":42.1926910299,\""momentum_stoch_signal\"":27.1024688945,\""momentum_wr\"":-57.8073089701,\""momentum_ao\"":-1.5078571429,\""momentum_roc\"":0.0,\""momentum_ppo\"":-0.1660808295,\""momentum_ppo_signal\"":-0.0671169298,\""momentum_ppo_hist\"":-0.0989638997,\""momentum_pvo\"":-13.8349290908,\""momentum_pvo_signal\"":-9.5223681569,\""momentum_pvo_hist\"":-4.3125609339,\""momentum_kama\"":559.7767135125,\""others_dr\"":0.4562125414,\""others_dlr\"":0.4551750462,\""others_cr\"":-0.5754758743},{\""DATE\"":1700438400000,\""SERIES\"":\""EQ\"",\""OPEN\"":564.0,\""HIGH\"":566.8,\""LOW\"":560.6,\""PREV. CLOSE\"":563.05,\""LTP\"":563.5,\""CLOSE\"":563.75,\""VWAP\"":564.68,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":12714585,\""VALUE\"":7179613743.4499998093,\""NO OF TRADES\"":264832,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-18473900.5291737802,\""volume_obv\"":21084184,\""volume_cmf\"":-0.1985990845,\""volume_fi\"":18175088.1142912731,\""volume_em\"":-6.0953621373,\""volume_sma_em\"":-30.7525445764,\""volume_vpt\"":48190.2027394317,\""volume_vwap\"":563.4581953752,\""volume_mfi\"":51.0677784087,\""volume_nvi\"":1000.0152058968,\""volatility_bbm\"":562.7875,\""volatility_bbh\"":568.7751435265,\""volatility_bbl\"":556.7998564735,\""volatility_bbw\"":2.1278523515,\""volatility_bbp\"":0.5803738562,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":563.1145833333,\""volatility_kch\"":568.6333333333,\""volatility_kcl\"":557.5958333333,\""volatility_kcw\"":1.9600806526,\""volatility_kcp\"":0.5575688939,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":570.2,\""volatility_dcm\"":562.675,\""volatility_dcw\"":2.674188748,\""volatility_dcp\"":0.5714285714,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":49.9653787458,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":81.6778290504,\""momentum_uo\"":40.5264538263,\""momentum_stoch\"":57.1428571429,\""momentum_stoch_signal\"":41.5282392027,\""momentum_wr\"":-42.8571428571,\""momentum_ao\"":-1.458125,\""momentum_roc\"":0.0,\""momentum_ppo\"":-0.1401849133,\""momentum_ppo_signal\"":-0.0817305265,\""momentum_ppo_hist\"":-0.0584543869,\""momentum_pvo\"":-13.6770952854,\""momentum_pvo_signal\"":-10.3533135826,\""momentum_pvo_hist\"":-3.3237817028,\""momentum_kama\"":560.7666650889,\""others_dr\"":0.4007123776,\""others_dlr\"":0.3999116638,\""others_cr\"":-0.1770694998},{\""DATE\"":1700179200000,\""SERIES\"":\""EQ\"",\""OPEN\"":574.5,\""HIGH\"":574.5,\""LOW\"":562.1,\""PREV. CLOSE\"":584.65,\""LTP\"":562.9,\""CLOSE\"":563.05,\""VWAP\"":566.16,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":37173221,\""VALUE\"":21045979976.2999992371,\""NO OF TRADES\"":682644,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-49951224.7630451471,\""volume_obv\"":-16089037,\""volume_cmf\"":-0.3836667651,\""volume_fi\"":11861324.8551065642,\""volume_em\"":153.4437922396,\""volume_sma_em\"":-7.7280024744,\""volume_vpt\"":2032.7664644837,\""volume_vwap\"":564.3409708827,\""volume_mfi\"":67.612898454,\""volume_nvi\"":1000.0152058968,\""volatility_bbm\"":562.8166666667,\""volatility_bbh\"":568.4642816992,\""volatility_bbl\"":557.1690516341,\""volatility_bbw\"":2.0069110838,\""volatility_bbp\"":0.5206576875,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":563.4962962963,\""volatility_kch\"":569.7796296296,\""volatility_kcl\"":557.212962963,\""volatility_kcw\"":2.230124093,\""volatility_kcp\"":0.4644857059,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":574.5,\""volatility_dcm\"":564.825,\""volatility_dcw\"":3.438064497,\""volatility_dcp\"":0.4082687339,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":47.4434480021,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":79.5592740778,\""momentum_uo\"":31.2468110787,\""momentum_stoch\"":40.826873385,\""momentum_stoch_signal\"":46.7208071859,\""momentum_wr\"":-59.173126615,\""momentum_ao\"":-0.5261111111,\""momentum_roc\"":0.0,\""momentum_ppo\"":-0.1282159479,\""momentum_ppo_signal\"":-0.0910276108,\""momentum_ppo_hist\"":-0.0371883371,\""momentum_pvo\"":-0.946469927,\""momentum_pvo_signal\"":-8.4719448515,\""momentum_pvo_hist\"":7.5254749245,\""momentum_kama\"":561.0129338184,\""others_dr\"":-0.1241685144,\""others_dlr\"":-0.1242456674,\""others_cr\"":-0.3010181496},{\""DATE\"":1700092800000,\""SERIES\"":\""EQ\"",\""OPEN\"":584.7,\""HIGH\"":588.0,\""LOW\"":582.9,\""PREV. CLOSE\"":584.7,\""LTP\"":584.1,\""CLOSE\"":584.65,\""VWAP\"":585.4,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":8622660,\""VALUE\"":5047671165.75,\""NO OF TRADES\"":179964,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-52656372.9983392879,\""volume_obv\"":-7466377,\""volume_cmf\"":-0.3793223403,\""volume_fi\"":36773915.0186627954,\""volume_em\"":1014.3621573853,\""volume_sma_em\"":105.8375708433,\""volume_vpt\"":332819.4745721126,\""volume_vwap\"":565.6356009401,\""volume_mfi\"":70.0399670357,\""volume_nvi\"":1038.3782792426,\""volatility_bbm\"":565.0,\""volatility_bbh\"":579.1533035013,\""volatility_bbl\"":550.8466964987,\""volatility_bbw\"":5.0100189385,\""volatility_bbp\"":1.1941842234,\""volatility_bbhi\"":1.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":565.665,\""volatility_kch\"":571.83,\""volatility_kcl\"":559.5,\""volatility_kcw\"":2.1797353557,\""volatility_kcp\"":2.0397404704,\""volatility_kchi\"":1.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.814159292,\""volatility_dcp\"":0.898021309,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":80.3693886674,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":79.2188829621,\""momentum_uo\"":48.1648232058,\""momentum_stoch\"":89.802130898,\""momentum_stoch_signal\"":62.5906204753,\""momentum_wr\"":-10.197869102,\""momentum_ao\"":2.2225,\""momentum_roc\"":0.0,\""momentum_ppo\"":0.1877997946,\""momentum_ppo_signal\"":-0.0352621297,\""momentum_ppo_hist\"":0.2230619243,\""momentum_pvo\"":-5.057790754,\""momentum_pvo_signal\"":-7.789114032,\""momentum_pvo_hist\"":2.731323278,\""momentum_kama\"":563.5037126917,\""others_dr\"":3.836249001,\""others_dlr\"":3.7644943443,\""others_cr\"":3.5236830456},{\""DATE\"":1700006400000,\""SERIES\"":\""EQ\"",\""OPEN\"":587.25,\""HIGH\"":588.0,\""LOW\"":582.6,\""PREV. CLOSE\"":581.35,\""LTP\"":584.0,\""CLOSE\"":584.7,\""VWAP\"":584.93,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":11397676,\""VALUE\"":6666856921.25,\""NO OF TRADES\"":221182,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-55189189.8872280419,\""volume_obv\"":3931299,\""volume_cmf\"":-0.3674022216,\""volume_fi\"":31601910.5588539355,\""volume_em\"":-7.1067119297,\""volume_sma_em\"":94.543142566,\""volume_vpt\"":333794.2180938786,\""volume_vwap\"":567.112480453,\""volume_mfi\"":63.7281490673,\""volume_nvi\"":1038.3782792426,\""volatility_bbm\"":566.7909090909,\""volatility_bbh\"":584.4090721456,\""volatility_bbl\"":549.1727460362,\""volatility_bbw\"":6.2168121514,\""volatility_bbp\"":1.0082564752,\""volatility_bbhi\"":1.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":567.555,\""volatility_kch\"":573.605,\""volatility_kcl\"":561.505,\""volatility_kcw\"":2.1319519694,\""volatility_kcp\"":1.9169421488,\""volatility_kchi\"":1.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7957880892,\""volatility_dcp\"":0.899543379,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":80.3999990216,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":78.9668233449,\""momentum_uo\"":48.6897035667,\""momentum_stoch\"":89.9543378995,\""momentum_stoch_signal\"":73.5277807275,\""momentum_wr\"":-10.0456621005,\""momentum_ao\"":5.5627272727,\""momentum_roc\"":0.0,\""momentum_ppo\"":0.4325761692,\""momentum_ppo_signal\"":0.0583055301,\""momentum_ppo_hist\"":0.3742706391,\""momentum_pvo\"":-7.0546470537,\""momentum_pvo_signal\"":-7.6422206363,\""momentum_pvo_hist\"":0.5875735826,\""momentum_kama\"":566.2513086483,\""others_dr\"":0.0085521252,\""others_dlr\"":0.0085517595,\""others_cr\"":3.5325365206},{\""DATE\"":1699833600000,\""SERIES\"":\""EQ\"",\""OPEN\"":581.0,\""HIGH\"":582.5,\""LOW\"":575.2,\""PREV. CLOSE\"":581.3,\""LTP\"":581.3,\""CLOSE\"":581.35,\""VWAP\"":578.71,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":11282362,\""VALUE\"":6529178380.0500001907,\""NO OF TRADES\"":177978,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-47461544.6817485392,\""volume_obv\"":-7351063,\""volume_cmf\"":-0.2938850019,\""volume_fi\"":21687935.8075890541,\""volume_em\"":-417.3328244564,\""volume_sma_em\"":48.0089637458,\""volume_vpt\"":269152.6708046697,\""volume_vwap\"":567.9906943965,\""volume_mfi\"":58.5532270827,\""volume_nvi\"":1032.4289595308,\""volatility_bbm\"":568.0041666667,\""volatility_bbh\"":586.6937563863,\""volatility_bbl\"":549.314576947,\""volatility_bbw\"":6.580793176,\""volatility_bbp\"":0.857039227,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":568.7666666667,\""volatility_kch\"":575.1566666667,\""volatility_kcl\"":562.3766666667,\""volatility_kcw\"":2.2469671218,\""volatility_kcp\"":1.4846113719,\""volatility_kchi\"":1.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.783408279,\""volatility_dcp\"":0.797564688,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":72.2689637581,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":77.043634751,\""momentum_uo\"":51.1690911699,\""momentum_stoch\"":79.7564687976,\""momentum_stoch_signal\"":86.5043125317,\""momentum_wr\"":-20.2435312024,\""momentum_ao\"":7.6429166667,\""momentum_roc\"":0.0,\""momentum_ppo\"":0.5715419647,\""momentum_ppo_signal\"":0.160952817,\""momentum_ppo_hist\"":0.4105891477,\""momentum_pvo\"":-8.7003894938,\""momentum_pvo_signal\"":-7.8538544078,\""momentum_pvo_hist\"":-0.846535086,\""momentum_kama\"":567.238765432,\""others_dr\"":-0.5729433898,\""others_dlr\"":-0.5745910067,\""others_cr\"":2.9393536963},{\""DATE\"":1699747200000,\""SERIES\"":\""EQ\"",\""OPEN\"":584.75,\""HIGH\"":584.75,\""LOW\"":580.1,\""PREV. CLOSE\"":579.5,\""LTP\"":580.85,\""CLOSE\"":581.3,\""VWAP\"":581.85,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":1947722,\""VALUE\"":1133273076.5499999523,\""NO OF TRADES\"":53935,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-48403990.810780853,\""volume_obv\"":-9298785,\""volume_cmf\"":-0.2961490081,\""volume_fi\"":18575746.9636477418,\""volume_em\"":853.497059642,\""volume_sma_em\"":115.1329717372,\""volume_vpt\"":268985.153646331,\""volume_vwap\"":568.1582349529,\""volume_mfi\"":59.1285203372,\""volume_nvi\"":1032.3401637142,\""volatility_bbm\"":569.0269230769,\""volatility_bbh\"":588.3308332183,\""volatility_bbl\"":549.7230129356,\""volatility_bbw\"":6.7848846367,\""volatility_bbp\"":0.8178909566,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":570.6133333333,\""volatility_kch\"":577.0583333333,\""volatility_kcl\"":564.1683333333,\""volatility_kcw\"":2.2589728012,\""volatility_kcp\"":1.3290664598,\""volatility_kchi\"":1.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7730133088,\""volatility_dcp\"":0.796042618,\""volatility_atr\"":0.0,\""volatility_ui\"":0.0,\""momentum_rsi\"":72.151677774,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":75.5137296838,\""momentum_uo\"":50.6849990528,\""momentum_stoch\"":79.604261796,\""momentum_stoch_signal\"":83.1050228311,\""momentum_wr\"":-20.395738204,\""momentum_ao\"":10.3303846154,\""momentum_roc\"":2.9305002213,\""momentum_ppo\"":0.6726631973,\""momentum_ppo_signal\"":0.2632948931,\""momentum_ppo_hist\"":0.4093683042,\""momentum_pvo\"":-15.3770241619,\""momentum_pvo_signal\"":-9.3584883586,\""momentum_pvo_hist\"":-6.0185358033,\""momentum_kama\"":568.9217809285,\""others_dr\"":-0.0086006709,\""others_dlr\"":-0.0086010407,\""others_cr\"":2.9305002213},{\""DATE\"":1699574400000,\""SERIES\"":\""EQ\"",\""OPEN\"":577.8,\""HIGH\"":581.0,\""LOW\"":575.4,\""PREV. CLOSE\"":578.35,\""LTP\"":580.25,\""CLOSE\"":579.5,\""VWAP\"":578.7,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":6773038,\""VALUE\"":3919567928.9000000954,\""NO OF TRADES\"":119792,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-45259366.0250665545,\""volume_obv\"":-16071823,\""volume_cmf\"":-0.265890982,\""volume_fi\"":14180430.4831266813,\""volume_em\"":-349.326255072,\""volume_sma_em\"":79.4053389057,\""volume_vpt\"":248012.3884648422,\""volume_vwap\"":568.5750436424,\""volume_mfi\"":56.4211944646,\""volume_nvi\"":1032.3401637142,\""volatility_bbm\"":569.775,\""volatility_bbh\"":589.1431162887,\""volatility_bbl\"":550.4068837113,\""volatility_bbw\"":6.7985139006,\""volatility_bbp\"":0.7510569395,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":572.4083333333,\""volatility_kch\"":579.0933333333,\""volatility_kcl\"":565.7233333333,\""volatility_kcw\"":2.3357451703,\""volatility_kcp\"":1.030416355,\""volatility_kchi\"":1.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7654337238,\""volatility_dcp\"":0.7412480974,\""volatility_atr\"":0.0,\""volatility_ui\"":0.9460200317,\""momentum_rsi\"":67.8806956185,\""momentum_stoch_rsi\"":0.5670891672,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":73.3754818331,\""momentum_uo\"":53.2521414363,\""momentum_stoch\"":74.1248097412,\""momentum_stoch_signal\"":77.828513445,\""momentum_wr\"":-25.8751902588,\""momentum_ao\"":11.7057142857,\""momentum_roc\"":1.916989096,\""momentum_ppo\"":0.7187621424,\""momentum_ppo_signal\"":0.3543883429,\""momentum_ppo_hist\"":0.3643737994,\""momentum_pvo\"":-18.144794805,\""momentum_pvo_signal\"":-11.1157496479,\""momentum_pvo_hist\"":-7.0290451571,\""momentum_kama\"":570.6670056156,\""others_dr\"":-0.3096507827,\""others_dlr\"":-0.3101311927,\""others_cr\"":2.6117751217},{\""DATE\"":1699488000000,\""SERIES\"":\""EQ\"",\""OPEN\"":581.0,\""HIGH\"":581.85,\""LOW\"":576.5,\""PREV. CLOSE\"":580.3,\""LTP\"":577.6,\""CLOSE\"":578.35,\""VWAP\"":578.27,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":12434363,\""VALUE\"":7190409015.1499996185,\""NO OF TRADES\"":167584,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-49094263.0250664875,\""volume_obv\"":-28506186,\""volume_cmf\"":-0.2687856213,\""volume_fi\"":10111866.4926800542,\""volume_em\"":41.9502792383,\""volume_sma_em\"":76.7299775009,\""volume_vpt\"":223336.7759540577,\""volume_vwap\"":569.6512876108,\""volume_mfi\"":59.8017695665,\""volume_nvi\"":1032.3401637142,\""volatility_bbm\"":570.3466666667,\""volatility_bbh\"":589.5408486555,\""volatility_bbl\"":551.1524846778,\""volatility_bbw\"":6.7307071683,\""volatility_bbp\"":0.7084833138,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":574.24,\""volatility_kch\"":580.94,\""volatility_kcl\"":567.54,\""volatility_kcw\"":2.3335190861,\""volatility_kcp\"":0.8067164179,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7596549467,\""volatility_dcp\"":0.7062404871,\""volatility_atr\"":0.0,\""volatility_ui\"":0.9895456857,\""momentum_rsi\"":65.2242529068,\""momentum_stoch_rsi\"":0.5312850659,\""momentum_stoch_rsi_k\"":0.0,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":71.062921517,\""momentum_uo\"":53.0911841748,\""momentum_stoch\"":70.6240487062,\""momentum_stoch_signal\"":74.7843734145,\""momentum_wr\"":-29.3759512938,\""momentum_ao\"":9.8616666667,\""momentum_roc\"":2.4625741873,\""momentum_ppo\"":0.730482376,\""momentum_ppo_signal\"":0.4296071495,\""momentum_ppo_hist\"":0.3008752265,\""momentum_pvo\"":-16.6437761011,\""momentum_pvo_signal\"":-12.2213549386,\""momentum_pvo_hist\"":-4.4224211625,\""momentum_kama\"":571.8097435694,\""others_dr\"":-0.198446937,\""others_dlr\"":-0.1986441038,\""others_cr\"":2.408145197},{\""DATE\"":1699401600000,\""SERIES\"":\""EQ\"",\""OPEN\"":581.9,\""HIGH\"":582.6,\""LOW\"":579.0,\""PREV. CLOSE\"":579.75,\""LTP\"":580.4,\""CLOSE\"":580.3,\""VWAP\"":580.52,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":15434808,\""VALUE\"":8960148835.6499996185,\""NO OF TRADES\"":391419,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-53381709.6917336136,\""volume_obv\"":-13071378,\""volume_cmf\"":-0.2694862845,\""volume_fi\"":12967010.650868468,\""volume_em\"":37.9013461003,\""volume_sma_em\"":79.0991126764,\""volume_vpt\"":275377.7124112202,\""volume_vwap\"":570.7878501662,\""volume_mfi\"":61.1080184002,\""volume_nvi\"":1032.3401637142,\""volatility_bbm\"":570.96875,\""volatility_bbh\"":590.1679646649,\""volatility_bbl\"":551.7695353351,\""volatility_bbw\"":6.7251367662,\""volatility_bbp\"":0.7430112419,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":576.35,\""volatility_kch\"":582.475,\""volatility_kcl\"":570.225,\""volatility_kcw\"":2.1254446083,\""volatility_kcp\"":0.8224489796,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7533796727,\""volatility_dcp\"":0.7656012177,\""volatility_atr\"":0.0,\""volatility_ui\"":1.0097771909,\""momentum_rsi\"":67.5436509677,\""momentum_stoch_rsi\"":0.7645090512,\""momentum_stoch_rsi_k\"":0.6209610948,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":69.348726488,\""momentum_uo\"":57.8784495,\""momentum_stoch\"":76.5601217656,\""momentum_stoch_signal\"":73.769660071,\""momentum_wr\"":-23.4398782344,\""momentum_ao\"":8.3446875,\""momentum_roc\"":3.5602748282,\""momentum_ppo\"":0.7582699258,\""momentum_ppo_signal\"":0.4953397048,\""momentum_ppo_hist\"":0.262930221,\""momentum_pvo\"":-13.3785657445,\""momentum_pvo_signal\"":-12.4527970998,\""momentum_pvo_hist\"":-0.9257686448,\""momentum_kama\"":573.3589624809,\""others_dr\"":0.3371660759,\""others_dlr\"":0.3365989455,\""others_cr\"":2.7534307216},{\""DATE\"":1699315200000,\""SERIES\"":\""EQ\"",\""OPEN\"":574.8,\""HIGH\"":581.2,\""LOW\"":572.6,\""PREV. CLOSE\"":574.35,\""LTP\"":580.35,\""CLOSE\"":579.75,\""VWAP\"":577.36,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":17923281,\""VALUE\"":10348172706.1000003815,\""NO OF TRADES\"":466280,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-41502325.7731291354,\""volume_obv\"":-30994659,\""volume_cmf\"":-0.192131318,\""volume_fi\"":9706322.7650302332,\""volume_em\"":-187.1309164879,\""volume_sma_em\"":79.7409452821,\""volume_vpt\"":258390.2842706056,\""volume_vwap\"":571.8301379674,\""volume_mfi\"":57.6448978949,\""volume_nvi\"":1032.3401637142,\""volatility_bbm\"":571.4852941176,\""volatility_bbh\"":590.564163387,\""volatility_bbl\"":552.4064248483,\""volatility_bbw\"":6.6769414596,\""volatility_bbp\"":0.7165931787,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":577.83,\""volatility_kch\"":584.25,\""volatility_kcl\"":571.41,\""volatility_kcw\"":2.222106848,\""volatility_kcp\"":0.6495327103,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7481794087,\""volatility_dcp\"":0.7488584475,\""volatility_atr\"":0.0,\""volatility_ui\"":1.0162645587,\""momentum_rsi\"":66.2024748618,\""momentum_stoch_rsi\"":0.739942601,\""momentum_stoch_rsi_k\"":0.678578906,\""momentum_stoch_rsi_d\"":0.0,\""momentum_tsi\"":67.6552341868,\""momentum_uo\"":53.9967493194,\""momentum_stoch\"":74.8858447489,\""momentum_stoch_signal\"":74.0233384069,\""momentum_wr\"":-25.1141552511,\""momentum_ao\"":7.6397058824,\""momentum_roc\"":3.5360300027,\""momentum_ppo\"":0.7635932745,\""momentum_ppo_signal\"":0.5489904187,\""momentum_ppo_hist\"":0.2146028557,\""momentum_pvo\"":-9.1971392354,\""momentum_pvo_signal\"":-11.8016655269,\""momentum_pvo_hist\"":2.6045262915,\""momentum_kama\"":574.346271377,\""others_dr\"":-0.0947785628,\""others_dlr\"":-0.0948235061,\""others_cr\"":2.6560424967},{\""DATE\"":1699228800000,\""SERIES\"":\""EQ\"",\""OPEN\"":582.0,\""HIGH\"":582.5,\""LOW\"":573.25,\""PREV. CLOSE\"":578.15,\""LTP\"":573.95,\""CLOSE\"":574.35,\""VWAP\"":575.7,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":16499138,\""VALUE\"":9498482029.9500007629,\""NO OF TRADES\"":330138,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-54077344.4650209472,\""volume_obv\"":-47493797,\""volume_cmf\"":-0.2325813806,\""volume_fi\"":-4408201.2299740314,\""volume_em\"":54.6619465817,\""volume_sma_em\"":91.6963085006,\""volume_vpt\"":104711.3792253287,\""volume_vwap\"":572.6456942837,\""volume_mfi\"":54.5065842755,\""volume_nvi\"":1022.7245761609,\""volatility_bbm\"":571.6444444444,\""volatility_bbh\"":590.2321610841,\""volatility_bbl\"":553.0567278048,\""volatility_bbw\"":6.5032440428,\""volatility_bbp\"":0.5727780504,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":579.1283333333,\""volatility_kch\"":585.8533333333,\""volatility_kcl\"":572.4033333333,\""volatility_kcw\"":2.3224558748,\""volatility_kcp\"":0.1447335812,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7465790701,\""volatility_dcp\"":0.5844748858,\""volatility_atr\"":0.0,\""volatility_ui\"":1.0517773248,\""momentum_rsi\"":54.7150249499,\""momentum_stoch_rsi\"":0.5295258895,\""momentum_stoch_rsi_k\"":0.6779925139,\""momentum_stoch_rsi_d\"":0.6591775049,\""momentum_tsi\"":63.4651233938,\""momentum_uo\"":49.3355955375,\""momentum_stoch\"":58.4474885845,\""momentum_stoch_signal\"":69.964485033,\""momentum_wr\"":-41.5525114155,\""momentum_ao\"":6.3955555556,\""momentum_roc\"":2.7551659361,\""momentum_ppo\"":0.6841350555,\""momentum_ppo_signal\"":0.5760193461,\""momentum_ppo_hist\"":0.1081157094,\""momentum_pvo\"":-6.7158829717,\""momentum_pvo_signal\"":-10.7845090158,\""momentum_pvo_hist\"":4.0686260442,\""momentum_kama\"":574.3464841983,\""others_dr\"":-0.9314359638,\""others_dlr\"":-0.9358009544,\""others_cr\"":1.6998671979},{\""DATE\"":1698969600000,\""SERIES\"":\""EQ\"",\""OPEN\"":576.0,\""HIGH\"":579.5,\""LOW\"":573.45,\""PREV. CLOSE\"":572.1,\""LTP\"":579.15,\""CLOSE\"":578.15,\""VWAP\"":577.35,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":11371371,\""VALUE\"":6565279355.4499998093,\""NO OF TRADES\"":205720,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-47780800.1922938004,\""volume_obv\"":-36122426,\""volume_cmf\"":-0.1959187302,\""volume_fi\"":2394571.7743078982,\""volume_em\"":-74.4853017283,\""volume_sma_em\"":86.0846685616,\""volume_vpt\"":179946.3575486501,\""volume_vwap\"":573.3035249602,\""volume_mfi\"":58.970724108,\""volume_nvi\"":1029.4911007355,\""volatility_bbm\"":571.9868421053,\""volatility_bbh\"":590.3105926437,\""volatility_bbl\"":553.6630915668,\""volatility_bbw\"":6.4070531661,\""volatility_bbp\"":0.6681740286,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":580.1766666667,\""volatility_kch\"":586.2666666667,\""volatility_kcl\"":574.0866666667,\""volatility_kcw\"":2.0993605396,\""volatility_kcp\"":0.333607006,\""volatility_kchi\"":0.0,\""volatility_kcli\"":0.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7431391042,\""volatility_dcp\"":0.700152207,\""volatility_atr\"":0.0,\""volatility_ui\"":1.0151679938,\""momentum_rsi\"":59.9779079189,\""momentum_stoch_rsi\"":0.6259266169,\""momentum_stoch_rsi_k\"":0.6317983691,\""momentum_stoch_rsi_d\"":0.6627899297,\""momentum_tsi\"":60.5124395692,\""momentum_uo\"":50.4719724405,\""momentum_stoch\"":70.0152207002,\""momentum_stoch_signal\"":67.7828513445,\""momentum_wr\"":-29.9847792998,\""momentum_ao\"":5.8252631579,\""momentum_roc\"":2.9652715939,\""momentum_ppo\"":0.6667629503,\""momentum_ppo_signal\"":0.5941680669,\""momentum_ppo_hist\"":0.0725948834,\""momentum_pvo\"":-7.6584702872,\""momentum_pvo_signal\"":-10.1593012701,\""momentum_pvo_hist\"":2.5008309829,\""momentum_kama\"":574.6742296003,\""others_dr\"":0.6616174806,\""others_dlr\"":0.6594383984,\""others_cr\"":2.372731297},{\""DATE\"":1698883200000,\""SERIES\"":\""EQ\"",\""OPEN\"":571.1,\""HIGH\"":575.45,\""LOW\"":567.6,\""PREV. CLOSE\"":566.4,\""LTP\"":572.0,\""CLOSE\"":572.1,\""VWAP\"":571.18,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":11148516,\""VALUE\"":6367849377.0,\""NO OF TRADES\"":212716,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-46147578.1031218618,\""volume_obv\"":-47270942,\""volume_cmf\"":-0.1809501512,\""volume_fi\"":-7583013.0220217276,\""volume_em\"":-348.5441470416,\""volume_sma_em\"":66.0041588271,\""volume_vpt\"":63283.6890370189,\""volume_vwap\"":574.2961541448,\""volume_mfi\"":60.0400997438,\""volume_nvi\"":1018.718081347,\""volatility_bbm\"":571.9925,\""volatility_bbh\"":589.8523509232,\""volatility_bbl\"":554.1326490768,\""volatility_bbw\"":6.2447850009,\""volatility_bbp\"":0.5030095436,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":578.83,\""volatility_kch\"":585.195,\""volatility_kcl\"":572.465,\""volatility_kcw\"":2.1992640326,\""volatility_kcp\"":-0.0286724273,\""volatility_kchi\"":0.0,\""volatility_kcli\"":1.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7430822957,\""volatility_dcp\"":0.5159817352,\""volatility_atr\"":0.0,\""volatility_ui\"":1.0754207991,\""momentum_rsi\"":50.0123177487,\""momentum_stoch_rsi\"":0.2378285833,\""momentum_stoch_rsi_k\"":0.4644270299,\""momentum_stoch_rsi_d\"":0.591405971,\""momentum_tsi\"":55.1249625855,\""momentum_uo\"":50.8280521754,\""momentum_stoch\"":41.9708029197,\""momentum_stoch_signal\"":56.8111707348,\""momentum_wr\"":-58.0291970803,\""momentum_ao\"":4.34,\""momentum_roc\"":1.4811529933,\""momentum_ppo\"":0.5616739206,\""momentum_ppo_signal\"":0.5876692377,\""momentum_ppo_hist\"":-0.0259953171,\""momentum_pvo\"":-8.4868543726,\""momentum_pvo_signal\"":-9.8248118906,\""momentum_pvo_hist\"":1.337957518,\""momentum_kama\"":574.3075122324,\""others_dr\"":-1.046441235,\""others_dlr\"":-1.05195493,\""others_cr\"":1.3014608234},{\""DATE\"":1698796800000,\""SERIES\"":\""EQ\"",\""OPEN\"":566.25,\""HIGH\"":569.65,\""LOW\"":563.85,\""PREV. CLOSE\"":565.55,\""LTP\"":567.3,\""CLOSE\"":566.4,\""VWAP\"":567.16,\""52W H\"":629.55,\""52W L\"":499.35,\""VOLUME\"":13575575,\""VALUE\"":7699515897.6499996185,\""NO OF TRADES\"":240220,\""SYMBOL\"":\""SBIN\"",\""volume_adi\"":-47786009.5686392188,\""volume_obv\"":-60846517,\""volume_cmf\"":-0.1392975904,\""volume_fi\"":-17554122.2331615686,\""volume_em\"":-204.006091823,\""volume_sma_em\"":40.1277836079,\""volume_vpt\"":-71973.7441040413,\""volume_vwap\"":574.5965712167,\""volume_mfi\"":52.8018836042,\""volume_nvi\"":1018.718081347,\""volatility_bbm\"":572.075,\""volatility_bbh\"":589.8151099207,\""volatility_bbl\"":554.3348900793,\""volatility_bbw\"":6.2020224344,\""volatility_bbp\"":0.3400517239,\""volatility_bbhi\"":0.0,\""volatility_bbli\"":0.0,\""volatility_kcc\"":576.9833333333,\""volatility_kch\"":583.3883333333,\""volatility_kcl\"":570.5783333333,\""volatility_kcw\"":2.2201681158,\""volatility_kcp\"":-0.3261774655,\""volatility_kchi\"":0.0,\""volatility_kcli\"":1.0,\""volatility_dcl\"":555.15,\""volatility_dch\"":588.0,\""volatility_dcm\"":571.575,\""volatility_dcw\"":5.7422540751,\""volatility_dcp\"":0.3424657534,\""volatility_atr\"":7.9547619048,\""volatility_ui\"":1.3209284072,\""momentum_rsi\"":42.7973733337,\""momentum_stoch_rsi\"":0.0,\""momentum_stoch_rsi_k\"":0.2879184001,\""momentum_stoch_rsi_d\"":0.4613812664,\""momentum_tsi\"":48.1564982797,\""momentum_uo\"":47.6406640002,\""momentum_stoch\"":21.1678832117,\""momentum_stoch_signal\"":44.3846356105,\""momentum_wr\"":-78.8321167883,\""momentum_ao\"":1.7978571429,\""momentum_roc\"":0.5949738034,\""momentum_ppo\"":0.3937692131,\""momentum_ppo_signal\"":0.5488892328,\""momentum_ppo_hist\"":-0.1551200196,\""momentum_pvo\"":-7.6066398516,\""momentum_pvo_signal\"":-9.3811774828,\""momentum_pvo_hist\"":1.7745376312,\""momentum_kama\"":572.8160721681,\""others_dr\"":-0.9963293131,\""others_dlr\"":-1.0013258895,\""others_cr\"":0.2921646746}]""
========================

STARTING TURN 2
---------------

Traceback (most recent call last):
  File ""/Users/saurabhc/Documents/kernelPI/pifin/Agent.py"", line 28, in <module>
    response = agent.chat(""What was the stock price movement of SBIN stock during November 2023?"")
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/llama_index/callbacks/utils.py"", line 39, in wrapper
    return func(self, *args, **kwargs)
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/llama_index/agent/openai_agent.py"", line 438, in chat
    chat_response = self._chat(
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/llama_index/agent/openai_agent.py"", line 360, in _chat
    agent_chat_response = self._get_agent_response(mode=mode, **llm_chat_kwargs)
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/llama_index/agent/openai_agent.py"", line 322, in _get_agent_response
    chat_response: ChatResponse = self._llm.chat(**llm_chat_kwargs)
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/llama_index/llms/base.py"", line 187, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/llama_index/llms/openai.py"", line 200, in chat
    return chat_fn(messages, **kwargs)
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/llama_index/llms/openai.py"", line 254, in _chat
    response = self._client.chat.completions.create(
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/openai/_utils/_utils.py"", line 301, in wrapper
    return func(*args, **kwargs)
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/openai/resources/chat/completions.py"", line 598, in create
    return self._post(
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/openai/_base_client.py"", line 1096, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/openai/_base_client.py"", line 856, in request
    return self._request(
  File ""/Users/saurabhc/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/openai/_base_client.py"", line 908, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': ""[] is too short - 'messages'"", 'type': 'invalid_request_error', 'param': None, 'code': None}}
```

![Screenshot 2023-12-07 at 5 57 23â€¯PM](https://github.com/run-llama/llama_index/assets/19235748/00fd37d7-295f-43ea-9400-7ca268a67457)
![Screenshot 2023-12-07 at 5 57 28â€¯PM](https://github.com/run-llama/llama_index/assets/19235748/e11d48ac-108f-4e58-8667-597609fb4827)

![Screenshot 2023-12-07 at 5 55 48â€¯PM](https://github.com/run-llama/llama_index/assets/19235748/04c22d62-d74d-450e-a624-b541c8f0e8f0)
![Screenshot 2023-12-07 at 5 57 05â€¯PM](https://github.com/run-llama/llama_index/assets/19235748/7c00a3c1-1554-43b7-b8c6-706b096b7607)

![Screenshot 2023-12-07 at 5 57 13â€¯PM](https://github.com/run-llama/llama_index/assets/19235748/8b90ac12-f29e-4fbd-a740-ebb9c87d7ff7)
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
97,0143263b-cb6b-4af6-a36a-925a8889ed0c,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 9, 'assignee': '', 'size': '', 'index_id': '9408'}",[],[],{},6d38dabbf9725e6c242e31a25bdf56b0e8b763834460f7c650f96bd4b2551625,"[Question]: About setting the context_window and num_output
### Question Validation

- [X] I have searched both the documentation and discord for an answer.

### Question

I'm wondering what is the right strategy to avoid running over the token limit after deploying llamaindex RAG in a Q&A setting. 

Is my understanding correct that context_window + number of tokens in the prompt template should be less than the max context_window of an llm? ",,,"{metadata_str}

{content}",{key}: {value},"
",Document
98,06902593-890a-4f25-b58b-08de1249e90c,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 7, 'assignee': '', 'size': 'S', 'index_id': '9372'}",[],[],{},4721aeeddd0cdac459190fc459f519cc97334b0127ccbc95ba3a6696d3286b4e,"Proposal : Update the evaluation correctness function to be more robust.
# Description

Proposal : Update the evaluation correctness function to be more robust.

When using RagEvaluatorPack on a large dataset, sometime GPT3/4 will return a malformated answer, raising an error in correctness.py and interumpting the benchmark (that could be costly).

Such case emerge when the LLM :
 - prefix the answer with ```\n```
 - do not answer correctly such as: ```I'm not sure how to evaluate this case so I will say 3.0```
 - Something in the content made the LLM go off-road
 
To make the parsing more robust, I change the prompt to output the score in the form ```[SCORE:4.2]``` instead of only a number.

I then use a regexp to retrieve the score instead of assuming first line.

I use a regexp to remove the ```[SCORE:2.3]``` pattern from the llm answer to get a reasoning, without relying on line marker.

## Type of Change

Please delete options that are not relevant.

- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

# How Has This Been Tested?

Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration

- [ ] Added new unit/integration tests
- [ ] Added new notebook (that tests end-to-end)
- [x] I stared at the code and made sure it makes sense

# Suggested Checklist:

- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added Google Colab support for the newly added notebooks.
- [x] My changes generate no new warnings
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes
- [ ] I ran `make format; make lint` to appease the lint gods
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
99,d124b918-a528-4991-ac3d-aa80fa687dde,,"{'state': 'open', 'year': 2023, 'month': 12, 'day': 9, 'assignee': '', 'size': '', 'index_id': '9415'}",[],[],{},9b1f3fe302f876a5fb5968e251ce4d2f2f1456340f7becaae5135eb017dc1873,"[Bug]: Invalid JSON Path -Simple JSON Query Engine Error
### Bug Description

Loading the JSON value and schema from files, and trying to run a simple JSON Query Engine

```
f = open('SBIN.json')
json_value = json.load(f)
f.close()

f = open('stock-schema.json')
json_schema = json.load(f)
f.close()
```

But getting an error - `ValueError: Invalid JSON Path: The JSONPath query for this task would depend on the specific structure of the JSON data.`

Any help would be appreciated.

### Version

0.9.10

### Steps to Reproduce

python main.py

### Relevant Logs/Tracbacks

```shell
JsonPathParserError                       Traceback (most recent call last)
File ~/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/llama_index/indices/struct_store/json_query.py:56, in default_output_processor(llm_output, json_value)
     55 try:
---> 56     datum: List[DatumInContext] = parse(expression).find(json_value)
     57     if datum:

File ~/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/jsonpath_ng/ext/parser.py:172, in parse(path, debug)
    171 def parse(path, debug=False):
--> 172     return ExtentedJsonPathParser(debug=debug).parse(path)

File ~/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/jsonpath_ng/parser.py:45, in JsonPathParser.parse(self, string, lexer)
     44 lexer = lexer or self.lexer_class()
---> 45 return self.parse_token_stream(lexer.tokenize(string))

File ~/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/jsonpath_ng/parser.py:69, in JsonPathParser.parse_token_stream(self, token_iterator, start_symbol)
     61 new_parser = ply.yacc.yacc(module=self,
     62                            debug=self.debug,
     63                            tabmodule = parsing_table_module,
   (...)
     66                            start = start_symbol,
     67                            errorlog = logger)
---> 69 return new_parser.parse(lexer = IteratorToTokenStream(token_iterator))

File ~/Documents/kernelPI/pifin/env/lib/python3.10/site-packages/ply/yacc.py:333, in LRParser.parse(self, input, lexer, debug, tracking, tokenfunc)
...
     62     except Exception as exc:
---> 63         raise ValueError(f""Invalid JSON Path: {expression}"") from exc
     65 return results

ValueError: Invalid JSON Path: Since the task requires specific data about a stock symbol (SBIN) and a specific month (November)
```
",,,"{metadata_str}

{content}",{key}: {value},"
",Document
