import json
import logging
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, cast

from openai import AsyncOpenAI, AsyncStream, BadRequestError, OpenAI, Stream
from openai.types.responses import (
    Response,
    ResponseCompletedEvent,
    ResponseFailedEvent,
    ResponseFunctionCallArgumentsDeltaEvent,
    ResponseIncompleteEvent,
    ResponseInputItemParam,
    ResponseOutputItemAddedEvent,
    ResponseReasoningTextDeltaEvent,
    ResponseStreamEvent,
    ResponseTextDeltaEvent,
)
from pydantic import Field

from langroid.agent.base import async_noop_fn, noop_fn
from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig
from langroid.agent.chat_document import ChatDocument
from langroid.language_models.base import (
    LLMFunctionCall,
    LLMResponse,
    LLMTokenUsage,
    OpenAIToolCall,
    Role,
    StreamEventType,
    StreamingIfAllowed,
    ToolChoiceTypes,
)
from langroid.language_models.openai_gpt import (
    OpenAIGPT,
    OpenAIGPTConfig,
)
from langroid.language_models.utils import (
    async_retry_with_exponential_backoff,
    retry_with_exponential_backoff,
)
from langroid.utils.configuration import settings

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class ResponseStatus(str, Enum):
    QUEUED = "queued"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"
    INCOMPLETE = "incomplete"


@dataclass
class ProcessStreamData:
    """
    Data structure to hold data while processing streaming response.
    See _process_stream_event() for details.
    """
    output_text: str = ''
    reasoning_text: str = ''
    function_name: str = ''
    function_args: str = ''
    llm_response: Optional[LLMResponse] = None
    reasoning_data: List[Dict[str, Any]] = Field(default_factory=list)
    response_id: str = ''


class OpenAIResponsesAgentConfig(ChatAgentConfig):
    # stateful operation mode
    stateful: bool = False
    # Level of reasoning effort to be used by the model.
    # Options are 'none', 'minimal', 'low', 'medium', 'high'.
    # Note that 'none' is supported only by gpt-5.1 and later models;
    # for original gpt-5 models it will be treated as 'minimal'.
    reasoning_effort: str = 'none'
    # Include reasoning summary in the response.
    # Options are 'auto', 'detailed' or 'concise'.
    reasoning_summary: Optional[str] = None
    # enable API logging
    llm_logs: bool = False


class OpenAIResponsesAgent(ChatAgent):
    """
    A drop-in replacement for ChatAgent powered by OpenAI Responses API:
    - `llm_response` and `llm_response_async` methods use Responses API
      (instead of Chat Completion API used by regular ChatAgent)
    - stateless mode of the Responses API is implemented by default:
      - context is maintained inside the agent (in self.message_history)
        and can be manipulated via regular ChatAgent methods
      - resoning items generated by the model are "memorized" and included
        in the follow-up instructions sent to the model; this significantly
        improves model's instruction following and function calling
        performance
    - stateful mode may be enabled by setting `config.stateful = True`
      - we memorize response IDs returned by the model for each assistant
        message and use previous_response_id parameter to send only the
        new messages to the model
      - if context contains assistant messages with unknown response IDs
        (e.g. we may share message history with other agent or message history
        can be modified - e.g. by chat_doc_agent in RAG mode) - we will send
        multiple elements in 'input' list
      - it is recommended to call `finish_async()` method after the agent is
        done to to delete all created responses from OpenAI servers
    """

    def __init__(self, config: OpenAIResponsesAgentConfig) -> None:
        super().__init__(config)
        self.config: OpenAIResponsesAgentConfig = config

        # reasoning data produced by model
        # - key: id of ChatDocument that corresponds to LLM Message
        # - value: list of encrypted_content values (in case more than one is generated)
        self.reasoning_data: Dict[str, List[Dict[str, Any]]] = {}

        # IDs of responses produced by the model
        # - key: id of ChatDocument that corresponds to LLM Message
        # - value: response ID
        self.response_ids: Dict[str, str] = {}

    def _add_user_message(self, message: Optional[str | ChatDocument]) -> bool:
        """Add user message to the message history."""
        # the code is copied from ChatAgent._prep_llm_messages()
        if message is None and len(self.message_history) > 0:
            # this means agent has been used to get LLM response already,
            # and so the last message is an "assistant" response.
            # We delete this last assistant response and re-generate it.
            self.clear_history(-1)
            logger.warning(
                "Re-generating the last assistant response since message is None"
            )

        if len(self.message_history) == 0:
            # initial messages have not yet been loaded, so load them
            self.init_message_history()
        else:
            assert self.message_history[0].role == Role.SYSTEM
            # update the system message with the latest tool instructions
            self.message_history[0] = self._create_system_and_tools_message()

        if message is not None:
            if (
                isinstance(message, str)
                or message.id() != self.message_history[-1].chat_document_id
            ):
                # either the message is a str, or it is a fresh ChatDocument
                # different from the last message in the history
                llm_msgs = ChatDocument.to_LLMMessage(message, self.oai_tool_calls)
                # LLM only responds to the content, so only those msgs with
                # non-empty content should be kept
                llm_msgs = [m for m in llm_msgs if m.content.strip() != ""]
                if len(llm_msgs) == 0:
                    return False
                # process tools if any
                done_tools = [m.tool_call_id for m in llm_msgs if m.role == Role.TOOL]
                self.oai_tool_calls = [
                    t for t in self.oai_tool_calls if t.id not in done_tools
                ]
                self.message_history.extend(llm_msgs)

        return True

    def _prep_llm_inputs(
        self,
        message: Optional[str | ChatDocument] = None,
        truncate: bool = True
    ) -> Tuple[str, List[ResponseInputItemParam], Optional[str], int]:
        """
        Prepare inputs to be sent to the "create response" API.
        If desired output tokens + message history exceeds the model context length,
        then first the max output tokens is reduced to fit, and if that is not
        possible, older messages may be truncated to accommodate at least
        self.config.llm.min_output_tokens of output.

        Returns:
            Tuple[str, List[ResponseInputItemParam], int]:
            (instructions, input, previous_response_id, output_len)
                instructions: The instructions for the model.
                input: The input messages for the model.
                previous_response_id: ID of the previous response from
                                      the model, if any.
                output_len: max number of tokens expected in response.
        """
        if (
            not self.llm_can_respond(message)
            or self.config.llm is None
            or self.llm is None
        ):
            return '', [], None, 0

        # add user message to history
        if not self._add_user_message(message):
            return '', [], None, 0

        input: List[ResponseInputItemParam] = []

        previous_response_id: Optional[str] = None
        first_idx: int = 1  # skip system message
        if self.config.stateful:
            # find last ASSISTANT message in history that has known model response_id
            for i in range(len(self.message_history) - 1, 0, -1):
                msg = self.message_history[i]
                if msg.role == Role.ASSISTANT:
                    previous_response_id = self.response_ids.get(msg.chat_document_id)
                    if previous_response_id:
                        first_idx = i + 1
                        break

        for i in range(first_idx, len(self.message_history)):
            msg = self.message_history[i]

            if not self.config.stateful:
                reasoning_data = self.reasoning_data.get(msg.chat_document_id)
                if reasoning_data is not None:
                    for rd in reasoning_data:
                        input.append(rd)  # type: ignore

            if msg.content:
                if msg.role == Role.USER:
                    input.append({'role': 'user', 'content': msg.content})
                elif msg.role == Role.ASSISTANT:
                    input.append({'role': 'assistant', 'content': msg.content})

            if msg.role == Role.ASSISTANT and msg.tool_calls:
                for tool_call in msg.tool_calls:
                    if tool_call.function is not None:
                        input.append({
                            'type': 'function_call',
                            'call_id': tool_call.id or '',
                            'name': tool_call.function.name,
                            'arguments': json.dumps(tool_call.function.arguments) \
                                if tool_call.function.arguments is not None else '',
                        })

            if msg.role == Role.TOOL and msg.content:
                input.append({
                    'type': 'function_call_output',
                    'call_id': msg.tool_call_id or '',
                    'output': msg.content
                })

        instructions = self.message_history[0].content

        if not input:
            input.append({'role': 'user', 'content': ' '})

        output_len = self.config.llm.model_max_output_tokens
        # TODO: truncate message history if needed

        return instructions, input, previous_response_id, output_len

    def _parse_response(
        self,
        openai_response: Response
    ) -> Tuple[LLMResponse, List[Dict[str, Any]], str]:
        """
        Parse the response object from Responses API and convert to LLMResponse.

        Args:
            openai_response: The response object from Responses API.

        Returns:
            LLMResponse: The parsed LLM response.
            List[str]: List of generated reasoning data, if any.
            str: The response ID.
        """
        response: LLMResponse = LLMResponse(message='')
        reasoning_data: List[Dict[str, Any]] = []
        for item in openai_response.output:
            if item.type == 'message':
                texts: List[str] = []
                for content in item.content:
                    if content.type == 'output_text':
                        texts.append(content.text)
                if texts:
                    response.message = '\n'.join(texts)

            elif item.type == 'function_call':
                if response.oai_tool_calls is None:
                    response.oai_tool_calls = []
                try:
                    response.oai_tool_calls.append(
                        OpenAIToolCall(
                            id=item.call_id,
                            function=LLMFunctionCall(
                                name=item.name,
                                arguments=json.loads(item.arguments or "{}"),
                            )
                        )
                    )
                    response.tool_id = item.id or ''
                except (ValueError, SyntaxError):
                    logger.warning(
                        "Could not parse function arguments: "
                        f"{item.arguments} "
                        f"for function {item.name}"
                    )

            elif item.type == 'reasoning':
                if item.content is not None:
                    reasonings: List[str] = []
                    for reasoning_content in item.content:
                        if reasoning_content.type == 'reasoning_text':
                            reasonings.append(reasoning_content.text)
                    if reasonings:
                        response.reasoning += '\n'.join(reasonings)
                elif item.summary is not None:
                    summaries: List[str] = []
                    for reasoning_summary in item.summary:
                        if reasoning_summary.type == 'summary_text':
                            summaries.append(reasoning_summary.text)
                    if summaries:
                        response.reasoning += '\n'.join(summaries)

                if item.encrypted_content is not None and \
                   not self.config.stateful:
                    reasoning_data.append(item.model_dump(exclude_none=True))

        if openai_response.usage is not None:
            response.usage = LLMTokenUsage(
                prompt_tokens = openai_response.usage.input_tokens,
                cached_tokens = \
                    openai_response.usage.input_tokens_details.cached_tokens,
                completion_tokens = openai_response.usage.output_tokens,
                cost = self.compute_token_cost(
                    openai_response.usage.input_tokens,
                    openai_response.usage.input_tokens_details.cached_tokens,
                    openai_response.usage.output_tokens
                )
            )

        return response, reasoning_data, openai_response.id

    def _stream_response(
        self,
        openai_stream: Stream[ResponseStreamEvent]
    ) -> Tuple[Optional[LLMResponse], List[Dict[str, Any]], str]:
        """
        Parse the response object from Stream[ResponseStreamEvent] and
        convert to LLMResponse.

        Args:
            openai_response: The response stream object from Responses API.

        Returns:
            LLMResponse: The parsed LLM response.
            List[str]: List of generated reasoning data, if any.
            str: The response ID.
        """
        data = ProcessStreamData()
        try:
            for event in openai_stream:
                is_done = self._process_stream_event(
                    event,
                    data
                )
                if is_done:
                    break

        except Exception as e:
            logging.warning("Error while processing stream response: %s", str(e))

        return data.llm_response, data.reasoning_data, data.response_id

    async def _stream_response_async(
        self,
        openai_stream: AsyncStream[ResponseStreamEvent]
    ) -> Tuple[Optional[LLMResponse], List[Dict[str, Any]], str]:
        """
        Async version of _stream_response().
        """
        data = ProcessStreamData()
        try:
            async for event in openai_stream:
                is_done = await self._process_stream_event_async(
                    event,
                    data
                )
                if is_done:
                    break

        except Exception as e:
            logging.warning("Error while processing stream response: %s", str(e))

        return data.llm_response, data.reasoning_data, data.response_id

    def _process_stream_event(
        self,
        event: ResponseStreamEvent,
        data: ProcessStreamData
    ) -> bool:
        """
        Process streaming event and update data dictionary.
        """
        assert self.llm is not None
        if isinstance(event, (
            ResponseCompletedEvent,
            ResponseFailedEvent,
            ResponseIncompleteEvent)
        ):
            if self.config.llm_logs:
                logger.info(f'EVENT: {event}')
            data.llm_response, data.reasoning_data, data.response_id = \
                self._parse_response(event.response)
            return True
        elif isinstance(event, ResponseTextDeltaEvent):
            data.output_text += event.delta
            if self.llm.config.streamer is not None:
                self.llm.config.streamer(
                    event.delta,
                    StreamEventType.TEXT
                )  # type: ignore
        elif isinstance(event, ResponseReasoningTextDeltaEvent):
            data.reasoning_text += event.delta
            if self.llm.config.streamer is not None:
                self.llm.config.streamer(
                    event.delta,
                    StreamEventType.REASONING
                )  # type: ignore
        elif isinstance(event, ResponseOutputItemAddedEvent):
            if event.item.type == 'function_call':
                data.function_name = event.item.name
                data.function_args = event.item.arguments
                if self.llm.config.streamer is not None:
                    self.llm.config.streamer(
                        event.item.name,
                        StreamEventType.TOOL_NAME
                    )  # type: ignore
                    if event.item.arguments:
                        self.llm.config.streamer(
                            event.item.arguments,
                            StreamEventType.TOOL_ARGS
                        )  # type: ignore
        elif isinstance(event, ResponseFunctionCallArgumentsDeltaEvent):
            data.function_args += event.delta
            if self.llm.config.streamer is not None:
                self.llm.config.streamer(
                    event.delta,
                    StreamEventType.TOOL_ARGS
                )  # type: ignore

        return False

    async def _process_stream_event_async(
        self,
        event: ResponseStreamEvent,
        data: ProcessStreamData
    ) -> bool:
        """
        Async version of _process_stream_event().
        """
        assert self.llm is not None
        if isinstance(event, (
            ResponseCompletedEvent,
            ResponseFailedEvent,
            ResponseIncompleteEvent
            )
        ):
            if self.config.llm_logs:
                logger.info(f'EVENT: {event}')
            data.llm_response, data.reasoning_data, data.response_id = \
                self._parse_response(event.response)
            return True
        elif isinstance(event, ResponseTextDeltaEvent):
            data.output_text += event.delta
            if self.llm.config.streamer_async is not None:
                await self.llm.config.streamer_async(
                    event.delta,
                    StreamEventType.TEXT
                )
        elif isinstance(event, ResponseReasoningTextDeltaEvent):
            data.reasoning_text += event.delta
            if self.llm.config.streamer_async is not None:
                await self.llm.config.streamer_async(
                    event.delta,
                    StreamEventType.REASONING
                )
        elif isinstance(event, ResponseOutputItemAddedEvent):
            if event.item.type == 'function_call':
                data.function_name = event.item.name
                data.function_args = event.item.arguments
                if self.llm.config.streamer_async is not None:
                    await self.llm.config.streamer_async(
                        event.item.name,
                        StreamEventType.TOOL_NAME
                    )
                    if event.item.arguments:
                        await self.llm.config.streamer_async(
                            event.item.arguments,
                            StreamEventType.TOOL_ARGS
                        )
        elif isinstance(event, ResponseFunctionCallArgumentsDeltaEvent):
            data.function_args += event.delta
            if self.llm.config.streamer_async is not None:
                await self.llm.config.streamer_async(
                    event.delta,
                    StreamEventType.TOOL_ARGS
                )

        return False

    def _create_response_kwargs(
        self,
        instructions: str,
        input: List[ResponseInputItemParam],
        previous_response_id: Optional[str] = None,
        output_len: Optional[int] = None,
        tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = "auto"
    ) -> Dict[str, Any]:
        """
        Create parameters' dictionary for the create response API.

        Args:
            instructions: The instructions for the model.
            input: The input messages for the model.
            previous_response_id: The ID of the previous response, if any.
            output_len: max number of tokens expected in response.
                    If None, use the LLM's default model_max_output_tokens.
            tool_choice: The tool choice configuration.
        """
        if self.llm is None:
            return {}

        openai_config = cast(OpenAIGPTConfig, self.llm.config)

        kwargs: Dict[str, Any] = {
            'model': openai_config.chat_model,
            'instructions': instructions,
            'input': input,
            'max_output_tokens': output_len or openai_config.max_output_tokens,
            'store': False
        }

        if self.config.stateful:
            kwargs['store'] = True
            if previous_response_id is not None:
                kwargs['previous_response_id'] = previous_response_id

        if self.llm.get_stream():
            kwargs['stream'] = True

        # reasoning configuration
        if openai_config.chat_model.startswith('gpt-5'):
            reasoning_effort = self.config.reasoning_effort
            if reasoning_effort == 'none' and \
               openai_config.chat_model in ('gpt-5', 'gpt-5-mini', 'gpt-5-nano'):
                # reasoning effort 'none' is supported by gpt-5.1 and later
                reasoning_effort = 'minimal'

            kwargs['reasoning'] = {'effort': reasoning_effort}
            if reasoning_effort != 'none':
                if self.config.reasoning_summary is not None:
                    kwargs['reasoning']['summary'] = self.config.reasoning_summary

                if not self.config.stateful:
                    kwargs['include'] = ['reasoning.encrypted_content']

        # tools configuration
        functions, fun_call, tools, force_tool, output_format = (
            self._function_args()
        )
        if tools is not None:
            kwargs['tools'] = [
                {"type": "function"} | t.function.model_dump()
                | ({"strict": t.strict} if t.strict is not None else {})
                for t in tools
            ]
            kwargs['tool_choice'] = force_tool or tool_choice

            if openai_config.parallel_tool_calls is not None:
                    kwargs["parallel_tool_calls"] = openai_config.parallel_tool_calls

        # additional configuration
        if 'temperature' not in self.llm.info().unsupported_params:
            kwargs['temperature'] = openai_config.temperature

        if output_format is not None:
            kwargs['text'] = {'format': output_format}

        if self.config.llm_logs:
            logger.info(f'CREATE RESPONSE: {kwargs}')

        return kwargs

    def _create_response_finalize(
        self,
        response: Optional[LLMResponse],
        reasoning_data: List[Dict[str, Any]],
        response_id: str
    ) -> Optional[ChatDocument]:
        """
        Convert the Response object from Responses API to ChatDocument.

        Args:
            response: The response object from Responses API.
            reasoning_data: List of generated reasoning data, if any.
            response_id: The response ID.
        Returns:
            ChatDocument: The converted ChatDocument.
        """
        if not self.llm:
            return None

        if response is None:
            if self.llm.get_stream():
                self.callbacks.finish_llm_stream(content='', is_tool=False)
            return None

        self.update_token_usage(
            response,
            '',
            self.llm.get_stream(),
            chat=True,
            print_response_stats=self.config.show_stats and not settings.quiet,
        )

        chat_doc = ChatDocument.from_LLMResponse(response, displayed=True)

        if reasoning_data:
            self.reasoning_data[chat_doc.id()] = reasoning_data

        if response_id:
            self.response_ids[chat_doc.id()] = response_id

        if self.llm.get_stream():
            self.callbacks.finish_llm_stream(
                content=str(response),
                is_tool=self.has_tool_message_attempt(chat_doc)
            )
        else:
            self.callbacks.show_llm_response(
                content=str(response),
                is_tool=self.has_tool_message_attempt(chat_doc),
                cached=False
            )

        self.oai_tool_calls = response.oai_tool_calls or []
        self.oai_tool_id2call.update(
            {t.id: t for t in self.oai_tool_calls if t.id is not None}
        )

        # If using strict output format, parse the output JSON
        self._load_output_format(chat_doc)

        return chat_doc

    def _create_response(
        self,
        instructions: str,
        input: List[ResponseInputItemParam],
        previous_response_id: Optional[str] = None,
        output_len: Optional[int] = None,
        tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = "auto"
    ) -> Optional[ChatDocument]:
        """
        Create a response using the Responses API.

        Args:
            instructions: The instructions for the model.
            input: The input messages for the model.
            previous_response_id: The ID of the previous response, if any.
            output_len: max number of tokens expected in response.
                    If None, use the LLM's default model_max_output_tokens.
            tool_choice: The tool choice configuration.
        """
        if self.llm is None:
            return None

        kwargs = self._create_response_kwargs(
            instructions,
            input,
            previous_response_id,
            output_len,
            tool_choice
        )

        streamer = noop_fn
        if self.llm.get_stream():
            streamer = self.callbacks.start_llm_stream()
        self.llm.config.streamer = streamer

        response: Optional[LLMResponse] = None
        response_id: str = ''
        try:
            openai_response_or_stream = self._create_response_with_backoff(**kwargs)
            if openai_response_or_stream is None:
                return None

            if self.llm is not None and self.llm.get_stream():
                openai_stream = cast(
                    Stream[ResponseStreamEvent],
                    openai_response_or_stream
                )
                response, reasoning_data, response_id = \
                    self._stream_response(openai_stream)
            else:
                openai_response = cast(Response, openai_response_or_stream)
                if self.config.llm_logs:
                    logger.info(f'RESPONSE: {openai_response}')
                response, reasoning_data, response_id = \
                    self._parse_response(openai_response)
        finally:
            self.llm.config.streamer = noop_fn

        chat_doc = self._create_response_finalize(response, reasoning_data, response_id)

        return chat_doc

    async def _create_response_async(
        self,
        instructions: str,
        input: List[ResponseInputItemParam],
        previous_response_id: Optional[str] = None,
        output_len: Optional[int] = None,
        tool_choice: ToolChoiceTypes | Dict[str, str | Dict[str, str]] = "auto"
    ) -> Optional[ChatDocument]:
        """
        Async version of _create_response().
        """
        if self.llm is None:
            return None

        kwargs = self._create_response_kwargs(
            instructions,
            input,
            previous_response_id,
            output_len,
            tool_choice
        )

        streamer_async = async_noop_fn
        if self.llm.get_stream():
            streamer_async = await self.callbacks.start_llm_stream_async()
        self.llm.config.streamer_async = streamer_async

        response: Optional[LLMResponse] = None
        response_id: str = ''
        try:
            openai_response_or_stream = \
                await self._create_response_with_backoff_async(**kwargs)
            if openai_response_or_stream is None:
                return None

            if self.llm is not None and self.llm.get_stream():
                openai_stream = cast(
                    AsyncStream[ResponseStreamEvent],
                    openai_response_or_stream
                )
                response, reasoning_data, response_id = \
                    await self._stream_response_async(openai_stream)
            else:
                openai_response = cast(Response, openai_response_or_stream)
                if self.config.llm_logs:
                    logger.info(f'RESPONSE: {openai_response}')
                response, reasoning_data, response_id = \
                    self._parse_response(openai_response)
        finally:
            self.llm.config.streamer_async = async_noop_fn

        chat_doc = self._create_response_finalize(response, reasoning_data, response_id)

        return chat_doc

    def _create_response_with_backoff(
        self,
        **kwargs: Any
    ) -> Response | AsyncStream[ResponseStreamEvent] | None:
        if self.llm is None:
            return None

        retry_func = retry_with_exponential_backoff(
            self._create_response_with_backoff_body,
            initial_delay=self.llm.config.retry_params.initial_delay,
            max_retries=self.llm.config.retry_params.max_retries,
            exponential_base=self.llm.config.retry_params.exponential_base,
            jitter=self.llm.config.retry_params.jitter,
        )
        return retry_func(**kwargs)  # type: ignore

    def _create_response_with_backoff_body(
        self,
        **kwargs: Any
    ) -> Response | Stream[ResponseStreamEvent] | None:
        if not isinstance(self.llm, OpenAIGPT) or \
           not isinstance(self.llm.client, OpenAI):
            return None

        return self.llm.client.responses.create(**kwargs)  # type: ignore

    async def _create_response_with_backoff_async(
        self,
        **kwargs: Any
    ) -> Response | AsyncStream[ResponseStreamEvent] | None:
        if self.llm is None:
            return None

        retry_func = async_retry_with_exponential_backoff(
            self._create_response_with_backoff_body_async,
            initial_delay=self.llm.config.retry_params.initial_delay,
            max_retries=self.llm.config.retry_params.max_retries,
            exponential_base=self.llm.config.retry_params.exponential_base,
            jitter=self.llm.config.retry_params.jitter,
        )
        return await retry_func(**kwargs)  # type: ignore

    async def _create_response_with_backoff_body_async(
        self,
        **kwargs: Any
    ) -> Response | AsyncStream[ResponseStreamEvent] | None:
        if not isinstance(self.llm, OpenAIGPT) or \
           not isinstance(self.llm.async_client, AsyncOpenAI):
            return None

        return await self.llm.async_client.responses.create(**kwargs)  # type: ignore

    def _finalize_response(
        self,
        message: Optional[str | ChatDocument],
        response: ChatDocument
    ) -> None:
        """
        Finalize LLM response processing

        Args:
            response: LLM response created by self._create_response()
        """
        self.message_history.extend(ChatDocument.to_LLMMessage(response))
        response.metadata.msg_idx = len(self.message_history) - 1
        response.metadata.agent_id = self.id
        if isinstance(message, ChatDocument):
            self._reduce_raw_tool_results(message)

        # Preserve trail of tool_ids for OpenAI Assistant fn-calls
        response.metadata.tool_ids = (
            []
            if isinstance(message, str)
            else message.metadata.tool_ids if message is not None else []
        )

    def llm_response(
        self,
        message: Optional[str | ChatDocument] = None
    ) -> Optional[ChatDocument]:
        """
        Override ChatAgent's main LLM response method.

        Args:
            message: message to respond to (if absent, the LLM response
                     will be based on the instructions in the system_message).
        Returns:
            Optional[ChatDocument]: LLM response
        """
        if self.llm is None:
            return None

        (
            instructions,
            input,
            previous_response_id,
            output_len
        ) = self._prep_llm_inputs(message)

        if len(input) == 0:
            return None

        tool_choice = (
            "auto"
            if isinstance(message, str)
            else (message.oai_tool_choice if message is not None else "auto")
        )

        with StreamingIfAllowed(self.llm, self.llm.get_stream()):
            try:
                response = self._create_response(
                    instructions=instructions,
                    input=input,
                    previous_response_id=previous_response_id,
                    output_len=output_len,
                    tool_choice=tool_choice
                )
            except BadRequestError as e:
                if self.any_strict:
                    self.disable_strict = True
                    self.set_output_format(None)
                    logging.warning(
                        f"""
                        OpenAI BadRequestError raised with strict mode enabled.
                        Message: {e.message}
                        Disabling strict mode and retrying.
                        """
                    )
                    return self.llm_response(message)
                else:
                    raise e

        if response is None:
            return None

        self._finalize_response(message, response)

        return response

    async def llm_response_async(
        self,
        message: Optional[str | ChatDocument] = None
    ) -> Optional[ChatDocument]:
        """
        Async version of llm_response().
        """
        if self.llm is None:
            return None

        (
            instructions,
            input,
            previous_response_id,
            output_len
        ) = self._prep_llm_inputs(message)

        if len(input) == 0:
            return None

        tool_choice = (
            "auto"
            if isinstance(message, str)
            else (message.oai_tool_choice if message is not None else "auto")
        )

        with StreamingIfAllowed(self.llm, self.llm.get_stream()):
            try:
                response = await self._create_response_async(
                    instructions=instructions,
                    input=input,
                    previous_response_id=previous_response_id,
                    output_len=output_len,
                    tool_choice=tool_choice
                )
            except BadRequestError as e:
                if self.any_strict:
                    self.disable_strict = True
                    self.set_output_format(None)
                    logging.warning(
                        f"""
                        OpenAI BadRequestError raised with strict mode enabled.
                        Message: {e.message}
                        Disabling strict mode and retrying.
                        """
                    )
                    return await self.llm_response_async(message)
                else:
                    raise e

        if response is None:
            return None

        self._finalize_response(message, response)

        return response

    def finish(self) -> None:
        """
        Delete all created responses from OpenAI servers.
        """
        if not self.config.stateful or \
           self.llm is None or \
           not isinstance(self.llm, OpenAIGPT) or \
           not isinstance(self.llm.client, OpenAI):
            return

        for response_id in self.response_ids.values():
            if self.config.llm_logs:
                logger.info(f'DELETE RESPONSE: {response_id}')
            try:
                self.llm.client.responses.delete(response_id)
            except Exception:
                pass

    async def finish_async(self) -> None:
        """
        Async version of finish().
        """
        if not self.config.stateful or \
           self.llm is None or \
           not isinstance(self.llm, OpenAIGPT) or \
           not isinstance(self.llm.async_client, AsyncOpenAI):
            return

        for response_id in self.response_ids.values():
            if self.config.llm_logs:
                logger.info(f'DELETE RESPONSE: {response_id}')
            try:
                await self.llm.async_client.responses.delete(response_id)
            except Exception:
                pass
